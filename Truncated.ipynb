{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Kl-FedDis-Research-/blob/main/Truncated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLz8giC4gfQX"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import truncnorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4wqnUn3hRMt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKc2deHuhXU8"
      },
      "outputs": [],
      "source": [
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmZRSl5Lhaoo"
      },
      "outputs": [],
      "source": [
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49PV9mSHhdvl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMqdpHMGhhGQ",
        "outputId": "5391376c-787a-42da-da1c-4074dbff333d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48844265.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ1HQc6mhrld"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGCJttd7hvRQ"
      },
      "outputs": [],
      "source": [
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZJ4CgBdhy0X"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-C19bb3h6vG"
      },
      "outputs": [],
      "source": [
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1bW5Dv0iA5v"
      },
      "outputs": [],
      "source": [
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrwsQLEwiEGr"
      },
      "outputs": [],
      "source": [
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2lmbOtxiHn7"
      },
      "outputs": [],
      "source": [
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "        \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3N0NvLXiLuB"
      },
      "outputs": [],
      "source": [
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwWHoKDXiS8F"
      },
      "outputs": [],
      "source": [
        "# Define logic to generate augmented data using Truncated Normal distribution\n",
        "def generate_augmented_data(vae: VAE, distribution_info: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using Truncated Normal distribution\n",
        "    mean = distribution_info[\"mean\"]\n",
        "    std = distribution_info[\"std\"]\n",
        "    a = (0 - mean) / std\n",
        "    b = np.inf\n",
        "    augmented_data = torch.from_numpy(truncnorm.rvs(a, b, loc=mean, scale=std, size=(64, vae.z_dim))).float()\n",
        "    return augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MzkD3WpiZwS"
      },
      "outputs": [],
      "source": [
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info)\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg3348heidh_"
      },
      "outputs": [],
      "source": [
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "        \"std\": np.ones(20)\n",
        "    }\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5oS4WKDig11"
      },
      "outputs": [],
      "source": [
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OnEoB8KikVn",
        "outputId": "7a5c2934-45f9-4fca-c252-5e47f8d9d3d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 9.84%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 10.01%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 10.99%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 11.45%\n",
            "Epoch [5/10], Training Loss: 2.300, Validation Accuracy: 11.84%\n",
            "Epoch [6/10], Training Loss: 2.299, Validation Accuracy: 12.89%\n",
            "Epoch [7/10], Training Loss: 2.297, Validation Accuracy: 13.90%\n",
            "Epoch [8/10], Training Loss: 2.295, Validation Accuracy: 15.55%\n",
            "Epoch [9/10], Training Loss: 2.292, Validation Accuracy: 16.91%\n",
            "Epoch [10/10], Training Loss: 2.288, Validation Accuracy: 17.83%\n",
            "Epoch [1/10], Training Loss: 2.285, Validation Accuracy: 18.46%\n",
            "Epoch [2/10], Training Loss: 2.277, Validation Accuracy: 18.37%\n",
            "Epoch [3/10], Training Loss: 2.265, Validation Accuracy: 20.00%\n",
            "Epoch [4/10], Training Loss: 2.245, Validation Accuracy: 23.10%\n",
            "Epoch [5/10], Training Loss: 2.211, Validation Accuracy: 23.20%\n",
            "Epoch [6/10], Training Loss: 2.162, Validation Accuracy: 23.54%\n",
            "Epoch [7/10], Training Loss: 2.113, Validation Accuracy: 24.72%\n",
            "Epoch [8/10], Training Loss: 2.071, Validation Accuracy: 26.71%\n",
            "Epoch [9/10], Training Loss: 2.033, Validation Accuracy: 28.63%\n",
            "Epoch [10/10], Training Loss: 1.994, Validation Accuracy: 28.78%\n",
            "Epoch [1/10], Training Loss: 1.983, Validation Accuracy: 30.09%\n",
            "Epoch [2/10], Training Loss: 1.958, Validation Accuracy: 31.20%\n",
            "Epoch [3/10], Training Loss: 1.937, Validation Accuracy: 31.59%\n",
            "Epoch [4/10], Training Loss: 1.917, Validation Accuracy: 31.82%\n",
            "Epoch [5/10], Training Loss: 1.901, Validation Accuracy: 32.98%\n",
            "Epoch [6/10], Training Loss: 1.881, Validation Accuracy: 32.98%\n",
            "Epoch [7/10], Training Loss: 1.865, Validation Accuracy: 32.99%\n",
            "Epoch [8/10], Training Loss: 1.847, Validation Accuracy: 33.46%\n",
            "Epoch [9/10], Training Loss: 1.829, Validation Accuracy: 34.88%\n",
            "Epoch [10/10], Training Loss: 1.813, Validation Accuracy: 35.02%\n",
            "Epoch [1/10], Training Loss: 1.805, Validation Accuracy: 35.90%\n",
            "Epoch [2/10], Training Loss: 1.783, Validation Accuracy: 35.69%\n",
            "Epoch [3/10], Training Loss: 1.767, Validation Accuracy: 36.36%\n",
            "Epoch [4/10], Training Loss: 1.749, Validation Accuracy: 37.15%\n",
            "Epoch [5/10], Training Loss: 1.735, Validation Accuracy: 36.55%\n",
            "Epoch [6/10], Training Loss: 1.720, Validation Accuracy: 37.81%\n",
            "Epoch [7/10], Training Loss: 1.697, Validation Accuracy: 38.19%\n",
            "Epoch [8/10], Training Loss: 1.684, Validation Accuracy: 38.59%\n",
            "Epoch [9/10], Training Loss: 1.675, Validation Accuracy: 38.71%\n",
            "Epoch [10/10], Training Loss: 1.660, Validation Accuracy: 39.13%\n",
            "Epoch [1/10], Training Loss: 1.643, Validation Accuracy: 38.68%\n",
            "Epoch [2/10], Training Loss: 1.629, Validation Accuracy: 40.04%\n",
            "Epoch [3/10], Training Loss: 1.620, Validation Accuracy: 40.03%\n",
            "Epoch [4/10], Training Loss: 1.603, Validation Accuracy: 40.44%\n",
            "Epoch [5/10], Training Loss: 1.587, Validation Accuracy: 40.80%\n",
            "Epoch [6/10], Training Loss: 1.574, Validation Accuracy: 40.45%\n",
            "Epoch [7/10], Training Loss: 1.565, Validation Accuracy: 40.11%\n",
            "Epoch [8/10], Training Loss: 1.556, Validation Accuracy: 41.57%\n",
            "Epoch [9/10], Training Loss: 1.544, Validation Accuracy: 41.14%\n",
            "Epoch [10/10], Training Loss: 1.539, Validation Accuracy: 41.52%\n",
            "Epoch [1/10], Training Loss: 1.583, Validation Accuracy: 41.63%\n",
            "Epoch [2/10], Training Loss: 1.576, Validation Accuracy: 42.46%\n",
            "Epoch [3/10], Training Loss: 1.560, Validation Accuracy: 42.92%\n",
            "Epoch [4/10], Training Loss: 1.550, Validation Accuracy: 43.42%\n",
            "Epoch [5/10], Training Loss: 1.538, Validation Accuracy: 43.17%\n",
            "Epoch [6/10], Training Loss: 1.526, Validation Accuracy: 42.88%\n",
            "Epoch [7/10], Training Loss: 1.516, Validation Accuracy: 43.30%\n",
            "Epoch [8/10], Training Loss: 1.510, Validation Accuracy: 44.09%\n",
            "Epoch [9/10], Training Loss: 1.500, Validation Accuracy: 43.32%\n",
            "Epoch [10/10], Training Loss: 1.496, Validation Accuracy: 44.40%\n",
            "Epoch [1/10], Training Loss: 1.516, Validation Accuracy: 44.44%\n",
            "Epoch [2/10], Training Loss: 1.498, Validation Accuracy: 45.17%\n",
            "Epoch [3/10], Training Loss: 1.491, Validation Accuracy: 45.16%\n",
            "Epoch [4/10], Training Loss: 1.485, Validation Accuracy: 44.71%\n",
            "Epoch [5/10], Training Loss: 1.468, Validation Accuracy: 45.29%\n",
            "Epoch [6/10], Training Loss: 1.462, Validation Accuracy: 45.16%\n",
            "Epoch [7/10], Training Loss: 1.451, Validation Accuracy: 46.05%\n",
            "Epoch [8/10], Training Loss: 1.439, Validation Accuracy: 45.68%\n",
            "Epoch [9/10], Training Loss: 1.434, Validation Accuracy: 46.38%\n",
            "Epoch [10/10], Training Loss: 1.422, Validation Accuracy: 46.50%\n",
            "Epoch [1/10], Training Loss: 1.474, Validation Accuracy: 46.73%\n",
            "Epoch [2/10], Training Loss: 1.454, Validation Accuracy: 46.07%\n",
            "Epoch [3/10], Training Loss: 1.442, Validation Accuracy: 46.64%\n",
            "Epoch [4/10], Training Loss: 1.428, Validation Accuracy: 46.33%\n",
            "Epoch [5/10], Training Loss: 1.427, Validation Accuracy: 46.54%\n",
            "Epoch [6/10], Training Loss: 1.411, Validation Accuracy: 46.49%\n",
            "Epoch [7/10], Training Loss: 1.400, Validation Accuracy: 47.30%\n",
            "Epoch [8/10], Training Loss: 1.391, Validation Accuracy: 47.42%\n",
            "Epoch [9/10], Training Loss: 1.392, Validation Accuracy: 46.92%\n",
            "Epoch [10/10], Training Loss: 1.382, Validation Accuracy: 47.61%\n",
            "Epoch [1/10], Training Loss: 1.440, Validation Accuracy: 46.92%\n",
            "Epoch [2/10], Training Loss: 1.431, Validation Accuracy: 47.71%\n",
            "Epoch [3/10], Training Loss: 1.411, Validation Accuracy: 48.26%\n",
            "Epoch [4/10], Training Loss: 1.405, Validation Accuracy: 48.08%\n",
            "Epoch [5/10], Training Loss: 1.392, Validation Accuracy: 48.56%\n",
            "Epoch [6/10], Training Loss: 1.385, Validation Accuracy: 48.52%\n",
            "Epoch [7/10], Training Loss: 1.378, Validation Accuracy: 49.13%\n",
            "Epoch [8/10], Training Loss: 1.362, Validation Accuracy: 48.72%\n",
            "Epoch [9/10], Training Loss: 1.357, Validation Accuracy: 48.73%\n",
            "Epoch [10/10], Training Loss: 1.359, Validation Accuracy: 47.90%\n",
            "Epoch [1/10], Training Loss: 1.375, Validation Accuracy: 48.94%\n",
            "Epoch [2/10], Training Loss: 1.355, Validation Accuracy: 49.58%\n",
            "Epoch [3/10], Training Loss: 1.339, Validation Accuracy: 49.04%\n",
            "Epoch [4/10], Training Loss: 1.338, Validation Accuracy: 49.68%\n",
            "Epoch [5/10], Training Loss: 1.320, Validation Accuracy: 49.51%\n",
            "Epoch [6/10], Training Loss: 1.309, Validation Accuracy: 49.10%\n",
            "Epoch [7/10], Training Loss: 1.300, Validation Accuracy: 49.80%\n",
            "Epoch [8/10], Training Loss: 1.291, Validation Accuracy: 49.23%\n",
            "Epoch [9/10], Training Loss: 1.291, Validation Accuracy: 48.89%\n",
            "Epoch [10/10], Training Loss: 1.282, Validation Accuracy: 49.81%\n",
            "Epoch [1/10], Training Loss: 1.367, Validation Accuracy: 50.78%\n",
            "Epoch [2/10], Training Loss: 1.345, Validation Accuracy: 50.90%\n",
            "Epoch [3/10], Training Loss: 1.335, Validation Accuracy: 50.99%\n",
            "Epoch [4/10], Training Loss: 1.325, Validation Accuracy: 50.68%\n",
            "Epoch [5/10], Training Loss: 1.307, Validation Accuracy: 50.77%\n",
            "Epoch [6/10], Training Loss: 1.302, Validation Accuracy: 51.54%\n",
            "Epoch [7/10], Training Loss: 1.290, Validation Accuracy: 50.60%\n",
            "Epoch [8/10], Training Loss: 1.282, Validation Accuracy: 51.10%\n",
            "Epoch [9/10], Training Loss: 1.272, Validation Accuracy: 51.45%\n",
            "Epoch [10/10], Training Loss: 1.259, Validation Accuracy: 52.08%\n",
            "Epoch [1/10], Training Loss: 1.328, Validation Accuracy: 51.90%\n",
            "Epoch [2/10], Training Loss: 1.294, Validation Accuracy: 51.79%\n",
            "Epoch [3/10], Training Loss: 1.297, Validation Accuracy: 52.03%\n",
            "Epoch [4/10], Training Loss: 1.277, Validation Accuracy: 52.35%\n",
            "Epoch [5/10], Training Loss: 1.258, Validation Accuracy: 52.19%\n",
            "Epoch [6/10], Training Loss: 1.257, Validation Accuracy: 51.74%\n",
            "Epoch [7/10], Training Loss: 1.252, Validation Accuracy: 52.91%\n",
            "Epoch [8/10], Training Loss: 1.242, Validation Accuracy: 52.13%\n",
            "Epoch [9/10], Training Loss: 1.227, Validation Accuracy: 52.45%\n",
            "Epoch [10/10], Training Loss: 1.220, Validation Accuracy: 52.12%\n",
            "Epoch [1/10], Training Loss: 1.286, Validation Accuracy: 52.31%\n",
            "Epoch [2/10], Training Loss: 1.270, Validation Accuracy: 52.12%\n",
            "Epoch [3/10], Training Loss: 1.244, Validation Accuracy: 52.59%\n",
            "Epoch [4/10], Training Loss: 1.235, Validation Accuracy: 52.32%\n",
            "Epoch [5/10], Training Loss: 1.225, Validation Accuracy: 52.86%\n",
            "Epoch [6/10], Training Loss: 1.214, Validation Accuracy: 52.49%\n",
            "Epoch [7/10], Training Loss: 1.207, Validation Accuracy: 53.09%\n",
            "Epoch [8/10], Training Loss: 1.194, Validation Accuracy: 52.95%\n",
            "Epoch [9/10], Training Loss: 1.185, Validation Accuracy: 52.49%\n",
            "Epoch [10/10], Training Loss: 1.172, Validation Accuracy: 53.25%\n",
            "Epoch [1/10], Training Loss: 1.265, Validation Accuracy: 52.93%\n",
            "Epoch [2/10], Training Loss: 1.240, Validation Accuracy: 53.63%\n",
            "Epoch [3/10], Training Loss: 1.221, Validation Accuracy: 53.71%\n",
            "Epoch [4/10], Training Loss: 1.219, Validation Accuracy: 53.80%\n",
            "Epoch [5/10], Training Loss: 1.199, Validation Accuracy: 53.01%\n",
            "Epoch [6/10], Training Loss: 1.185, Validation Accuracy: 53.70%\n",
            "Epoch [7/10], Training Loss: 1.175, Validation Accuracy: 53.77%\n",
            "Epoch [8/10], Training Loss: 1.157, Validation Accuracy: 54.20%\n",
            "Epoch [9/10], Training Loss: 1.145, Validation Accuracy: 53.65%\n",
            "Epoch [10/10], Training Loss: 1.138, Validation Accuracy: 54.37%\n",
            "Epoch [1/10], Training Loss: 1.226, Validation Accuracy: 54.26%\n",
            "Epoch [2/10], Training Loss: 1.198, Validation Accuracy: 54.11%\n",
            "Epoch [3/10], Training Loss: 1.172, Validation Accuracy: 54.35%\n",
            "Epoch [4/10], Training Loss: 1.165, Validation Accuracy: 54.55%\n",
            "Epoch [5/10], Training Loss: 1.147, Validation Accuracy: 55.20%\n",
            "Epoch [6/10], Training Loss: 1.144, Validation Accuracy: 54.20%\n",
            "Epoch [7/10], Training Loss: 1.125, Validation Accuracy: 54.28%\n",
            "Epoch [8/10], Training Loss: 1.116, Validation Accuracy: 54.04%\n",
            "Epoch [9/10], Training Loss: 1.097, Validation Accuracy: 55.12%\n",
            "Epoch [10/10], Training Loss: 1.084, Validation Accuracy: 54.34%\n",
            "Epoch [1/10], Training Loss: 1.227, Validation Accuracy: 54.52%\n",
            "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 55.67%\n",
            "Epoch [3/10], Training Loss: 1.171, Validation Accuracy: 55.54%\n",
            "Epoch [4/10], Training Loss: 1.159, Validation Accuracy: 55.51%\n",
            "Epoch [5/10], Training Loss: 1.145, Validation Accuracy: 55.61%\n",
            "Epoch [6/10], Training Loss: 1.127, Validation Accuracy: 55.56%\n",
            "Epoch [7/10], Training Loss: 1.121, Validation Accuracy: 56.23%\n",
            "Epoch [8/10], Training Loss: 1.109, Validation Accuracy: 56.31%\n",
            "Epoch [9/10], Training Loss: 1.087, Validation Accuracy: 56.28%\n",
            "Epoch [10/10], Training Loss: 1.075, Validation Accuracy: 55.72%\n",
            "Epoch [1/10], Training Loss: 1.208, Validation Accuracy: 55.15%\n",
            "Epoch [2/10], Training Loss: 1.173, Validation Accuracy: 56.31%\n",
            "Epoch [3/10], Training Loss: 1.149, Validation Accuracy: 56.39%\n",
            "Epoch [4/10], Training Loss: 1.134, Validation Accuracy: 56.29%\n",
            "Epoch [5/10], Training Loss: 1.123, Validation Accuracy: 55.96%\n",
            "Epoch [6/10], Training Loss: 1.111, Validation Accuracy: 55.31%\n",
            "Epoch [7/10], Training Loss: 1.099, Validation Accuracy: 55.70%\n",
            "Epoch [8/10], Training Loss: 1.089, Validation Accuracy: 56.04%\n",
            "Epoch [9/10], Training Loss: 1.073, Validation Accuracy: 55.72%\n",
            "Epoch [10/10], Training Loss: 1.065, Validation Accuracy: 56.32%\n",
            "Epoch [1/10], Training Loss: 1.171, Validation Accuracy: 55.41%\n",
            "Epoch [2/10], Training Loss: 1.139, Validation Accuracy: 56.70%\n",
            "Epoch [3/10], Training Loss: 1.112, Validation Accuracy: 56.67%\n",
            "Epoch [4/10], Training Loss: 1.093, Validation Accuracy: 56.51%\n",
            "Epoch [5/10], Training Loss: 1.076, Validation Accuracy: 56.06%\n",
            "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 56.70%\n",
            "Epoch [7/10], Training Loss: 1.051, Validation Accuracy: 56.51%\n",
            "Epoch [8/10], Training Loss: 1.038, Validation Accuracy: 55.72%\n",
            "Epoch [9/10], Training Loss: 1.029, Validation Accuracy: 55.82%\n",
            "Epoch [10/10], Training Loss: 1.010, Validation Accuracy: 56.57%\n",
            "Epoch [1/10], Training Loss: 1.159, Validation Accuracy: 57.01%\n",
            "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 57.12%\n",
            "Epoch [3/10], Training Loss: 1.092, Validation Accuracy: 56.73%\n",
            "Epoch [4/10], Training Loss: 1.069, Validation Accuracy: 56.78%\n",
            "Epoch [5/10], Training Loss: 1.063, Validation Accuracy: 57.34%\n",
            "Epoch [6/10], Training Loss: 1.047, Validation Accuracy: 57.16%\n",
            "Epoch [7/10], Training Loss: 1.032, Validation Accuracy: 57.14%\n",
            "Epoch [8/10], Training Loss: 1.025, Validation Accuracy: 57.35%\n",
            "Epoch [9/10], Training Loss: 1.011, Validation Accuracy: 57.20%\n",
            "Epoch [10/10], Training Loss: 0.989, Validation Accuracy: 56.70%\n",
            "Epoch [1/10], Training Loss: 1.127, Validation Accuracy: 57.79%\n",
            "Epoch [2/10], Training Loss: 1.078, Validation Accuracy: 57.26%\n",
            "Epoch [3/10], Training Loss: 1.058, Validation Accuracy: 57.30%\n",
            "Epoch [4/10], Training Loss: 1.036, Validation Accuracy: 56.64%\n",
            "Epoch [5/10], Training Loss: 1.024, Validation Accuracy: 56.29%\n",
            "Epoch [6/10], Training Loss: 1.012, Validation Accuracy: 58.05%\n",
            "Epoch [7/10], Training Loss: 0.995, Validation Accuracy: 56.94%\n",
            "Epoch [8/10], Training Loss: 0.989, Validation Accuracy: 57.40%\n",
            "Epoch [9/10], Training Loss: 0.972, Validation Accuracy: 57.09%\n",
            "Epoch [10/10], Training Loss: 0.961, Validation Accuracy: 57.67%\n",
            "Epoch [1/10], Training Loss: 1.119, Validation Accuracy: 58.17%\n",
            "Epoch [2/10], Training Loss: 1.079, Validation Accuracy: 57.66%\n",
            "Epoch [3/10], Training Loss: 1.056, Validation Accuracy: 57.49%\n",
            "Epoch [4/10], Training Loss: 1.041, Validation Accuracy: 57.67%\n",
            "Epoch [5/10], Training Loss: 1.014, Validation Accuracy: 57.76%\n",
            "Epoch [6/10], Training Loss: 1.001, Validation Accuracy: 58.15%\n",
            "Epoch [7/10], Training Loss: 0.982, Validation Accuracy: 58.38%\n",
            "Epoch [8/10], Training Loss: 0.971, Validation Accuracy: 58.29%\n",
            "Epoch [9/10], Training Loss: 0.959, Validation Accuracy: 58.37%\n",
            "Epoch [10/10], Training Loss: 0.942, Validation Accuracy: 57.99%\n",
            "Epoch [1/10], Training Loss: 1.130, Validation Accuracy: 58.74%\n",
            "Epoch [2/10], Training Loss: 1.074, Validation Accuracy: 55.93%\n",
            "Epoch [3/10], Training Loss: 1.050, Validation Accuracy: 58.64%\n",
            "Epoch [4/10], Training Loss: 1.021, Validation Accuracy: 58.42%\n",
            "Epoch [5/10], Training Loss: 1.005, Validation Accuracy: 58.61%\n",
            "Epoch [6/10], Training Loss: 0.994, Validation Accuracy: 58.20%\n",
            "Epoch [7/10], Training Loss: 0.971, Validation Accuracy: 58.57%\n",
            "Epoch [8/10], Training Loss: 0.971, Validation Accuracy: 58.58%\n",
            "Epoch [9/10], Training Loss: 0.949, Validation Accuracy: 58.32%\n",
            "Epoch [10/10], Training Loss: 0.934, Validation Accuracy: 58.47%\n",
            "Epoch [1/10], Training Loss: 1.073, Validation Accuracy: 58.13%\n",
            "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 58.57%\n",
            "Epoch [3/10], Training Loss: 1.010, Validation Accuracy: 58.98%\n",
            "Epoch [4/10], Training Loss: 0.991, Validation Accuracy: 57.63%\n",
            "Epoch [5/10], Training Loss: 0.967, Validation Accuracy: 58.42%\n",
            "Epoch [6/10], Training Loss: 0.953, Validation Accuracy: 59.07%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 58.03%\n",
            "Epoch [8/10], Training Loss: 0.923, Validation Accuracy: 58.95%\n",
            "Epoch [9/10], Training Loss: 0.901, Validation Accuracy: 58.37%\n",
            "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 57.81%\n",
            "Epoch [1/10], Training Loss: 1.070, Validation Accuracy: 58.94%\n",
            "Epoch [2/10], Training Loss: 1.022, Validation Accuracy: 58.47%\n",
            "Epoch [3/10], Training Loss: 0.995, Validation Accuracy: 59.06%\n",
            "Epoch [4/10], Training Loss: 0.971, Validation Accuracy: 59.01%\n",
            "Epoch [5/10], Training Loss: 0.950, Validation Accuracy: 58.43%\n",
            "Epoch [6/10], Training Loss: 0.941, Validation Accuracy: 59.56%\n",
            "Epoch [7/10], Training Loss: 0.920, Validation Accuracy: 59.11%\n",
            "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 58.59%\n",
            "Epoch [9/10], Training Loss: 0.899, Validation Accuracy: 59.24%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 58.39%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 59.55%\n",
            "Epoch [2/10], Training Loss: 0.993, Validation Accuracy: 59.64%\n",
            "Epoch [3/10], Training Loss: 0.961, Validation Accuracy: 58.74%\n",
            "Epoch [4/10], Training Loss: 0.935, Validation Accuracy: 59.59%\n",
            "Epoch [5/10], Training Loss: 0.917, Validation Accuracy: 58.20%\n",
            "Epoch [6/10], Training Loss: 0.905, Validation Accuracy: 59.39%\n",
            "Epoch [7/10], Training Loss: 0.878, Validation Accuracy: 59.45%\n",
            "Epoch [8/10], Training Loss: 0.870, Validation Accuracy: 58.91%\n",
            "Epoch [9/10], Training Loss: 0.849, Validation Accuracy: 58.92%\n",
            "Epoch [10/10], Training Loss: 0.848, Validation Accuracy: 57.39%\n",
            "Epoch [1/10], Training Loss: 1.044, Validation Accuracy: 58.44%\n",
            "Epoch [2/10], Training Loss: 0.985, Validation Accuracy: 58.96%\n",
            "Epoch [3/10], Training Loss: 0.951, Validation Accuracy: 59.44%\n",
            "Epoch [4/10], Training Loss: 0.919, Validation Accuracy: 59.11%\n",
            "Epoch [5/10], Training Loss: 0.904, Validation Accuracy: 59.76%\n",
            "Epoch [6/10], Training Loss: 0.893, Validation Accuracy: 59.05%\n",
            "Epoch [7/10], Training Loss: 0.870, Validation Accuracy: 59.59%\n",
            "Epoch [8/10], Training Loss: 0.859, Validation Accuracy: 59.38%\n",
            "Epoch [9/10], Training Loss: 0.830, Validation Accuracy: 59.44%\n",
            "Epoch [10/10], Training Loss: 0.825, Validation Accuracy: 59.68%\n",
            "Epoch [1/10], Training Loss: 1.051, Validation Accuracy: 58.69%\n",
            "Epoch [2/10], Training Loss: 0.993, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 0.956, Validation Accuracy: 59.61%\n",
            "Epoch [4/10], Training Loss: 0.923, Validation Accuracy: 58.62%\n",
            "Epoch [5/10], Training Loss: 0.905, Validation Accuracy: 59.64%\n",
            "Epoch [6/10], Training Loss: 0.889, Validation Accuracy: 59.63%\n",
            "Epoch [7/10], Training Loss: 0.873, Validation Accuracy: 59.56%\n",
            "Epoch [8/10], Training Loss: 0.860, Validation Accuracy: 59.67%\n",
            "Epoch [9/10], Training Loss: 0.844, Validation Accuracy: 59.93%\n",
            "Epoch [10/10], Training Loss: 0.823, Validation Accuracy: 58.88%\n",
            "Epoch [1/10], Training Loss: 1.008, Validation Accuracy: 59.18%\n",
            "Epoch [2/10], Training Loss: 0.956, Validation Accuracy: 60.49%\n",
            "Epoch [3/10], Training Loss: 0.924, Validation Accuracy: 59.30%\n",
            "Epoch [4/10], Training Loss: 0.894, Validation Accuracy: 58.68%\n",
            "Epoch [5/10], Training Loss: 0.869, Validation Accuracy: 60.01%\n",
            "Epoch [6/10], Training Loss: 0.845, Validation Accuracy: 59.49%\n",
            "Epoch [7/10], Training Loss: 0.834, Validation Accuracy: 58.66%\n",
            "Epoch [8/10], Training Loss: 0.823, Validation Accuracy: 59.23%\n",
            "Epoch [9/10], Training Loss: 0.797, Validation Accuracy: 59.49%\n",
            "Epoch [10/10], Training Loss: 0.778, Validation Accuracy: 59.19%\n",
            "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 60.06%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 60.23%\n",
            "Epoch [3/10], Training Loss: 0.912, Validation Accuracy: 60.41%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 60.16%\n",
            "Epoch [5/10], Training Loss: 0.857, Validation Accuracy: 60.40%\n",
            "Epoch [6/10], Training Loss: 0.838, Validation Accuracy: 59.90%\n",
            "Epoch [7/10], Training Loss: 0.819, Validation Accuracy: 60.56%\n",
            "Epoch [8/10], Training Loss: 0.805, Validation Accuracy: 59.02%\n",
            "Epoch [9/10], Training Loss: 0.784, Validation Accuracy: 59.63%\n",
            "Epoch [10/10], Training Loss: 0.763, Validation Accuracy: 60.12%\n",
            "Epoch [1/10], Training Loss: 0.990, Validation Accuracy: 59.89%\n",
            "Epoch [2/10], Training Loss: 0.912, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.878, Validation Accuracy: 60.17%\n",
            "Epoch [4/10], Training Loss: 0.848, Validation Accuracy: 60.40%\n",
            "Epoch [5/10], Training Loss: 0.814, Validation Accuracy: 59.23%\n",
            "Epoch [6/10], Training Loss: 0.804, Validation Accuracy: 59.93%\n",
            "Epoch [7/10], Training Loss: 0.778, Validation Accuracy: 59.49%\n",
            "Epoch [8/10], Training Loss: 0.773, Validation Accuracy: 59.25%\n",
            "Epoch [9/10], Training Loss: 0.763, Validation Accuracy: 59.10%\n",
            "Epoch [10/10], Training Loss: 0.737, Validation Accuracy: 59.66%\n",
            "Epoch [1/10], Training Loss: 0.978, Validation Accuracy: 60.43%\n",
            "Epoch [2/10], Training Loss: 0.906, Validation Accuracy: 59.46%\n",
            "Epoch [3/10], Training Loss: 0.865, Validation Accuracy: 60.32%\n",
            "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 60.57%\n",
            "Epoch [5/10], Training Loss: 0.805, Validation Accuracy: 60.57%\n",
            "Epoch [6/10], Training Loss: 0.787, Validation Accuracy: 60.74%\n",
            "Epoch [7/10], Training Loss: 0.764, Validation Accuracy: 59.47%\n",
            "Epoch [8/10], Training Loss: 0.745, Validation Accuracy: 59.74%\n",
            "Epoch [9/10], Training Loss: 0.728, Validation Accuracy: 59.79%\n",
            "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 60.38%\n",
            "Epoch [1/10], Training Loss: 0.983, Validation Accuracy: 58.81%\n",
            "Epoch [2/10], Training Loss: 0.915, Validation Accuracy: 60.45%\n",
            "Epoch [3/10], Training Loss: 0.862, Validation Accuracy: 60.59%\n",
            "Epoch [4/10], Training Loss: 0.837, Validation Accuracy: 60.45%\n",
            "Epoch [5/10], Training Loss: 0.825, Validation Accuracy: 60.10%\n",
            "Epoch [6/10], Training Loss: 0.792, Validation Accuracy: 60.32%\n",
            "Epoch [7/10], Training Loss: 0.770, Validation Accuracy: 60.51%\n",
            "Epoch [8/10], Training Loss: 0.757, Validation Accuracy: 60.50%\n",
            "Epoch [9/10], Training Loss: 0.735, Validation Accuracy: 59.89%\n",
            "Epoch [10/10], Training Loss: 0.723, Validation Accuracy: 59.46%\n",
            "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 59.92%\n",
            "Epoch [2/10], Training Loss: 0.878, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.832, Validation Accuracy: 59.78%\n",
            "Epoch [4/10], Training Loss: 0.807, Validation Accuracy: 60.35%\n",
            "Epoch [5/10], Training Loss: 0.773, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.755, Validation Accuracy: 60.39%\n",
            "Epoch [7/10], Training Loss: 0.732, Validation Accuracy: 59.89%\n",
            "Epoch [8/10], Training Loss: 0.716, Validation Accuracy: 60.55%\n",
            "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 59.84%\n",
            "Epoch [10/10], Training Loss: 0.683, Validation Accuracy: 60.51%\n",
            "Epoch [1/10], Training Loss: 0.966, Validation Accuracy: 59.78%\n",
            "Epoch [2/10], Training Loss: 0.889, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.851, Validation Accuracy: 60.83%\n",
            "Epoch [4/10], Training Loss: 0.807, Validation Accuracy: 59.41%\n",
            "Epoch [5/10], Training Loss: 0.771, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.752, Validation Accuracy: 60.79%\n",
            "Epoch [7/10], Training Loss: 0.731, Validation Accuracy: 60.30%\n",
            "Epoch [8/10], Training Loss: 0.704, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.690, Validation Accuracy: 60.69%\n",
            "Epoch [10/10], Training Loss: 0.673, Validation Accuracy: 59.47%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 60.29%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 60.04%\n",
            "Epoch [3/10], Training Loss: 0.800, Validation Accuracy: 59.63%\n",
            "Epoch [4/10], Training Loss: 0.759, Validation Accuracy: 59.97%\n",
            "Epoch [5/10], Training Loss: 0.740, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.710, Validation Accuracy: 60.00%\n",
            "Epoch [7/10], Training Loss: 0.689, Validation Accuracy: 60.35%\n",
            "Epoch [8/10], Training Loss: 0.673, Validation Accuracy: 60.00%\n",
            "Epoch [9/10], Training Loss: 0.644, Validation Accuracy: 60.06%\n",
            "Epoch [10/10], Training Loss: 0.630, Validation Accuracy: 60.33%\n",
            "Epoch [1/10], Training Loss: 0.940, Validation Accuracy: 59.37%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 60.61%\n",
            "Epoch [3/10], Training Loss: 0.788, Validation Accuracy: 59.37%\n",
            "Epoch [4/10], Training Loss: 0.765, Validation Accuracy: 60.62%\n",
            "Epoch [5/10], Training Loss: 0.717, Validation Accuracy: 60.74%\n",
            "Epoch [6/10], Training Loss: 0.688, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.668, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.667, Validation Accuracy: 59.98%\n",
            "Epoch [9/10], Training Loss: 0.641, Validation Accuracy: 59.82%\n",
            "Epoch [10/10], Training Loss: 0.624, Validation Accuracy: 59.97%\n",
            "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 59.38%\n",
            "Epoch [2/10], Training Loss: 0.844, Validation Accuracy: 60.03%\n",
            "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.751, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.719, Validation Accuracy: 59.76%\n",
            "Epoch [6/10], Training Loss: 0.709, Validation Accuracy: 60.56%\n",
            "Epoch [7/10], Training Loss: 0.685, Validation Accuracy: 60.56%\n",
            "Epoch [8/10], Training Loss: 0.656, Validation Accuracy: 60.36%\n",
            "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 60.38%\n",
            "Epoch [10/10], Training Loss: 0.622, Validation Accuracy: 59.93%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.814, Validation Accuracy: 60.93%\n",
            "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 60.71%\n",
            "Epoch [4/10], Training Loss: 0.726, Validation Accuracy: 60.54%\n",
            "Epoch [5/10], Training Loss: 0.691, Validation Accuracy: 61.07%\n",
            "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.638, Validation Accuracy: 60.21%\n",
            "Epoch [8/10], Training Loss: 0.626, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 60.09%\n",
            "Epoch [10/10], Training Loss: 0.583, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.823, Validation Accuracy: 59.98%\n",
            "Epoch [3/10], Training Loss: 0.760, Validation Accuracy: 60.78%\n",
            "Epoch [4/10], Training Loss: 0.728, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.706, Validation Accuracy: 60.23%\n",
            "Epoch [6/10], Training Loss: 0.667, Validation Accuracy: 60.37%\n",
            "Epoch [7/10], Training Loss: 0.648, Validation Accuracy: 61.07%\n",
            "Epoch [8/10], Training Loss: 0.640, Validation Accuracy: 59.96%\n",
            "Epoch [9/10], Training Loss: 0.596, Validation Accuracy: 60.06%\n",
            "Epoch [10/10], Training Loss: 0.579, Validation Accuracy: 60.15%\n",
            "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 60.44%\n",
            "Epoch [3/10], Training Loss: 0.724, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.687, Validation Accuracy: 60.02%\n",
            "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 60.81%\n",
            "Epoch [6/10], Training Loss: 0.617, Validation Accuracy: 60.24%\n",
            "Epoch [7/10], Training Loss: 0.604, Validation Accuracy: 59.36%\n",
            "Epoch [8/10], Training Loss: 0.582, Validation Accuracy: 60.32%\n",
            "Epoch [9/10], Training Loss: 0.556, Validation Accuracy: 60.42%\n",
            "Epoch [10/10], Training Loss: 0.551, Validation Accuracy: 60.08%\n",
            "Epoch [1/10], Training Loss: 0.891, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.766, Validation Accuracy: 60.05%\n",
            "Epoch [3/10], Training Loss: 0.713, Validation Accuracy: 60.21%\n",
            "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 59.91%\n",
            "Epoch [5/10], Training Loss: 0.641, Validation Accuracy: 60.25%\n",
            "Epoch [6/10], Training Loss: 0.610, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.592, Validation Accuracy: 60.82%\n",
            "Epoch [8/10], Training Loss: 0.566, Validation Accuracy: 60.42%\n",
            "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 60.19%\n",
            "Epoch [10/10], Training Loss: 0.520, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 0.889, Validation Accuracy: 60.40%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 60.64%\n",
            "Epoch [3/10], Training Loss: 0.715, Validation Accuracy: 59.41%\n",
            "Epoch [4/10], Training Loss: 0.680, Validation Accuracy: 61.01%\n",
            "Epoch [5/10], Training Loss: 0.637, Validation Accuracy: 60.89%\n",
            "Epoch [6/10], Training Loss: 0.610, Validation Accuracy: 59.46%\n",
            "Epoch [7/10], Training Loss: 0.595, Validation Accuracy: 60.87%\n",
            "Epoch [8/10], Training Loss: 0.571, Validation Accuracy: 60.25%\n",
            "Epoch [9/10], Training Loss: 0.552, Validation Accuracy: 60.20%\n",
            "Epoch [10/10], Training Loss: 0.526, Validation Accuracy: 59.94%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 59.42%\n",
            "Epoch [2/10], Training Loss: 0.743, Validation Accuracy: 60.52%\n",
            "Epoch [3/10], Training Loss: 0.683, Validation Accuracy: 60.93%\n",
            "Epoch [4/10], Training Loss: 0.646, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.612, Validation Accuracy: 60.45%\n",
            "Epoch [6/10], Training Loss: 0.584, Validation Accuracy: 60.98%\n",
            "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 60.92%\n",
            "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 60.49%\n",
            "Epoch [9/10], Training Loss: 0.510, Validation Accuracy: 60.33%\n",
            "Epoch [10/10], Training Loss: 0.492, Validation Accuracy: 60.58%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 60.02%\n",
            "Epoch [2/10], Training Loss: 0.763, Validation Accuracy: 60.50%\n",
            "Epoch [3/10], Training Loss: 0.696, Validation Accuracy: 60.03%\n",
            "Epoch [4/10], Training Loss: 0.666, Validation Accuracy: 60.58%\n",
            "Epoch [5/10], Training Loss: 0.617, Validation Accuracy: 60.39%\n",
            "Epoch [6/10], Training Loss: 0.589, Validation Accuracy: 60.62%\n",
            "Epoch [7/10], Training Loss: 0.556, Validation Accuracy: 60.36%\n",
            "Epoch [8/10], Training Loss: 0.539, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.514, Validation Accuracy: 60.54%\n",
            "Epoch [10/10], Training Loss: 0.491, Validation Accuracy: 59.87%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 59.59%\n",
            "Epoch [2/10], Training Loss: 0.729, Validation Accuracy: 59.85%\n",
            "Epoch [3/10], Training Loss: 0.665, Validation Accuracy: 60.66%\n",
            "Epoch [4/10], Training Loss: 0.610, Validation Accuracy: 60.68%\n",
            "Epoch [5/10], Training Loss: 0.580, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.550, Validation Accuracy: 60.57%\n",
            "Epoch [7/10], Training Loss: 0.514, Validation Accuracy: 60.94%\n",
            "Epoch [8/10], Training Loss: 0.494, Validation Accuracy: 59.55%\n",
            "Epoch [9/10], Training Loss: 0.473, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.449, Validation Accuracy: 59.78%\n",
            "Epoch [1/10], Training Loss: 0.853, Validation Accuracy: 59.67%\n",
            "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 60.36%\n",
            "Epoch [3/10], Training Loss: 0.641, Validation Accuracy: 60.23%\n",
            "Epoch [4/10], Training Loss: 0.590, Validation Accuracy: 60.54%\n",
            "Epoch [5/10], Training Loss: 0.570, Validation Accuracy: 60.84%\n",
            "Epoch [6/10], Training Loss: 0.534, Validation Accuracy: 60.48%\n",
            "Epoch [7/10], Training Loss: 0.502, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.476, Validation Accuracy: 60.60%\n",
            "Epoch [9/10], Training Loss: 0.469, Validation Accuracy: 60.17%\n",
            "Epoch [10/10], Training Loss: 0.442, Validation Accuracy: 60.04%\n",
            "Epoch [1/10], Training Loss: 0.851, Validation Accuracy: 59.74%\n",
            "Epoch [2/10], Training Loss: 0.716, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 0.659, Validation Accuracy: 60.57%\n",
            "Epoch [4/10], Training Loss: 0.595, Validation Accuracy: 60.78%\n",
            "Epoch [5/10], Training Loss: 0.561, Validation Accuracy: 59.75%\n",
            "Epoch [6/10], Training Loss: 0.532, Validation Accuracy: 59.52%\n",
            "Epoch [7/10], Training Loss: 0.496, Validation Accuracy: 60.12%\n",
            "Epoch [8/10], Training Loss: 0.480, Validation Accuracy: 60.01%\n",
            "Epoch [9/10], Training Loss: 0.458, Validation Accuracy: 60.13%\n",
            "Epoch [10/10], Training Loss: 0.432, Validation Accuracy: 59.85%\n",
            "Epoch [1/10], Training Loss: 0.835, Validation Accuracy: 59.91%\n",
            "Epoch [2/10], Training Loss: 0.685, Validation Accuracy: 60.24%\n",
            "Epoch [3/10], Training Loss: 0.622, Validation Accuracy: 60.44%\n",
            "Epoch [4/10], Training Loss: 0.569, Validation Accuracy: 59.88%\n",
            "Epoch [5/10], Training Loss: 0.541, Validation Accuracy: 60.49%\n",
            "Epoch [6/10], Training Loss: 0.501, Validation Accuracy: 59.88%\n",
            "Epoch [7/10], Training Loss: 0.469, Validation Accuracy: 59.83%\n",
            "Epoch [8/10], Training Loss: 0.449, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.435, Validation Accuracy: 59.78%\n",
            "Epoch [10/10], Training Loss: 0.404, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 0.862, Validation Accuracy: 59.77%\n",
            "Epoch [2/10], Training Loss: 0.704, Validation Accuracy: 60.25%\n",
            "Epoch [3/10], Training Loss: 0.641, Validation Accuracy: 60.03%\n",
            "Epoch [4/10], Training Loss: 0.577, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.541, Validation Accuracy: 60.67%\n",
            "Epoch [6/10], Training Loss: 0.505, Validation Accuracy: 59.77%\n",
            "Epoch [7/10], Training Loss: 0.473, Validation Accuracy: 59.86%\n",
            "Epoch [8/10], Training Loss: 0.453, Validation Accuracy: 59.67%\n",
            "Epoch [9/10], Training Loss: 0.427, Validation Accuracy: 59.95%\n",
            "Epoch [10/10], Training Loss: 0.412, Validation Accuracy: 60.24%\n",
            "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 59.66%\n",
            "Epoch [2/10], Training Loss: 0.659, Validation Accuracy: 59.84%\n",
            "Epoch [3/10], Training Loss: 0.587, Validation Accuracy: 60.47%\n",
            "Epoch [4/10], Training Loss: 0.534, Validation Accuracy: 60.21%\n",
            "Epoch [5/10], Training Loss: 0.498, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.471, Validation Accuracy: 59.94%\n",
            "Epoch [7/10], Training Loss: 0.438, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.415, Validation Accuracy: 60.08%\n",
            "Epoch [9/10], Training Loss: 0.389, Validation Accuracy: 60.01%\n",
            "Epoch [10/10], Training Loss: 0.385, Validation Accuracy: 60.20%\n",
            "Test Accuracy: 59.98%\n"
          ]
        }
      ],
      "source": [
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arE7wOng3yKA",
        "outputId": "383355e4-78fa-4ef6-f150-5144defb09b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 74705682.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 10.19%\n",
            "Epoch [2/10], Training Loss: 2.305, Validation Accuracy: 10.19%\n",
            "Epoch [3/10], Training Loss: 2.304, Validation Accuracy: 10.19%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 10.19%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 10.19%\n",
            "Epoch [6/10], Training Loss: 2.302, Validation Accuracy: 10.19%\n",
            "Epoch [7/10], Training Loss: 2.301, Validation Accuracy: 10.19%\n",
            "Epoch [8/10], Training Loss: 2.300, Validation Accuracy: 10.19%\n",
            "Epoch [9/10], Training Loss: 2.299, Validation Accuracy: 10.19%\n",
            "Epoch [10/10], Training Loss: 2.298, Validation Accuracy: 10.19%\n",
            "Epoch [1/10], Training Loss: 2.298, Validation Accuracy: 10.23%\n",
            "Epoch [2/10], Training Loss: 2.296, Validation Accuracy: 11.73%\n",
            "Epoch [3/10], Training Loss: 2.293, Validation Accuracy: 13.83%\n",
            "Epoch [4/10], Training Loss: 2.290, Validation Accuracy: 15.32%\n",
            "Epoch [5/10], Training Loss: 2.286, Validation Accuracy: 15.77%\n",
            "Epoch [6/10], Training Loss: 2.279, Validation Accuracy: 15.72%\n",
            "Epoch [7/10], Training Loss: 2.270, Validation Accuracy: 16.17%\n",
            "Epoch [8/10], Training Loss: 2.258, Validation Accuracy: 16.19%\n",
            "Epoch [9/10], Training Loss: 2.244, Validation Accuracy: 17.49%\n",
            "Epoch [10/10], Training Loss: 2.228, Validation Accuracy: 19.95%\n",
            "Epoch [1/10], Training Loss: 2.209, Validation Accuracy: 20.51%\n",
            "Epoch [2/10], Training Loss: 2.182, Validation Accuracy: 22.83%\n",
            "Epoch [3/10], Training Loss: 2.149, Validation Accuracy: 24.06%\n",
            "Epoch [4/10], Training Loss: 2.114, Validation Accuracy: 25.65%\n",
            "Epoch [5/10], Training Loss: 2.085, Validation Accuracy: 26.35%\n",
            "Epoch [6/10], Training Loss: 2.056, Validation Accuracy: 26.95%\n",
            "Epoch [7/10], Training Loss: 2.032, Validation Accuracy: 28.01%\n",
            "Epoch [8/10], Training Loss: 2.000, Validation Accuracy: 28.63%\n",
            "Epoch [9/10], Training Loss: 1.979, Validation Accuracy: 29.02%\n",
            "Epoch [10/10], Training Loss: 1.954, Validation Accuracy: 29.43%\n",
            "Epoch [1/10], Training Loss: 1.962, Validation Accuracy: 30.20%\n",
            "Epoch [2/10], Training Loss: 1.945, Validation Accuracy: 30.28%\n",
            "Epoch [3/10], Training Loss: 1.923, Validation Accuracy: 31.03%\n",
            "Epoch [4/10], Training Loss: 1.905, Validation Accuracy: 31.59%\n",
            "Epoch [5/10], Training Loss: 1.883, Validation Accuracy: 31.39%\n",
            "Epoch [6/10], Training Loss: 1.866, Validation Accuracy: 32.58%\n",
            "Epoch [7/10], Training Loss: 1.849, Validation Accuracy: 32.15%\n",
            "Epoch [8/10], Training Loss: 1.830, Validation Accuracy: 33.58%\n",
            "Epoch [9/10], Training Loss: 1.816, Validation Accuracy: 33.60%\n",
            "Epoch [10/10], Training Loss: 1.800, Validation Accuracy: 34.00%\n",
            "Epoch [1/10], Training Loss: 1.794, Validation Accuracy: 34.47%\n",
            "Epoch [2/10], Training Loss: 1.773, Validation Accuracy: 35.38%\n",
            "Epoch [3/10], Training Loss: 1.758, Validation Accuracy: 35.83%\n",
            "Epoch [4/10], Training Loss: 1.744, Validation Accuracy: 36.30%\n",
            "Epoch [5/10], Training Loss: 1.735, Validation Accuracy: 37.03%\n",
            "Epoch [6/10], Training Loss: 1.725, Validation Accuracy: 36.79%\n",
            "Epoch [7/10], Training Loss: 1.711, Validation Accuracy: 37.55%\n",
            "Epoch [8/10], Training Loss: 1.701, Validation Accuracy: 37.30%\n",
            "Epoch [9/10], Training Loss: 1.694, Validation Accuracy: 38.06%\n",
            "Epoch [10/10], Training Loss: 1.680, Validation Accuracy: 38.61%\n",
            "Epoch [1/10], Training Loss: 1.684, Validation Accuracy: 37.97%\n",
            "Epoch [2/10], Training Loss: 1.673, Validation Accuracy: 38.96%\n",
            "Epoch [3/10], Training Loss: 1.661, Validation Accuracy: 38.78%\n",
            "Epoch [4/10], Training Loss: 1.652, Validation Accuracy: 39.69%\n",
            "Epoch [5/10], Training Loss: 1.631, Validation Accuracy: 40.19%\n",
            "Epoch [6/10], Training Loss: 1.625, Validation Accuracy: 40.25%\n",
            "Epoch [7/10], Training Loss: 1.610, Validation Accuracy: 41.20%\n",
            "Epoch [8/10], Training Loss: 1.602, Validation Accuracy: 40.74%\n",
            "Epoch [9/10], Training Loss: 1.599, Validation Accuracy: 40.78%\n",
            "Epoch [10/10], Training Loss: 1.588, Validation Accuracy: 42.09%\n",
            "Epoch [1/10], Training Loss: 1.625, Validation Accuracy: 40.69%\n",
            "Epoch [2/10], Training Loss: 1.618, Validation Accuracy: 42.38%\n",
            "Epoch [3/10], Training Loss: 1.603, Validation Accuracy: 42.32%\n",
            "Epoch [4/10], Training Loss: 1.590, Validation Accuracy: 42.91%\n",
            "Epoch [5/10], Training Loss: 1.579, Validation Accuracy: 42.31%\n",
            "Epoch [6/10], Training Loss: 1.572, Validation Accuracy: 42.94%\n",
            "Epoch [7/10], Training Loss: 1.567, Validation Accuracy: 43.27%\n",
            "Epoch [8/10], Training Loss: 1.557, Validation Accuracy: 43.66%\n",
            "Epoch [9/10], Training Loss: 1.540, Validation Accuracy: 43.57%\n",
            "Epoch [10/10], Training Loss: 1.534, Validation Accuracy: 44.28%\n",
            "Epoch [1/10], Training Loss: 1.537, Validation Accuracy: 43.33%\n",
            "Epoch [2/10], Training Loss: 1.521, Validation Accuracy: 44.30%\n",
            "Epoch [3/10], Training Loss: 1.511, Validation Accuracy: 44.63%\n",
            "Epoch [4/10], Training Loss: 1.497, Validation Accuracy: 44.99%\n",
            "Epoch [5/10], Training Loss: 1.489, Validation Accuracy: 44.68%\n",
            "Epoch [6/10], Training Loss: 1.478, Validation Accuracy: 45.45%\n",
            "Epoch [7/10], Training Loss: 1.467, Validation Accuracy: 45.43%\n",
            "Epoch [8/10], Training Loss: 1.456, Validation Accuracy: 45.89%\n",
            "Epoch [9/10], Training Loss: 1.453, Validation Accuracy: 44.98%\n",
            "Epoch [10/10], Training Loss: 1.446, Validation Accuracy: 45.89%\n",
            "Epoch [1/10], Training Loss: 1.493, Validation Accuracy: 45.98%\n",
            "Epoch [2/10], Training Loss: 1.476, Validation Accuracy: 46.38%\n",
            "Epoch [3/10], Training Loss: 1.471, Validation Accuracy: 46.62%\n",
            "Epoch [4/10], Training Loss: 1.455, Validation Accuracy: 46.82%\n",
            "Epoch [5/10], Training Loss: 1.452, Validation Accuracy: 47.03%\n",
            "Epoch [6/10], Training Loss: 1.451, Validation Accuracy: 47.21%\n",
            "Epoch [7/10], Training Loss: 1.430, Validation Accuracy: 47.23%\n",
            "Epoch [8/10], Training Loss: 1.427, Validation Accuracy: 47.47%\n",
            "Epoch [9/10], Training Loss: 1.413, Validation Accuracy: 47.38%\n",
            "Epoch [10/10], Training Loss: 1.412, Validation Accuracy: 46.68%\n",
            "Epoch [1/10], Training Loss: 1.434, Validation Accuracy: 47.44%\n",
            "Epoch [2/10], Training Loss: 1.430, Validation Accuracy: 48.01%\n",
            "Epoch [3/10], Training Loss: 1.397, Validation Accuracy: 48.10%\n",
            "Epoch [4/10], Training Loss: 1.390, Validation Accuracy: 48.17%\n",
            "Epoch [5/10], Training Loss: 1.390, Validation Accuracy: 48.14%\n",
            "Epoch [6/10], Training Loss: 1.370, Validation Accuracy: 48.60%\n",
            "Epoch [7/10], Training Loss: 1.358, Validation Accuracy: 49.11%\n",
            "Epoch [8/10], Training Loss: 1.358, Validation Accuracy: 49.01%\n",
            "Epoch [9/10], Training Loss: 1.347, Validation Accuracy: 48.15%\n",
            "Epoch [10/10], Training Loss: 1.348, Validation Accuracy: 48.64%\n",
            "Epoch [1/10], Training Loss: 1.377, Validation Accuracy: 48.87%\n",
            "Epoch [2/10], Training Loss: 1.364, Validation Accuracy: 49.75%\n",
            "Epoch [3/10], Training Loss: 1.347, Validation Accuracy: 50.07%\n",
            "Epoch [4/10], Training Loss: 1.350, Validation Accuracy: 49.82%\n",
            "Epoch [5/10], Training Loss: 1.328, Validation Accuracy: 49.81%\n",
            "Epoch [6/10], Training Loss: 1.329, Validation Accuracy: 49.62%\n",
            "Epoch [7/10], Training Loss: 1.315, Validation Accuracy: 48.84%\n",
            "Epoch [8/10], Training Loss: 1.299, Validation Accuracy: 49.80%\n",
            "Epoch [9/10], Training Loss: 1.293, Validation Accuracy: 50.16%\n",
            "Epoch [10/10], Training Loss: 1.282, Validation Accuracy: 50.24%\n",
            "Epoch [1/10], Training Loss: 1.370, Validation Accuracy: 50.33%\n",
            "Epoch [2/10], Training Loss: 1.351, Validation Accuracy: 51.18%\n",
            "Epoch [3/10], Training Loss: 1.342, Validation Accuracy: 51.60%\n",
            "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 51.77%\n",
            "Epoch [5/10], Training Loss: 1.319, Validation Accuracy: 51.42%\n",
            "Epoch [6/10], Training Loss: 1.317, Validation Accuracy: 51.14%\n",
            "Epoch [7/10], Training Loss: 1.310, Validation Accuracy: 51.63%\n",
            "Epoch [8/10], Training Loss: 1.291, Validation Accuracy: 51.72%\n",
            "Epoch [9/10], Training Loss: 1.290, Validation Accuracy: 51.19%\n",
            "Epoch [10/10], Training Loss: 1.278, Validation Accuracy: 51.95%\n",
            "Epoch [1/10], Training Loss: 1.323, Validation Accuracy: 51.39%\n",
            "Epoch [2/10], Training Loss: 1.300, Validation Accuracy: 52.10%\n",
            "Epoch [3/10], Training Loss: 1.287, Validation Accuracy: 51.82%\n",
            "Epoch [4/10], Training Loss: 1.264, Validation Accuracy: 52.83%\n",
            "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 52.94%\n",
            "Epoch [6/10], Training Loss: 1.243, Validation Accuracy: 52.45%\n",
            "Epoch [7/10], Training Loss: 1.244, Validation Accuracy: 52.77%\n",
            "Epoch [8/10], Training Loss: 1.224, Validation Accuracy: 53.01%\n",
            "Epoch [9/10], Training Loss: 1.216, Validation Accuracy: 53.24%\n",
            "Epoch [10/10], Training Loss: 1.224, Validation Accuracy: 52.56%\n",
            "Epoch [1/10], Training Loss: 1.311, Validation Accuracy: 52.87%\n",
            "Epoch [2/10], Training Loss: 1.296, Validation Accuracy: 53.24%\n",
            "Epoch [3/10], Training Loss: 1.274, Validation Accuracy: 53.54%\n",
            "Epoch [4/10], Training Loss: 1.266, Validation Accuracy: 53.73%\n",
            "Epoch [5/10], Training Loss: 1.256, Validation Accuracy: 53.99%\n",
            "Epoch [6/10], Training Loss: 1.243, Validation Accuracy: 54.51%\n",
            "Epoch [7/10], Training Loss: 1.243, Validation Accuracy: 52.56%\n",
            "Epoch [8/10], Training Loss: 1.233, Validation Accuracy: 54.03%\n",
            "Epoch [9/10], Training Loss: 1.221, Validation Accuracy: 52.50%\n",
            "Epoch [10/10], Training Loss: 1.215, Validation Accuracy: 54.41%\n",
            "Epoch [1/10], Training Loss: 1.260, Validation Accuracy: 54.19%\n",
            "Epoch [2/10], Training Loss: 1.234, Validation Accuracy: 55.46%\n",
            "Epoch [3/10], Training Loss: 1.224, Validation Accuracy: 53.94%\n",
            "Epoch [4/10], Training Loss: 1.203, Validation Accuracy: 54.31%\n",
            "Epoch [5/10], Training Loss: 1.196, Validation Accuracy: 53.39%\n",
            "Epoch [6/10], Training Loss: 1.188, Validation Accuracy: 54.37%\n",
            "Epoch [7/10], Training Loss: 1.172, Validation Accuracy: 54.99%\n",
            "Epoch [8/10], Training Loss: 1.165, Validation Accuracy: 54.86%\n",
            "Epoch [9/10], Training Loss: 1.153, Validation Accuracy: 54.47%\n",
            "Epoch [10/10], Training Loss: 1.147, Validation Accuracy: 54.50%\n",
            "Epoch [1/10], Training Loss: 1.216, Validation Accuracy: 54.97%\n",
            "Epoch [2/10], Training Loss: 1.196, Validation Accuracy: 54.72%\n",
            "Epoch [3/10], Training Loss: 1.175, Validation Accuracy: 54.43%\n",
            "Epoch [4/10], Training Loss: 1.163, Validation Accuracy: 55.73%\n",
            "Epoch [5/10], Training Loss: 1.152, Validation Accuracy: 55.28%\n",
            "Epoch [6/10], Training Loss: 1.147, Validation Accuracy: 55.10%\n",
            "Epoch [7/10], Training Loss: 1.137, Validation Accuracy: 53.93%\n",
            "Epoch [8/10], Training Loss: 1.131, Validation Accuracy: 55.31%\n",
            "Epoch [9/10], Training Loss: 1.121, Validation Accuracy: 55.52%\n",
            "Epoch [10/10], Training Loss: 1.108, Validation Accuracy: 55.27%\n",
            "Epoch [1/10], Training Loss: 1.239, Validation Accuracy: 56.17%\n",
            "Epoch [2/10], Training Loss: 1.203, Validation Accuracy: 54.94%\n",
            "Epoch [3/10], Training Loss: 1.181, Validation Accuracy: 56.26%\n",
            "Epoch [4/10], Training Loss: 1.173, Validation Accuracy: 56.22%\n",
            "Epoch [5/10], Training Loss: 1.169, Validation Accuracy: 56.19%\n",
            "Epoch [6/10], Training Loss: 1.161, Validation Accuracy: 56.55%\n",
            "Epoch [7/10], Training Loss: 1.146, Validation Accuracy: 56.04%\n",
            "Epoch [8/10], Training Loss: 1.136, Validation Accuracy: 56.48%\n",
            "Epoch [9/10], Training Loss: 1.119, Validation Accuracy: 56.45%\n",
            "Epoch [10/10], Training Loss: 1.113, Validation Accuracy: 55.96%\n",
            "Epoch [1/10], Training Loss: 1.172, Validation Accuracy: 57.17%\n",
            "Epoch [2/10], Training Loss: 1.149, Validation Accuracy: 56.52%\n",
            "Epoch [3/10], Training Loss: 1.134, Validation Accuracy: 57.25%\n",
            "Epoch [4/10], Training Loss: 1.130, Validation Accuracy: 56.79%\n",
            "Epoch [5/10], Training Loss: 1.112, Validation Accuracy: 55.56%\n",
            "Epoch [6/10], Training Loss: 1.113, Validation Accuracy: 56.66%\n",
            "Epoch [7/10], Training Loss: 1.088, Validation Accuracy: 56.56%\n",
            "Epoch [8/10], Training Loss: 1.078, Validation Accuracy: 56.93%\n",
            "Epoch [9/10], Training Loss: 1.065, Validation Accuracy: 56.99%\n",
            "Epoch [10/10], Training Loss: 1.066, Validation Accuracy: 56.89%\n",
            "Epoch [1/10], Training Loss: 1.200, Validation Accuracy: 56.95%\n",
            "Epoch [2/10], Training Loss: 1.173, Validation Accuracy: 56.95%\n",
            "Epoch [3/10], Training Loss: 1.159, Validation Accuracy: 57.10%\n",
            "Epoch [4/10], Training Loss: 1.146, Validation Accuracy: 57.33%\n",
            "Epoch [5/10], Training Loss: 1.138, Validation Accuracy: 57.60%\n",
            "Epoch [6/10], Training Loss: 1.125, Validation Accuracy: 57.74%\n",
            "Epoch [7/10], Training Loss: 1.116, Validation Accuracy: 57.46%\n",
            "Epoch [8/10], Training Loss: 1.095, Validation Accuracy: 57.01%\n",
            "Epoch [9/10], Training Loss: 1.087, Validation Accuracy: 56.60%\n",
            "Epoch [10/10], Training Loss: 1.081, Validation Accuracy: 57.39%\n",
            "Epoch [1/10], Training Loss: 1.146, Validation Accuracy: 57.07%\n",
            "Epoch [2/10], Training Loss: 1.119, Validation Accuracy: 57.30%\n",
            "Epoch [3/10], Training Loss: 1.097, Validation Accuracy: 57.99%\n",
            "Epoch [4/10], Training Loss: 1.082, Validation Accuracy: 57.84%\n",
            "Epoch [5/10], Training Loss: 1.078, Validation Accuracy: 57.91%\n",
            "Epoch [6/10], Training Loss: 1.063, Validation Accuracy: 58.31%\n",
            "Epoch [7/10], Training Loss: 1.056, Validation Accuracy: 57.98%\n",
            "Epoch [8/10], Training Loss: 1.033, Validation Accuracy: 58.47%\n",
            "Epoch [9/10], Training Loss: 1.027, Validation Accuracy: 57.95%\n",
            "Epoch [10/10], Training Loss: 1.017, Validation Accuracy: 56.94%\n",
            "Epoch [1/10], Training Loss: 1.119, Validation Accuracy: 57.93%\n",
            "Epoch [2/10], Training Loss: 1.088, Validation Accuracy: 58.07%\n",
            "Epoch [3/10], Training Loss: 1.067, Validation Accuracy: 57.39%\n",
            "Epoch [4/10], Training Loss: 1.049, Validation Accuracy: 58.30%\n",
            "Epoch [5/10], Training Loss: 1.043, Validation Accuracy: 57.82%\n",
            "Epoch [6/10], Training Loss: 1.018, Validation Accuracy: 58.31%\n",
            "Epoch [7/10], Training Loss: 1.009, Validation Accuracy: 57.74%\n",
            "Epoch [8/10], Training Loss: 1.002, Validation Accuracy: 56.58%\n",
            "Epoch [9/10], Training Loss: 0.994, Validation Accuracy: 57.85%\n",
            "Epoch [10/10], Training Loss: 0.976, Validation Accuracy: 57.56%\n",
            "Epoch [1/10], Training Loss: 1.141, Validation Accuracy: 57.96%\n",
            "Epoch [2/10], Training Loss: 1.102, Validation Accuracy: 57.63%\n",
            "Epoch [3/10], Training Loss: 1.076, Validation Accuracy: 58.62%\n",
            "Epoch [4/10], Training Loss: 1.064, Validation Accuracy: 57.52%\n",
            "Epoch [5/10], Training Loss: 1.050, Validation Accuracy: 57.55%\n",
            "Epoch [6/10], Training Loss: 1.044, Validation Accuracy: 57.45%\n",
            "Epoch [7/10], Training Loss: 1.023, Validation Accuracy: 58.27%\n",
            "Epoch [8/10], Training Loss: 1.011, Validation Accuracy: 58.95%\n",
            "Epoch [9/10], Training Loss: 1.000, Validation Accuracy: 58.72%\n",
            "Epoch [10/10], Training Loss: 0.988, Validation Accuracy: 58.83%\n",
            "Epoch [1/10], Training Loss: 1.095, Validation Accuracy: 58.42%\n",
            "Epoch [2/10], Training Loss: 1.068, Validation Accuracy: 58.74%\n",
            "Epoch [3/10], Training Loss: 1.037, Validation Accuracy: 59.06%\n",
            "Epoch [4/10], Training Loss: 1.020, Validation Accuracy: 59.27%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 57.90%\n",
            "Epoch [6/10], Training Loss: 0.992, Validation Accuracy: 58.98%\n",
            "Epoch [7/10], Training Loss: 0.978, Validation Accuracy: 58.27%\n",
            "Epoch [8/10], Training Loss: 0.960, Validation Accuracy: 58.82%\n",
            "Epoch [9/10], Training Loss: 0.950, Validation Accuracy: 59.05%\n",
            "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 58.07%\n",
            "Epoch [1/10], Training Loss: 1.136, Validation Accuracy: 59.40%\n",
            "Epoch [2/10], Training Loss: 1.083, Validation Accuracy: 59.59%\n",
            "Epoch [3/10], Training Loss: 1.071, Validation Accuracy: 58.10%\n",
            "Epoch [4/10], Training Loss: 1.055, Validation Accuracy: 59.25%\n",
            "Epoch [5/10], Training Loss: 1.029, Validation Accuracy: 59.60%\n",
            "Epoch [6/10], Training Loss: 1.015, Validation Accuracy: 59.76%\n",
            "Epoch [7/10], Training Loss: 1.003, Validation Accuracy: 58.78%\n",
            "Epoch [8/10], Training Loss: 0.996, Validation Accuracy: 58.75%\n",
            "Epoch [9/10], Training Loss: 0.981, Validation Accuracy: 60.15%\n",
            "Epoch [10/10], Training Loss: 0.969, Validation Accuracy: 59.24%\n",
            "Epoch [1/10], Training Loss: 1.092, Validation Accuracy: 59.35%\n",
            "Epoch [2/10], Training Loss: 1.044, Validation Accuracy: 59.95%\n",
            "Epoch [3/10], Training Loss: 1.014, Validation Accuracy: 59.42%\n",
            "Epoch [4/10], Training Loss: 0.991, Validation Accuracy: 58.86%\n",
            "Epoch [5/10], Training Loss: 0.979, Validation Accuracy: 59.97%\n",
            "Epoch [6/10], Training Loss: 0.961, Validation Accuracy: 59.48%\n",
            "Epoch [7/10], Training Loss: 0.954, Validation Accuracy: 59.16%\n",
            "Epoch [8/10], Training Loss: 0.936, Validation Accuracy: 59.76%\n",
            "Epoch [9/10], Training Loss: 0.922, Validation Accuracy: 59.78%\n",
            "Epoch [10/10], Training Loss: 0.913, Validation Accuracy: 59.65%\n",
            "Epoch [1/10], Training Loss: 1.044, Validation Accuracy: 59.22%\n",
            "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 59.27%\n",
            "Epoch [3/10], Training Loss: 0.980, Validation Accuracy: 59.27%\n",
            "Epoch [4/10], Training Loss: 0.958, Validation Accuracy: 59.50%\n",
            "Epoch [5/10], Training Loss: 0.933, Validation Accuracy: 58.89%\n",
            "Epoch [6/10], Training Loss: 0.923, Validation Accuracy: 59.41%\n",
            "Epoch [7/10], Training Loss: 0.907, Validation Accuracy: 59.83%\n",
            "Epoch [8/10], Training Loss: 0.893, Validation Accuracy: 59.09%\n",
            "Epoch [9/10], Training Loss: 0.891, Validation Accuracy: 59.77%\n",
            "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 59.38%\n",
            "Epoch [1/10], Training Loss: 1.074, Validation Accuracy: 59.96%\n",
            "Epoch [2/10], Training Loss: 1.036, Validation Accuracy: 59.66%\n",
            "Epoch [3/10], Training Loss: 1.000, Validation Accuracy: 60.34%\n",
            "Epoch [4/10], Training Loss: 0.968, Validation Accuracy: 60.22%\n",
            "Epoch [5/10], Training Loss: 0.958, Validation Accuracy: 59.50%\n",
            "Epoch [6/10], Training Loss: 0.946, Validation Accuracy: 59.92%\n",
            "Epoch [7/10], Training Loss: 0.925, Validation Accuracy: 59.75%\n",
            "Epoch [8/10], Training Loss: 0.908, Validation Accuracy: 59.72%\n",
            "Epoch [9/10], Training Loss: 0.893, Validation Accuracy: 59.41%\n",
            "Epoch [10/10], Training Loss: 0.893, Validation Accuracy: 59.69%\n",
            "Epoch [1/10], Training Loss: 1.019, Validation Accuracy: 60.16%\n",
            "Epoch [2/10], Training Loss: 0.979, Validation Accuracy: 60.31%\n",
            "Epoch [3/10], Training Loss: 0.958, Validation Accuracy: 60.11%\n",
            "Epoch [4/10], Training Loss: 0.935, Validation Accuracy: 60.24%\n",
            "Epoch [5/10], Training Loss: 0.918, Validation Accuracy: 60.49%\n",
            "Epoch [6/10], Training Loss: 0.897, Validation Accuracy: 59.81%\n",
            "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 59.48%\n",
            "Epoch [8/10], Training Loss: 0.871, Validation Accuracy: 59.33%\n",
            "Epoch [9/10], Training Loss: 0.859, Validation Accuracy: 60.33%\n",
            "Epoch [10/10], Training Loss: 0.838, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 1.069, Validation Accuracy: 60.09%\n",
            "Epoch [2/10], Training Loss: 1.019, Validation Accuracy: 60.92%\n",
            "Epoch [3/10], Training Loss: 0.990, Validation Accuracy: 61.29%\n",
            "Epoch [4/10], Training Loss: 0.956, Validation Accuracy: 60.41%\n",
            "Epoch [5/10], Training Loss: 0.951, Validation Accuracy: 60.68%\n",
            "Epoch [6/10], Training Loss: 0.924, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.911, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.903, Validation Accuracy: 60.69%\n",
            "Epoch [9/10], Training Loss: 0.886, Validation Accuracy: 59.74%\n",
            "Epoch [10/10], Training Loss: 0.869, Validation Accuracy: 61.26%\n",
            "Epoch [1/10], Training Loss: 1.023, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.972, Validation Accuracy: 60.80%\n",
            "Epoch [3/10], Training Loss: 0.940, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.911, Validation Accuracy: 60.59%\n",
            "Epoch [5/10], Training Loss: 0.902, Validation Accuracy: 60.88%\n",
            "Epoch [6/10], Training Loss: 0.879, Validation Accuracy: 59.78%\n",
            "Epoch [7/10], Training Loss: 0.872, Validation Accuracy: 60.65%\n",
            "Epoch [8/10], Training Loss: 0.844, Validation Accuracy: 59.47%\n",
            "Epoch [9/10], Training Loss: 0.833, Validation Accuracy: 60.57%\n",
            "Epoch [10/10], Training Loss: 0.818, Validation Accuracy: 60.60%\n",
            "Epoch [1/10], Training Loss: 0.995, Validation Accuracy: 60.16%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.36%\n",
            "Epoch [3/10], Training Loss: 0.895, Validation Accuracy: 60.57%\n",
            "Epoch [4/10], Training Loss: 0.876, Validation Accuracy: 60.40%\n",
            "Epoch [5/10], Training Loss: 0.850, Validation Accuracy: 61.13%\n",
            "Epoch [6/10], Training Loss: 0.842, Validation Accuracy: 60.33%\n",
            "Epoch [7/10], Training Loss: 0.812, Validation Accuracy: 61.02%\n",
            "Epoch [8/10], Training Loss: 0.800, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.779, Validation Accuracy: 60.41%\n",
            "Epoch [10/10], Training Loss: 0.766, Validation Accuracy: 60.69%\n",
            "Epoch [1/10], Training Loss: 1.018, Validation Accuracy: 60.50%\n",
            "Epoch [2/10], Training Loss: 0.948, Validation Accuracy: 61.06%\n",
            "Epoch [3/10], Training Loss: 0.911, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 60.20%\n",
            "Epoch [5/10], Training Loss: 0.871, Validation Accuracy: 60.16%\n",
            "Epoch [6/10], Training Loss: 0.868, Validation Accuracy: 60.49%\n",
            "Epoch [7/10], Training Loss: 0.835, Validation Accuracy: 60.42%\n",
            "Epoch [8/10], Training Loss: 0.826, Validation Accuracy: 61.00%\n",
            "Epoch [9/10], Training Loss: 0.802, Validation Accuracy: 60.63%\n",
            "Epoch [10/10], Training Loss: 0.780, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.969, Validation Accuracy: 60.67%\n",
            "Epoch [2/10], Training Loss: 0.908, Validation Accuracy: 61.11%\n",
            "Epoch [3/10], Training Loss: 0.880, Validation Accuracy: 60.68%\n",
            "Epoch [4/10], Training Loss: 0.853, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.837, Validation Accuracy: 60.83%\n",
            "Epoch [6/10], Training Loss: 0.812, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.789, Validation Accuracy: 60.43%\n",
            "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.768, Validation Accuracy: 60.73%\n",
            "Epoch [10/10], Training Loss: 0.748, Validation Accuracy: 61.12%\n",
            "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.955, Validation Accuracy: 60.92%\n",
            "Epoch [3/10], Training Loss: 0.922, Validation Accuracy: 61.38%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 61.01%\n",
            "Epoch [5/10], Training Loss: 0.860, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.845, Validation Accuracy: 61.76%\n",
            "Epoch [7/10], Training Loss: 0.822, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.815, Validation Accuracy: 60.85%\n",
            "Epoch [9/10], Training Loss: 0.805, Validation Accuracy: 61.49%\n",
            "Epoch [10/10], Training Loss: 0.783, Validation Accuracy: 60.68%\n",
            "Epoch [1/10], Training Loss: 0.971, Validation Accuracy: 61.43%\n",
            "Epoch [2/10], Training Loss: 0.900, Validation Accuracy: 61.37%\n",
            "Epoch [3/10], Training Loss: 0.862, Validation Accuracy: 62.11%\n",
            "Epoch [4/10], Training Loss: 0.841, Validation Accuracy: 61.65%\n",
            "Epoch [5/10], Training Loss: 0.815, Validation Accuracy: 60.19%\n",
            "Epoch [6/10], Training Loss: 0.797, Validation Accuracy: 61.92%\n",
            "Epoch [7/10], Training Loss: 0.767, Validation Accuracy: 61.44%\n",
            "Epoch [8/10], Training Loss: 0.758, Validation Accuracy: 61.33%\n",
            "Epoch [9/10], Training Loss: 0.738, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.729, Validation Accuracy: 61.31%\n",
            "Epoch [1/10], Training Loss: 0.933, Validation Accuracy: 61.82%\n",
            "Epoch [2/10], Training Loss: 0.853, Validation Accuracy: 60.11%\n",
            "Epoch [3/10], Training Loss: 0.834, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.802, Validation Accuracy: 61.22%\n",
            "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.736, Validation Accuracy: 61.62%\n",
            "Epoch [8/10], Training Loss: 0.711, Validation Accuracy: 60.77%\n",
            "Epoch [9/10], Training Loss: 0.684, Validation Accuracy: 60.88%\n",
            "Epoch [10/10], Training Loss: 0.675, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.959, Validation Accuracy: 61.10%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 61.56%\n",
            "Epoch [3/10], Training Loss: 0.837, Validation Accuracy: 61.03%\n",
            "Epoch [4/10], Training Loss: 0.818, Validation Accuracy: 61.19%\n",
            "Epoch [5/10], Training Loss: 0.786, Validation Accuracy: 60.65%\n",
            "Epoch [6/10], Training Loss: 0.764, Validation Accuracy: 61.92%\n",
            "Epoch [7/10], Training Loss: 0.742, Validation Accuracy: 61.77%\n",
            "Epoch [8/10], Training Loss: 0.727, Validation Accuracy: 61.38%\n",
            "Epoch [9/10], Training Loss: 0.708, Validation Accuracy: 61.46%\n",
            "Epoch [10/10], Training Loss: 0.701, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 0.922, Validation Accuracy: 61.94%\n",
            "Epoch [2/10], Training Loss: 0.850, Validation Accuracy: 61.13%\n",
            "Epoch [3/10], Training Loss: 0.804, Validation Accuracy: 61.68%\n",
            "Epoch [4/10], Training Loss: 0.776, Validation Accuracy: 61.24%\n",
            "Epoch [5/10], Training Loss: 0.748, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.734, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.705, Validation Accuracy: 61.15%\n",
            "Epoch [8/10], Training Loss: 0.703, Validation Accuracy: 60.58%\n",
            "Epoch [9/10], Training Loss: 0.677, Validation Accuracy: 60.95%\n",
            "Epoch [10/10], Training Loss: 0.663, Validation Accuracy: 61.16%\n",
            "Epoch [1/10], Training Loss: 0.977, Validation Accuracy: 61.17%\n",
            "Epoch [2/10], Training Loss: 0.892, Validation Accuracy: 61.51%\n",
            "Epoch [3/10], Training Loss: 0.845, Validation Accuracy: 61.85%\n",
            "Epoch [4/10], Training Loss: 0.822, Validation Accuracy: 61.43%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 61.64%\n",
            "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.751, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.727, Validation Accuracy: 61.87%\n",
            "Epoch [9/10], Training Loss: 0.701, Validation Accuracy: 61.34%\n",
            "Epoch [10/10], Training Loss: 0.686, Validation Accuracy: 60.74%\n",
            "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 61.48%\n",
            "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 61.57%\n",
            "Epoch [3/10], Training Loss: 0.797, Validation Accuracy: 61.19%\n",
            "Epoch [4/10], Training Loss: 0.769, Validation Accuracy: 61.43%\n",
            "Epoch [5/10], Training Loss: 0.749, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.706, Validation Accuracy: 61.56%\n",
            "Epoch [7/10], Training Loss: 0.681, Validation Accuracy: 61.40%\n",
            "Epoch [8/10], Training Loss: 0.666, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.650, Validation Accuracy: 61.25%\n",
            "Epoch [10/10], Training Loss: 0.630, Validation Accuracy: 61.16%\n",
            "Epoch [1/10], Training Loss: 0.875, Validation Accuracy: 60.71%\n",
            "Epoch [2/10], Training Loss: 0.810, Validation Accuracy: 61.37%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.715, Validation Accuracy: 61.37%\n",
            "Epoch [5/10], Training Loss: 0.693, Validation Accuracy: 61.32%\n",
            "Epoch [6/10], Training Loss: 0.668, Validation Accuracy: 61.26%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 60.60%\n",
            "Epoch [8/10], Training Loss: 0.620, Validation Accuracy: 61.39%\n",
            "Epoch [9/10], Training Loss: 0.602, Validation Accuracy: 60.98%\n",
            "Epoch [10/10], Training Loss: 0.579, Validation Accuracy: 61.08%\n",
            "Epoch [1/10], Training Loss: 0.916, Validation Accuracy: 61.53%\n",
            "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 61.53%\n",
            "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 62.19%\n",
            "Epoch [4/10], Training Loss: 0.734, Validation Accuracy: 61.65%\n",
            "Epoch [5/10], Training Loss: 0.696, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.678, Validation Accuracy: 61.42%\n",
            "Epoch [7/10], Training Loss: 0.648, Validation Accuracy: 61.66%\n",
            "Epoch [8/10], Training Loss: 0.626, Validation Accuracy: 61.97%\n",
            "Epoch [9/10], Training Loss: 0.614, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.593, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 60.88%\n",
            "Epoch [2/10], Training Loss: 0.799, Validation Accuracy: 61.69%\n",
            "Epoch [3/10], Training Loss: 0.746, Validation Accuracy: 61.39%\n",
            "Epoch [4/10], Training Loss: 0.703, Validation Accuracy: 61.26%\n",
            "Epoch [5/10], Training Loss: 0.678, Validation Accuracy: 61.48%\n",
            "Epoch [6/10], Training Loss: 0.653, Validation Accuracy: 61.27%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.610, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.587, Validation Accuracy: 61.39%\n",
            "Epoch [10/10], Training Loss: 0.566, Validation Accuracy: 60.16%\n",
            "Epoch [1/10], Training Loss: 0.944, Validation Accuracy: 60.86%\n",
            "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 61.49%\n",
            "Epoch [3/10], Training Loss: 0.772, Validation Accuracy: 61.57%\n",
            "Epoch [4/10], Training Loss: 0.749, Validation Accuracy: 62.04%\n",
            "Epoch [5/10], Training Loss: 0.712, Validation Accuracy: 61.33%\n",
            "Epoch [6/10], Training Loss: 0.681, Validation Accuracy: 61.56%\n",
            "Epoch [7/10], Training Loss: 0.653, Validation Accuracy: 61.03%\n",
            "Epoch [8/10], Training Loss: 0.634, Validation Accuracy: 61.51%\n",
            "Epoch [9/10], Training Loss: 0.616, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.595, Validation Accuracy: 61.20%\n",
            "Epoch [1/10], Training Loss: 0.883, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.778, Validation Accuracy: 61.61%\n",
            "Epoch [3/10], Training Loss: 0.729, Validation Accuracy: 61.29%\n",
            "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 61.25%\n",
            "Epoch [5/10], Training Loss: 0.655, Validation Accuracy: 61.34%\n",
            "Epoch [6/10], Training Loss: 0.630, Validation Accuracy: 61.32%\n",
            "Epoch [7/10], Training Loss: 0.602, Validation Accuracy: 61.60%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 61.38%\n",
            "Epoch [9/10], Training Loss: 0.562, Validation Accuracy: 60.88%\n",
            "Epoch [10/10], Training Loss: 0.538, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.745, Validation Accuracy: 60.70%\n",
            "Epoch [3/10], Training Loss: 0.688, Validation Accuracy: 60.82%\n",
            "Epoch [4/10], Training Loss: 0.636, Validation Accuracy: 60.52%\n",
            "Epoch [5/10], Training Loss: 0.603, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.582, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.550, Validation Accuracy: 60.52%\n",
            "Epoch [8/10], Training Loss: 0.532, Validation Accuracy: 60.51%\n",
            "Epoch [9/10], Training Loss: 0.520, Validation Accuracy: 60.30%\n",
            "Epoch [10/10], Training Loss: 0.495, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 61.14%\n",
            "Epoch [2/10], Training Loss: 0.763, Validation Accuracy: 61.36%\n",
            "Epoch [3/10], Training Loss: 0.697, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.654, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.628, Validation Accuracy: 61.26%\n",
            "Epoch [6/10], Training Loss: 0.594, Validation Accuracy: 61.30%\n",
            "Epoch [7/10], Training Loss: 0.574, Validation Accuracy: 61.12%\n",
            "Epoch [8/10], Training Loss: 0.542, Validation Accuracy: 61.26%\n",
            "Epoch [9/10], Training Loss: 0.525, Validation Accuracy: 61.61%\n",
            "Epoch [10/10], Training Loss: 0.504, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.845, Validation Accuracy: 60.81%\n",
            "Epoch [2/10], Training Loss: 0.727, Validation Accuracy: 61.22%\n",
            "Epoch [3/10], Training Loss: 0.667, Validation Accuracy: 60.69%\n",
            "Epoch [4/10], Training Loss: 0.636, Validation Accuracy: 60.79%\n",
            "Epoch [5/10], Training Loss: 0.599, Validation Accuracy: 61.30%\n",
            "Epoch [6/10], Training Loss: 0.559, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.533, Validation Accuracy: 61.19%\n",
            "Epoch [8/10], Training Loss: 0.514, Validation Accuracy: 61.01%\n",
            "Epoch [9/10], Training Loss: 0.492, Validation Accuracy: 60.63%\n",
            "Epoch [10/10], Training Loss: 0.483, Validation Accuracy: 61.16%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 60.83%\n",
            "Epoch [2/10], Training Loss: 0.772, Validation Accuracy: 61.28%\n",
            "Epoch [3/10], Training Loss: 0.703, Validation Accuracy: 60.82%\n",
            "Epoch [4/10], Training Loss: 0.664, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.630, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.597, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.572, Validation Accuracy: 60.91%\n",
            "Epoch [8/10], Training Loss: 0.539, Validation Accuracy: 61.11%\n",
            "Epoch [9/10], Training Loss: 0.523, Validation Accuracy: 61.03%\n",
            "Epoch [10/10], Training Loss: 0.503, Validation Accuracy: 60.58%\n",
            "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 61.21%\n",
            "Epoch [2/10], Training Loss: 0.715, Validation Accuracy: 60.95%\n",
            "Epoch [3/10], Training Loss: 0.655, Validation Accuracy: 61.76%\n",
            "Epoch [4/10], Training Loss: 0.608, Validation Accuracy: 60.87%\n",
            "Epoch [5/10], Training Loss: 0.564, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 61.46%\n",
            "Epoch [7/10], Training Loss: 0.515, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.482, Validation Accuracy: 61.01%\n",
            "Epoch [9/10], Training Loss: 0.462, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.435, Validation Accuracy: 60.38%\n",
            "Test Accuracy: 60.35%\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import truncnorm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "        \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "\n",
        "# Define logic to generate augmented data using Truncated Normal distribution\n",
        "def generate_augmented_data(vae: VAE, distribution_info: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using Truncated Normal distribution\n",
        "    mean = distribution_info[\"mean\"]\n",
        "    std = distribution_info[\"std\"]\n",
        "    a = (0 - mean) / std\n",
        "    b = np.inf\n",
        "    augmented_data = torch.from_numpy(truncnorm.rvs(a, b, loc=mean, scale=std, size=(64, vae.z_dim))).float()\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info)\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Implement the logic to receive the distribution information from the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to receive the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Receive the distribution information from the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to receive the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to receive the information\n",
        "    distribution_info = {\n",
        "        \"mean\": np.zeros(20),\n",
        "        \"std\": np.ones(20)\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH7WKXbAVTIe",
        "outputId": "4eb35dba-be53-4799-fc29-e299dc780037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 63859793.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.75%\n",
            "Epoch [2/10], Training Loss: 2.301, Validation Accuracy: 12.22%\n",
            "Epoch [3/10], Training Loss: 2.299, Validation Accuracy: 14.96%\n",
            "Epoch [4/10], Training Loss: 2.297, Validation Accuracy: 14.55%\n",
            "Epoch [5/10], Training Loss: 2.294, Validation Accuracy: 13.37%\n",
            "Epoch [6/10], Training Loss: 2.290, Validation Accuracy: 14.07%\n",
            "Epoch [7/10], Training Loss: 2.285, Validation Accuracy: 16.00%\n",
            "Epoch [8/10], Training Loss: 2.278, Validation Accuracy: 16.99%\n",
            "Epoch [9/10], Training Loss: 2.269, Validation Accuracy: 16.84%\n",
            "Epoch [10/10], Training Loss: 2.254, Validation Accuracy: 17.89%\n",
            "Epoch [1/10], Training Loss: 2.239, Validation Accuracy: 19.78%\n",
            "Epoch [2/10], Training Loss: 2.207, Validation Accuracy: 21.91%\n",
            "Epoch [3/10], Training Loss: 2.165, Validation Accuracy: 23.47%\n",
            "Epoch [4/10], Training Loss: 2.123, Validation Accuracy: 24.92%\n",
            "Epoch [5/10], Training Loss: 2.088, Validation Accuracy: 26.15%\n",
            "Epoch [6/10], Training Loss: 2.058, Validation Accuracy: 26.80%\n",
            "Epoch [7/10], Training Loss: 2.028, Validation Accuracy: 28.03%\n",
            "Epoch [8/10], Training Loss: 1.998, Validation Accuracy: 28.58%\n",
            "Epoch [9/10], Training Loss: 1.972, Validation Accuracy: 29.07%\n",
            "Epoch [10/10], Training Loss: 1.953, Validation Accuracy: 29.60%\n",
            "Epoch [1/10], Training Loss: 1.938, Validation Accuracy: 29.41%\n",
            "Epoch [2/10], Training Loss: 1.925, Validation Accuracy: 30.06%\n",
            "Epoch [3/10], Training Loss: 1.908, Validation Accuracy: 30.35%\n",
            "Epoch [4/10], Training Loss: 1.897, Validation Accuracy: 30.56%\n",
            "Epoch [5/10], Training Loss: 1.885, Validation Accuracy: 31.46%\n",
            "Epoch [6/10], Training Loss: 1.870, Validation Accuracy: 32.00%\n",
            "Epoch [7/10], Training Loss: 1.857, Validation Accuracy: 32.21%\n",
            "Epoch [8/10], Training Loss: 1.843, Validation Accuracy: 33.06%\n",
            "Epoch [9/10], Training Loss: 1.829, Validation Accuracy: 33.21%\n",
            "Epoch [10/10], Training Loss: 1.809, Validation Accuracy: 33.59%\n",
            "Epoch [1/10], Training Loss: 1.817, Validation Accuracy: 33.50%\n",
            "Epoch [2/10], Training Loss: 1.799, Validation Accuracy: 34.90%\n",
            "Epoch [3/10], Training Loss: 1.779, Validation Accuracy: 34.94%\n",
            "Epoch [4/10], Training Loss: 1.764, Validation Accuracy: 36.12%\n",
            "Epoch [5/10], Training Loss: 1.742, Validation Accuracy: 36.42%\n",
            "Epoch [6/10], Training Loss: 1.722, Validation Accuracy: 36.51%\n",
            "Epoch [7/10], Training Loss: 1.700, Validation Accuracy: 37.89%\n",
            "Epoch [8/10], Training Loss: 1.681, Validation Accuracy: 38.89%\n",
            "Epoch [9/10], Training Loss: 1.664, Validation Accuracy: 39.38%\n",
            "Epoch [10/10], Training Loss: 1.645, Validation Accuracy: 39.29%\n",
            "Epoch [1/10], Training Loss: 1.678, Validation Accuracy: 40.31%\n",
            "Epoch [2/10], Training Loss: 1.656, Validation Accuracy: 40.58%\n",
            "Epoch [3/10], Training Loss: 1.643, Validation Accuracy: 41.17%\n",
            "Epoch [4/10], Training Loss: 1.631, Validation Accuracy: 41.30%\n",
            "Epoch [5/10], Training Loss: 1.617, Validation Accuracy: 41.20%\n",
            "Epoch [6/10], Training Loss: 1.601, Validation Accuracy: 43.14%\n",
            "Epoch [7/10], Training Loss: 1.592, Validation Accuracy: 42.31%\n",
            "Epoch [8/10], Training Loss: 1.574, Validation Accuracy: 42.53%\n",
            "Epoch [9/10], Training Loss: 1.570, Validation Accuracy: 43.27%\n",
            "Epoch [10/10], Training Loss: 1.556, Validation Accuracy: 43.35%\n",
            "Epoch [1/10], Training Loss: 1.550, Validation Accuracy: 44.38%\n",
            "Epoch [2/10], Training Loss: 1.544, Validation Accuracy: 43.99%\n",
            "Epoch [3/10], Training Loss: 1.525, Validation Accuracy: 44.24%\n",
            "Epoch [4/10], Training Loss: 1.510, Validation Accuracy: 45.09%\n",
            "Epoch [5/10], Training Loss: 1.502, Validation Accuracy: 44.40%\n",
            "Epoch [6/10], Training Loss: 1.495, Validation Accuracy: 44.95%\n",
            "Epoch [7/10], Training Loss: 1.484, Validation Accuracy: 44.86%\n",
            "Epoch [8/10], Training Loss: 1.476, Validation Accuracy: 45.87%\n",
            "Epoch [9/10], Training Loss: 1.466, Validation Accuracy: 45.88%\n",
            "Epoch [10/10], Training Loss: 1.462, Validation Accuracy: 45.97%\n",
            "Epoch [1/10], Training Loss: 1.495, Validation Accuracy: 46.04%\n",
            "Epoch [2/10], Training Loss: 1.484, Validation Accuracy: 44.78%\n",
            "Epoch [3/10], Training Loss: 1.467, Validation Accuracy: 46.37%\n",
            "Epoch [4/10], Training Loss: 1.456, Validation Accuracy: 46.48%\n",
            "Epoch [5/10], Training Loss: 1.444, Validation Accuracy: 45.00%\n",
            "Epoch [6/10], Training Loss: 1.438, Validation Accuracy: 46.11%\n",
            "Epoch [7/10], Training Loss: 1.430, Validation Accuracy: 47.73%\n",
            "Epoch [8/10], Training Loss: 1.417, Validation Accuracy: 46.55%\n",
            "Epoch [9/10], Training Loss: 1.412, Validation Accuracy: 47.39%\n",
            "Epoch [10/10], Training Loss: 1.401, Validation Accuracy: 47.55%\n",
            "Epoch [1/10], Training Loss: 1.447, Validation Accuracy: 46.85%\n",
            "Epoch [2/10], Training Loss: 1.444, Validation Accuracy: 47.81%\n",
            "Epoch [3/10], Training Loss: 1.418, Validation Accuracy: 48.53%\n",
            "Epoch [4/10], Training Loss: 1.403, Validation Accuracy: 47.72%\n",
            "Epoch [5/10], Training Loss: 1.389, Validation Accuracy: 48.32%\n",
            "Epoch [6/10], Training Loss: 1.387, Validation Accuracy: 48.97%\n",
            "Epoch [7/10], Training Loss: 1.386, Validation Accuracy: 48.08%\n",
            "Epoch [8/10], Training Loss: 1.378, Validation Accuracy: 49.20%\n",
            "Epoch [9/10], Training Loss: 1.361, Validation Accuracy: 48.95%\n",
            "Epoch [10/10], Training Loss: 1.350, Validation Accuracy: 48.90%\n",
            "Epoch [1/10], Training Loss: 1.399, Validation Accuracy: 48.67%\n",
            "Epoch [2/10], Training Loss: 1.404, Validation Accuracy: 49.39%\n",
            "Epoch [3/10], Training Loss: 1.384, Validation Accuracy: 49.50%\n",
            "Epoch [4/10], Training Loss: 1.366, Validation Accuracy: 50.09%\n",
            "Epoch [5/10], Training Loss: 1.355, Validation Accuracy: 49.58%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 49.85%\n",
            "Epoch [7/10], Training Loss: 1.333, Validation Accuracy: 49.38%\n",
            "Epoch [8/10], Training Loss: 1.332, Validation Accuracy: 49.95%\n",
            "Epoch [9/10], Training Loss: 1.325, Validation Accuracy: 49.52%\n",
            "Epoch [10/10], Training Loss: 1.317, Validation Accuracy: 49.48%\n",
            "Epoch [1/10], Training Loss: 1.402, Validation Accuracy: 50.61%\n",
            "Epoch [2/10], Training Loss: 1.385, Validation Accuracy: 49.86%\n",
            "Epoch [3/10], Training Loss: 1.363, Validation Accuracy: 51.39%\n",
            "Epoch [4/10], Training Loss: 1.350, Validation Accuracy: 50.31%\n",
            "Epoch [5/10], Training Loss: 1.338, Validation Accuracy: 51.06%\n",
            "Epoch [6/10], Training Loss: 1.331, Validation Accuracy: 51.26%\n",
            "Epoch [7/10], Training Loss: 1.320, Validation Accuracy: 51.25%\n",
            "Epoch [8/10], Training Loss: 1.308, Validation Accuracy: 51.34%\n",
            "Epoch [9/10], Training Loss: 1.303, Validation Accuracy: 51.73%\n",
            "Epoch [10/10], Training Loss: 1.295, Validation Accuracy: 51.03%\n",
            "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 51.80%\n",
            "Epoch [2/10], Training Loss: 1.315, Validation Accuracy: 51.69%\n",
            "Epoch [3/10], Training Loss: 1.306, Validation Accuracy: 51.73%\n",
            "Epoch [4/10], Training Loss: 1.291, Validation Accuracy: 52.09%\n",
            "Epoch [5/10], Training Loss: 1.286, Validation Accuracy: 51.65%\n",
            "Epoch [6/10], Training Loss: 1.275, Validation Accuracy: 52.54%\n",
            "Epoch [7/10], Training Loss: 1.261, Validation Accuracy: 52.99%\n",
            "Epoch [8/10], Training Loss: 1.250, Validation Accuracy: 51.97%\n",
            "Epoch [9/10], Training Loss: 1.252, Validation Accuracy: 52.91%\n",
            "Epoch [10/10], Training Loss: 1.230, Validation Accuracy: 52.17%\n",
            "Epoch [1/10], Training Loss: 1.321, Validation Accuracy: 52.32%\n",
            "Epoch [2/10], Training Loss: 1.297, Validation Accuracy: 52.50%\n",
            "Epoch [3/10], Training Loss: 1.279, Validation Accuracy: 51.98%\n",
            "Epoch [4/10], Training Loss: 1.265, Validation Accuracy: 53.10%\n",
            "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 53.31%\n",
            "Epoch [6/10], Training Loss: 1.246, Validation Accuracy: 53.55%\n",
            "Epoch [7/10], Training Loss: 1.223, Validation Accuracy: 52.59%\n",
            "Epoch [8/10], Training Loss: 1.225, Validation Accuracy: 53.26%\n",
            "Epoch [9/10], Training Loss: 1.213, Validation Accuracy: 53.29%\n",
            "Epoch [10/10], Training Loss: 1.210, Validation Accuracy: 53.37%\n",
            "Epoch [1/10], Training Loss: 1.289, Validation Accuracy: 53.89%\n",
            "Epoch [2/10], Training Loss: 1.261, Validation Accuracy: 53.18%\n",
            "Epoch [3/10], Training Loss: 1.244, Validation Accuracy: 53.82%\n",
            "Epoch [4/10], Training Loss: 1.229, Validation Accuracy: 54.18%\n",
            "Epoch [5/10], Training Loss: 1.213, Validation Accuracy: 53.99%\n",
            "Epoch [6/10], Training Loss: 1.206, Validation Accuracy: 54.12%\n",
            "Epoch [7/10], Training Loss: 1.204, Validation Accuracy: 54.25%\n",
            "Epoch [8/10], Training Loss: 1.185, Validation Accuracy: 54.70%\n",
            "Epoch [9/10], Training Loss: 1.173, Validation Accuracy: 54.14%\n",
            "Epoch [10/10], Training Loss: 1.168, Validation Accuracy: 54.66%\n",
            "Epoch [1/10], Training Loss: 1.260, Validation Accuracy: 53.79%\n",
            "Epoch [2/10], Training Loss: 1.237, Validation Accuracy: 54.48%\n",
            "Epoch [3/10], Training Loss: 1.227, Validation Accuracy: 55.40%\n",
            "Epoch [4/10], Training Loss: 1.206, Validation Accuracy: 55.12%\n",
            "Epoch [5/10], Training Loss: 1.190, Validation Accuracy: 54.17%\n",
            "Epoch [6/10], Training Loss: 1.186, Validation Accuracy: 55.28%\n",
            "Epoch [7/10], Training Loss: 1.174, Validation Accuracy: 55.13%\n",
            "Epoch [8/10], Training Loss: 1.166, Validation Accuracy: 54.98%\n",
            "Epoch [9/10], Training Loss: 1.155, Validation Accuracy: 54.58%\n",
            "Epoch [10/10], Training Loss: 1.141, Validation Accuracy: 54.91%\n",
            "Epoch [1/10], Training Loss: 1.257, Validation Accuracy: 55.20%\n",
            "Epoch [2/10], Training Loss: 1.225, Validation Accuracy: 55.63%\n",
            "Epoch [3/10], Training Loss: 1.210, Validation Accuracy: 55.18%\n",
            "Epoch [4/10], Training Loss: 1.188, Validation Accuracy: 54.97%\n",
            "Epoch [5/10], Training Loss: 1.185, Validation Accuracy: 54.84%\n",
            "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 54.73%\n",
            "Epoch [7/10], Training Loss: 1.154, Validation Accuracy: 55.09%\n",
            "Epoch [8/10], Training Loss: 1.148, Validation Accuracy: 53.81%\n",
            "Epoch [9/10], Training Loss: 1.136, Validation Accuracy: 54.99%\n",
            "Epoch [10/10], Training Loss: 1.128, Validation Accuracy: 55.13%\n",
            "Epoch [1/10], Training Loss: 1.223, Validation Accuracy: 54.77%\n",
            "Epoch [2/10], Training Loss: 1.190, Validation Accuracy: 55.85%\n",
            "Epoch [3/10], Training Loss: 1.167, Validation Accuracy: 55.71%\n",
            "Epoch [4/10], Training Loss: 1.147, Validation Accuracy: 55.74%\n",
            "Epoch [5/10], Training Loss: 1.137, Validation Accuracy: 55.47%\n",
            "Epoch [6/10], Training Loss: 1.125, Validation Accuracy: 55.80%\n",
            "Epoch [7/10], Training Loss: 1.111, Validation Accuracy: 56.09%\n",
            "Epoch [8/10], Training Loss: 1.109, Validation Accuracy: 55.88%\n",
            "Epoch [9/10], Training Loss: 1.094, Validation Accuracy: 56.12%\n",
            "Epoch [10/10], Training Loss: 1.077, Validation Accuracy: 55.99%\n",
            "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 55.06%\n",
            "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.26%\n",
            "Epoch [3/10], Training Loss: 1.155, Validation Accuracy: 54.83%\n",
            "Epoch [4/10], Training Loss: 1.135, Validation Accuracy: 56.65%\n",
            "Epoch [5/10], Training Loss: 1.115, Validation Accuracy: 55.80%\n",
            "Epoch [6/10], Training Loss: 1.108, Validation Accuracy: 56.58%\n",
            "Epoch [7/10], Training Loss: 1.096, Validation Accuracy: 57.01%\n",
            "Epoch [8/10], Training Loss: 1.089, Validation Accuracy: 56.65%\n",
            "Epoch [9/10], Training Loss: 1.066, Validation Accuracy: 55.41%\n",
            "Epoch [10/10], Training Loss: 1.062, Validation Accuracy: 56.28%\n",
            "Epoch [1/10], Training Loss: 1.183, Validation Accuracy: 56.32%\n",
            "Epoch [2/10], Training Loss: 1.147, Validation Accuracy: 55.79%\n",
            "Epoch [3/10], Training Loss: 1.135, Validation Accuracy: 57.08%\n",
            "Epoch [4/10], Training Loss: 1.106, Validation Accuracy: 57.83%\n",
            "Epoch [5/10], Training Loss: 1.085, Validation Accuracy: 56.70%\n",
            "Epoch [6/10], Training Loss: 1.088, Validation Accuracy: 56.37%\n",
            "Epoch [7/10], Training Loss: 1.065, Validation Accuracy: 57.17%\n",
            "Epoch [8/10], Training Loss: 1.047, Validation Accuracy: 57.15%\n",
            "Epoch [9/10], Training Loss: 1.041, Validation Accuracy: 57.08%\n",
            "Epoch [10/10], Training Loss: 1.030, Validation Accuracy: 56.99%\n",
            "Epoch [1/10], Training Loss: 1.177, Validation Accuracy: 57.65%\n",
            "Epoch [2/10], Training Loss: 1.133, Validation Accuracy: 56.81%\n",
            "Epoch [3/10], Training Loss: 1.104, Validation Accuracy: 56.95%\n",
            "Epoch [4/10], Training Loss: 1.090, Validation Accuracy: 57.00%\n",
            "Epoch [5/10], Training Loss: 1.073, Validation Accuracy: 57.57%\n",
            "Epoch [6/10], Training Loss: 1.068, Validation Accuracy: 57.33%\n",
            "Epoch [7/10], Training Loss: 1.035, Validation Accuracy: 57.66%\n",
            "Epoch [8/10], Training Loss: 1.024, Validation Accuracy: 56.14%\n",
            "Epoch [9/10], Training Loss: 1.016, Validation Accuracy: 57.50%\n",
            "Epoch [10/10], Training Loss: 1.008, Validation Accuracy: 57.02%\n",
            "Epoch [1/10], Training Loss: 1.157, Validation Accuracy: 57.44%\n",
            "Epoch [2/10], Training Loss: 1.120, Validation Accuracy: 58.49%\n",
            "Epoch [3/10], Training Loss: 1.095, Validation Accuracy: 57.27%\n",
            "Epoch [4/10], Training Loss: 1.078, Validation Accuracy: 58.17%\n",
            "Epoch [5/10], Training Loss: 1.055, Validation Accuracy: 57.95%\n",
            "Epoch [6/10], Training Loss: 1.040, Validation Accuracy: 57.58%\n",
            "Epoch [7/10], Training Loss: 1.028, Validation Accuracy: 58.01%\n",
            "Epoch [8/10], Training Loss: 1.013, Validation Accuracy: 57.34%\n",
            "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 57.41%\n",
            "Epoch [10/10], Training Loss: 0.992, Validation Accuracy: 55.79%\n",
            "Epoch [1/10], Training Loss: 1.136, Validation Accuracy: 57.63%\n",
            "Epoch [2/10], Training Loss: 1.087, Validation Accuracy: 57.93%\n",
            "Epoch [3/10], Training Loss: 1.067, Validation Accuracy: 58.69%\n",
            "Epoch [4/10], Training Loss: 1.039, Validation Accuracy: 57.28%\n",
            "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 57.58%\n",
            "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 57.83%\n",
            "Epoch [7/10], Training Loss: 0.989, Validation Accuracy: 58.23%\n",
            "Epoch [8/10], Training Loss: 0.979, Validation Accuracy: 58.16%\n",
            "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 58.70%\n",
            "Epoch [10/10], Training Loss: 0.954, Validation Accuracy: 58.29%\n",
            "Epoch [1/10], Training Loss: 1.113, Validation Accuracy: 58.54%\n",
            "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 58.43%\n",
            "Epoch [3/10], Training Loss: 1.045, Validation Accuracy: 58.80%\n",
            "Epoch [4/10], Training Loss: 1.015, Validation Accuracy: 58.56%\n",
            "Epoch [5/10], Training Loss: 1.001, Validation Accuracy: 58.64%\n",
            "Epoch [6/10], Training Loss: 0.980, Validation Accuracy: 58.27%\n",
            "Epoch [7/10], Training Loss: 0.970, Validation Accuracy: 57.46%\n",
            "Epoch [8/10], Training Loss: 0.953, Validation Accuracy: 58.32%\n",
            "Epoch [9/10], Training Loss: 0.943, Validation Accuracy: 58.43%\n",
            "Epoch [10/10], Training Loss: 0.927, Validation Accuracy: 58.90%\n",
            "Epoch [1/10], Training Loss: 1.099, Validation Accuracy: 57.97%\n",
            "Epoch [2/10], Training Loss: 1.059, Validation Accuracy: 58.53%\n",
            "Epoch [3/10], Training Loss: 1.021, Validation Accuracy: 58.74%\n",
            "Epoch [4/10], Training Loss: 0.996, Validation Accuracy: 58.18%\n",
            "Epoch [5/10], Training Loss: 0.974, Validation Accuracy: 58.88%\n",
            "Epoch [6/10], Training Loss: 0.959, Validation Accuracy: 59.06%\n",
            "Epoch [7/10], Training Loss: 0.942, Validation Accuracy: 59.20%\n",
            "Epoch [8/10], Training Loss: 0.925, Validation Accuracy: 58.98%\n",
            "Epoch [9/10], Training Loss: 0.915, Validation Accuracy: 59.17%\n",
            "Epoch [10/10], Training Loss: 0.898, Validation Accuracy: 59.16%\n",
            "Epoch [1/10], Training Loss: 1.084, Validation Accuracy: 59.21%\n",
            "Epoch [2/10], Training Loss: 1.036, Validation Accuracy: 59.11%\n",
            "Epoch [3/10], Training Loss: 1.004, Validation Accuracy: 59.65%\n",
            "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 59.48%\n",
            "Epoch [5/10], Training Loss: 0.959, Validation Accuracy: 59.29%\n",
            "Epoch [6/10], Training Loss: 0.945, Validation Accuracy: 59.28%\n",
            "Epoch [7/10], Training Loss: 0.924, Validation Accuracy: 59.05%\n",
            "Epoch [8/10], Training Loss: 0.908, Validation Accuracy: 58.99%\n",
            "Epoch [9/10], Training Loss: 0.906, Validation Accuracy: 59.20%\n",
            "Epoch [10/10], Training Loss: 0.882, Validation Accuracy: 58.93%\n",
            "Epoch [1/10], Training Loss: 1.074, Validation Accuracy: 59.00%\n",
            "Epoch [2/10], Training Loss: 1.032, Validation Accuracy: 59.18%\n",
            "Epoch [3/10], Training Loss: 0.992, Validation Accuracy: 59.39%\n",
            "Epoch [4/10], Training Loss: 0.971, Validation Accuracy: 59.28%\n",
            "Epoch [5/10], Training Loss: 0.943, Validation Accuracy: 59.39%\n",
            "Epoch [6/10], Training Loss: 0.925, Validation Accuracy: 59.53%\n",
            "Epoch [7/10], Training Loss: 0.911, Validation Accuracy: 58.83%\n",
            "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 58.81%\n",
            "Epoch [9/10], Training Loss: 0.882, Validation Accuracy: 58.41%\n",
            "Epoch [10/10], Training Loss: 0.859, Validation Accuracy: 59.05%\n",
            "Epoch [1/10], Training Loss: 1.046, Validation Accuracy: 59.80%\n",
            "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 59.57%\n",
            "Epoch [3/10], Training Loss: 0.961, Validation Accuracy: 59.66%\n",
            "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 60.23%\n",
            "Epoch [5/10], Training Loss: 0.919, Validation Accuracy: 59.11%\n",
            "Epoch [6/10], Training Loss: 0.888, Validation Accuracy: 59.25%\n",
            "Epoch [7/10], Training Loss: 0.876, Validation Accuracy: 59.74%\n",
            "Epoch [8/10], Training Loss: 0.871, Validation Accuracy: 59.29%\n",
            "Epoch [9/10], Training Loss: 0.844, Validation Accuracy: 59.95%\n",
            "Epoch [10/10], Training Loss: 0.831, Validation Accuracy: 59.50%\n",
            "Epoch [1/10], Training Loss: 1.049, Validation Accuracy: 59.77%\n",
            "Epoch [2/10], Training Loss: 0.980, Validation Accuracy: 58.98%\n",
            "Epoch [3/10], Training Loss: 0.954, Validation Accuracy: 59.68%\n",
            "Epoch [4/10], Training Loss: 0.918, Validation Accuracy: 58.98%\n",
            "Epoch [5/10], Training Loss: 0.894, Validation Accuracy: 59.79%\n",
            "Epoch [6/10], Training Loss: 0.870, Validation Accuracy: 59.83%\n",
            "Epoch [7/10], Training Loss: 0.861, Validation Accuracy: 58.60%\n",
            "Epoch [8/10], Training Loss: 0.842, Validation Accuracy: 59.04%\n",
            "Epoch [9/10], Training Loss: 0.826, Validation Accuracy: 59.50%\n",
            "Epoch [10/10], Training Loss: 0.807, Validation Accuracy: 59.10%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 59.71%\n",
            "Epoch [2/10], Training Loss: 0.968, Validation Accuracy: 58.39%\n",
            "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 59.55%\n",
            "Epoch [4/10], Training Loss: 0.901, Validation Accuracy: 59.53%\n",
            "Epoch [5/10], Training Loss: 0.876, Validation Accuracy: 59.20%\n",
            "Epoch [6/10], Training Loss: 0.858, Validation Accuracy: 59.24%\n",
            "Epoch [7/10], Training Loss: 0.842, Validation Accuracy: 58.87%\n",
            "Epoch [8/10], Training Loss: 0.816, Validation Accuracy: 59.82%\n",
            "Epoch [9/10], Training Loss: 0.796, Validation Accuracy: 59.86%\n",
            "Epoch [10/10], Training Loss: 0.785, Validation Accuracy: 59.79%\n",
            "Epoch [1/10], Training Loss: 1.025, Validation Accuracy: 58.61%\n",
            "Epoch [2/10], Training Loss: 0.964, Validation Accuracy: 59.65%\n",
            "Epoch [3/10], Training Loss: 0.915, Validation Accuracy: 59.85%\n",
            "Epoch [4/10], Training Loss: 0.895, Validation Accuracy: 60.28%\n",
            "Epoch [5/10], Training Loss: 0.862, Validation Accuracy: 59.44%\n",
            "Epoch [6/10], Training Loss: 0.839, Validation Accuracy: 60.17%\n",
            "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.805, Validation Accuracy: 59.58%\n",
            "Epoch [9/10], Training Loss: 0.782, Validation Accuracy: 60.13%\n",
            "Epoch [10/10], Training Loss: 0.769, Validation Accuracy: 58.70%\n",
            "Epoch [1/10], Training Loss: 1.015, Validation Accuracy: 60.41%\n",
            "Epoch [2/10], Training Loss: 0.946, Validation Accuracy: 60.13%\n",
            "Epoch [3/10], Training Loss: 0.910, Validation Accuracy: 59.99%\n",
            "Epoch [4/10], Training Loss: 0.881, Validation Accuracy: 60.18%\n",
            "Epoch [5/10], Training Loss: 0.845, Validation Accuracy: 60.26%\n",
            "Epoch [6/10], Training Loss: 0.814, Validation Accuracy: 60.02%\n",
            "Epoch [7/10], Training Loss: 0.803, Validation Accuracy: 58.56%\n",
            "Epoch [8/10], Training Loss: 0.792, Validation Accuracy: 59.95%\n",
            "Epoch [9/10], Training Loss: 0.773, Validation Accuracy: 59.65%\n",
            "Epoch [10/10], Training Loss: 0.751, Validation Accuracy: 59.15%\n",
            "Epoch [1/10], Training Loss: 1.006, Validation Accuracy: 60.29%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 58.62%\n",
            "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 60.32%\n",
            "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 60.46%\n",
            "Epoch [5/10], Training Loss: 0.823, Validation Accuracy: 60.80%\n",
            "Epoch [6/10], Training Loss: 0.796, Validation Accuracy: 60.79%\n",
            "Epoch [7/10], Training Loss: 0.769, Validation Accuracy: 60.37%\n",
            "Epoch [8/10], Training Loss: 0.757, Validation Accuracy: 59.99%\n",
            "Epoch [9/10], Training Loss: 0.738, Validation Accuracy: 60.29%\n",
            "Epoch [10/10], Training Loss: 0.722, Validation Accuracy: 59.66%\n",
            "Epoch [1/10], Training Loss: 0.987, Validation Accuracy: 60.28%\n",
            "Epoch [2/10], Training Loss: 0.902, Validation Accuracy: 59.93%\n",
            "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 60.77%\n",
            "Epoch [4/10], Training Loss: 0.827, Validation Accuracy: 59.92%\n",
            "Epoch [5/10], Training Loss: 0.805, Validation Accuracy: 60.38%\n",
            "Epoch [6/10], Training Loss: 0.776, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.763, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 60.28%\n",
            "Epoch [9/10], Training Loss: 0.722, Validation Accuracy: 59.76%\n",
            "Epoch [10/10], Training Loss: 0.710, Validation Accuracy: 60.40%\n",
            "Epoch [1/10], Training Loss: 0.985, Validation Accuracy: 59.15%\n",
            "Epoch [2/10], Training Loss: 0.891, Validation Accuracy: 59.80%\n",
            "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 60.98%\n",
            "Epoch [4/10], Training Loss: 0.804, Validation Accuracy: 60.57%\n",
            "Epoch [5/10], Training Loss: 0.778, Validation Accuracy: 60.90%\n",
            "Epoch [6/10], Training Loss: 0.762, Validation Accuracy: 60.43%\n",
            "Epoch [7/10], Training Loss: 0.731, Validation Accuracy: 60.28%\n",
            "Epoch [8/10], Training Loss: 0.713, Validation Accuracy: 61.00%\n",
            "Epoch [9/10], Training Loss: 0.693, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.674, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.977, Validation Accuracy: 60.70%\n",
            "Epoch [2/10], Training Loss: 0.893, Validation Accuracy: 60.53%\n",
            "Epoch [3/10], Training Loss: 0.835, Validation Accuracy: 61.15%\n",
            "Epoch [4/10], Training Loss: 0.795, Validation Accuracy: 60.73%\n",
            "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 59.67%\n",
            "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 60.27%\n",
            "Epoch [7/10], Training Loss: 0.727, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.710, Validation Accuracy: 60.49%\n",
            "Epoch [9/10], Training Loss: 0.685, Validation Accuracy: 60.18%\n",
            "Epoch [10/10], Training Loss: 0.660, Validation Accuracy: 60.43%\n",
            "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 60.67%\n",
            "Epoch [2/10], Training Loss: 0.870, Validation Accuracy: 60.57%\n",
            "Epoch [3/10], Training Loss: 0.818, Validation Accuracy: 60.83%\n",
            "Epoch [4/10], Training Loss: 0.787, Validation Accuracy: 60.69%\n",
            "Epoch [5/10], Training Loss: 0.748, Validation Accuracy: 60.85%\n",
            "Epoch [6/10], Training Loss: 0.723, Validation Accuracy: 60.49%\n",
            "Epoch [7/10], Training Loss: 0.708, Validation Accuracy: 59.95%\n",
            "Epoch [8/10], Training Loss: 0.681, Validation Accuracy: 60.11%\n",
            "Epoch [9/10], Training Loss: 0.661, Validation Accuracy: 60.29%\n",
            "Epoch [10/10], Training Loss: 0.638, Validation Accuracy: 60.08%\n",
            "Epoch [1/10], Training Loss: 0.941, Validation Accuracy: 61.20%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 61.34%\n",
            "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 61.03%\n",
            "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 61.21%\n",
            "Epoch [5/10], Training Loss: 0.735, Validation Accuracy: 59.42%\n",
            "Epoch [6/10], Training Loss: 0.702, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.683, Validation Accuracy: 60.66%\n",
            "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 59.84%\n",
            "Epoch [9/10], Training Loss: 0.638, Validation Accuracy: 61.00%\n",
            "Epoch [10/10], Training Loss: 0.618, Validation Accuracy: 61.00%\n",
            "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 60.70%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 61.26%\n",
            "Epoch [3/10], Training Loss: 0.788, Validation Accuracy: 60.18%\n",
            "Epoch [4/10], Training Loss: 0.745, Validation Accuracy: 60.51%\n",
            "Epoch [5/10], Training Loss: 0.710, Validation Accuracy: 60.41%\n",
            "Epoch [6/10], Training Loss: 0.694, Validation Accuracy: 60.85%\n",
            "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 60.59%\n",
            "Epoch [8/10], Training Loss: 0.644, Validation Accuracy: 61.21%\n",
            "Epoch [9/10], Training Loss: 0.627, Validation Accuracy: 60.22%\n",
            "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 60.75%\n",
            "Epoch [2/10], Training Loss: 0.822, Validation Accuracy: 60.35%\n",
            "Epoch [3/10], Training Loss: 0.765, Validation Accuracy: 60.58%\n",
            "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 60.24%\n",
            "Epoch [5/10], Training Loss: 0.686, Validation Accuracy: 61.10%\n",
            "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.623, Validation Accuracy: 59.76%\n",
            "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 59.98%\n",
            "Epoch [9/10], Training Loss: 0.603, Validation Accuracy: 60.49%\n",
            "Epoch [10/10], Training Loss: 0.576, Validation Accuracy: 59.84%\n",
            "Epoch [1/10], Training Loss: 0.929, Validation Accuracy: 60.47%\n",
            "Epoch [2/10], Training Loss: 0.820, Validation Accuracy: 60.51%\n",
            "Epoch [3/10], Training Loss: 0.751, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.717, Validation Accuracy: 59.94%\n",
            "Epoch [5/10], Training Loss: 0.692, Validation Accuracy: 60.14%\n",
            "Epoch [6/10], Training Loss: 0.652, Validation Accuracy: 60.57%\n",
            "Epoch [7/10], Training Loss: 0.628, Validation Accuracy: 60.60%\n",
            "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 60.14%\n",
            "Epoch [9/10], Training Loss: 0.585, Validation Accuracy: 60.45%\n",
            "Epoch [10/10], Training Loss: 0.565, Validation Accuracy: 60.09%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 60.25%\n",
            "Epoch [2/10], Training Loss: 0.800, Validation Accuracy: 61.09%\n",
            "Epoch [3/10], Training Loss: 0.738, Validation Accuracy: 61.00%\n",
            "Epoch [4/10], Training Loss: 0.694, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 61.00%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 60.11%\n",
            "Epoch [7/10], Training Loss: 0.620, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.582, Validation Accuracy: 59.52%\n",
            "Epoch [9/10], Training Loss: 0.563, Validation Accuracy: 59.78%\n",
            "Epoch [10/10], Training Loss: 0.555, Validation Accuracy: 60.49%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 61.07%\n",
            "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 60.74%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 61.31%\n",
            "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 61.57%\n",
            "Epoch [5/10], Training Loss: 0.645, Validation Accuracy: 60.80%\n",
            "Epoch [6/10], Training Loss: 0.610, Validation Accuracy: 60.83%\n",
            "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 60.60%\n",
            "Epoch [9/10], Training Loss: 0.537, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.516, Validation Accuracy: 60.82%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.52%\n",
            "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 61.40%\n",
            "Epoch [3/10], Training Loss: 0.713, Validation Accuracy: 60.38%\n",
            "Epoch [4/10], Training Loss: 0.671, Validation Accuracy: 61.33%\n",
            "Epoch [5/10], Training Loss: 0.636, Validation Accuracy: 61.09%\n",
            "Epoch [6/10], Training Loss: 0.598, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.574, Validation Accuracy: 60.22%\n",
            "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 60.86%\n",
            "Epoch [9/10], Training Loss: 0.532, Validation Accuracy: 60.22%\n",
            "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 60.27%\n",
            "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 60.15%\n",
            "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 60.42%\n",
            "Epoch [3/10], Training Loss: 0.682, Validation Accuracy: 60.36%\n",
            "Epoch [4/10], Training Loss: 0.650, Validation Accuracy: 60.48%\n",
            "Epoch [5/10], Training Loss: 0.603, Validation Accuracy: 60.64%\n",
            "Epoch [6/10], Training Loss: 0.573, Validation Accuracy: 60.68%\n",
            "Epoch [7/10], Training Loss: 0.551, Validation Accuracy: 59.55%\n",
            "Epoch [8/10], Training Loss: 0.520, Validation Accuracy: 59.78%\n",
            "Epoch [9/10], Training Loss: 0.495, Validation Accuracy: 60.54%\n",
            "Epoch [10/10], Training Loss: 0.484, Validation Accuracy: 60.68%\n",
            "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 59.23%\n",
            "Epoch [2/10], Training Loss: 0.747, Validation Accuracy: 59.69%\n",
            "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.643, Validation Accuracy: 60.77%\n",
            "Epoch [5/10], Training Loss: 0.608, Validation Accuracy: 61.17%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 60.36%\n",
            "Epoch [7/10], Training Loss: 0.541, Validation Accuracy: 60.86%\n",
            "Epoch [8/10], Training Loss: 0.519, Validation Accuracy: 60.60%\n",
            "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.475, Validation Accuracy: 60.22%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 60.80%\n",
            "Epoch [2/10], Training Loss: 0.734, Validation Accuracy: 59.52%\n",
            "Epoch [3/10], Training Loss: 0.667, Validation Accuracy: 60.82%\n",
            "Epoch [4/10], Training Loss: 0.618, Validation Accuracy: 60.80%\n",
            "Epoch [5/10], Training Loss: 0.579, Validation Accuracy: 60.60%\n",
            "Epoch [6/10], Training Loss: 0.548, Validation Accuracy: 60.32%\n",
            "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 60.78%\n",
            "Epoch [8/10], Training Loss: 0.494, Validation Accuracy: 59.67%\n",
            "Epoch [9/10], Training Loss: 0.478, Validation Accuracy: 60.24%\n",
            "Epoch [10/10], Training Loss: 0.458, Validation Accuracy: 60.07%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.711, Validation Accuracy: 61.52%\n",
            "Epoch [3/10], Training Loss: 0.632, Validation Accuracy: 61.45%\n",
            "Epoch [4/10], Training Loss: 0.609, Validation Accuracy: 61.00%\n",
            "Epoch [5/10], Training Loss: 0.565, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.523, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.498, Validation Accuracy: 61.22%\n",
            "Epoch [8/10], Training Loss: 0.474, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 60.97%\n",
            "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 60.79%\n",
            "Epoch [1/10], Training Loss: 0.885, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.732, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.592, Validation Accuracy: 61.02%\n",
            "Epoch [5/10], Training Loss: 0.546, Validation Accuracy: 61.12%\n",
            "Epoch [6/10], Training Loss: 0.518, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.489, Validation Accuracy: 60.68%\n",
            "Epoch [8/10], Training Loss: 0.468, Validation Accuracy: 60.49%\n",
            "Epoch [9/10], Training Loss: 0.444, Validation Accuracy: 60.34%\n",
            "Epoch [10/10], Training Loss: 0.417, Validation Accuracy: 60.08%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 59.92%\n",
            "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.612, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.554, Validation Accuracy: 59.98%\n",
            "Epoch [5/10], Training Loss: 0.518, Validation Accuracy: 60.13%\n",
            "Epoch [6/10], Training Loss: 0.487, Validation Accuracy: 60.58%\n",
            "Epoch [7/10], Training Loss: 0.468, Validation Accuracy: 59.58%\n",
            "Epoch [8/10], Training Loss: 0.442, Validation Accuracy: 60.56%\n",
            "Epoch [9/10], Training Loss: 0.411, Validation Accuracy: 60.52%\n",
            "Epoch [10/10], Training Loss: 0.393, Validation Accuracy: 60.38%\n",
            "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.28%\n",
            "Epoch [2/10], Training Loss: 0.678, Validation Accuracy: 60.24%\n",
            "Epoch [3/10], Training Loss: 0.604, Validation Accuracy: 61.14%\n",
            "Epoch [4/10], Training Loss: 0.558, Validation Accuracy: 60.30%\n",
            "Epoch [5/10], Training Loss: 0.528, Validation Accuracy: 59.88%\n",
            "Epoch [6/10], Training Loss: 0.489, Validation Accuracy: 60.34%\n",
            "Epoch [7/10], Training Loss: 0.458, Validation Accuracy: 59.62%\n",
            "Epoch [8/10], Training Loss: 0.436, Validation Accuracy: 60.23%\n",
            "Epoch [9/10], Training Loss: 0.409, Validation Accuracy: 60.09%\n",
            "Epoch [10/10], Training Loss: 0.405, Validation Accuracy: 59.79%\n",
            "Epoch [1/10], Training Loss: 0.834, Validation Accuracy: 60.54%\n",
            "Epoch [2/10], Training Loss: 0.671, Validation Accuracy: 60.20%\n",
            "Epoch [3/10], Training Loss: 0.598, Validation Accuracy: 60.30%\n",
            "Epoch [4/10], Training Loss: 0.535, Validation Accuracy: 59.64%\n",
            "Epoch [5/10], Training Loss: 0.499, Validation Accuracy: 60.54%\n",
            "Epoch [6/10], Training Loss: 0.468, Validation Accuracy: 59.64%\n",
            "Epoch [7/10], Training Loss: 0.432, Validation Accuracy: 59.60%\n",
            "Epoch [8/10], Training Loss: 0.410, Validation Accuracy: 59.80%\n",
            "Epoch [9/10], Training Loss: 0.385, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.365, Validation Accuracy: 59.90%\n",
            "Confusion Matrix:\n",
            "[[682  23  50  29  25  12  25  29  90  35]\n",
            " [ 46 724  11  12   9   8  16  19  58  97]\n",
            " [ 88  15 427  82 119  76  80  71  33   9]\n",
            " [ 32  16  90 382  93 165 105  70  28  19]\n",
            " [ 37   7  83  67 527  43  98 112  21   5]\n",
            " [ 19   7  65 197  87 446  48 102  20   9]\n",
            " [ 15   9  50  51  70  39 727  17  14   8]\n",
            " [ 22  11  23  53  98  57  13 706   5  12]\n",
            " [112  51  13  18  18  10  15   6 724  33]\n",
            " [ 68 146   9  24  18  20  31  39  69 576]]\n",
            "Test Accuracy: 59.21%\n",
            "True Positives (TP): [682 724 427 382 527 446 727 706 724 576]\n",
            "False Positives (FP): [439 285 394 533 537 430 431 465 338 227]\n",
            "True Negatives (TN): [8561 8715 8606 8467 8463 8570 8569 8535 8662 8773]\n",
            "False Negatives (FN): [318 276 573 618 473 554 273 294 276 424]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.60838537 0.71754212 0.52009744 0.41748634 0.49530075 0.50913242\n",
            " 0.62780656 0.6029035  0.68173258 0.71731009]\n",
            "Recall: [0.682 0.724 0.427 0.382 0.527 0.446 0.727 0.706 0.724 0.576]\n",
            "F1 Score: [0.64309288 0.7207566  0.46897309 0.39895561 0.51065891 0.47547974\n",
            " 0.67377201 0.65039152 0.70223084 0.63893511]\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import truncnorm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to get the distribution information from the VAE model\n",
        "    # This can involve extracting the mean and standard deviation of the latent space\n",
        "    # and sending this information to the global server for use in generating augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "        \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define logic to generate augmented data using Truncated Normal distribution\n",
        "def generate_augmented_data(vae: VAE, distribution_info: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using Truncated Normal distribution\n",
        "    mean = distribution_info[\"mean\"]\n",
        "    std = distribution_info[\"std\"]\n",
        "    a = (0 - mean) / std\n",
        "    b = np.inf\n",
        "    augmented_data = torch.from_numpy(truncnorm.rvs(a, b, loc=mean, scale=std, size=(64, vae.z_dim))).float()\n",
        "    return augmented_data\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info)\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Implement the logic to receive the distribution information from the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to receive the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Receive the distribution information from the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to receive the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to receive the information\n",
        "    distribution_info = {\n",
        "        \"mean\": np.zeros(20),\n",
        "        \"std\": np.ones(20)\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBhUNL_TxpFP",
        "outputId": "ec4a368d-c1bd-4446-f98a-c5f93167d85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracies: [10.75, 12.22, 14.96, 14.55, 13.37, 14.07, 16.0, 16.99, 16.84, 17.89, 19.78, 21.91, 23.47, 24.92, 26.15, 26.8, 28.03, 28.58, 29.07, 29.6, 29.41, 30.06, 30.35, 30.56, 31.46, 32.0, 32.21, 33.06, 33.21, 33.59, 33.5, 34.9, 34.94, 36.12, 36.42, 36.51, 37.89, 38.89, 39.38, 39.29, 40.31, 40.58, 41.17, 41.3, 41.2, 43.14, 42.31, 42.53, 43.27, 43.35, 44.38, 43.99, 44.24, 45.09, 44.4, 44.95, 44.86, 45.87, 45.88, 45.97, 46.04, 44.78, 46.37, 46.48, 45.0, 46.11, 47.73, 46.55, 47.39, 47.55, 46.85, 47.81, 48.53, 47.72, 48.32, 48.97, 48.08, 49.2, 48.95, 48.9, 48.67, 49.39, 49.5, 50.09, 49.58, 49.85, 49.38, 49.95, 49.52, 49.48, 50.61, 49.86, 51.39, 50.31, 51.06, 51.26, 51.25, 51.34, 51.73, 51.03, 51.8, 51.69, 51.73, 52.09, 51.65, 52.54, 52.99, 51.97, 52.91, 52.17, 52.32, 52.5, 51.98, 53.1, 53.31, 53.55, 52.59, 53.26, 53.29, 53.37, 53.89, 53.18, 53.82, 54.18, 53.99, 54.12, 54.25, 54.7, 54.14, 54.66, 53.79, 54.48, 55.4, 55.12, 54.17, 55.28, 55.13, 54.98, 54.58, 54.91, 55.2, 55.63, 55.18, 54.97, 54.84, 54.73, 55.09, 53.81, 54.99, 55.13, 54.77, 55.85, 55.71, 55.74, 55.47, 55.8, 56.09, 55.88, 56.12, 55.99, 55.06, 56.26, 54.83, 56.65, 55.8, 56.58, 57.01, 56.65, 55.41, 56.28, 56.32, 55.79, 57.08, 57.83, 56.7, 56.37, 57.17, 57.15, 57.08, 56.99, 57.65, 56.81, 56.95, 57.0, 57.57, 57.33, 57.66, 56.14, 57.5, 57.02, 57.44, 58.49, 57.27, 58.17, 57.95, 57.58, 58.01, 57.34, 57.41, 55.79, 57.63, 57.93, 58.69, 57.28, 57.58, 57.83, 58.23, 58.16, 58.7, 58.29, 58.54, 58.43, 58.8, 58.56, 58.64, 58.27, 57.46, 58.32, 58.43, 58.9, 57.97, 58.53, 58.74, 58.18, 58.88, 59.06, 59.2, 58.98, 59.17, 59.16, 59.21, 59.11, 59.65, 59.48, 59.29, 59.28, 59.05, 58.99, 59.2, 58.93, 59.0, 59.18, 59.39, 59.28, 59.39, 59.53, 58.83, 58.81, 58.41, 59.05, 59.8, 59.57, 59.66, 60.23, 59.11, 59.25, 59.74, 59.29, 59.95, 59.5, 59.77, 58.98, 59.68, 58.98, 59.79, 59.83, 58.6, 59.04, 59.5, 59.1, 59.71, 58.39, 59.55, 59.53, 59.2, 59.24, 58.87, 59.82, 59.86, 59.79, 58.61, 59.65, 59.85, 60.28, 59.44, 60.17, 60.55, 59.58, 60.13, 58.7, 60.41, 60.13, 59.99, 60.18, 60.26, 60.02, 58.56, 59.95, 59.65, 59.15, 60.29, 58.62, 60.32, 60.46, 60.8, 60.79, 60.37, 59.99, 60.29, 59.66, 60.28, 59.93, 60.77, 59.92, 60.38, 60.88, 60.41, 60.28, 59.76, 60.4, 59.15, 59.8, 60.98, 60.57, 60.9, 60.43, 60.28, 61.0, 60.68, 60.03, 60.7, 60.53, 61.15, 60.73, 59.67, 60.27, 60.41, 60.49, 60.18, 60.43, 60.67, 60.57, 60.83, 60.69, 60.85, 60.49, 59.95, 60.11, 60.29, 60.08, 61.2, 61.34, 61.03, 61.21, 59.42, 60.59, 60.66, 59.84, 61.0, 61.0, 60.7, 61.26, 60.18, 60.51, 60.41, 60.85, 60.59, 61.21, 60.22, 60.47, 60.75, 60.35, 60.58, 60.24, 61.1, 61.18, 59.76, 59.98, 60.49, 59.84, 60.47, 60.51, 61.09, 59.94, 60.14, 60.57, 60.6, 60.14, 60.45, 60.09, 60.25, 61.09, 61.0, 60.14, 61.0, 60.11, 60.27, 59.52, 59.78, 60.49, 61.07, 60.74, 61.31, 61.57, 60.8, 60.83, 60.41, 60.6, 60.7, 60.82, 60.52, 61.4, 60.38, 61.33, 61.09, 60.96, 60.22, 60.86, 60.22, 60.27, 60.15, 60.42, 60.36, 60.48, 60.64, 60.68, 59.55, 59.78, 60.54, 60.68, 59.23, 59.69, 60.6, 60.77, 61.17, 60.36, 60.86, 60.6, 60.67, 60.22, 60.8, 59.52, 60.82, 60.8, 60.6, 60.32, 60.78, 59.67, 60.24, 60.07, 60.55, 61.52, 61.45, 61.0, 60.96, 61.15, 61.22, 60.79, 60.97, 60.79, 60.62, 60.76, 61.17, 61.02, 61.12, 60.72, 60.68, 60.49, 60.34, 60.08, 59.92, 60.47, 60.6, 59.98, 60.13, 60.58, 59.58, 60.56, 60.52, 60.38, 60.28, 60.24, 61.14, 60.3, 59.88, 60.34, 59.62, 60.23, 60.09, 59.79, 60.54, 60.2, 60.3, 59.64, 60.54, 59.64, 59.6, 59.8, 60.16, 59.9]\n",
            "Size of array: 500\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.75%\n",
        "Epoch [2/10], Training Loss: 2.301, Validation Accuracy: 12.22%\n",
        "Epoch [3/10], Training Loss: 2.299, Validation Accuracy: 14.96%\n",
        "Epoch [4/10], Training Loss: 2.297, Validation Accuracy: 14.55%\n",
        "Epoch [5/10], Training Loss: 2.294, Validation Accuracy: 13.37%\n",
        "Epoch [6/10], Training Loss: 2.290, Validation Accuracy: 14.07%\n",
        "Epoch [7/10], Training Loss: 2.285, Validation Accuracy: 16.00%\n",
        "Epoch [8/10], Training Loss: 2.278, Validation Accuracy: 16.99%\n",
        "Epoch [9/10], Training Loss: 2.269, Validation Accuracy: 16.84%\n",
        "Epoch [10/10], Training Loss: 2.254, Validation Accuracy: 17.89%\n",
        "Epoch [1/10], Training Loss: 2.239, Validation Accuracy: 19.78%\n",
        "Epoch [2/10], Training Loss: 2.207, Validation Accuracy: 21.91%\n",
        "Epoch [3/10], Training Loss: 2.165, Validation Accuracy: 23.47%\n",
        "Epoch [4/10], Training Loss: 2.123, Validation Accuracy: 24.92%\n",
        "Epoch [5/10], Training Loss: 2.088, Validation Accuracy: 26.15%\n",
        "Epoch [6/10], Training Loss: 2.058, Validation Accuracy: 26.80%\n",
        "Epoch [7/10], Training Loss: 2.028, Validation Accuracy: 28.03%\n",
        "Epoch [8/10], Training Loss: 1.998, Validation Accuracy: 28.58%\n",
        "Epoch [9/10], Training Loss: 1.972, Validation Accuracy: 29.07%\n",
        "Epoch [10/10], Training Loss: 1.953, Validation Accuracy: 29.60%\n",
        "Epoch [1/10], Training Loss: 1.938, Validation Accuracy: 29.41%\n",
        "Epoch [2/10], Training Loss: 1.925, Validation Accuracy: 30.06%\n",
        "Epoch [3/10], Training Loss: 1.908, Validation Accuracy: 30.35%\n",
        "Epoch [4/10], Training Loss: 1.897, Validation Accuracy: 30.56%\n",
        "Epoch [5/10], Training Loss: 1.885, Validation Accuracy: 31.46%\n",
        "Epoch [6/10], Training Loss: 1.870, Validation Accuracy: 32.00%\n",
        "Epoch [7/10], Training Loss: 1.857, Validation Accuracy: 32.21%\n",
        "Epoch [8/10], Training Loss: 1.843, Validation Accuracy: 33.06%\n",
        "Epoch [9/10], Training Loss: 1.829, Validation Accuracy: 33.21%\n",
        "Epoch [10/10], Training Loss: 1.809, Validation Accuracy: 33.59%\n",
        "Epoch [1/10], Training Loss: 1.817, Validation Accuracy: 33.50%\n",
        "Epoch [2/10], Training Loss: 1.799, Validation Accuracy: 34.90%\n",
        "Epoch [3/10], Training Loss: 1.779, Validation Accuracy: 34.94%\n",
        "Epoch [4/10], Training Loss: 1.764, Validation Accuracy: 36.12%\n",
        "Epoch [5/10], Training Loss: 1.742, Validation Accuracy: 36.42%\n",
        "Epoch [6/10], Training Loss: 1.722, Validation Accuracy: 36.51%\n",
        "Epoch [7/10], Training Loss: 1.700, Validation Accuracy: 37.89%\n",
        "Epoch [8/10], Training Loss: 1.681, Validation Accuracy: 38.89%\n",
        "Epoch [9/10], Training Loss: 1.664, Validation Accuracy: 39.38%\n",
        "Epoch [10/10], Training Loss: 1.645, Validation Accuracy: 39.29%\n",
        "Epoch [1/10], Training Loss: 1.678, Validation Accuracy: 40.31%\n",
        "Epoch [2/10], Training Loss: 1.656, Validation Accuracy: 40.58%\n",
        "Epoch [3/10], Training Loss: 1.643, Validation Accuracy: 41.17%\n",
        "Epoch [4/10], Training Loss: 1.631, Validation Accuracy: 41.30%\n",
        "Epoch [5/10], Training Loss: 1.617, Validation Accuracy: 41.20%\n",
        "Epoch [6/10], Training Loss: 1.601, Validation Accuracy: 43.14%\n",
        "Epoch [7/10], Training Loss: 1.592, Validation Accuracy: 42.31%\n",
        "Epoch [8/10], Training Loss: 1.574, Validation Accuracy: 42.53%\n",
        "Epoch [9/10], Training Loss: 1.570, Validation Accuracy: 43.27%\n",
        "Epoch [10/10], Training Loss: 1.556, Validation Accuracy: 43.35%\n",
        "Epoch [1/10], Training Loss: 1.550, Validation Accuracy: 44.38%\n",
        "Epoch [2/10], Training Loss: 1.544, Validation Accuracy: 43.99%\n",
        "Epoch [3/10], Training Loss: 1.525, Validation Accuracy: 44.24%\n",
        "Epoch [4/10], Training Loss: 1.510, Validation Accuracy: 45.09%\n",
        "Epoch [5/10], Training Loss: 1.502, Validation Accuracy: 44.40%\n",
        "Epoch [6/10], Training Loss: 1.495, Validation Accuracy: 44.95%\n",
        "Epoch [7/10], Training Loss: 1.484, Validation Accuracy: 44.86%\n",
        "Epoch [8/10], Training Loss: 1.476, Validation Accuracy: 45.87%\n",
        "Epoch [9/10], Training Loss: 1.466, Validation Accuracy: 45.88%\n",
        "Epoch [10/10], Training Loss: 1.462, Validation Accuracy: 45.97%\n",
        "Epoch [1/10], Training Loss: 1.495, Validation Accuracy: 46.04%\n",
        "Epoch [2/10], Training Loss: 1.484, Validation Accuracy: 44.78%\n",
        "Epoch [3/10], Training Loss: 1.467, Validation Accuracy: 46.37%\n",
        "Epoch [4/10], Training Loss: 1.456, Validation Accuracy: 46.48%\n",
        "Epoch [5/10], Training Loss: 1.444, Validation Accuracy: 45.00%\n",
        "Epoch [6/10], Training Loss: 1.438, Validation Accuracy: 46.11%\n",
        "Epoch [7/10], Training Loss: 1.430, Validation Accuracy: 47.73%\n",
        "Epoch [8/10], Training Loss: 1.417, Validation Accuracy: 46.55%\n",
        "Epoch [9/10], Training Loss: 1.412, Validation Accuracy: 47.39%\n",
        "Epoch [10/10], Training Loss: 1.401, Validation Accuracy: 47.55%\n",
        "Epoch [1/10], Training Loss: 1.447, Validation Accuracy: 46.85%\n",
        "Epoch [2/10], Training Loss: 1.444, Validation Accuracy: 47.81%\n",
        "Epoch [3/10], Training Loss: 1.418, Validation Accuracy: 48.53%\n",
        "Epoch [4/10], Training Loss: 1.403, Validation Accuracy: 47.72%\n",
        "Epoch [5/10], Training Loss: 1.389, Validation Accuracy: 48.32%\n",
        "Epoch [6/10], Training Loss: 1.387, Validation Accuracy: 48.97%\n",
        "Epoch [7/10], Training Loss: 1.386, Validation Accuracy: 48.08%\n",
        "Epoch [8/10], Training Loss: 1.378, Validation Accuracy: 49.20%\n",
        "Epoch [9/10], Training Loss: 1.361, Validation Accuracy: 48.95%\n",
        "Epoch [10/10], Training Loss: 1.350, Validation Accuracy: 48.90%\n",
        "Epoch [1/10], Training Loss: 1.399, Validation Accuracy: 48.67%\n",
        "Epoch [2/10], Training Loss: 1.404, Validation Accuracy: 49.39%\n",
        "Epoch [3/10], Training Loss: 1.384, Validation Accuracy: 49.50%\n",
        "Epoch [4/10], Training Loss: 1.366, Validation Accuracy: 50.09%\n",
        "Epoch [5/10], Training Loss: 1.355, Validation Accuracy: 49.58%\n",
        "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 49.85%\n",
        "Epoch [7/10], Training Loss: 1.333, Validation Accuracy: 49.38%\n",
        "Epoch [8/10], Training Loss: 1.332, Validation Accuracy: 49.95%\n",
        "Epoch [9/10], Training Loss: 1.325, Validation Accuracy: 49.52%\n",
        "Epoch [10/10], Training Loss: 1.317, Validation Accuracy: 49.48%\n",
        "Epoch [1/10], Training Loss: 1.402, Validation Accuracy: 50.61%\n",
        "Epoch [2/10], Training Loss: 1.385, Validation Accuracy: 49.86%\n",
        "Epoch [3/10], Training Loss: 1.363, Validation Accuracy: 51.39%\n",
        "Epoch [4/10], Training Loss: 1.350, Validation Accuracy: 50.31%\n",
        "Epoch [5/10], Training Loss: 1.338, Validation Accuracy: 51.06%\n",
        "Epoch [6/10], Training Loss: 1.331, Validation Accuracy: 51.26%\n",
        "Epoch [7/10], Training Loss: 1.320, Validation Accuracy: 51.25%\n",
        "Epoch [8/10], Training Loss: 1.308, Validation Accuracy: 51.34%\n",
        "Epoch [9/10], Training Loss: 1.303, Validation Accuracy: 51.73%\n",
        "Epoch [10/10], Training Loss: 1.295, Validation Accuracy: 51.03%\n",
        "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 51.80%\n",
        "Epoch [2/10], Training Loss: 1.315, Validation Accuracy: 51.69%\n",
        "Epoch [3/10], Training Loss: 1.306, Validation Accuracy: 51.73%\n",
        "Epoch [4/10], Training Loss: 1.291, Validation Accuracy: 52.09%\n",
        "Epoch [5/10], Training Loss: 1.286, Validation Accuracy: 51.65%\n",
        "Epoch [6/10], Training Loss: 1.275, Validation Accuracy: 52.54%\n",
        "Epoch [7/10], Training Loss: 1.261, Validation Accuracy: 52.99%\n",
        "Epoch [8/10], Training Loss: 1.250, Validation Accuracy: 51.97%\n",
        "Epoch [9/10], Training Loss: 1.252, Validation Accuracy: 52.91%\n",
        "Epoch [10/10], Training Loss: 1.230, Validation Accuracy: 52.17%\n",
        "Epoch [1/10], Training Loss: 1.321, Validation Accuracy: 52.32%\n",
        "Epoch [2/10], Training Loss: 1.297, Validation Accuracy: 52.50%\n",
        "Epoch [3/10], Training Loss: 1.279, Validation Accuracy: 51.98%\n",
        "Epoch [4/10], Training Loss: 1.265, Validation Accuracy: 53.10%\n",
        "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 53.31%\n",
        "Epoch [6/10], Training Loss: 1.246, Validation Accuracy: 53.55%\n",
        "Epoch [7/10], Training Loss: 1.223, Validation Accuracy: 52.59%\n",
        "Epoch [8/10], Training Loss: 1.225, Validation Accuracy: 53.26%\n",
        "Epoch [9/10], Training Loss: 1.213, Validation Accuracy: 53.29%\n",
        "Epoch [10/10], Training Loss: 1.210, Validation Accuracy: 53.37%\n",
        "Epoch [1/10], Training Loss: 1.289, Validation Accuracy: 53.89%\n",
        "Epoch [2/10], Training Loss: 1.261, Validation Accuracy: 53.18%\n",
        "Epoch [3/10], Training Loss: 1.244, Validation Accuracy: 53.82%\n",
        "Epoch [4/10], Training Loss: 1.229, Validation Accuracy: 54.18%\n",
        "Epoch [5/10], Training Loss: 1.213, Validation Accuracy: 53.99%\n",
        "Epoch [6/10], Training Loss: 1.206, Validation Accuracy: 54.12%\n",
        "Epoch [7/10], Training Loss: 1.204, Validation Accuracy: 54.25%\n",
        "Epoch [8/10], Training Loss: 1.185, Validation Accuracy: 54.70%\n",
        "Epoch [9/10], Training Loss: 1.173, Validation Accuracy: 54.14%\n",
        "Epoch [10/10], Training Loss: 1.168, Validation Accuracy: 54.66%\n",
        "Epoch [1/10], Training Loss: 1.260, Validation Accuracy: 53.79%\n",
        "Epoch [2/10], Training Loss: 1.237, Validation Accuracy: 54.48%\n",
        "Epoch [3/10], Training Loss: 1.227, Validation Accuracy: 55.40%\n",
        "Epoch [4/10], Training Loss: 1.206, Validation Accuracy: 55.12%\n",
        "Epoch [5/10], Training Loss: 1.190, Validation Accuracy: 54.17%\n",
        "Epoch [6/10], Training Loss: 1.186, Validation Accuracy: 55.28%\n",
        "Epoch [7/10], Training Loss: 1.174, Validation Accuracy: 55.13%\n",
        "Epoch [8/10], Training Loss: 1.166, Validation Accuracy: 54.98%\n",
        "Epoch [9/10], Training Loss: 1.155, Validation Accuracy: 54.58%\n",
        "Epoch [10/10], Training Loss: 1.141, Validation Accuracy: 54.91%\n",
        "Epoch [1/10], Training Loss: 1.257, Validation Accuracy: 55.20%\n",
        "Epoch [2/10], Training Loss: 1.225, Validation Accuracy: 55.63%\n",
        "Epoch [3/10], Training Loss: 1.210, Validation Accuracy: 55.18%\n",
        "Epoch [4/10], Training Loss: 1.188, Validation Accuracy: 54.97%\n",
        "Epoch [5/10], Training Loss: 1.185, Validation Accuracy: 54.84%\n",
        "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 54.73%\n",
        "Epoch [7/10], Training Loss: 1.154, Validation Accuracy: 55.09%\n",
        "Epoch [8/10], Training Loss: 1.148, Validation Accuracy: 53.81%\n",
        "Epoch [9/10], Training Loss: 1.136, Validation Accuracy: 54.99%\n",
        "Epoch [10/10], Training Loss: 1.128, Validation Accuracy: 55.13%\n",
        "Epoch [1/10], Training Loss: 1.223, Validation Accuracy: 54.77%\n",
        "Epoch [2/10], Training Loss: 1.190, Validation Accuracy: 55.85%\n",
        "Epoch [3/10], Training Loss: 1.167, Validation Accuracy: 55.71%\n",
        "Epoch [4/10], Training Loss: 1.147, Validation Accuracy: 55.74%\n",
        "Epoch [5/10], Training Loss: 1.137, Validation Accuracy: 55.47%\n",
        "Epoch [6/10], Training Loss: 1.125, Validation Accuracy: 55.80%\n",
        "Epoch [7/10], Training Loss: 1.111, Validation Accuracy: 56.09%\n",
        "Epoch [8/10], Training Loss: 1.109, Validation Accuracy: 55.88%\n",
        "Epoch [9/10], Training Loss: 1.094, Validation Accuracy: 56.12%\n",
        "Epoch [10/10], Training Loss: 1.077, Validation Accuracy: 55.99%\n",
        "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 55.06%\n",
        "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.26%\n",
        "Epoch [3/10], Training Loss: 1.155, Validation Accuracy: 54.83%\n",
        "Epoch [4/10], Training Loss: 1.135, Validation Accuracy: 56.65%\n",
        "Epoch [5/10], Training Loss: 1.115, Validation Accuracy: 55.80%\n",
        "Epoch [6/10], Training Loss: 1.108, Validation Accuracy: 56.58%\n",
        "Epoch [7/10], Training Loss: 1.096, Validation Accuracy: 57.01%\n",
        "Epoch [8/10], Training Loss: 1.089, Validation Accuracy: 56.65%\n",
        "Epoch [9/10], Training Loss: 1.066, Validation Accuracy: 55.41%\n",
        "Epoch [10/10], Training Loss: 1.062, Validation Accuracy: 56.28%\n",
        "Epoch [1/10], Training Loss: 1.183, Validation Accuracy: 56.32%\n",
        "Epoch [2/10], Training Loss: 1.147, Validation Accuracy: 55.79%\n",
        "Epoch [3/10], Training Loss: 1.135, Validation Accuracy: 57.08%\n",
        "Epoch [4/10], Training Loss: 1.106, Validation Accuracy: 57.83%\n",
        "Epoch [5/10], Training Loss: 1.085, Validation Accuracy: 56.70%\n",
        "Epoch [6/10], Training Loss: 1.088, Validation Accuracy: 56.37%\n",
        "Epoch [7/10], Training Loss: 1.065, Validation Accuracy: 57.17%\n",
        "Epoch [8/10], Training Loss: 1.047, Validation Accuracy: 57.15%\n",
        "Epoch [9/10], Training Loss: 1.041, Validation Accuracy: 57.08%\n",
        "Epoch [10/10], Training Loss: 1.030, Validation Accuracy: 56.99%\n",
        "Epoch [1/10], Training Loss: 1.177, Validation Accuracy: 57.65%\n",
        "Epoch [2/10], Training Loss: 1.133, Validation Accuracy: 56.81%\n",
        "Epoch [3/10], Training Loss: 1.104, Validation Accuracy: 56.95%\n",
        "Epoch [4/10], Training Loss: 1.090, Validation Accuracy: 57.00%\n",
        "Epoch [5/10], Training Loss: 1.073, Validation Accuracy: 57.57%\n",
        "Epoch [6/10], Training Loss: 1.068, Validation Accuracy: 57.33%\n",
        "Epoch [7/10], Training Loss: 1.035, Validation Accuracy: 57.66%\n",
        "Epoch [8/10], Training Loss: 1.024, Validation Accuracy: 56.14%\n",
        "Epoch [9/10], Training Loss: 1.016, Validation Accuracy: 57.50%\n",
        "Epoch [10/10], Training Loss: 1.008, Validation Accuracy: 57.02%\n",
        "Epoch [1/10], Training Loss: 1.157, Validation Accuracy: 57.44%\n",
        "Epoch [2/10], Training Loss: 1.120, Validation Accuracy: 58.49%\n",
        "Epoch [3/10], Training Loss: 1.095, Validation Accuracy: 57.27%\n",
        "Epoch [4/10], Training Loss: 1.078, Validation Accuracy: 58.17%\n",
        "Epoch [5/10], Training Loss: 1.055, Validation Accuracy: 57.95%\n",
        "Epoch [6/10], Training Loss: 1.040, Validation Accuracy: 57.58%\n",
        "Epoch [7/10], Training Loss: 1.028, Validation Accuracy: 58.01%\n",
        "Epoch [8/10], Training Loss: 1.013, Validation Accuracy: 57.34%\n",
        "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 57.41%\n",
        "Epoch [10/10], Training Loss: 0.992, Validation Accuracy: 55.79%\n",
        "Epoch [1/10], Training Loss: 1.136, Validation Accuracy: 57.63%\n",
        "Epoch [2/10], Training Loss: 1.087, Validation Accuracy: 57.93%\n",
        "Epoch [3/10], Training Loss: 1.067, Validation Accuracy: 58.69%\n",
        "Epoch [4/10], Training Loss: 1.039, Validation Accuracy: 57.28%\n",
        "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 57.58%\n",
        "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 57.83%\n",
        "Epoch [7/10], Training Loss: 0.989, Validation Accuracy: 58.23%\n",
        "Epoch [8/10], Training Loss: 0.979, Validation Accuracy: 58.16%\n",
        "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 58.70%\n",
        "Epoch [10/10], Training Loss: 0.954, Validation Accuracy: 58.29%\n",
        "Epoch [1/10], Training Loss: 1.113, Validation Accuracy: 58.54%\n",
        "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 58.43%\n",
        "Epoch [3/10], Training Loss: 1.045, Validation Accuracy: 58.80%\n",
        "Epoch [4/10], Training Loss: 1.015, Validation Accuracy: 58.56%\n",
        "Epoch [5/10], Training Loss: 1.001, Validation Accuracy: 58.64%\n",
        "Epoch [6/10], Training Loss: 0.980, Validation Accuracy: 58.27%\n",
        "Epoch [7/10], Training Loss: 0.970, Validation Accuracy: 57.46%\n",
        "Epoch [8/10], Training Loss: 0.953, Validation Accuracy: 58.32%\n",
        "Epoch [9/10], Training Loss: 0.943, Validation Accuracy: 58.43%\n",
        "Epoch [10/10], Training Loss: 0.927, Validation Accuracy: 58.90%\n",
        "Epoch [1/10], Training Loss: 1.099, Validation Accuracy: 57.97%\n",
        "Epoch [2/10], Training Loss: 1.059, Validation Accuracy: 58.53%\n",
        "Epoch [3/10], Training Loss: 1.021, Validation Accuracy: 58.74%\n",
        "Epoch [4/10], Training Loss: 0.996, Validation Accuracy: 58.18%\n",
        "Epoch [5/10], Training Loss: 0.974, Validation Accuracy: 58.88%\n",
        "Epoch [6/10], Training Loss: 0.959, Validation Accuracy: 59.06%\n",
        "Epoch [7/10], Training Loss: 0.942, Validation Accuracy: 59.20%\n",
        "Epoch [8/10], Training Loss: 0.925, Validation Accuracy: 58.98%\n",
        "Epoch [9/10], Training Loss: 0.915, Validation Accuracy: 59.17%\n",
        "Epoch [10/10], Training Loss: 0.898, Validation Accuracy: 59.16%\n",
        "Epoch [1/10], Training Loss: 1.084, Validation Accuracy: 59.21%\n",
        "Epoch [2/10], Training Loss: 1.036, Validation Accuracy: 59.11%\n",
        "Epoch [3/10], Training Loss: 1.004, Validation Accuracy: 59.65%\n",
        "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 59.48%\n",
        "Epoch [5/10], Training Loss: 0.959, Validation Accuracy: 59.29%\n",
        "Epoch [6/10], Training Loss: 0.945, Validation Accuracy: 59.28%\n",
        "Epoch [7/10], Training Loss: 0.924, Validation Accuracy: 59.05%\n",
        "Epoch [8/10], Training Loss: 0.908, Validation Accuracy: 58.99%\n",
        "Epoch [9/10], Training Loss: 0.906, Validation Accuracy: 59.20%\n",
        "Epoch [10/10], Training Loss: 0.882, Validation Accuracy: 58.93%\n",
        "Epoch [1/10], Training Loss: 1.074, Validation Accuracy: 59.00%\n",
        "Epoch [2/10], Training Loss: 1.032, Validation Accuracy: 59.18%\n",
        "Epoch [3/10], Training Loss: 0.992, Validation Accuracy: 59.39%\n",
        "Epoch [4/10], Training Loss: 0.971, Validation Accuracy: 59.28%\n",
        "Epoch [5/10], Training Loss: 0.943, Validation Accuracy: 59.39%\n",
        "Epoch [6/10], Training Loss: 0.925, Validation Accuracy: 59.53%\n",
        "Epoch [7/10], Training Loss: 0.911, Validation Accuracy: 58.83%\n",
        "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 58.81%\n",
        "Epoch [9/10], Training Loss: 0.882, Validation Accuracy: 58.41%\n",
        "Epoch [10/10], Training Loss: 0.859, Validation Accuracy: 59.05%\n",
        "Epoch [1/10], Training Loss: 1.046, Validation Accuracy: 59.80%\n",
        "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 59.57%\n",
        "Epoch [3/10], Training Loss: 0.961, Validation Accuracy: 59.66%\n",
        "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 60.23%\n",
        "Epoch [5/10], Training Loss: 0.919, Validation Accuracy: 59.11%\n",
        "Epoch [6/10], Training Loss: 0.888, Validation Accuracy: 59.25%\n",
        "Epoch [7/10], Training Loss: 0.876, Validation Accuracy: 59.74%\n",
        "Epoch [8/10], Training Loss: 0.871, Validation Accuracy: 59.29%\n",
        "Epoch [9/10], Training Loss: 0.844, Validation Accuracy: 59.95%\n",
        "Epoch [10/10], Training Loss: 0.831, Validation Accuracy: 59.50%\n",
        "Epoch [1/10], Training Loss: 1.049, Validation Accuracy: 59.77%\n",
        "Epoch [2/10], Training Loss: 0.980, Validation Accuracy: 58.98%\n",
        "Epoch [3/10], Training Loss: 0.954, Validation Accuracy: 59.68%\n",
        "Epoch [4/10], Training Loss: 0.918, Validation Accuracy: 58.98%\n",
        "Epoch [5/10], Training Loss: 0.894, Validation Accuracy: 59.79%\n",
        "Epoch [6/10], Training Loss: 0.870, Validation Accuracy: 59.83%\n",
        "Epoch [7/10], Training Loss: 0.861, Validation Accuracy: 58.60%\n",
        "Epoch [8/10], Training Loss: 0.842, Validation Accuracy: 59.04%\n",
        "Epoch [9/10], Training Loss: 0.826, Validation Accuracy: 59.50%\n",
        "Epoch [10/10], Training Loss: 0.807, Validation Accuracy: 59.10%\n",
        "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 59.71%\n",
        "Epoch [2/10], Training Loss: 0.968, Validation Accuracy: 58.39%\n",
        "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 59.55%\n",
        "Epoch [4/10], Training Loss: 0.901, Validation Accuracy: 59.53%\n",
        "Epoch [5/10], Training Loss: 0.876, Validation Accuracy: 59.20%\n",
        "Epoch [6/10], Training Loss: 0.858, Validation Accuracy: 59.24%\n",
        "Epoch [7/10], Training Loss: 0.842, Validation Accuracy: 58.87%\n",
        "Epoch [8/10], Training Loss: 0.816, Validation Accuracy: 59.82%\n",
        "Epoch [9/10], Training Loss: 0.796, Validation Accuracy: 59.86%\n",
        "Epoch [10/10], Training Loss: 0.785, Validation Accuracy: 59.79%\n",
        "Epoch [1/10], Training Loss: 1.025, Validation Accuracy: 58.61%\n",
        "Epoch [2/10], Training Loss: 0.964, Validation Accuracy: 59.65%\n",
        "Epoch [3/10], Training Loss: 0.915, Validation Accuracy: 59.85%\n",
        "Epoch [4/10], Training Loss: 0.895, Validation Accuracy: 60.28%\n",
        "Epoch [5/10], Training Loss: 0.862, Validation Accuracy: 59.44%\n",
        "Epoch [6/10], Training Loss: 0.839, Validation Accuracy: 60.17%\n",
        "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.55%\n",
        "Epoch [8/10], Training Loss: 0.805, Validation Accuracy: 59.58%\n",
        "Epoch [9/10], Training Loss: 0.782, Validation Accuracy: 60.13%\n",
        "Epoch [10/10], Training Loss: 0.769, Validation Accuracy: 58.70%\n",
        "Epoch [1/10], Training Loss: 1.015, Validation Accuracy: 60.41%\n",
        "Epoch [2/10], Training Loss: 0.946, Validation Accuracy: 60.13%\n",
        "Epoch [3/10], Training Loss: 0.910, Validation Accuracy: 59.99%\n",
        "Epoch [4/10], Training Loss: 0.881, Validation Accuracy: 60.18%\n",
        "Epoch [5/10], Training Loss: 0.845, Validation Accuracy: 60.26%\n",
        "Epoch [6/10], Training Loss: 0.814, Validation Accuracy: 60.02%\n",
        "Epoch [7/10], Training Loss: 0.803, Validation Accuracy: 58.56%\n",
        "Epoch [8/10], Training Loss: 0.792, Validation Accuracy: 59.95%\n",
        "Epoch [9/10], Training Loss: 0.773, Validation Accuracy: 59.65%\n",
        "Epoch [10/10], Training Loss: 0.751, Validation Accuracy: 59.15%\n",
        "Epoch [1/10], Training Loss: 1.006, Validation Accuracy: 60.29%\n",
        "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 58.62%\n",
        "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 60.32%\n",
        "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 60.46%\n",
        "Epoch [5/10], Training Loss: 0.823, Validation Accuracy: 60.80%\n",
        "Epoch [6/10], Training Loss: 0.796, Validation Accuracy: 60.79%\n",
        "Epoch [7/10], Training Loss: 0.769, Validation Accuracy: 60.37%\n",
        "Epoch [8/10], Training Loss: 0.757, Validation Accuracy: 59.99%\n",
        "Epoch [9/10], Training Loss: 0.738, Validation Accuracy: 60.29%\n",
        "Epoch [10/10], Training Loss: 0.722, Validation Accuracy: 59.66%\n",
        "Epoch [1/10], Training Loss: 0.987, Validation Accuracy: 60.28%\n",
        "Epoch [2/10], Training Loss: 0.902, Validation Accuracy: 59.93%\n",
        "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 60.77%\n",
        "Epoch [4/10], Training Loss: 0.827, Validation Accuracy: 59.92%\n",
        "Epoch [5/10], Training Loss: 0.805, Validation Accuracy: 60.38%\n",
        "Epoch [6/10], Training Loss: 0.776, Validation Accuracy: 60.88%\n",
        "Epoch [7/10], Training Loss: 0.763, Validation Accuracy: 60.41%\n",
        "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 60.28%\n",
        "Epoch [9/10], Training Loss: 0.722, Validation Accuracy: 59.76%\n",
        "Epoch [10/10], Training Loss: 0.710, Validation Accuracy: 60.40%\n",
        "Epoch [1/10], Training Loss: 0.985, Validation Accuracy: 59.15%\n",
        "Epoch [2/10], Training Loss: 0.891, Validation Accuracy: 59.80%\n",
        "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 60.98%\n",
        "Epoch [4/10], Training Loss: 0.804, Validation Accuracy: 60.57%\n",
        "Epoch [5/10], Training Loss: 0.778, Validation Accuracy: 60.90%\n",
        "Epoch [6/10], Training Loss: 0.762, Validation Accuracy: 60.43%\n",
        "Epoch [7/10], Training Loss: 0.731, Validation Accuracy: 60.28%\n",
        "Epoch [8/10], Training Loss: 0.713, Validation Accuracy: 61.00%\n",
        "Epoch [9/10], Training Loss: 0.693, Validation Accuracy: 60.68%\n",
        "Epoch [10/10], Training Loss: 0.674, Validation Accuracy: 60.03%\n",
        "Epoch [1/10], Training Loss: 0.977, Validation Accuracy: 60.70%\n",
        "Epoch [2/10], Training Loss: 0.893, Validation Accuracy: 60.53%\n",
        "Epoch [3/10], Training Loss: 0.835, Validation Accuracy: 61.15%\n",
        "Epoch [4/10], Training Loss: 0.795, Validation Accuracy: 60.73%\n",
        "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 59.67%\n",
        "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 60.27%\n",
        "Epoch [7/10], Training Loss: 0.727, Validation Accuracy: 60.41%\n",
        "Epoch [8/10], Training Loss: 0.710, Validation Accuracy: 60.49%\n",
        "Epoch [9/10], Training Loss: 0.685, Validation Accuracy: 60.18%\n",
        "Epoch [10/10], Training Loss: 0.660, Validation Accuracy: 60.43%\n",
        "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 60.67%\n",
        "Epoch [2/10], Training Loss: 0.870, Validation Accuracy: 60.57%\n",
        "Epoch [3/10], Training Loss: 0.818, Validation Accuracy: 60.83%\n",
        "Epoch [4/10], Training Loss: 0.787, Validation Accuracy: 60.69%\n",
        "Epoch [5/10], Training Loss: 0.748, Validation Accuracy: 60.85%\n",
        "Epoch [6/10], Training Loss: 0.723, Validation Accuracy: 60.49%\n",
        "Epoch [7/10], Training Loss: 0.708, Validation Accuracy: 59.95%\n",
        "Epoch [8/10], Training Loss: 0.681, Validation Accuracy: 60.11%\n",
        "Epoch [9/10], Training Loss: 0.661, Validation Accuracy: 60.29%\n",
        "Epoch [10/10], Training Loss: 0.638, Validation Accuracy: 60.08%\n",
        "Epoch [1/10], Training Loss: 0.941, Validation Accuracy: 61.20%\n",
        "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 61.34%\n",
        "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 61.03%\n",
        "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 61.21%\n",
        "Epoch [5/10], Training Loss: 0.735, Validation Accuracy: 59.42%\n",
        "Epoch [6/10], Training Loss: 0.702, Validation Accuracy: 60.59%\n",
        "Epoch [7/10], Training Loss: 0.683, Validation Accuracy: 60.66%\n",
        "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 59.84%\n",
        "Epoch [9/10], Training Loss: 0.638, Validation Accuracy: 61.00%\n",
        "Epoch [10/10], Training Loss: 0.618, Validation Accuracy: 61.00%\n",
        "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 60.70%\n",
        "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 61.26%\n",
        "Epoch [3/10], Training Loss: 0.788, Validation Accuracy: 60.18%\n",
        "Epoch [4/10], Training Loss: 0.745, Validation Accuracy: 60.51%\n",
        "Epoch [5/10], Training Loss: 0.710, Validation Accuracy: 60.41%\n",
        "Epoch [6/10], Training Loss: 0.694, Validation Accuracy: 60.85%\n",
        "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 60.59%\n",
        "Epoch [8/10], Training Loss: 0.644, Validation Accuracy: 61.21%\n",
        "Epoch [9/10], Training Loss: 0.627, Validation Accuracy: 60.22%\n",
        "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 60.47%\n",
        "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 60.75%\n",
        "Epoch [2/10], Training Loss: 0.822, Validation Accuracy: 60.35%\n",
        "Epoch [3/10], Training Loss: 0.765, Validation Accuracy: 60.58%\n",
        "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 60.24%\n",
        "Epoch [5/10], Training Loss: 0.686, Validation Accuracy: 61.10%\n",
        "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 61.18%\n",
        "Epoch [7/10], Training Loss: 0.623, Validation Accuracy: 59.76%\n",
        "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 59.98%\n",
        "Epoch [9/10], Training Loss: 0.603, Validation Accuracy: 60.49%\n",
        "Epoch [10/10], Training Loss: 0.576, Validation Accuracy: 59.84%\n",
        "Epoch [1/10], Training Loss: 0.929, Validation Accuracy: 60.47%\n",
        "Epoch [2/10], Training Loss: 0.820, Validation Accuracy: 60.51%\n",
        "Epoch [3/10], Training Loss: 0.751, Validation Accuracy: 61.09%\n",
        "Epoch [4/10], Training Loss: 0.717, Validation Accuracy: 59.94%\n",
        "Epoch [5/10], Training Loss: 0.692, Validation Accuracy: 60.14%\n",
        "Epoch [6/10], Training Loss: 0.652, Validation Accuracy: 60.57%\n",
        "Epoch [7/10], Training Loss: 0.628, Validation Accuracy: 60.60%\n",
        "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 60.14%\n",
        "Epoch [9/10], Training Loss: 0.585, Validation Accuracy: 60.45%\n",
        "Epoch [10/10], Training Loss: 0.565, Validation Accuracy: 60.09%\n",
        "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 60.25%\n",
        "Epoch [2/10], Training Loss: 0.800, Validation Accuracy: 61.09%\n",
        "Epoch [3/10], Training Loss: 0.738, Validation Accuracy: 61.00%\n",
        "Epoch [4/10], Training Loss: 0.694, Validation Accuracy: 60.14%\n",
        "Epoch [5/10], Training Loss: 0.664, Validation Accuracy: 61.00%\n",
        "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 60.11%\n",
        "Epoch [7/10], Training Loss: 0.620, Validation Accuracy: 60.27%\n",
        "Epoch [8/10], Training Loss: 0.582, Validation Accuracy: 59.52%\n",
        "Epoch [9/10], Training Loss: 0.563, Validation Accuracy: 59.78%\n",
        "Epoch [10/10], Training Loss: 0.555, Validation Accuracy: 60.49%\n",
        "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 61.07%\n",
        "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 60.74%\n",
        "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 61.31%\n",
        "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 61.57%\n",
        "Epoch [5/10], Training Loss: 0.645, Validation Accuracy: 60.80%\n",
        "Epoch [6/10], Training Loss: 0.610, Validation Accuracy: 60.83%\n",
        "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 60.41%\n",
        "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 60.60%\n",
        "Epoch [9/10], Training Loss: 0.537, Validation Accuracy: 60.70%\n",
        "Epoch [10/10], Training Loss: 0.516, Validation Accuracy: 60.82%\n",
        "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.52%\n",
        "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 61.40%\n",
        "Epoch [3/10], Training Loss: 0.713, Validation Accuracy: 60.38%\n",
        "Epoch [4/10], Training Loss: 0.671, Validation Accuracy: 61.33%\n",
        "Epoch [5/10], Training Loss: 0.636, Validation Accuracy: 61.09%\n",
        "Epoch [6/10], Training Loss: 0.598, Validation Accuracy: 60.96%\n",
        "Epoch [7/10], Training Loss: 0.574, Validation Accuracy: 60.22%\n",
        "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 60.86%\n",
        "Epoch [9/10], Training Loss: 0.532, Validation Accuracy: 60.22%\n",
        "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 60.27%\n",
        "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 60.15%\n",
        "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 60.42%\n",
        "Epoch [3/10], Training Loss: 0.682, Validation Accuracy: 60.36%\n",
        "Epoch [4/10], Training Loss: 0.650, Validation Accuracy: 60.48%\n",
        "Epoch [5/10], Training Loss: 0.603, Validation Accuracy: 60.64%\n",
        "Epoch [6/10], Training Loss: 0.573, Validation Accuracy: 60.68%\n",
        "Epoch [7/10], Training Loss: 0.551, Validation Accuracy: 59.55%\n",
        "Epoch [8/10], Training Loss: 0.520, Validation Accuracy: 59.78%\n",
        "Epoch [9/10], Training Loss: 0.495, Validation Accuracy: 60.54%\n",
        "Epoch [10/10], Training Loss: 0.484, Validation Accuracy: 60.68%\n",
        "Epoch [1/10], Training Loss: 0.884, Validation Accuracy: 59.23%\n",
        "Epoch [2/10], Training Loss: 0.747, Validation Accuracy: 59.69%\n",
        "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 60.60%\n",
        "Epoch [4/10], Training Loss: 0.643, Validation Accuracy: 60.77%\n",
        "Epoch [5/10], Training Loss: 0.608, Validation Accuracy: 61.17%\n",
        "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 60.36%\n",
        "Epoch [7/10], Training Loss: 0.541, Validation Accuracy: 60.86%\n",
        "Epoch [8/10], Training Loss: 0.519, Validation Accuracy: 60.60%\n",
        "Epoch [9/10], Training Loss: 0.500, Validation Accuracy: 60.67%\n",
        "Epoch [10/10], Training Loss: 0.475, Validation Accuracy: 60.22%\n",
        "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 60.80%\n",
        "Epoch [2/10], Training Loss: 0.734, Validation Accuracy: 59.52%\n",
        "Epoch [3/10], Training Loss: 0.667, Validation Accuracy: 60.82%\n",
        "Epoch [4/10], Training Loss: 0.618, Validation Accuracy: 60.80%\n",
        "Epoch [5/10], Training Loss: 0.579, Validation Accuracy: 60.60%\n",
        "Epoch [6/10], Training Loss: 0.548, Validation Accuracy: 60.32%\n",
        "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 60.78%\n",
        "Epoch [8/10], Training Loss: 0.494, Validation Accuracy: 59.67%\n",
        "Epoch [9/10], Training Loss: 0.478, Validation Accuracy: 60.24%\n",
        "Epoch [10/10], Training Loss: 0.458, Validation Accuracy: 60.07%\n",
        "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 60.55%\n",
        "Epoch [2/10], Training Loss: 0.711, Validation Accuracy: 61.52%\n",
        "Epoch [3/10], Training Loss: 0.632, Validation Accuracy: 61.45%\n",
        "Epoch [4/10], Training Loss: 0.609, Validation Accuracy: 61.00%\n",
        "Epoch [5/10], Training Loss: 0.565, Validation Accuracy: 60.96%\n",
        "Epoch [6/10], Training Loss: 0.523, Validation Accuracy: 61.15%\n",
        "Epoch [7/10], Training Loss: 0.498, Validation Accuracy: 61.22%\n",
        "Epoch [8/10], Training Loss: 0.474, Validation Accuracy: 60.79%\n",
        "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 60.97%\n",
        "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 60.79%\n",
        "Epoch [1/10], Training Loss: 0.885, Validation Accuracy: 60.62%\n",
        "Epoch [2/10], Training Loss: 0.732, Validation Accuracy: 60.76%\n",
        "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 61.17%\n",
        "Epoch [4/10], Training Loss: 0.592, Validation Accuracy: 61.02%\n",
        "Epoch [5/10], Training Loss: 0.546, Validation Accuracy: 61.12%\n",
        "Epoch [6/10], Training Loss: 0.518, Validation Accuracy: 60.72%\n",
        "Epoch [7/10], Training Loss: 0.489, Validation Accuracy: 60.68%\n",
        "Epoch [8/10], Training Loss: 0.468, Validation Accuracy: 60.49%\n",
        "Epoch [9/10], Training Loss: 0.444, Validation Accuracy: 60.34%\n",
        "Epoch [10/10], Training Loss: 0.417, Validation Accuracy: 60.08%\n",
        "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 59.92%\n",
        "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 60.47%\n",
        "Epoch [3/10], Training Loss: 0.612, Validation Accuracy: 60.60%\n",
        "Epoch [4/10], Training Loss: 0.554, Validation Accuracy: 59.98%\n",
        "Epoch [5/10], Training Loss: 0.518, Validation Accuracy: 60.13%\n",
        "Epoch [6/10], Training Loss: 0.487, Validation Accuracy: 60.58%\n",
        "Epoch [7/10], Training Loss: 0.468, Validation Accuracy: 59.58%\n",
        "Epoch [8/10], Training Loss: 0.442, Validation Accuracy: 60.56%\n",
        "Epoch [9/10], Training Loss: 0.411, Validation Accuracy: 60.52%\n",
        "Epoch [10/10], Training Loss: 0.393, Validation Accuracy: 60.38%\n",
        "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.28%\n",
        "Epoch [2/10], Training Loss: 0.678, Validation Accuracy: 60.24%\n",
        "Epoch [3/10], Training Loss: 0.604, Validation Accuracy: 61.14%\n",
        "Epoch [4/10], Training Loss: 0.558, Validation Accuracy: 60.30%\n",
        "Epoch [5/10], Training Loss: 0.528, Validation Accuracy: 59.88%\n",
        "Epoch [6/10], Training Loss: 0.489, Validation Accuracy: 60.34%\n",
        "Epoch [7/10], Training Loss: 0.458, Validation Accuracy: 59.62%\n",
        "Epoch [8/10], Training Loss: 0.436, Validation Accuracy: 60.23%\n",
        "Epoch [9/10], Training Loss: 0.409, Validation Accuracy: 60.09%\n",
        "Epoch [10/10], Training Loss: 0.405, Validation Accuracy: 59.79%\n",
        "Epoch [1/10], Training Loss: 0.834, Validation Accuracy: 60.54%\n",
        "Epoch [2/10], Training Loss: 0.671, Validation Accuracy: 60.20%\n",
        "Epoch [3/10], Training Loss: 0.598, Validation Accuracy: 60.30%\n",
        "Epoch [4/10], Training Loss: 0.535, Validation Accuracy: 59.64%\n",
        "Epoch [5/10], Training Loss: 0.499, Validation Accuracy: 60.54%\n",
        "Epoch [6/10], Training Loss: 0.468, Validation Accuracy: 59.64%\n",
        "Epoch [7/10], Training Loss: 0.432, Validation Accuracy: 59.60%\n",
        "Epoch [8/10], Training Loss: 0.410, Validation Accuracy: 59.80%\n",
        "Epoch [9/10], Training Loss: 0.385, Validation Accuracy: 60.16%\n",
        "Epoch [10/10], Training Loss: 0.365, Validation Accuracy: 59.90%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SJYxfq4Scsl",
        "outputId": "535134b6-ccf3-4bd6-f430-21d156b561ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [5843 5936 6049 6011 6149 5995 6058 6085 5882 5992]\n",
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 9.09%\n",
            "Epoch [2/10], Training Loss: 2.302, Validation Accuracy: 8.80%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 8.83%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 8.95%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 9.52%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 9.62%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 9.65%\n",
            "Epoch [8/10], Training Loss: 2.300, Validation Accuracy: 9.66%\n",
            "Epoch [9/10], Training Loss: 2.299, Validation Accuracy: 9.66%\n",
            "Epoch [10/10], Training Loss: 2.298, Validation Accuracy: 9.66%\n",
            "Epoch [1/10], Training Loss: 2.299, Validation Accuracy: 9.66%\n",
            "Epoch [2/10], Training Loss: 2.298, Validation Accuracy: 9.72%\n",
            "Epoch [3/10], Training Loss: 2.296, Validation Accuracy: 9.78%\n",
            "Epoch [4/10], Training Loss: 2.294, Validation Accuracy: 10.35%\n",
            "Epoch [5/10], Training Loss: 2.292, Validation Accuracy: 11.38%\n",
            "Epoch [6/10], Training Loss: 2.288, Validation Accuracy: 13.31%\n",
            "Epoch [7/10], Training Loss: 2.283, Validation Accuracy: 14.51%\n",
            "Epoch [8/10], Training Loss: 2.274, Validation Accuracy: 15.77%\n",
            "Epoch [9/10], Training Loss: 2.260, Validation Accuracy: 16.55%\n",
            "Epoch [10/10], Training Loss: 2.240, Validation Accuracy: 16.95%\n",
            "Epoch [1/10], Training Loss: 2.213, Validation Accuracy: 17.66%\n",
            "Epoch [2/10], Training Loss: 2.179, Validation Accuracy: 21.05%\n",
            "Epoch [3/10], Training Loss: 2.141, Validation Accuracy: 23.12%\n",
            "Epoch [4/10], Training Loss: 2.099, Validation Accuracy: 24.57%\n",
            "Epoch [5/10], Training Loss: 2.058, Validation Accuracy: 26.33%\n",
            "Epoch [6/10], Training Loss: 2.021, Validation Accuracy: 27.11%\n",
            "Epoch [7/10], Training Loss: 1.989, Validation Accuracy: 28.22%\n",
            "Epoch [8/10], Training Loss: 1.963, Validation Accuracy: 28.16%\n",
            "Epoch [9/10], Training Loss: 1.941, Validation Accuracy: 29.41%\n",
            "Epoch [10/10], Training Loss: 1.919, Validation Accuracy: 30.06%\n",
            "Epoch [1/10], Training Loss: 1.902, Validation Accuracy: 30.18%\n",
            "Epoch [2/10], Training Loss: 1.881, Validation Accuracy: 31.11%\n",
            "Epoch [3/10], Training Loss: 1.864, Validation Accuracy: 31.47%\n",
            "Epoch [4/10], Training Loss: 1.851, Validation Accuracy: 31.89%\n",
            "Epoch [5/10], Training Loss: 1.836, Validation Accuracy: 32.67%\n",
            "Epoch [6/10], Training Loss: 1.820, Validation Accuracy: 32.83%\n",
            "Epoch [7/10], Training Loss: 1.806, Validation Accuracy: 33.57%\n",
            "Epoch [8/10], Training Loss: 1.793, Validation Accuracy: 34.17%\n",
            "Epoch [9/10], Training Loss: 1.779, Validation Accuracy: 34.26%\n",
            "Epoch [10/10], Training Loss: 1.769, Validation Accuracy: 35.14%\n",
            "Epoch [1/10], Training Loss: 1.769, Validation Accuracy: 35.42%\n",
            "Epoch [2/10], Training Loss: 1.752, Validation Accuracy: 36.06%\n",
            "Epoch [3/10], Training Loss: 1.735, Validation Accuracy: 36.02%\n",
            "Epoch [4/10], Training Loss: 1.725, Validation Accuracy: 36.89%\n",
            "Epoch [5/10], Training Loss: 1.708, Validation Accuracy: 37.26%\n",
            "Epoch [6/10], Training Loss: 1.702, Validation Accuracy: 37.56%\n",
            "Epoch [7/10], Training Loss: 1.686, Validation Accuracy: 38.08%\n",
            "Epoch [8/10], Training Loss: 1.671, Validation Accuracy: 38.22%\n",
            "Epoch [9/10], Training Loss: 1.663, Validation Accuracy: 38.43%\n",
            "Epoch [10/10], Training Loss: 1.651, Validation Accuracy: 39.11%\n",
            "Epoch [1/10], Training Loss: 1.651, Validation Accuracy: 39.45%\n",
            "Epoch [2/10], Training Loss: 1.633, Validation Accuracy: 40.51%\n",
            "Epoch [3/10], Training Loss: 1.623, Validation Accuracy: 39.55%\n",
            "Epoch [4/10], Training Loss: 1.607, Validation Accuracy: 40.74%\n",
            "Epoch [5/10], Training Loss: 1.593, Validation Accuracy: 41.25%\n",
            "Epoch [6/10], Training Loss: 1.583, Validation Accuracy: 41.52%\n",
            "Epoch [7/10], Training Loss: 1.575, Validation Accuracy: 40.70%\n",
            "Epoch [8/10], Training Loss: 1.559, Validation Accuracy: 42.09%\n",
            "Epoch [9/10], Training Loss: 1.552, Validation Accuracy: 42.08%\n",
            "Epoch [10/10], Training Loss: 1.540, Validation Accuracy: 42.14%\n",
            "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 43.02%\n",
            "Epoch [2/10], Training Loss: 1.570, Validation Accuracy: 42.68%\n",
            "Epoch [3/10], Training Loss: 1.554, Validation Accuracy: 43.28%\n",
            "Epoch [4/10], Training Loss: 1.550, Validation Accuracy: 43.59%\n",
            "Epoch [5/10], Training Loss: 1.533, Validation Accuracy: 43.75%\n",
            "Epoch [6/10], Training Loss: 1.524, Validation Accuracy: 44.03%\n",
            "Epoch [7/10], Training Loss: 1.516, Validation Accuracy: 43.75%\n",
            "Epoch [8/10], Training Loss: 1.503, Validation Accuracy: 44.79%\n",
            "Epoch [9/10], Training Loss: 1.494, Validation Accuracy: 44.87%\n",
            "Epoch [10/10], Training Loss: 1.490, Validation Accuracy: 44.57%\n",
            "Epoch [1/10], Training Loss: 1.514, Validation Accuracy: 45.16%\n",
            "Epoch [2/10], Training Loss: 1.495, Validation Accuracy: 45.63%\n",
            "Epoch [3/10], Training Loss: 1.482, Validation Accuracy: 45.46%\n",
            "Epoch [4/10], Training Loss: 1.470, Validation Accuracy: 45.66%\n",
            "Epoch [5/10], Training Loss: 1.462, Validation Accuracy: 46.49%\n",
            "Epoch [6/10], Training Loss: 1.457, Validation Accuracy: 46.26%\n",
            "Epoch [7/10], Training Loss: 1.445, Validation Accuracy: 45.98%\n",
            "Epoch [8/10], Training Loss: 1.437, Validation Accuracy: 46.30%\n",
            "Epoch [9/10], Training Loss: 1.434, Validation Accuracy: 46.20%\n",
            "Epoch [10/10], Training Loss: 1.417, Validation Accuracy: 46.86%\n",
            "Epoch [1/10], Training Loss: 1.460, Validation Accuracy: 46.61%\n",
            "Epoch [2/10], Training Loss: 1.449, Validation Accuracy: 46.93%\n",
            "Epoch [3/10], Training Loss: 1.430, Validation Accuracy: 47.17%\n",
            "Epoch [4/10], Training Loss: 1.420, Validation Accuracy: 47.88%\n",
            "Epoch [5/10], Training Loss: 1.416, Validation Accuracy: 47.29%\n",
            "Epoch [6/10], Training Loss: 1.403, Validation Accuracy: 47.72%\n",
            "Epoch [7/10], Training Loss: 1.395, Validation Accuracy: 48.24%\n",
            "Epoch [8/10], Training Loss: 1.383, Validation Accuracy: 48.99%\n",
            "Epoch [9/10], Training Loss: 1.377, Validation Accuracy: 48.79%\n",
            "Epoch [10/10], Training Loss: 1.367, Validation Accuracy: 48.20%\n",
            "Epoch [1/10], Training Loss: 1.410, Validation Accuracy: 48.34%\n",
            "Epoch [2/10], Training Loss: 1.389, Validation Accuracy: 49.19%\n",
            "Epoch [3/10], Training Loss: 1.375, Validation Accuracy: 49.54%\n",
            "Epoch [4/10], Training Loss: 1.363, Validation Accuracy: 49.72%\n",
            "Epoch [5/10], Training Loss: 1.353, Validation Accuracy: 49.56%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 49.58%\n",
            "Epoch [7/10], Training Loss: 1.337, Validation Accuracy: 49.54%\n",
            "Epoch [8/10], Training Loss: 1.320, Validation Accuracy: 49.97%\n",
            "Epoch [9/10], Training Loss: 1.315, Validation Accuracy: 50.01%\n",
            "Epoch [10/10], Training Loss: 1.308, Validation Accuracy: 50.39%\n",
            "Epoch [1/10], Training Loss: 1.351, Validation Accuracy: 49.71%\n",
            "Epoch [2/10], Training Loss: 1.335, Validation Accuracy: 50.78%\n",
            "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 50.10%\n",
            "Epoch [4/10], Training Loss: 1.308, Validation Accuracy: 50.62%\n",
            "Epoch [5/10], Training Loss: 1.296, Validation Accuracy: 49.97%\n",
            "Epoch [6/10], Training Loss: 1.289, Validation Accuracy: 51.72%\n",
            "Epoch [7/10], Training Loss: 1.268, Validation Accuracy: 51.33%\n",
            "Epoch [8/10], Training Loss: 1.259, Validation Accuracy: 51.54%\n",
            "Epoch [9/10], Training Loss: 1.257, Validation Accuracy: 51.24%\n",
            "Epoch [10/10], Training Loss: 1.241, Validation Accuracy: 50.79%\n",
            "Epoch [1/10], Training Loss: 1.341, Validation Accuracy: 51.82%\n",
            "Epoch [2/10], Training Loss: 1.322, Validation Accuracy: 52.07%\n",
            "Epoch [3/10], Training Loss: 1.302, Validation Accuracy: 52.53%\n",
            "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 52.64%\n",
            "Epoch [5/10], Training Loss: 1.278, Validation Accuracy: 52.13%\n",
            "Epoch [6/10], Training Loss: 1.266, Validation Accuracy: 52.96%\n",
            "Epoch [7/10], Training Loss: 1.268, Validation Accuracy: 52.76%\n",
            "Epoch [8/10], Training Loss: 1.246, Validation Accuracy: 52.55%\n",
            "Epoch [9/10], Training Loss: 1.242, Validation Accuracy: 52.14%\n",
            "Epoch [10/10], Training Loss: 1.230, Validation Accuracy: 52.86%\n",
            "Epoch [1/10], Training Loss: 1.288, Validation Accuracy: 51.95%\n",
            "Epoch [2/10], Training Loss: 1.275, Validation Accuracy: 53.17%\n",
            "Epoch [3/10], Training Loss: 1.251, Validation Accuracy: 53.91%\n",
            "Epoch [4/10], Training Loss: 1.231, Validation Accuracy: 53.38%\n",
            "Epoch [5/10], Training Loss: 1.228, Validation Accuracy: 53.74%\n",
            "Epoch [6/10], Training Loss: 1.207, Validation Accuracy: 53.32%\n",
            "Epoch [7/10], Training Loss: 1.201, Validation Accuracy: 52.78%\n",
            "Epoch [8/10], Training Loss: 1.202, Validation Accuracy: 54.22%\n",
            "Epoch [9/10], Training Loss: 1.191, Validation Accuracy: 53.65%\n",
            "Epoch [10/10], Training Loss: 1.174, Validation Accuracy: 53.61%\n",
            "Epoch [1/10], Training Loss: 1.274, Validation Accuracy: 52.64%\n",
            "Epoch [2/10], Training Loss: 1.239, Validation Accuracy: 53.96%\n",
            "Epoch [3/10], Training Loss: 1.230, Validation Accuracy: 53.72%\n",
            "Epoch [4/10], Training Loss: 1.220, Validation Accuracy: 53.51%\n",
            "Epoch [5/10], Training Loss: 1.207, Validation Accuracy: 54.83%\n",
            "Epoch [6/10], Training Loss: 1.194, Validation Accuracy: 54.70%\n",
            "Epoch [7/10], Training Loss: 1.185, Validation Accuracy: 54.61%\n",
            "Epoch [8/10], Training Loss: 1.181, Validation Accuracy: 54.85%\n",
            "Epoch [9/10], Training Loss: 1.166, Validation Accuracy: 54.05%\n",
            "Epoch [10/10], Training Loss: 1.156, Validation Accuracy: 54.72%\n",
            "Epoch [1/10], Training Loss: 1.222, Validation Accuracy: 54.60%\n",
            "Epoch [2/10], Training Loss: 1.205, Validation Accuracy: 54.76%\n",
            "Epoch [3/10], Training Loss: 1.179, Validation Accuracy: 55.29%\n",
            "Epoch [4/10], Training Loss: 1.159, Validation Accuracy: 55.20%\n",
            "Epoch [5/10], Training Loss: 1.155, Validation Accuracy: 55.93%\n",
            "Epoch [6/10], Training Loss: 1.146, Validation Accuracy: 54.46%\n",
            "Epoch [7/10], Training Loss: 1.132, Validation Accuracy: 54.13%\n",
            "Epoch [8/10], Training Loss: 1.115, Validation Accuracy: 55.33%\n",
            "Epoch [9/10], Training Loss: 1.115, Validation Accuracy: 55.26%\n",
            "Epoch [10/10], Training Loss: 1.110, Validation Accuracy: 55.66%\n",
            "Epoch [1/10], Training Loss: 1.188, Validation Accuracy: 55.33%\n",
            "Epoch [2/10], Training Loss: 1.165, Validation Accuracy: 54.90%\n",
            "Epoch [3/10], Training Loss: 1.145, Validation Accuracy: 55.48%\n",
            "Epoch [4/10], Training Loss: 1.134, Validation Accuracy: 55.43%\n",
            "Epoch [5/10], Training Loss: 1.119, Validation Accuracy: 56.41%\n",
            "Epoch [6/10], Training Loss: 1.107, Validation Accuracy: 55.60%\n",
            "Epoch [7/10], Training Loss: 1.091, Validation Accuracy: 56.08%\n",
            "Epoch [8/10], Training Loss: 1.081, Validation Accuracy: 55.76%\n",
            "Epoch [9/10], Training Loss: 1.072, Validation Accuracy: 56.72%\n",
            "Epoch [10/10], Training Loss: 1.062, Validation Accuracy: 55.63%\n",
            "Epoch [1/10], Training Loss: 1.202, Validation Accuracy: 56.35%\n",
            "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.21%\n",
            "Epoch [3/10], Training Loss: 1.159, Validation Accuracy: 56.31%\n",
            "Epoch [4/10], Training Loss: 1.141, Validation Accuracy: 55.99%\n",
            "Epoch [5/10], Training Loss: 1.133, Validation Accuracy: 56.52%\n",
            "Epoch [6/10], Training Loss: 1.120, Validation Accuracy: 55.86%\n",
            "Epoch [7/10], Training Loss: 1.102, Validation Accuracy: 56.58%\n",
            "Epoch [8/10], Training Loss: 1.092, Validation Accuracy: 56.63%\n",
            "Epoch [9/10], Training Loss: 1.082, Validation Accuracy: 56.68%\n",
            "Epoch [10/10], Training Loss: 1.071, Validation Accuracy: 56.09%\n",
            "Epoch [1/10], Training Loss: 1.168, Validation Accuracy: 57.04%\n",
            "Epoch [2/10], Training Loss: 1.134, Validation Accuracy: 56.76%\n",
            "Epoch [3/10], Training Loss: 1.117, Validation Accuracy: 57.11%\n",
            "Epoch [4/10], Training Loss: 1.098, Validation Accuracy: 57.17%\n",
            "Epoch [5/10], Training Loss: 1.081, Validation Accuracy: 57.30%\n",
            "Epoch [6/10], Training Loss: 1.066, Validation Accuracy: 56.35%\n",
            "Epoch [7/10], Training Loss: 1.057, Validation Accuracy: 57.42%\n",
            "Epoch [8/10], Training Loss: 1.041, Validation Accuracy: 57.16%\n",
            "Epoch [9/10], Training Loss: 1.029, Validation Accuracy: 57.09%\n",
            "Epoch [10/10], Training Loss: 1.021, Validation Accuracy: 57.35%\n",
            "Epoch [1/10], Training Loss: 1.147, Validation Accuracy: 57.35%\n",
            "Epoch [2/10], Training Loss: 1.122, Validation Accuracy: 57.02%\n",
            "Epoch [3/10], Training Loss: 1.103, Validation Accuracy: 57.45%\n",
            "Epoch [4/10], Training Loss: 1.091, Validation Accuracy: 55.62%\n",
            "Epoch [5/10], Training Loss: 1.077, Validation Accuracy: 56.74%\n",
            "Epoch [6/10], Training Loss: 1.053, Validation Accuracy: 58.15%\n",
            "Epoch [7/10], Training Loss: 1.044, Validation Accuracy: 57.47%\n",
            "Epoch [8/10], Training Loss: 1.037, Validation Accuracy: 57.08%\n",
            "Epoch [9/10], Training Loss: 1.026, Validation Accuracy: 56.93%\n",
            "Epoch [10/10], Training Loss: 1.011, Validation Accuracy: 57.03%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 57.07%\n",
            "Epoch [2/10], Training Loss: 1.082, Validation Accuracy: 57.69%\n",
            "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 58.45%\n",
            "Epoch [4/10], Training Loss: 1.036, Validation Accuracy: 58.10%\n",
            "Epoch [5/10], Training Loss: 1.020, Validation Accuracy: 57.61%\n",
            "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 58.01%\n",
            "Epoch [7/10], Training Loss: 0.996, Validation Accuracy: 58.58%\n",
            "Epoch [8/10], Training Loss: 0.981, Validation Accuracy: 57.86%\n",
            "Epoch [9/10], Training Loss: 0.972, Validation Accuracy: 57.86%\n",
            "Epoch [10/10], Training Loss: 0.962, Validation Accuracy: 57.49%\n",
            "Epoch [1/10], Training Loss: 1.090, Validation Accuracy: 57.98%\n",
            "Epoch [2/10], Training Loss: 1.046, Validation Accuracy: 57.70%\n",
            "Epoch [3/10], Training Loss: 1.030, Validation Accuracy: 58.55%\n",
            "Epoch [4/10], Training Loss: 1.011, Validation Accuracy: 58.33%\n",
            "Epoch [5/10], Training Loss: 0.994, Validation Accuracy: 58.11%\n",
            "Epoch [6/10], Training Loss: 0.984, Validation Accuracy: 58.35%\n",
            "Epoch [7/10], Training Loss: 0.966, Validation Accuracy: 58.51%\n",
            "Epoch [8/10], Training Loss: 0.950, Validation Accuracy: 58.20%\n",
            "Epoch [9/10], Training Loss: 0.941, Validation Accuracy: 57.10%\n",
            "Epoch [10/10], Training Loss: 0.924, Validation Accuracy: 58.33%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 57.85%\n",
            "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.97%\n",
            "Epoch [3/10], Training Loss: 1.055, Validation Accuracy: 58.44%\n",
            "Epoch [4/10], Training Loss: 1.031, Validation Accuracy: 59.06%\n",
            "Epoch [5/10], Training Loss: 1.010, Validation Accuracy: 58.88%\n",
            "Epoch [6/10], Training Loss: 0.996, Validation Accuracy: 59.02%\n",
            "Epoch [7/10], Training Loss: 0.984, Validation Accuracy: 59.29%\n",
            "Epoch [8/10], Training Loss: 0.963, Validation Accuracy: 58.66%\n",
            "Epoch [9/10], Training Loss: 0.959, Validation Accuracy: 58.55%\n",
            "Epoch [10/10], Training Loss: 0.945, Validation Accuracy: 58.77%\n",
            "Epoch [1/10], Training Loss: 1.081, Validation Accuracy: 59.16%\n",
            "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 58.63%\n",
            "Epoch [3/10], Training Loss: 1.021, Validation Accuracy: 58.62%\n",
            "Epoch [4/10], Training Loss: 0.990, Validation Accuracy: 59.30%\n",
            "Epoch [5/10], Training Loss: 0.976, Validation Accuracy: 58.45%\n",
            "Epoch [6/10], Training Loss: 0.968, Validation Accuracy: 58.58%\n",
            "Epoch [7/10], Training Loss: 0.943, Validation Accuracy: 58.68%\n",
            "Epoch [8/10], Training Loss: 0.936, Validation Accuracy: 58.74%\n",
            "Epoch [9/10], Training Loss: 0.920, Validation Accuracy: 58.46%\n",
            "Epoch [10/10], Training Loss: 0.907, Validation Accuracy: 58.81%\n",
            "Epoch [1/10], Training Loss: 1.076, Validation Accuracy: 57.98%\n",
            "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 59.57%\n",
            "Epoch [3/10], Training Loss: 1.004, Validation Accuracy: 58.96%\n",
            "Epoch [4/10], Training Loss: 0.980, Validation Accuracy: 58.89%\n",
            "Epoch [5/10], Training Loss: 0.972, Validation Accuracy: 59.78%\n",
            "Epoch [6/10], Training Loss: 0.947, Validation Accuracy: 58.99%\n",
            "Epoch [7/10], Training Loss: 0.940, Validation Accuracy: 59.33%\n",
            "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 59.39%\n",
            "Epoch [9/10], Training Loss: 0.918, Validation Accuracy: 58.27%\n",
            "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 58.94%\n",
            "Epoch [1/10], Training Loss: 1.041, Validation Accuracy: 59.67%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 59.57%\n",
            "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.14%\n",
            "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 58.64%\n",
            "Epoch [5/10], Training Loss: 0.916, Validation Accuracy: 60.54%\n",
            "Epoch [6/10], Training Loss: 0.893, Validation Accuracy: 59.96%\n",
            "Epoch [7/10], Training Loss: 0.881, Validation Accuracy: 59.52%\n",
            "Epoch [8/10], Training Loss: 0.863, Validation Accuracy: 60.14%\n",
            "Epoch [9/10], Training Loss: 0.853, Validation Accuracy: 59.33%\n",
            "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 59.42%\n",
            "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 59.61%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 59.89%\n",
            "Epoch [3/10], Training Loss: 0.926, Validation Accuracy: 59.43%\n",
            "Epoch [4/10], Training Loss: 0.907, Validation Accuracy: 59.39%\n",
            "Epoch [5/10], Training Loss: 0.900, Validation Accuracy: 59.64%\n",
            "Epoch [6/10], Training Loss: 0.868, Validation Accuracy: 60.05%\n",
            "Epoch [7/10], Training Loss: 0.849, Validation Accuracy: 59.52%\n",
            "Epoch [8/10], Training Loss: 0.841, Validation Accuracy: 59.64%\n",
            "Epoch [9/10], Training Loss: 0.823, Validation Accuracy: 59.94%\n",
            "Epoch [10/10], Training Loss: 0.816, Validation Accuracy: 59.76%\n",
            "Epoch [1/10], Training Loss: 1.045, Validation Accuracy: 58.89%\n",
            "Epoch [2/10], Training Loss: 0.991, Validation Accuracy: 60.21%\n",
            "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 59.86%\n",
            "Epoch [4/10], Training Loss: 0.936, Validation Accuracy: 60.37%\n",
            "Epoch [5/10], Training Loss: 0.911, Validation Accuracy: 60.57%\n",
            "Epoch [6/10], Training Loss: 0.896, Validation Accuracy: 60.36%\n",
            "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 59.95%\n",
            "Epoch [8/10], Training Loss: 0.863, Validation Accuracy: 59.81%\n",
            "Epoch [9/10], Training Loss: 0.846, Validation Accuracy: 59.88%\n",
            "Epoch [10/10], Training Loss: 0.831, Validation Accuracy: 59.40%\n",
            "Epoch [1/10], Training Loss: 1.021, Validation Accuracy: 59.98%\n",
            "Epoch [2/10], Training Loss: 0.957, Validation Accuracy: 59.12%\n",
            "Epoch [3/10], Training Loss: 0.927, Validation Accuracy: 60.83%\n",
            "Epoch [4/10], Training Loss: 0.902, Validation Accuracy: 60.91%\n",
            "Epoch [5/10], Training Loss: 0.878, Validation Accuracy: 60.02%\n",
            "Epoch [6/10], Training Loss: 0.861, Validation Accuracy: 60.58%\n",
            "Epoch [7/10], Training Loss: 0.846, Validation Accuracy: 60.20%\n",
            "Epoch [8/10], Training Loss: 0.838, Validation Accuracy: 59.35%\n",
            "Epoch [9/10], Training Loss: 0.820, Validation Accuracy: 60.18%\n",
            "Epoch [10/10], Training Loss: 0.803, Validation Accuracy: 60.27%\n",
            "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.82%\n",
            "Epoch [2/10], Training Loss: 0.952, Validation Accuracy: 60.33%\n",
            "Epoch [3/10], Training Loss: 0.911, Validation Accuracy: 60.38%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 59.98%\n",
            "Epoch [5/10], Training Loss: 0.873, Validation Accuracy: 60.02%\n",
            "Epoch [6/10], Training Loss: 0.853, Validation Accuracy: 60.48%\n",
            "Epoch [7/10], Training Loss: 0.833, Validation Accuracy: 60.22%\n",
            "Epoch [8/10], Training Loss: 0.812, Validation Accuracy: 60.46%\n",
            "Epoch [9/10], Training Loss: 0.801, Validation Accuracy: 60.47%\n",
            "Epoch [10/10], Training Loss: 0.787, Validation Accuracy: 60.16%\n",
            "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 60.16%\n",
            "Epoch [2/10], Training Loss: 0.909, Validation Accuracy: 60.82%\n",
            "Epoch [3/10], Training Loss: 0.870, Validation Accuracy: 60.16%\n",
            "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.810, Validation Accuracy: 60.38%\n",
            "Epoch [6/10], Training Loss: 0.808, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.772, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.761, Validation Accuracy: 60.30%\n",
            "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 60.92%\n",
            "Epoch [10/10], Training Loss: 0.726, Validation Accuracy: 60.45%\n",
            "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 60.20%\n",
            "Epoch [2/10], Training Loss: 0.875, Validation Accuracy: 61.38%\n",
            "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.814, Validation Accuracy: 60.25%\n",
            "Epoch [5/10], Training Loss: 0.787, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.769, Validation Accuracy: 60.80%\n",
            "Epoch [7/10], Training Loss: 0.753, Validation Accuracy: 60.75%\n",
            "Epoch [8/10], Training Loss: 0.736, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.709, Validation Accuracy: 60.59%\n",
            "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 61.07%\n",
            "Epoch [2/10], Training Loss: 0.911, Validation Accuracy: 61.22%\n",
            "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 60.83%\n",
            "Epoch [4/10], Training Loss: 0.840, Validation Accuracy: 60.81%\n",
            "Epoch [5/10], Training Loss: 0.828, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.802, Validation Accuracy: 60.86%\n",
            "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 60.54%\n",
            "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 60.48%\n",
            "Epoch [9/10], Training Loss: 0.745, Validation Accuracy: 60.97%\n",
            "Epoch [10/10], Training Loss: 0.729, Validation Accuracy: 61.24%\n",
            "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 60.95%\n",
            "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.15%\n",
            "Epoch [3/10], Training Loss: 0.850, Validation Accuracy: 61.34%\n",
            "Epoch [4/10], Training Loss: 0.826, Validation Accuracy: 60.99%\n",
            "Epoch [5/10], Training Loss: 0.811, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.782, Validation Accuracy: 59.86%\n",
            "Epoch [7/10], Training Loss: 0.751, Validation Accuracy: 61.18%\n",
            "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.719, Validation Accuracy: 61.40%\n",
            "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 61.00%\n",
            "Epoch [2/10], Training Loss: 0.882, Validation Accuracy: 60.78%\n",
            "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.803, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 60.74%\n",
            "Epoch [6/10], Training Loss: 0.763, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.742, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 60.83%\n",
            "Epoch [9/10], Training Loss: 0.695, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.695, Validation Accuracy: 60.51%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.833, Validation Accuracy: 60.83%\n",
            "Epoch [3/10], Training Loss: 0.793, Validation Accuracy: 60.96%\n",
            "Epoch [4/10], Training Loss: 0.754, Validation Accuracy: 61.89%\n",
            "Epoch [5/10], Training Loss: 0.730, Validation Accuracy: 61.14%\n",
            "Epoch [6/10], Training Loss: 0.701, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.677, Validation Accuracy: 60.77%\n",
            "Epoch [8/10], Training Loss: 0.682, Validation Accuracy: 61.25%\n",
            "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 61.27%\n",
            "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 60.71%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.26%\n",
            "Epoch [2/10], Training Loss: 0.803, Validation Accuracy: 60.06%\n",
            "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 61.66%\n",
            "Epoch [4/10], Training Loss: 0.734, Validation Accuracy: 61.29%\n",
            "Epoch [5/10], Training Loss: 0.708, Validation Accuracy: 61.63%\n",
            "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.662, Validation Accuracy: 60.89%\n",
            "Epoch [8/10], Training Loss: 0.649, Validation Accuracy: 60.80%\n",
            "Epoch [9/10], Training Loss: 0.632, Validation Accuracy: 60.95%\n",
            "Epoch [10/10], Training Loss: 0.610, Validation Accuracy: 60.81%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 61.21%\n",
            "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 61.15%\n",
            "Epoch [3/10], Training Loss: 0.788, Validation Accuracy: 61.61%\n",
            "Epoch [4/10], Training Loss: 0.755, Validation Accuracy: 61.58%\n",
            "Epoch [5/10], Training Loss: 0.736, Validation Accuracy: 61.45%\n",
            "Epoch [6/10], Training Loss: 0.709, Validation Accuracy: 61.61%\n",
            "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 60.51%\n",
            "Epoch [8/10], Training Loss: 0.681, Validation Accuracy: 60.56%\n",
            "Epoch [9/10], Training Loss: 0.658, Validation Accuracy: 61.58%\n",
            "Epoch [10/10], Training Loss: 0.635, Validation Accuracy: 61.31%\n",
            "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 60.82%\n",
            "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.04%\n",
            "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 60.55%\n",
            "Epoch [4/10], Training Loss: 0.740, Validation Accuracy: 61.68%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.683, Validation Accuracy: 61.59%\n",
            "Epoch [7/10], Training Loss: 0.661, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.646, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.620, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 61.01%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.93%\n",
            "Epoch [2/10], Training Loss: 0.817, Validation Accuracy: 61.83%\n",
            "Epoch [3/10], Training Loss: 0.765, Validation Accuracy: 61.43%\n",
            "Epoch [4/10], Training Loss: 0.731, Validation Accuracy: 61.78%\n",
            "Epoch [5/10], Training Loss: 0.698, Validation Accuracy: 61.44%\n",
            "Epoch [6/10], Training Loss: 0.680, Validation Accuracy: 60.58%\n",
            "Epoch [7/10], Training Loss: 0.657, Validation Accuracy: 60.64%\n",
            "Epoch [8/10], Training Loss: 0.625, Validation Accuracy: 61.10%\n",
            "Epoch [9/10], Training Loss: 0.606, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.856, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 61.22%\n",
            "Epoch [3/10], Training Loss: 0.710, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 61.09%\n",
            "Epoch [5/10], Training Loss: 0.653, Validation Accuracy: 61.30%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.46%\n",
            "Epoch [7/10], Training Loss: 0.585, Validation Accuracy: 61.20%\n",
            "Epoch [8/10], Training Loss: 0.575, Validation Accuracy: 61.28%\n",
            "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 60.94%\n",
            "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 61.48%\n",
            "Epoch [1/10], Training Loss: 0.850, Validation Accuracy: 60.52%\n",
            "Epoch [2/10], Training Loss: 0.750, Validation Accuracy: 61.53%\n",
            "Epoch [3/10], Training Loss: 0.694, Validation Accuracy: 61.45%\n",
            "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 61.22%\n",
            "Epoch [5/10], Training Loss: 0.638, Validation Accuracy: 60.74%\n",
            "Epoch [6/10], Training Loss: 0.602, Validation Accuracy: 61.21%\n",
            "Epoch [7/10], Training Loss: 0.583, Validation Accuracy: 60.89%\n",
            "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.546, Validation Accuracy: 61.03%\n",
            "Epoch [10/10], Training Loss: 0.519, Validation Accuracy: 60.52%\n",
            "Epoch [1/10], Training Loss: 0.889, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.778, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.734, Validation Accuracy: 61.07%\n",
            "Epoch [4/10], Training Loss: 0.684, Validation Accuracy: 61.42%\n",
            "Epoch [5/10], Training Loss: 0.647, Validation Accuracy: 61.78%\n",
            "Epoch [6/10], Training Loss: 0.622, Validation Accuracy: 60.76%\n",
            "Epoch [7/10], Training Loss: 0.599, Validation Accuracy: 61.21%\n",
            "Epoch [8/10], Training Loss: 0.580, Validation Accuracy: 60.98%\n",
            "Epoch [9/10], Training Loss: 0.555, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 60.90%\n",
            "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.756, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.703, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 61.41%\n",
            "Epoch [5/10], Training Loss: 0.618, Validation Accuracy: 61.70%\n",
            "Epoch [6/10], Training Loss: 0.588, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.568, Validation Accuracy: 61.66%\n",
            "Epoch [8/10], Training Loss: 0.547, Validation Accuracy: 61.19%\n",
            "Epoch [9/10], Training Loss: 0.530, Validation Accuracy: 61.06%\n",
            "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 61.69%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 61.57%\n",
            "Epoch [2/10], Training Loss: 0.749, Validation Accuracy: 61.34%\n",
            "Epoch [3/10], Training Loss: 0.695, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.660, Validation Accuracy: 61.09%\n",
            "Epoch [5/10], Training Loss: 0.618, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.587, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.542, Validation Accuracy: 60.64%\n",
            "Epoch [9/10], Training Loss: 0.530, Validation Accuracy: 60.77%\n",
            "Epoch [10/10], Training Loss: 0.500, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.828, Validation Accuracy: 61.12%\n",
            "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 61.19%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.602, Validation Accuracy: 61.42%\n",
            "Epoch [5/10], Training Loss: 0.555, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.536, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.511, Validation Accuracy: 60.96%\n",
            "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 60.68%\n",
            "Epoch [9/10], Training Loss: 0.471, Validation Accuracy: 61.48%\n",
            "Epoch [10/10], Training Loss: 0.442, Validation Accuracy: 60.90%\n",
            "Epoch [1/10], Training Loss: 0.797, Validation Accuracy: 60.49%\n",
            "Epoch [2/10], Training Loss: 0.697, Validation Accuracy: 60.83%\n",
            "Epoch [3/10], Training Loss: 0.621, Validation Accuracy: 61.30%\n",
            "Epoch [4/10], Training Loss: 0.582, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.550, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.515, Validation Accuracy: 60.60%\n",
            "Epoch [7/10], Training Loss: 0.495, Validation Accuracy: 61.07%\n",
            "Epoch [8/10], Training Loss: 0.465, Validation Accuracy: 60.57%\n",
            "Epoch [9/10], Training Loss: 0.450, Validation Accuracy: 61.08%\n",
            "Epoch [10/10], Training Loss: 0.429, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 0.848, Validation Accuracy: 60.84%\n",
            "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.658, Validation Accuracy: 61.16%\n",
            "Epoch [4/10], Training Loss: 0.604, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.568, Validation Accuracy: 60.87%\n",
            "Epoch [6/10], Training Loss: 0.553, Validation Accuracy: 60.69%\n",
            "Epoch [7/10], Training Loss: 0.516, Validation Accuracy: 61.60%\n",
            "Epoch [8/10], Training Loss: 0.494, Validation Accuracy: 60.91%\n",
            "Epoch [9/10], Training Loss: 0.481, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.455, Validation Accuracy: 61.19%\n",
            "Epoch [1/10], Training Loss: 0.821, Validation Accuracy: 61.22%\n",
            "Epoch [2/10], Training Loss: 0.682, Validation Accuracy: 61.71%\n",
            "Epoch [3/10], Training Loss: 0.623, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.582, Validation Accuracy: 60.81%\n",
            "Epoch [5/10], Training Loss: 0.545, Validation Accuracy: 61.38%\n",
            "Epoch [6/10], Training Loss: 0.514, Validation Accuracy: 61.57%\n",
            "Epoch [7/10], Training Loss: 0.500, Validation Accuracy: 61.52%\n",
            "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.447, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 0.817, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.683, Validation Accuracy: 61.65%\n",
            "Epoch [3/10], Training Loss: 0.618, Validation Accuracy: 61.12%\n",
            "Epoch [4/10], Training Loss: 0.573, Validation Accuracy: 61.11%\n",
            "Epoch [5/10], Training Loss: 0.537, Validation Accuracy: 60.95%\n",
            "Epoch [6/10], Training Loss: 0.514, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.474, Validation Accuracy: 60.96%\n",
            "Epoch [8/10], Training Loss: 0.455, Validation Accuracy: 60.49%\n",
            "Epoch [9/10], Training Loss: 0.433, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 60.34%\n",
            "Epoch [1/10], Training Loss: 0.791, Validation Accuracy: 60.47%\n",
            "Epoch [2/10], Training Loss: 0.650, Validation Accuracy: 59.43%\n",
            "Epoch [3/10], Training Loss: 0.584, Validation Accuracy: 60.48%\n",
            "Epoch [4/10], Training Loss: 0.537, Validation Accuracy: 61.41%\n",
            "Epoch [5/10], Training Loss: 0.482, Validation Accuracy: 61.16%\n",
            "Epoch [6/10], Training Loss: 0.475, Validation Accuracy: 60.67%\n",
            "Epoch [7/10], Training Loss: 0.430, Validation Accuracy: 60.86%\n",
            "Epoch [8/10], Training Loss: 0.408, Validation Accuracy: 61.11%\n",
            "Epoch [9/10], Training Loss: 0.385, Validation Accuracy: 60.45%\n",
            "Epoch [10/10], Training Loss: 0.365, Validation Accuracy: 60.91%\n",
            "Confusion Matrix:\n",
            "[[641  31  89  28  23   7  13  12 100  56]\n",
            " [ 30 727  16  11   8  15  12   3  34 144]\n",
            " [ 61  10 577  77  84  55  63  40  11  22]\n",
            " [ 29  16 123 409  58 156  82  65  25  37]\n",
            " [ 24   7 149  67 505  50  75  93  20  10]\n",
            " [ 23  11 130 164  46 480  47  81   8  10]\n",
            " [ 12   9  81  64  47  44 698  10  12  23]\n",
            " [ 19   9  58  44  64  63  15 690   2  36]\n",
            " [ 86  52  29  24  13  10   6   2 737  41]\n",
            " [ 40 126  16  26   6  13  23  20  38 692]]\n",
            "Test Accuracy: 61.56%\n",
            "True Positives (TP): [641 727 577 409 505 480 698 690 737 692]\n",
            "False Positives (FP): [324 271 691 505 349 413 336 326 250 379]\n",
            "True Negatives (TN): [8676 8729 8309 8495 8651 8587 8664 8674 8750 8621]\n",
            "False Negatives (FN): [359 273 423 591 495 520 302 310 263 308]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.6642487  0.72845691 0.45504732 0.44748359 0.59133489 0.537514\n",
            " 0.67504836 0.67913386 0.74670719 0.64612512]\n",
            "Recall: [0.641 0.727 0.577 0.409 0.505 0.48  0.698 0.69  0.737 0.692]\n",
            "F1 Score: [0.6524173  0.72772773 0.50881834 0.42737722 0.54476807 0.50713154\n",
            " 0.68633235 0.68452381 0.74182184 0.6682762 ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE,  distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average =  augmented_data_truncated\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6li7kc3ot-B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPdKra2qnYGr",
        "outputId": "3cf07436-fcc9-45f0-f7db-cac6fc33a385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracies: [9.09, 8.8, 8.83, 8.95, 9.52, 9.62, 9.65, 9.66, 9.66, 9.66, 9.66, 9.72, 9.78, 10.35, 11.38, 13.31, 14.51, 15.77, 16.55, 16.95, 17.66, 21.05, 23.12, 24.57, 26.33, 27.11, 28.22, 28.16, 29.41, 30.06, 30.18, 31.11, 31.47, 31.89, 32.67, 32.83, 33.57, 34.17, 34.26, 35.14, 35.42, 36.06, 36.02, 36.89, 37.26, 37.56, 38.08, 38.22, 38.43, 39.11, 39.45, 40.51, 39.55, 40.74, 41.25, 41.52, 40.7, 42.09, 42.08, 42.14, 43.02, 42.68, 43.28, 43.59, 43.75, 44.03, 43.75, 44.79, 44.87, 44.57, 45.16, 45.63, 45.46, 45.66, 46.49, 46.26, 45.98, 46.3, 46.2, 46.86, 46.61, 46.93, 47.17, 47.88, 47.29, 47.72, 48.24, 48.99, 48.79, 48.2, 48.34, 49.19, 49.54, 49.72, 49.56, 49.58, 49.54, 49.97, 50.01, 50.39, 49.71, 50.78, 50.1, 50.62, 49.97, 51.72, 51.33, 51.54, 51.24, 50.79, 51.82, 52.07, 52.53, 52.64, 52.13, 52.96, 52.76, 52.55, 52.14, 52.86, 51.95, 53.17, 53.91, 53.38, 53.74, 53.32, 52.78, 54.22, 53.65, 53.61, 52.64, 53.96, 53.72, 53.51, 54.83, 54.7, 54.61, 54.85, 54.05, 54.72, 54.6, 54.76, 55.29, 55.2, 55.93, 54.46, 54.13, 55.33, 55.26, 55.66, 55.33, 54.9, 55.48, 55.43, 56.41, 55.6, 56.08, 55.76, 56.72, 55.63, 56.35, 56.21, 56.31, 55.99, 56.52, 55.86, 56.58, 56.63, 56.68, 56.09, 57.04, 56.76, 57.11, 57.17, 57.3, 56.35, 57.42, 57.16, 57.09, 57.35, 57.35, 57.02, 57.45, 55.62, 56.74, 58.15, 57.47, 57.08, 56.93, 57.03, 57.07, 57.69, 58.45, 58.1, 57.61, 58.01, 58.58, 57.86, 57.86, 57.49, 57.98, 57.7, 58.55, 58.33, 58.11, 58.35, 58.51, 58.2, 57.1, 58.33, 57.85, 58.97, 58.44, 59.06, 58.88, 59.02, 59.29, 58.66, 58.55, 58.77, 59.16, 58.63, 58.62, 59.3, 58.45, 58.58, 58.68, 58.74, 58.46, 58.81, 57.98, 59.57, 58.96, 58.89, 59.78, 58.99, 59.33, 59.39, 58.27, 58.94, 59.67, 59.57, 60.14, 58.64, 60.54, 59.96, 59.52, 60.14, 59.33, 59.42, 59.61, 59.89, 59.43, 59.39, 59.64, 60.05, 59.52, 59.64, 59.94, 59.76, 58.89, 60.21, 59.86, 60.37, 60.57, 60.36, 59.95, 59.81, 59.88, 59.4, 59.98, 59.12, 60.83, 60.91, 60.02, 60.58, 60.2, 59.35, 60.18, 60.27, 60.82, 60.33, 60.38, 59.98, 60.02, 60.48, 60.22, 60.46, 60.47, 60.16, 60.16, 60.82, 60.16, 60.75, 60.38, 61.06, 60.95, 60.3, 60.92, 60.45, 60.2, 61.38, 61.17, 60.25, 61.42, 60.8, 60.75, 60.38, 60.67, 60.59, 61.07, 61.22, 60.83, 60.81, 61.11, 60.86, 60.54, 60.48, 60.97, 61.24, 60.95, 61.15, 61.34, 60.99, 61.79, 59.86, 61.18, 61.49, 61.4, 60.64, 61.0, 60.78, 61.46, 61.31, 60.74, 60.59, 61.3, 60.83, 60.7, 60.51, 61.67, 60.83, 60.96, 61.89, 61.14, 61.02, 60.77, 61.25, 61.27, 60.71, 61.26, 60.06, 61.66, 61.29, 61.63, 61.06, 60.89, 60.8, 60.95, 60.81, 61.21, 61.15, 61.61, 61.58, 61.45, 61.61, 60.51, 60.56, 61.58, 61.31, 60.82, 61.04, 60.55, 61.68, 61.8, 61.59, 61.35, 60.9, 60.66, 61.01, 60.93, 61.83, 61.43, 61.78, 61.44, 60.58, 60.64, 61.1, 60.48, 60.75, 60.63, 61.22, 61.46, 61.09, 61.3, 61.46, 61.2, 61.28, 60.94, 61.48, 60.52, 61.53, 61.45, 61.22, 60.74, 61.21, 60.89, 60.75, 61.03, 60.52, 60.63, 60.97, 61.07, 61.42, 61.78, 60.76, 61.21, 60.98, 61.16, 60.9, 60.63, 61.39, 61.05, 61.41, 61.7, 61.47, 61.66, 61.19, 61.06, 61.69, 61.57, 61.34, 61.22, 61.09, 61.11, 61.15, 61.3, 60.64, 60.77, 60.86, 61.12, 61.19, 60.97, 61.42, 61.11, 61.23, 60.96, 60.68, 61.48, 60.9, 60.49, 60.83, 61.3, 61.18, 61.41, 60.6, 61.07, 60.57, 61.08, 60.64, 60.84, 60.76, 61.16, 60.94, 60.87, 60.69, 61.6, 60.91, 60.83, 61.19, 61.22, 61.71, 60.64, 60.81, 61.38, 61.57, 61.52, 61.03, 60.76, 60.42, 60.97, 61.65, 61.12, 61.11, 60.95, 61.18, 60.96, 60.49, 60.64, 60.34, 60.47, 59.43, 60.48, 61.41, 61.16, 60.67, 60.86, 61.11, 60.45, 60.91]\n",
            "Size of array: 500\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 9.09%\n",
        "Epoch [2/10], Training Loss: 2.302, Validation Accuracy: 8.80%\n",
        "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 8.83%\n",
        "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 8.95%\n",
        "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 9.52%\n",
        "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 9.62%\n",
        "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 9.65%\n",
        "Epoch [8/10], Training Loss: 2.300, Validation Accuracy: 9.66%\n",
        "Epoch [9/10], Training Loss: 2.299, Validation Accuracy: 9.66%\n",
        "Epoch [10/10], Training Loss: 2.298, Validation Accuracy: 9.66%\n",
        "Epoch [1/10], Training Loss: 2.299, Validation Accuracy: 9.66%\n",
        "Epoch [2/10], Training Loss: 2.298, Validation Accuracy: 9.72%\n",
        "Epoch [3/10], Training Loss: 2.296, Validation Accuracy: 9.78%\n",
        "Epoch [4/10], Training Loss: 2.294, Validation Accuracy: 10.35%\n",
        "Epoch [5/10], Training Loss: 2.292, Validation Accuracy: 11.38%\n",
        "Epoch [6/10], Training Loss: 2.288, Validation Accuracy: 13.31%\n",
        "Epoch [7/10], Training Loss: 2.283, Validation Accuracy: 14.51%\n",
        "Epoch [8/10], Training Loss: 2.274, Validation Accuracy: 15.77%\n",
        "Epoch [9/10], Training Loss: 2.260, Validation Accuracy: 16.55%\n",
        "Epoch [10/10], Training Loss: 2.240, Validation Accuracy: 16.95%\n",
        "Epoch [1/10], Training Loss: 2.213, Validation Accuracy: 17.66%\n",
        "Epoch [2/10], Training Loss: 2.179, Validation Accuracy: 21.05%\n",
        "Epoch [3/10], Training Loss: 2.141, Validation Accuracy: 23.12%\n",
        "Epoch [4/10], Training Loss: 2.099, Validation Accuracy: 24.57%\n",
        "Epoch [5/10], Training Loss: 2.058, Validation Accuracy: 26.33%\n",
        "Epoch [6/10], Training Loss: 2.021, Validation Accuracy: 27.11%\n",
        "Epoch [7/10], Training Loss: 1.989, Validation Accuracy: 28.22%\n",
        "Epoch [8/10], Training Loss: 1.963, Validation Accuracy: 28.16%\n",
        "Epoch [9/10], Training Loss: 1.941, Validation Accuracy: 29.41%\n",
        "Epoch [10/10], Training Loss: 1.919, Validation Accuracy: 30.06%\n",
        "Epoch [1/10], Training Loss: 1.902, Validation Accuracy: 30.18%\n",
        "Epoch [2/10], Training Loss: 1.881, Validation Accuracy: 31.11%\n",
        "Epoch [3/10], Training Loss: 1.864, Validation Accuracy: 31.47%\n",
        "Epoch [4/10], Training Loss: 1.851, Validation Accuracy: 31.89%\n",
        "Epoch [5/10], Training Loss: 1.836, Validation Accuracy: 32.67%\n",
        "Epoch [6/10], Training Loss: 1.820, Validation Accuracy: 32.83%\n",
        "Epoch [7/10], Training Loss: 1.806, Validation Accuracy: 33.57%\n",
        "Epoch [8/10], Training Loss: 1.793, Validation Accuracy: 34.17%\n",
        "Epoch [9/10], Training Loss: 1.779, Validation Accuracy: 34.26%\n",
        "Epoch [10/10], Training Loss: 1.769, Validation Accuracy: 35.14%\n",
        "Epoch [1/10], Training Loss: 1.769, Validation Accuracy: 35.42%\n",
        "Epoch [2/10], Training Loss: 1.752, Validation Accuracy: 36.06%\n",
        "Epoch [3/10], Training Loss: 1.735, Validation Accuracy: 36.02%\n",
        "Epoch [4/10], Training Loss: 1.725, Validation Accuracy: 36.89%\n",
        "Epoch [5/10], Training Loss: 1.708, Validation Accuracy: 37.26%\n",
        "Epoch [6/10], Training Loss: 1.702, Validation Accuracy: 37.56%\n",
        "Epoch [7/10], Training Loss: 1.686, Validation Accuracy: 38.08%\n",
        "Epoch [8/10], Training Loss: 1.671, Validation Accuracy: 38.22%\n",
        "Epoch [9/10], Training Loss: 1.663, Validation Accuracy: 38.43%\n",
        "Epoch [10/10], Training Loss: 1.651, Validation Accuracy: 39.11%\n",
        "Epoch [1/10], Training Loss: 1.651, Validation Accuracy: 39.45%\n",
        "Epoch [2/10], Training Loss: 1.633, Validation Accuracy: 40.51%\n",
        "Epoch [3/10], Training Loss: 1.623, Validation Accuracy: 39.55%\n",
        "Epoch [4/10], Training Loss: 1.607, Validation Accuracy: 40.74%\n",
        "Epoch [5/10], Training Loss: 1.593, Validation Accuracy: 41.25%\n",
        "Epoch [6/10], Training Loss: 1.583, Validation Accuracy: 41.52%\n",
        "Epoch [7/10], Training Loss: 1.575, Validation Accuracy: 40.70%\n",
        "Epoch [8/10], Training Loss: 1.559, Validation Accuracy: 42.09%\n",
        "Epoch [9/10], Training Loss: 1.552, Validation Accuracy: 42.08%\n",
        "Epoch [10/10], Training Loss: 1.540, Validation Accuracy: 42.14%\n",
        "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 43.02%\n",
        "Epoch [2/10], Training Loss: 1.570, Validation Accuracy: 42.68%\n",
        "Epoch [3/10], Training Loss: 1.554, Validation Accuracy: 43.28%\n",
        "Epoch [4/10], Training Loss: 1.550, Validation Accuracy: 43.59%\n",
        "Epoch [5/10], Training Loss: 1.533, Validation Accuracy: 43.75%\n",
        "Epoch [6/10], Training Loss: 1.524, Validation Accuracy: 44.03%\n",
        "Epoch [7/10], Training Loss: 1.516, Validation Accuracy: 43.75%\n",
        "Epoch [8/10], Training Loss: 1.503, Validation Accuracy: 44.79%\n",
        "Epoch [9/10], Training Loss: 1.494, Validation Accuracy: 44.87%\n",
        "Epoch [10/10], Training Loss: 1.490, Validation Accuracy: 44.57%\n",
        "Epoch [1/10], Training Loss: 1.514, Validation Accuracy: 45.16%\n",
        "Epoch [2/10], Training Loss: 1.495, Validation Accuracy: 45.63%\n",
        "Epoch [3/10], Training Loss: 1.482, Validation Accuracy: 45.46%\n",
        "Epoch [4/10], Training Loss: 1.470, Validation Accuracy: 45.66%\n",
        "Epoch [5/10], Training Loss: 1.462, Validation Accuracy: 46.49%\n",
        "Epoch [6/10], Training Loss: 1.457, Validation Accuracy: 46.26%\n",
        "Epoch [7/10], Training Loss: 1.445, Validation Accuracy: 45.98%\n",
        "Epoch [8/10], Training Loss: 1.437, Validation Accuracy: 46.30%\n",
        "Epoch [9/10], Training Loss: 1.434, Validation Accuracy: 46.20%\n",
        "Epoch [10/10], Training Loss: 1.417, Validation Accuracy: 46.86%\n",
        "Epoch [1/10], Training Loss: 1.460, Validation Accuracy: 46.61%\n",
        "Epoch [2/10], Training Loss: 1.449, Validation Accuracy: 46.93%\n",
        "Epoch [3/10], Training Loss: 1.430, Validation Accuracy: 47.17%\n",
        "Epoch [4/10], Training Loss: 1.420, Validation Accuracy: 47.88%\n",
        "Epoch [5/10], Training Loss: 1.416, Validation Accuracy: 47.29%\n",
        "Epoch [6/10], Training Loss: 1.403, Validation Accuracy: 47.72%\n",
        "Epoch [7/10], Training Loss: 1.395, Validation Accuracy: 48.24%\n",
        "Epoch [8/10], Training Loss: 1.383, Validation Accuracy: 48.99%\n",
        "Epoch [9/10], Training Loss: 1.377, Validation Accuracy: 48.79%\n",
        "Epoch [10/10], Training Loss: 1.367, Validation Accuracy: 48.20%\n",
        "Epoch [1/10], Training Loss: 1.410, Validation Accuracy: 48.34%\n",
        "Epoch [2/10], Training Loss: 1.389, Validation Accuracy: 49.19%\n",
        "Epoch [3/10], Training Loss: 1.375, Validation Accuracy: 49.54%\n",
        "Epoch [4/10], Training Loss: 1.363, Validation Accuracy: 49.72%\n",
        "Epoch [5/10], Training Loss: 1.353, Validation Accuracy: 49.56%\n",
        "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 49.58%\n",
        "Epoch [7/10], Training Loss: 1.337, Validation Accuracy: 49.54%\n",
        "Epoch [8/10], Training Loss: 1.320, Validation Accuracy: 49.97%\n",
        "Epoch [9/10], Training Loss: 1.315, Validation Accuracy: 50.01%\n",
        "Epoch [10/10], Training Loss: 1.308, Validation Accuracy: 50.39%\n",
        "Epoch [1/10], Training Loss: 1.351, Validation Accuracy: 49.71%\n",
        "Epoch [2/10], Training Loss: 1.335, Validation Accuracy: 50.78%\n",
        "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 50.10%\n",
        "Epoch [4/10], Training Loss: 1.308, Validation Accuracy: 50.62%\n",
        "Epoch [5/10], Training Loss: 1.296, Validation Accuracy: 49.97%\n",
        "Epoch [6/10], Training Loss: 1.289, Validation Accuracy: 51.72%\n",
        "Epoch [7/10], Training Loss: 1.268, Validation Accuracy: 51.33%\n",
        "Epoch [8/10], Training Loss: 1.259, Validation Accuracy: 51.54%\n",
        "Epoch [9/10], Training Loss: 1.257, Validation Accuracy: 51.24%\n",
        "Epoch [10/10], Training Loss: 1.241, Validation Accuracy: 50.79%\n",
        "Epoch [1/10], Training Loss: 1.341, Validation Accuracy: 51.82%\n",
        "Epoch [2/10], Training Loss: 1.322, Validation Accuracy: 52.07%\n",
        "Epoch [3/10], Training Loss: 1.302, Validation Accuracy: 52.53%\n",
        "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 52.64%\n",
        "Epoch [5/10], Training Loss: 1.278, Validation Accuracy: 52.13%\n",
        "Epoch [6/10], Training Loss: 1.266, Validation Accuracy: 52.96%\n",
        "Epoch [7/10], Training Loss: 1.268, Validation Accuracy: 52.76%\n",
        "Epoch [8/10], Training Loss: 1.246, Validation Accuracy: 52.55%\n",
        "Epoch [9/10], Training Loss: 1.242, Validation Accuracy: 52.14%\n",
        "Epoch [10/10], Training Loss: 1.230, Validation Accuracy: 52.86%\n",
        "Epoch [1/10], Training Loss: 1.288, Validation Accuracy: 51.95%\n",
        "Epoch [2/10], Training Loss: 1.275, Validation Accuracy: 53.17%\n",
        "Epoch [3/10], Training Loss: 1.251, Validation Accuracy: 53.91%\n",
        "Epoch [4/10], Training Loss: 1.231, Validation Accuracy: 53.38%\n",
        "Epoch [5/10], Training Loss: 1.228, Validation Accuracy: 53.74%\n",
        "Epoch [6/10], Training Loss: 1.207, Validation Accuracy: 53.32%\n",
        "Epoch [7/10], Training Loss: 1.201, Validation Accuracy: 52.78%\n",
        "Epoch [8/10], Training Loss: 1.202, Validation Accuracy: 54.22%\n",
        "Epoch [9/10], Training Loss: 1.191, Validation Accuracy: 53.65%\n",
        "Epoch [10/10], Training Loss: 1.174, Validation Accuracy: 53.61%\n",
        "Epoch [1/10], Training Loss: 1.274, Validation Accuracy: 52.64%\n",
        "Epoch [2/10], Training Loss: 1.239, Validation Accuracy: 53.96%\n",
        "Epoch [3/10], Training Loss: 1.230, Validation Accuracy: 53.72%\n",
        "Epoch [4/10], Training Loss: 1.220, Validation Accuracy: 53.51%\n",
        "Epoch [5/10], Training Loss: 1.207, Validation Accuracy: 54.83%\n",
        "Epoch [6/10], Training Loss: 1.194, Validation Accuracy: 54.70%\n",
        "Epoch [7/10], Training Loss: 1.185, Validation Accuracy: 54.61%\n",
        "Epoch [8/10], Training Loss: 1.181, Validation Accuracy: 54.85%\n",
        "Epoch [9/10], Training Loss: 1.166, Validation Accuracy: 54.05%\n",
        "Epoch [10/10], Training Loss: 1.156, Validation Accuracy: 54.72%\n",
        "Epoch [1/10], Training Loss: 1.222, Validation Accuracy: 54.60%\n",
        "Epoch [2/10], Training Loss: 1.205, Validation Accuracy: 54.76%\n",
        "Epoch [3/10], Training Loss: 1.179, Validation Accuracy: 55.29%\n",
        "Epoch [4/10], Training Loss: 1.159, Validation Accuracy: 55.20%\n",
        "Epoch [5/10], Training Loss: 1.155, Validation Accuracy: 55.93%\n",
        "Epoch [6/10], Training Loss: 1.146, Validation Accuracy: 54.46%\n",
        "Epoch [7/10], Training Loss: 1.132, Validation Accuracy: 54.13%\n",
        "Epoch [8/10], Training Loss: 1.115, Validation Accuracy: 55.33%\n",
        "Epoch [9/10], Training Loss: 1.115, Validation Accuracy: 55.26%\n",
        "Epoch [10/10], Training Loss: 1.110, Validation Accuracy: 55.66%\n",
        "Epoch [1/10], Training Loss: 1.188, Validation Accuracy: 55.33%\n",
        "Epoch [2/10], Training Loss: 1.165, Validation Accuracy: 54.90%\n",
        "Epoch [3/10], Training Loss: 1.145, Validation Accuracy: 55.48%\n",
        "Epoch [4/10], Training Loss: 1.134, Validation Accuracy: 55.43%\n",
        "Epoch [5/10], Training Loss: 1.119, Validation Accuracy: 56.41%\n",
        "Epoch [6/10], Training Loss: 1.107, Validation Accuracy: 55.60%\n",
        "Epoch [7/10], Training Loss: 1.091, Validation Accuracy: 56.08%\n",
        "Epoch [8/10], Training Loss: 1.081, Validation Accuracy: 55.76%\n",
        "Epoch [9/10], Training Loss: 1.072, Validation Accuracy: 56.72%\n",
        "Epoch [10/10], Training Loss: 1.062, Validation Accuracy: 55.63%\n",
        "Epoch [1/10], Training Loss: 1.202, Validation Accuracy: 56.35%\n",
        "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 56.21%\n",
        "Epoch [3/10], Training Loss: 1.159, Validation Accuracy: 56.31%\n",
        "Epoch [4/10], Training Loss: 1.141, Validation Accuracy: 55.99%\n",
        "Epoch [5/10], Training Loss: 1.133, Validation Accuracy: 56.52%\n",
        "Epoch [6/10], Training Loss: 1.120, Validation Accuracy: 55.86%\n",
        "Epoch [7/10], Training Loss: 1.102, Validation Accuracy: 56.58%\n",
        "Epoch [8/10], Training Loss: 1.092, Validation Accuracy: 56.63%\n",
        "Epoch [9/10], Training Loss: 1.082, Validation Accuracy: 56.68%\n",
        "Epoch [10/10], Training Loss: 1.071, Validation Accuracy: 56.09%\n",
        "Epoch [1/10], Training Loss: 1.168, Validation Accuracy: 57.04%\n",
        "Epoch [2/10], Training Loss: 1.134, Validation Accuracy: 56.76%\n",
        "Epoch [3/10], Training Loss: 1.117, Validation Accuracy: 57.11%\n",
        "Epoch [4/10], Training Loss: 1.098, Validation Accuracy: 57.17%\n",
        "Epoch [5/10], Training Loss: 1.081, Validation Accuracy: 57.30%\n",
        "Epoch [6/10], Training Loss: 1.066, Validation Accuracy: 56.35%\n",
        "Epoch [7/10], Training Loss: 1.057, Validation Accuracy: 57.42%\n",
        "Epoch [8/10], Training Loss: 1.041, Validation Accuracy: 57.16%\n",
        "Epoch [9/10], Training Loss: 1.029, Validation Accuracy: 57.09%\n",
        "Epoch [10/10], Training Loss: 1.021, Validation Accuracy: 57.35%\n",
        "Epoch [1/10], Training Loss: 1.147, Validation Accuracy: 57.35%\n",
        "Epoch [2/10], Training Loss: 1.122, Validation Accuracy: 57.02%\n",
        "Epoch [3/10], Training Loss: 1.103, Validation Accuracy: 57.45%\n",
        "Epoch [4/10], Training Loss: 1.091, Validation Accuracy: 55.62%\n",
        "Epoch [5/10], Training Loss: 1.077, Validation Accuracy: 56.74%\n",
        "Epoch [6/10], Training Loss: 1.053, Validation Accuracy: 58.15%\n",
        "Epoch [7/10], Training Loss: 1.044, Validation Accuracy: 57.47%\n",
        "Epoch [8/10], Training Loss: 1.037, Validation Accuracy: 57.08%\n",
        "Epoch [9/10], Training Loss: 1.026, Validation Accuracy: 56.93%\n",
        "Epoch [10/10], Training Loss: 1.011, Validation Accuracy: 57.03%\n",
        "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 57.07%\n",
        "Epoch [2/10], Training Loss: 1.082, Validation Accuracy: 57.69%\n",
        "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 58.45%\n",
        "Epoch [4/10], Training Loss: 1.036, Validation Accuracy: 58.10%\n",
        "Epoch [5/10], Training Loss: 1.020, Validation Accuracy: 57.61%\n",
        "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 58.01%\n",
        "Epoch [7/10], Training Loss: 0.996, Validation Accuracy: 58.58%\n",
        "Epoch [8/10], Training Loss: 0.981, Validation Accuracy: 57.86%\n",
        "Epoch [9/10], Training Loss: 0.972, Validation Accuracy: 57.86%\n",
        "Epoch [10/10], Training Loss: 0.962, Validation Accuracy: 57.49%\n",
        "Epoch [1/10], Training Loss: 1.090, Validation Accuracy: 57.98%\n",
        "Epoch [2/10], Training Loss: 1.046, Validation Accuracy: 57.70%\n",
        "Epoch [3/10], Training Loss: 1.030, Validation Accuracy: 58.55%\n",
        "Epoch [4/10], Training Loss: 1.011, Validation Accuracy: 58.33%\n",
        "Epoch [5/10], Training Loss: 0.994, Validation Accuracy: 58.11%\n",
        "Epoch [6/10], Training Loss: 0.984, Validation Accuracy: 58.35%\n",
        "Epoch [7/10], Training Loss: 0.966, Validation Accuracy: 58.51%\n",
        "Epoch [8/10], Training Loss: 0.950, Validation Accuracy: 58.20%\n",
        "Epoch [9/10], Training Loss: 0.941, Validation Accuracy: 57.10%\n",
        "Epoch [10/10], Training Loss: 0.924, Validation Accuracy: 58.33%\n",
        "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 57.85%\n",
        "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.97%\n",
        "Epoch [3/10], Training Loss: 1.055, Validation Accuracy: 58.44%\n",
        "Epoch [4/10], Training Loss: 1.031, Validation Accuracy: 59.06%\n",
        "Epoch [5/10], Training Loss: 1.010, Validation Accuracy: 58.88%\n",
        "Epoch [6/10], Training Loss: 0.996, Validation Accuracy: 59.02%\n",
        "Epoch [7/10], Training Loss: 0.984, Validation Accuracy: 59.29%\n",
        "Epoch [8/10], Training Loss: 0.963, Validation Accuracy: 58.66%\n",
        "Epoch [9/10], Training Loss: 0.959, Validation Accuracy: 58.55%\n",
        "Epoch [10/10], Training Loss: 0.945, Validation Accuracy: 58.77%\n",
        "Epoch [1/10], Training Loss: 1.081, Validation Accuracy: 59.16%\n",
        "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 58.63%\n",
        "Epoch [3/10], Training Loss: 1.021, Validation Accuracy: 58.62%\n",
        "Epoch [4/10], Training Loss: 0.990, Validation Accuracy: 59.30%\n",
        "Epoch [5/10], Training Loss: 0.976, Validation Accuracy: 58.45%\n",
        "Epoch [6/10], Training Loss: 0.968, Validation Accuracy: 58.58%\n",
        "Epoch [7/10], Training Loss: 0.943, Validation Accuracy: 58.68%\n",
        "Epoch [8/10], Training Loss: 0.936, Validation Accuracy: 58.74%\n",
        "Epoch [9/10], Training Loss: 0.920, Validation Accuracy: 58.46%\n",
        "Epoch [10/10], Training Loss: 0.907, Validation Accuracy: 58.81%\n",
        "Epoch [1/10], Training Loss: 1.076, Validation Accuracy: 57.98%\n",
        "Epoch [2/10], Training Loss: 1.037, Validation Accuracy: 59.57%\n",
        "Epoch [3/10], Training Loss: 1.004, Validation Accuracy: 58.96%\n",
        "Epoch [4/10], Training Loss: 0.980, Validation Accuracy: 58.89%\n",
        "Epoch [5/10], Training Loss: 0.972, Validation Accuracy: 59.78%\n",
        "Epoch [6/10], Training Loss: 0.947, Validation Accuracy: 58.99%\n",
        "Epoch [7/10], Training Loss: 0.940, Validation Accuracy: 59.33%\n",
        "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 59.39%\n",
        "Epoch [9/10], Training Loss: 0.918, Validation Accuracy: 58.27%\n",
        "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 58.94%\n",
        "Epoch [1/10], Training Loss: 1.041, Validation Accuracy: 59.67%\n",
        "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 59.57%\n",
        "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.14%\n",
        "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 58.64%\n",
        "Epoch [5/10], Training Loss: 0.916, Validation Accuracy: 60.54%\n",
        "Epoch [6/10], Training Loss: 0.893, Validation Accuracy: 59.96%\n",
        "Epoch [7/10], Training Loss: 0.881, Validation Accuracy: 59.52%\n",
        "Epoch [8/10], Training Loss: 0.863, Validation Accuracy: 60.14%\n",
        "Epoch [9/10], Training Loss: 0.853, Validation Accuracy: 59.33%\n",
        "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 59.42%\n",
        "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 59.61%\n",
        "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 59.89%\n",
        "Epoch [3/10], Training Loss: 0.926, Validation Accuracy: 59.43%\n",
        "Epoch [4/10], Training Loss: 0.907, Validation Accuracy: 59.39%\n",
        "Epoch [5/10], Training Loss: 0.900, Validation Accuracy: 59.64%\n",
        "Epoch [6/10], Training Loss: 0.868, Validation Accuracy: 60.05%\n",
        "Epoch [7/10], Training Loss: 0.849, Validation Accuracy: 59.52%\n",
        "Epoch [8/10], Training Loss: 0.841, Validation Accuracy: 59.64%\n",
        "Epoch [9/10], Training Loss: 0.823, Validation Accuracy: 59.94%\n",
        "Epoch [10/10], Training Loss: 0.816, Validation Accuracy: 59.76%\n",
        "Epoch [1/10], Training Loss: 1.045, Validation Accuracy: 58.89%\n",
        "Epoch [2/10], Training Loss: 0.991, Validation Accuracy: 60.21%\n",
        "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 59.86%\n",
        "Epoch [4/10], Training Loss: 0.936, Validation Accuracy: 60.37%\n",
        "Epoch [5/10], Training Loss: 0.911, Validation Accuracy: 60.57%\n",
        "Epoch [6/10], Training Loss: 0.896, Validation Accuracy: 60.36%\n",
        "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 59.95%\n",
        "Epoch [8/10], Training Loss: 0.863, Validation Accuracy: 59.81%\n",
        "Epoch [9/10], Training Loss: 0.846, Validation Accuracy: 59.88%\n",
        "Epoch [10/10], Training Loss: 0.831, Validation Accuracy: 59.40%\n",
        "Epoch [1/10], Training Loss: 1.021, Validation Accuracy: 59.98%\n",
        "Epoch [2/10], Training Loss: 0.957, Validation Accuracy: 59.12%\n",
        "Epoch [3/10], Training Loss: 0.927, Validation Accuracy: 60.83%\n",
        "Epoch [4/10], Training Loss: 0.902, Validation Accuracy: 60.91%\n",
        "Epoch [5/10], Training Loss: 0.878, Validation Accuracy: 60.02%\n",
        "Epoch [6/10], Training Loss: 0.861, Validation Accuracy: 60.58%\n",
        "Epoch [7/10], Training Loss: 0.846, Validation Accuracy: 60.20%\n",
        "Epoch [8/10], Training Loss: 0.838, Validation Accuracy: 59.35%\n",
        "Epoch [9/10], Training Loss: 0.820, Validation Accuracy: 60.18%\n",
        "Epoch [10/10], Training Loss: 0.803, Validation Accuracy: 60.27%\n",
        "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.82%\n",
        "Epoch [2/10], Training Loss: 0.952, Validation Accuracy: 60.33%\n",
        "Epoch [3/10], Training Loss: 0.911, Validation Accuracy: 60.38%\n",
        "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 59.98%\n",
        "Epoch [5/10], Training Loss: 0.873, Validation Accuracy: 60.02%\n",
        "Epoch [6/10], Training Loss: 0.853, Validation Accuracy: 60.48%\n",
        "Epoch [7/10], Training Loss: 0.833, Validation Accuracy: 60.22%\n",
        "Epoch [8/10], Training Loss: 0.812, Validation Accuracy: 60.46%\n",
        "Epoch [9/10], Training Loss: 0.801, Validation Accuracy: 60.47%\n",
        "Epoch [10/10], Training Loss: 0.787, Validation Accuracy: 60.16%\n",
        "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 60.16%\n",
        "Epoch [2/10], Training Loss: 0.909, Validation Accuracy: 60.82%\n",
        "Epoch [3/10], Training Loss: 0.870, Validation Accuracy: 60.16%\n",
        "Epoch [4/10], Training Loss: 0.845, Validation Accuracy: 60.75%\n",
        "Epoch [5/10], Training Loss: 0.810, Validation Accuracy: 60.38%\n",
        "Epoch [6/10], Training Loss: 0.808, Validation Accuracy: 61.06%\n",
        "Epoch [7/10], Training Loss: 0.772, Validation Accuracy: 60.95%\n",
        "Epoch [8/10], Training Loss: 0.761, Validation Accuracy: 60.30%\n",
        "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 60.92%\n",
        "Epoch [10/10], Training Loss: 0.726, Validation Accuracy: 60.45%\n",
        "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 60.20%\n",
        "Epoch [2/10], Training Loss: 0.875, Validation Accuracy: 61.38%\n",
        "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 61.17%\n",
        "Epoch [4/10], Training Loss: 0.814, Validation Accuracy: 60.25%\n",
        "Epoch [5/10], Training Loss: 0.787, Validation Accuracy: 61.42%\n",
        "Epoch [6/10], Training Loss: 0.769, Validation Accuracy: 60.80%\n",
        "Epoch [7/10], Training Loss: 0.753, Validation Accuracy: 60.75%\n",
        "Epoch [8/10], Training Loss: 0.736, Validation Accuracy: 60.38%\n",
        "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 60.67%\n",
        "Epoch [10/10], Training Loss: 0.709, Validation Accuracy: 60.59%\n",
        "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 61.07%\n",
        "Epoch [2/10], Training Loss: 0.911, Validation Accuracy: 61.22%\n",
        "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 60.83%\n",
        "Epoch [4/10], Training Loss: 0.840, Validation Accuracy: 60.81%\n",
        "Epoch [5/10], Training Loss: 0.828, Validation Accuracy: 61.11%\n",
        "Epoch [6/10], Training Loss: 0.802, Validation Accuracy: 60.86%\n",
        "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 60.54%\n",
        "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 60.48%\n",
        "Epoch [9/10], Training Loss: 0.745, Validation Accuracy: 60.97%\n",
        "Epoch [10/10], Training Loss: 0.729, Validation Accuracy: 61.24%\n",
        "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 60.95%\n",
        "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.15%\n",
        "Epoch [3/10], Training Loss: 0.850, Validation Accuracy: 61.34%\n",
        "Epoch [4/10], Training Loss: 0.826, Validation Accuracy: 60.99%\n",
        "Epoch [5/10], Training Loss: 0.811, Validation Accuracy: 61.79%\n",
        "Epoch [6/10], Training Loss: 0.782, Validation Accuracy: 59.86%\n",
        "Epoch [7/10], Training Loss: 0.751, Validation Accuracy: 61.18%\n",
        "Epoch [8/10], Training Loss: 0.737, Validation Accuracy: 61.49%\n",
        "Epoch [9/10], Training Loss: 0.719, Validation Accuracy: 61.40%\n",
        "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 60.64%\n",
        "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 61.00%\n",
        "Epoch [2/10], Training Loss: 0.882, Validation Accuracy: 60.78%\n",
        "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 61.46%\n",
        "Epoch [4/10], Training Loss: 0.803, Validation Accuracy: 61.31%\n",
        "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 60.74%\n",
        "Epoch [6/10], Training Loss: 0.763, Validation Accuracy: 60.59%\n",
        "Epoch [7/10], Training Loss: 0.742, Validation Accuracy: 61.30%\n",
        "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 60.83%\n",
        "Epoch [9/10], Training Loss: 0.695, Validation Accuracy: 60.70%\n",
        "Epoch [10/10], Training Loss: 0.695, Validation Accuracy: 60.51%\n",
        "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.67%\n",
        "Epoch [2/10], Training Loss: 0.833, Validation Accuracy: 60.83%\n",
        "Epoch [3/10], Training Loss: 0.793, Validation Accuracy: 60.96%\n",
        "Epoch [4/10], Training Loss: 0.754, Validation Accuracy: 61.89%\n",
        "Epoch [5/10], Training Loss: 0.730, Validation Accuracy: 61.14%\n",
        "Epoch [6/10], Training Loss: 0.701, Validation Accuracy: 61.02%\n",
        "Epoch [7/10], Training Loss: 0.677, Validation Accuracy: 60.77%\n",
        "Epoch [8/10], Training Loss: 0.682, Validation Accuracy: 61.25%\n",
        "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 61.27%\n",
        "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 60.71%\n",
        "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.26%\n",
        "Epoch [2/10], Training Loss: 0.803, Validation Accuracy: 60.06%\n",
        "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 61.66%\n",
        "Epoch [4/10], Training Loss: 0.734, Validation Accuracy: 61.29%\n",
        "Epoch [5/10], Training Loss: 0.708, Validation Accuracy: 61.63%\n",
        "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 61.06%\n",
        "Epoch [7/10], Training Loss: 0.662, Validation Accuracy: 60.89%\n",
        "Epoch [8/10], Training Loss: 0.649, Validation Accuracy: 60.80%\n",
        "Epoch [9/10], Training Loss: 0.632, Validation Accuracy: 60.95%\n",
        "Epoch [10/10], Training Loss: 0.610, Validation Accuracy: 60.81%\n",
        "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 61.21%\n",
        "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 61.15%\n",
        "Epoch [3/10], Training Loss: 0.788, Validation Accuracy: 61.61%\n",
        "Epoch [4/10], Training Loss: 0.755, Validation Accuracy: 61.58%\n",
        "Epoch [5/10], Training Loss: 0.736, Validation Accuracy: 61.45%\n",
        "Epoch [6/10], Training Loss: 0.709, Validation Accuracy: 61.61%\n",
        "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 60.51%\n",
        "Epoch [8/10], Training Loss: 0.681, Validation Accuracy: 60.56%\n",
        "Epoch [9/10], Training Loss: 0.658, Validation Accuracy: 61.58%\n",
        "Epoch [10/10], Training Loss: 0.635, Validation Accuracy: 61.31%\n",
        "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 60.82%\n",
        "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.04%\n",
        "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 60.55%\n",
        "Epoch [4/10], Training Loss: 0.740, Validation Accuracy: 61.68%\n",
        "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.80%\n",
        "Epoch [6/10], Training Loss: 0.683, Validation Accuracy: 61.59%\n",
        "Epoch [7/10], Training Loss: 0.661, Validation Accuracy: 61.35%\n",
        "Epoch [8/10], Training Loss: 0.646, Validation Accuracy: 60.90%\n",
        "Epoch [9/10], Training Loss: 0.620, Validation Accuracy: 60.66%\n",
        "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 61.01%\n",
        "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.93%\n",
        "Epoch [2/10], Training Loss: 0.817, Validation Accuracy: 61.83%\n",
        "Epoch [3/10], Training Loss: 0.765, Validation Accuracy: 61.43%\n",
        "Epoch [4/10], Training Loss: 0.731, Validation Accuracy: 61.78%\n",
        "Epoch [5/10], Training Loss: 0.698, Validation Accuracy: 61.44%\n",
        "Epoch [6/10], Training Loss: 0.680, Validation Accuracy: 60.58%\n",
        "Epoch [7/10], Training Loss: 0.657, Validation Accuracy: 60.64%\n",
        "Epoch [8/10], Training Loss: 0.625, Validation Accuracy: 61.10%\n",
        "Epoch [9/10], Training Loss: 0.606, Validation Accuracy: 60.48%\n",
        "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 60.75%\n",
        "Epoch [1/10], Training Loss: 0.856, Validation Accuracy: 60.63%\n",
        "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 61.22%\n",
        "Epoch [3/10], Training Loss: 0.710, Validation Accuracy: 61.46%\n",
        "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 61.09%\n",
        "Epoch [5/10], Training Loss: 0.653, Validation Accuracy: 61.30%\n",
        "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.46%\n",
        "Epoch [7/10], Training Loss: 0.585, Validation Accuracy: 61.20%\n",
        "Epoch [8/10], Training Loss: 0.575, Validation Accuracy: 61.28%\n",
        "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 60.94%\n",
        "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 61.48%\n",
        "Epoch [1/10], Training Loss: 0.850, Validation Accuracy: 60.52%\n",
        "Epoch [2/10], Training Loss: 0.750, Validation Accuracy: 61.53%\n",
        "Epoch [3/10], Training Loss: 0.694, Validation Accuracy: 61.45%\n",
        "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 61.22%\n",
        "Epoch [5/10], Training Loss: 0.638, Validation Accuracy: 60.74%\n",
        "Epoch [6/10], Training Loss: 0.602, Validation Accuracy: 61.21%\n",
        "Epoch [7/10], Training Loss: 0.583, Validation Accuracy: 60.89%\n",
        "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 60.75%\n",
        "Epoch [9/10], Training Loss: 0.546, Validation Accuracy: 61.03%\n",
        "Epoch [10/10], Training Loss: 0.519, Validation Accuracy: 60.52%\n",
        "Epoch [1/10], Training Loss: 0.889, Validation Accuracy: 60.63%\n",
        "Epoch [2/10], Training Loss: 0.778, Validation Accuracy: 60.97%\n",
        "Epoch [3/10], Training Loss: 0.734, Validation Accuracy: 61.07%\n",
        "Epoch [4/10], Training Loss: 0.684, Validation Accuracy: 61.42%\n",
        "Epoch [5/10], Training Loss: 0.647, Validation Accuracy: 61.78%\n",
        "Epoch [6/10], Training Loss: 0.622, Validation Accuracy: 60.76%\n",
        "Epoch [7/10], Training Loss: 0.599, Validation Accuracy: 61.21%\n",
        "Epoch [8/10], Training Loss: 0.580, Validation Accuracy: 60.98%\n",
        "Epoch [9/10], Training Loss: 0.555, Validation Accuracy: 61.16%\n",
        "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 60.90%\n",
        "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 60.63%\n",
        "Epoch [2/10], Training Loss: 0.756, Validation Accuracy: 61.39%\n",
        "Epoch [3/10], Training Loss: 0.703, Validation Accuracy: 61.05%\n",
        "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 61.41%\n",
        "Epoch [5/10], Training Loss: 0.618, Validation Accuracy: 61.70%\n",
        "Epoch [6/10], Training Loss: 0.588, Validation Accuracy: 61.47%\n",
        "Epoch [7/10], Training Loss: 0.568, Validation Accuracy: 61.66%\n",
        "Epoch [8/10], Training Loss: 0.547, Validation Accuracy: 61.19%\n",
        "Epoch [9/10], Training Loss: 0.530, Validation Accuracy: 61.06%\n",
        "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 61.69%\n",
        "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 61.57%\n",
        "Epoch [2/10], Training Loss: 0.749, Validation Accuracy: 61.34%\n",
        "Epoch [3/10], Training Loss: 0.695, Validation Accuracy: 61.22%\n",
        "Epoch [4/10], Training Loss: 0.660, Validation Accuracy: 61.09%\n",
        "Epoch [5/10], Training Loss: 0.618, Validation Accuracy: 61.11%\n",
        "Epoch [6/10], Training Loss: 0.587, Validation Accuracy: 61.15%\n",
        "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 61.30%\n",
        "Epoch [8/10], Training Loss: 0.542, Validation Accuracy: 60.64%\n",
        "Epoch [9/10], Training Loss: 0.530, Validation Accuracy: 60.77%\n",
        "Epoch [10/10], Training Loss: 0.500, Validation Accuracy: 60.86%\n",
        "Epoch [1/10], Training Loss: 0.828, Validation Accuracy: 61.12%\n",
        "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 61.19%\n",
        "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 60.97%\n",
        "Epoch [4/10], Training Loss: 0.602, Validation Accuracy: 61.42%\n",
        "Epoch [5/10], Training Loss: 0.555, Validation Accuracy: 61.11%\n",
        "Epoch [6/10], Training Loss: 0.536, Validation Accuracy: 61.23%\n",
        "Epoch [7/10], Training Loss: 0.511, Validation Accuracy: 60.96%\n",
        "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 60.68%\n",
        "Epoch [9/10], Training Loss: 0.471, Validation Accuracy: 61.48%\n",
        "Epoch [10/10], Training Loss: 0.442, Validation Accuracy: 60.90%\n",
        "Epoch [1/10], Training Loss: 0.797, Validation Accuracy: 60.49%\n",
        "Epoch [2/10], Training Loss: 0.697, Validation Accuracy: 60.83%\n",
        "Epoch [3/10], Training Loss: 0.621, Validation Accuracy: 61.30%\n",
        "Epoch [4/10], Training Loss: 0.582, Validation Accuracy: 61.18%\n",
        "Epoch [5/10], Training Loss: 0.550, Validation Accuracy: 61.41%\n",
        "Epoch [6/10], Training Loss: 0.515, Validation Accuracy: 60.60%\n",
        "Epoch [7/10], Training Loss: 0.495, Validation Accuracy: 61.07%\n",
        "Epoch [8/10], Training Loss: 0.465, Validation Accuracy: 60.57%\n",
        "Epoch [9/10], Training Loss: 0.450, Validation Accuracy: 61.08%\n",
        "Epoch [10/10], Training Loss: 0.429, Validation Accuracy: 60.64%\n",
        "Epoch [1/10], Training Loss: 0.848, Validation Accuracy: 60.84%\n",
        "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 60.76%\n",
        "Epoch [3/10], Training Loss: 0.658, Validation Accuracy: 61.16%\n",
        "Epoch [4/10], Training Loss: 0.604, Validation Accuracy: 60.94%\n",
        "Epoch [5/10], Training Loss: 0.568, Validation Accuracy: 60.87%\n",
        "Epoch [6/10], Training Loss: 0.553, Validation Accuracy: 60.69%\n",
        "Epoch [7/10], Training Loss: 0.516, Validation Accuracy: 61.60%\n",
        "Epoch [8/10], Training Loss: 0.494, Validation Accuracy: 60.91%\n",
        "Epoch [9/10], Training Loss: 0.481, Validation Accuracy: 60.83%\n",
        "Epoch [10/10], Training Loss: 0.455, Validation Accuracy: 61.19%\n",
        "Epoch [1/10], Training Loss: 0.821, Validation Accuracy: 61.22%\n",
        "Epoch [2/10], Training Loss: 0.682, Validation Accuracy: 61.71%\n",
        "Epoch [3/10], Training Loss: 0.623, Validation Accuracy: 60.64%\n",
        "Epoch [4/10], Training Loss: 0.582, Validation Accuracy: 60.81%\n",
        "Epoch [5/10], Training Loss: 0.545, Validation Accuracy: 61.38%\n",
        "Epoch [6/10], Training Loss: 0.514, Validation Accuracy: 61.57%\n",
        "Epoch [7/10], Training Loss: 0.500, Validation Accuracy: 61.52%\n",
        "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 61.03%\n",
        "Epoch [9/10], Training Loss: 0.447, Validation Accuracy: 60.76%\n",
        "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 60.42%\n",
        "Epoch [1/10], Training Loss: 0.817, Validation Accuracy: 60.97%\n",
        "Epoch [2/10], Training Loss: 0.683, Validation Accuracy: 61.65%\n",
        "Epoch [3/10], Training Loss: 0.618, Validation Accuracy: 61.12%\n",
        "Epoch [4/10], Training Loss: 0.573, Validation Accuracy: 61.11%\n",
        "Epoch [5/10], Training Loss: 0.537, Validation Accuracy: 60.95%\n",
        "Epoch [6/10], Training Loss: 0.514, Validation Accuracy: 61.18%\n",
        "Epoch [7/10], Training Loss: 0.474, Validation Accuracy: 60.96%\n",
        "Epoch [8/10], Training Loss: 0.455, Validation Accuracy: 60.49%\n",
        "Epoch [9/10], Training Loss: 0.433, Validation Accuracy: 60.64%\n",
        "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 60.34%\n",
        "Epoch [1/10], Training Loss: 0.791, Validation Accuracy: 60.47%\n",
        "Epoch [2/10], Training Loss: 0.650, Validation Accuracy: 59.43%\n",
        "Epoch [3/10], Training Loss: 0.584, Validation Accuracy: 60.48%\n",
        "Epoch [4/10], Training Loss: 0.537, Validation Accuracy: 61.41%\n",
        "Epoch [5/10], Training Loss: 0.482, Validation Accuracy: 61.16%\n",
        "Epoch [6/10], Training Loss: 0.475, Validation Accuracy: 60.67%\n",
        "Epoch [7/10], Training Loss: 0.430, Validation Accuracy: 60.86%\n",
        "Epoch [8/10], Training Loss: 0.408, Validation Accuracy: 61.11%\n",
        "Epoch [9/10], Training Loss: 0.385, Validation Accuracy: 60.45%\n",
        "Epoch [10/10], Training Loss: 0.365, Validation Accuracy: 60.91%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7j8NTE8nGeW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYAwS10pnHSq",
        "outputId": "829a32d4-a423-451d-8824-37db7a9eecf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 80.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [5958 6147 6057 5950 6003 6035 5879 5965 6068 5938]\n",
            "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 10.56%\n",
            "Epoch [2/10], Training Loss: 2.305, Validation Accuracy: 10.56%\n",
            "Epoch [3/10], Training Loss: 2.304, Validation Accuracy: 10.56%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 10.56%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 10.56%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 10.65%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 10.97%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 12.26%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 14.45%\n",
            "Epoch [10/10], Training Loss: 2.296, Validation Accuracy: 16.50%\n",
            "Epoch [1/10], Training Loss: 2.295, Validation Accuracy: 16.73%\n",
            "Epoch [2/10], Training Loss: 2.293, Validation Accuracy: 16.87%\n",
            "Epoch [3/10], Training Loss: 2.289, Validation Accuracy: 16.48%\n",
            "Epoch [4/10], Training Loss: 2.284, Validation Accuracy: 16.53%\n",
            "Epoch [5/10], Training Loss: 2.276, Validation Accuracy: 16.16%\n",
            "Epoch [6/10], Training Loss: 2.263, Validation Accuracy: 16.37%\n",
            "Epoch [7/10], Training Loss: 2.241, Validation Accuracy: 17.23%\n",
            "Epoch [8/10], Training Loss: 2.209, Validation Accuracy: 18.37%\n",
            "Epoch [9/10], Training Loss: 2.173, Validation Accuracy: 21.08%\n",
            "Epoch [10/10], Training Loss: 2.139, Validation Accuracy: 22.74%\n",
            "Epoch [1/10], Training Loss: 2.113, Validation Accuracy: 23.67%\n",
            "Epoch [2/10], Training Loss: 2.082, Validation Accuracy: 24.80%\n",
            "Epoch [3/10], Training Loss: 2.050, Validation Accuracy: 25.62%\n",
            "Epoch [4/10], Training Loss: 2.021, Validation Accuracy: 26.11%\n",
            "Epoch [5/10], Training Loss: 1.992, Validation Accuracy: 27.66%\n",
            "Epoch [6/10], Training Loss: 1.966, Validation Accuracy: 28.00%\n",
            "Epoch [7/10], Training Loss: 1.945, Validation Accuracy: 28.79%\n",
            "Epoch [8/10], Training Loss: 1.925, Validation Accuracy: 29.70%\n",
            "Epoch [9/10], Training Loss: 1.906, Validation Accuracy: 30.30%\n",
            "Epoch [10/10], Training Loss: 1.886, Validation Accuracy: 30.62%\n",
            "Epoch [1/10], Training Loss: 1.882, Validation Accuracy: 32.56%\n",
            "Epoch [2/10], Training Loss: 1.859, Validation Accuracy: 32.28%\n",
            "Epoch [3/10], Training Loss: 1.842, Validation Accuracy: 32.93%\n",
            "Epoch [4/10], Training Loss: 1.819, Validation Accuracy: 33.67%\n",
            "Epoch [5/10], Training Loss: 1.795, Validation Accuracy: 33.91%\n",
            "Epoch [6/10], Training Loss: 1.778, Validation Accuracy: 34.31%\n",
            "Epoch [7/10], Training Loss: 1.760, Validation Accuracy: 34.68%\n",
            "Epoch [8/10], Training Loss: 1.743, Validation Accuracy: 35.97%\n",
            "Epoch [9/10], Training Loss: 1.721, Validation Accuracy: 36.62%\n",
            "Epoch [10/10], Training Loss: 1.709, Validation Accuracy: 36.97%\n",
            "Epoch [1/10], Training Loss: 1.703, Validation Accuracy: 37.19%\n",
            "Epoch [2/10], Training Loss: 1.685, Validation Accuracy: 36.87%\n",
            "Epoch [3/10], Training Loss: 1.670, Validation Accuracy: 38.03%\n",
            "Epoch [4/10], Training Loss: 1.657, Validation Accuracy: 39.17%\n",
            "Epoch [5/10], Training Loss: 1.653, Validation Accuracy: 39.74%\n",
            "Epoch [6/10], Training Loss: 1.637, Validation Accuracy: 39.25%\n",
            "Epoch [7/10], Training Loss: 1.622, Validation Accuracy: 40.01%\n",
            "Epoch [8/10], Training Loss: 1.622, Validation Accuracy: 40.27%\n",
            "Epoch [9/10], Training Loss: 1.608, Validation Accuracy: 40.10%\n",
            "Epoch [10/10], Training Loss: 1.601, Validation Accuracy: 40.88%\n",
            "Epoch [1/10], Training Loss: 1.636, Validation Accuracy: 41.06%\n",
            "Epoch [2/10], Training Loss: 1.623, Validation Accuracy: 40.84%\n",
            "Epoch [3/10], Training Loss: 1.609, Validation Accuracy: 40.75%\n",
            "Epoch [4/10], Training Loss: 1.604, Validation Accuracy: 42.02%\n",
            "Epoch [5/10], Training Loss: 1.586, Validation Accuracy: 42.12%\n",
            "Epoch [6/10], Training Loss: 1.571, Validation Accuracy: 42.50%\n",
            "Epoch [7/10], Training Loss: 1.563, Validation Accuracy: 42.76%\n",
            "Epoch [8/10], Training Loss: 1.548, Validation Accuracy: 42.55%\n",
            "Epoch [9/10], Training Loss: 1.539, Validation Accuracy: 43.37%\n",
            "Epoch [10/10], Training Loss: 1.527, Validation Accuracy: 43.55%\n",
            "Epoch [1/10], Training Loss: 1.545, Validation Accuracy: 43.55%\n",
            "Epoch [2/10], Training Loss: 1.534, Validation Accuracy: 43.90%\n",
            "Epoch [3/10], Training Loss: 1.528, Validation Accuracy: 44.13%\n",
            "Epoch [4/10], Training Loss: 1.520, Validation Accuracy: 44.94%\n",
            "Epoch [5/10], Training Loss: 1.497, Validation Accuracy: 45.25%\n",
            "Epoch [6/10], Training Loss: 1.496, Validation Accuracy: 45.60%\n",
            "Epoch [7/10], Training Loss: 1.479, Validation Accuracy: 46.53%\n",
            "Epoch [8/10], Training Loss: 1.469, Validation Accuracy: 46.45%\n",
            "Epoch [9/10], Training Loss: 1.463, Validation Accuracy: 46.52%\n",
            "Epoch [10/10], Training Loss: 1.458, Validation Accuracy: 46.43%\n",
            "Epoch [1/10], Training Loss: 1.494, Validation Accuracy: 47.00%\n",
            "Epoch [2/10], Training Loss: 1.469, Validation Accuracy: 48.04%\n",
            "Epoch [3/10], Training Loss: 1.459, Validation Accuracy: 47.97%\n",
            "Epoch [4/10], Training Loss: 1.443, Validation Accuracy: 47.32%\n",
            "Epoch [5/10], Training Loss: 1.434, Validation Accuracy: 48.05%\n",
            "Epoch [6/10], Training Loss: 1.423, Validation Accuracy: 47.30%\n",
            "Epoch [7/10], Training Loss: 1.417, Validation Accuracy: 47.86%\n",
            "Epoch [8/10], Training Loss: 1.406, Validation Accuracy: 48.35%\n",
            "Epoch [9/10], Training Loss: 1.398, Validation Accuracy: 48.78%\n",
            "Epoch [10/10], Training Loss: 1.388, Validation Accuracy: 48.73%\n",
            "Epoch [1/10], Training Loss: 1.413, Validation Accuracy: 48.35%\n",
            "Epoch [2/10], Training Loss: 1.400, Validation Accuracy: 49.41%\n",
            "Epoch [3/10], Training Loss: 1.382, Validation Accuracy: 48.20%\n",
            "Epoch [4/10], Training Loss: 1.371, Validation Accuracy: 49.39%\n",
            "Epoch [5/10], Training Loss: 1.361, Validation Accuracy: 48.43%\n",
            "Epoch [6/10], Training Loss: 1.349, Validation Accuracy: 49.23%\n",
            "Epoch [7/10], Training Loss: 1.341, Validation Accuracy: 49.16%\n",
            "Epoch [8/10], Training Loss: 1.335, Validation Accuracy: 49.96%\n",
            "Epoch [9/10], Training Loss: 1.325, Validation Accuracy: 49.79%\n",
            "Epoch [10/10], Training Loss: 1.322, Validation Accuracy: 50.34%\n",
            "Epoch [1/10], Training Loss: 1.359, Validation Accuracy: 50.86%\n",
            "Epoch [2/10], Training Loss: 1.351, Validation Accuracy: 50.98%\n",
            "Epoch [3/10], Training Loss: 1.339, Validation Accuracy: 50.61%\n",
            "Epoch [4/10], Training Loss: 1.324, Validation Accuracy: 50.92%\n",
            "Epoch [5/10], Training Loss: 1.307, Validation Accuracy: 51.89%\n",
            "Epoch [6/10], Training Loss: 1.302, Validation Accuracy: 51.98%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 51.65%\n",
            "Epoch [8/10], Training Loss: 1.293, Validation Accuracy: 51.44%\n",
            "Epoch [9/10], Training Loss: 1.285, Validation Accuracy: 51.78%\n",
            "Epoch [10/10], Training Loss: 1.264, Validation Accuracy: 51.64%\n",
            "Epoch [1/10], Training Loss: 1.328, Validation Accuracy: 52.59%\n",
            "Epoch [2/10], Training Loss: 1.315, Validation Accuracy: 52.71%\n",
            "Epoch [3/10], Training Loss: 1.297, Validation Accuracy: 52.71%\n",
            "Epoch [4/10], Training Loss: 1.286, Validation Accuracy: 52.96%\n",
            "Epoch [5/10], Training Loss: 1.275, Validation Accuracy: 53.20%\n",
            "Epoch [6/10], Training Loss: 1.269, Validation Accuracy: 53.37%\n",
            "Epoch [7/10], Training Loss: 1.262, Validation Accuracy: 53.63%\n",
            "Epoch [8/10], Training Loss: 1.255, Validation Accuracy: 53.06%\n",
            "Epoch [9/10], Training Loss: 1.247, Validation Accuracy: 53.15%\n",
            "Epoch [10/10], Training Loss: 1.245, Validation Accuracy: 53.42%\n",
            "Epoch [1/10], Training Loss: 1.304, Validation Accuracy: 53.60%\n",
            "Epoch [2/10], Training Loss: 1.289, Validation Accuracy: 54.64%\n",
            "Epoch [3/10], Training Loss: 1.272, Validation Accuracy: 54.21%\n",
            "Epoch [4/10], Training Loss: 1.254, Validation Accuracy: 54.58%\n",
            "Epoch [5/10], Training Loss: 1.244, Validation Accuracy: 54.74%\n",
            "Epoch [6/10], Training Loss: 1.241, Validation Accuracy: 54.20%\n",
            "Epoch [7/10], Training Loss: 1.234, Validation Accuracy: 54.56%\n",
            "Epoch [8/10], Training Loss: 1.221, Validation Accuracy: 53.94%\n",
            "Epoch [9/10], Training Loss: 1.214, Validation Accuracy: 53.71%\n",
            "Epoch [10/10], Training Loss: 1.208, Validation Accuracy: 55.37%\n",
            "Epoch [1/10], Training Loss: 1.277, Validation Accuracy: 55.67%\n",
            "Epoch [2/10], Training Loss: 1.250, Validation Accuracy: 55.13%\n",
            "Epoch [3/10], Training Loss: 1.238, Validation Accuracy: 55.74%\n",
            "Epoch [4/10], Training Loss: 1.215, Validation Accuracy: 54.99%\n",
            "Epoch [5/10], Training Loss: 1.204, Validation Accuracy: 55.21%\n",
            "Epoch [6/10], Training Loss: 1.201, Validation Accuracy: 54.89%\n",
            "Epoch [7/10], Training Loss: 1.190, Validation Accuracy: 55.46%\n",
            "Epoch [8/10], Training Loss: 1.187, Validation Accuracy: 55.76%\n",
            "Epoch [9/10], Training Loss: 1.173, Validation Accuracy: 55.34%\n",
            "Epoch [10/10], Training Loss: 1.163, Validation Accuracy: 55.26%\n",
            "Epoch [1/10], Training Loss: 1.223, Validation Accuracy: 55.61%\n",
            "Epoch [2/10], Training Loss: 1.199, Validation Accuracy: 55.26%\n",
            "Epoch [3/10], Training Loss: 1.190, Validation Accuracy: 55.06%\n",
            "Epoch [4/10], Training Loss: 1.170, Validation Accuracy: 55.65%\n",
            "Epoch [5/10], Training Loss: 1.164, Validation Accuracy: 56.06%\n",
            "Epoch [6/10], Training Loss: 1.151, Validation Accuracy: 55.79%\n",
            "Epoch [7/10], Training Loss: 1.143, Validation Accuracy: 56.11%\n",
            "Epoch [8/10], Training Loss: 1.134, Validation Accuracy: 56.61%\n",
            "Epoch [9/10], Training Loss: 1.132, Validation Accuracy: 55.85%\n",
            "Epoch [10/10], Training Loss: 1.132, Validation Accuracy: 56.20%\n",
            "Epoch [1/10], Training Loss: 1.202, Validation Accuracy: 56.03%\n",
            "Epoch [2/10], Training Loss: 1.179, Validation Accuracy: 56.41%\n",
            "Epoch [3/10], Training Loss: 1.161, Validation Accuracy: 55.93%\n",
            "Epoch [4/10], Training Loss: 1.151, Validation Accuracy: 57.08%\n",
            "Epoch [5/10], Training Loss: 1.134, Validation Accuracy: 57.30%\n",
            "Epoch [6/10], Training Loss: 1.126, Validation Accuracy: 56.02%\n",
            "Epoch [7/10], Training Loss: 1.114, Validation Accuracy: 57.24%\n",
            "Epoch [8/10], Training Loss: 1.100, Validation Accuracy: 57.30%\n",
            "Epoch [9/10], Training Loss: 1.097, Validation Accuracy: 56.16%\n",
            "Epoch [10/10], Training Loss: 1.088, Validation Accuracy: 56.96%\n",
            "Epoch [1/10], Training Loss: 1.189, Validation Accuracy: 55.49%\n",
            "Epoch [2/10], Training Loss: 1.171, Validation Accuracy: 57.49%\n",
            "Epoch [3/10], Training Loss: 1.151, Validation Accuracy: 57.65%\n",
            "Epoch [4/10], Training Loss: 1.136, Validation Accuracy: 58.12%\n",
            "Epoch [5/10], Training Loss: 1.127, Validation Accuracy: 57.53%\n",
            "Epoch [6/10], Training Loss: 1.112, Validation Accuracy: 58.13%\n",
            "Epoch [7/10], Training Loss: 1.097, Validation Accuracy: 58.23%\n",
            "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 57.84%\n",
            "Epoch [9/10], Training Loss: 1.077, Validation Accuracy: 57.99%\n",
            "Epoch [10/10], Training Loss: 1.066, Validation Accuracy: 58.08%\n",
            "Epoch [1/10], Training Loss: 1.175, Validation Accuracy: 58.23%\n",
            "Epoch [2/10], Training Loss: 1.155, Validation Accuracy: 57.64%\n",
            "Epoch [3/10], Training Loss: 1.136, Validation Accuracy: 57.74%\n",
            "Epoch [4/10], Training Loss: 1.112, Validation Accuracy: 58.19%\n",
            "Epoch [5/10], Training Loss: 1.109, Validation Accuracy: 57.42%\n",
            "Epoch [6/10], Training Loss: 1.093, Validation Accuracy: 58.72%\n",
            "Epoch [7/10], Training Loss: 1.081, Validation Accuracy: 58.81%\n",
            "Epoch [8/10], Training Loss: 1.076, Validation Accuracy: 58.64%\n",
            "Epoch [9/10], Training Loss: 1.062, Validation Accuracy: 57.89%\n",
            "Epoch [10/10], Training Loss: 1.055, Validation Accuracy: 58.79%\n",
            "Epoch [1/10], Training Loss: 1.140, Validation Accuracy: 59.26%\n",
            "Epoch [2/10], Training Loss: 1.122, Validation Accuracy: 59.27%\n",
            "Epoch [3/10], Training Loss: 1.094, Validation Accuracy: 58.71%\n",
            "Epoch [4/10], Training Loss: 1.077, Validation Accuracy: 58.49%\n",
            "Epoch [5/10], Training Loss: 1.073, Validation Accuracy: 59.19%\n",
            "Epoch [6/10], Training Loss: 1.059, Validation Accuracy: 58.63%\n",
            "Epoch [7/10], Training Loss: 1.045, Validation Accuracy: 59.08%\n",
            "Epoch [8/10], Training Loss: 1.039, Validation Accuracy: 57.43%\n",
            "Epoch [9/10], Training Loss: 1.023, Validation Accuracy: 59.18%\n",
            "Epoch [10/10], Training Loss: 1.028, Validation Accuracy: 59.28%\n",
            "Epoch [1/10], Training Loss: 1.119, Validation Accuracy: 58.68%\n",
            "Epoch [2/10], Training Loss: 1.095, Validation Accuracy: 59.45%\n",
            "Epoch [3/10], Training Loss: 1.066, Validation Accuracy: 58.69%\n",
            "Epoch [4/10], Training Loss: 1.053, Validation Accuracy: 59.87%\n",
            "Epoch [5/10], Training Loss: 1.037, Validation Accuracy: 58.93%\n",
            "Epoch [6/10], Training Loss: 1.022, Validation Accuracy: 59.83%\n",
            "Epoch [7/10], Training Loss: 1.011, Validation Accuracy: 59.34%\n",
            "Epoch [8/10], Training Loss: 1.004, Validation Accuracy: 57.64%\n",
            "Epoch [9/10], Training Loss: 1.003, Validation Accuracy: 58.84%\n",
            "Epoch [10/10], Training Loss: 0.991, Validation Accuracy: 58.57%\n",
            "Epoch [1/10], Training Loss: 1.107, Validation Accuracy: 58.92%\n",
            "Epoch [2/10], Training Loss: 1.080, Validation Accuracy: 59.07%\n",
            "Epoch [3/10], Training Loss: 1.048, Validation Accuracy: 59.23%\n",
            "Epoch [4/10], Training Loss: 1.038, Validation Accuracy: 59.44%\n",
            "Epoch [5/10], Training Loss: 1.022, Validation Accuracy: 59.79%\n",
            "Epoch [6/10], Training Loss: 1.016, Validation Accuracy: 58.53%\n",
            "Epoch [7/10], Training Loss: 0.988, Validation Accuracy: 58.84%\n",
            "Epoch [8/10], Training Loss: 0.977, Validation Accuracy: 59.94%\n",
            "Epoch [9/10], Training Loss: 0.969, Validation Accuracy: 59.18%\n",
            "Epoch [10/10], Training Loss: 0.958, Validation Accuracy: 59.32%\n",
            "Epoch [1/10], Training Loss: 1.102, Validation Accuracy: 59.78%\n",
            "Epoch [2/10], Training Loss: 1.062, Validation Accuracy: 60.62%\n",
            "Epoch [3/10], Training Loss: 1.038, Validation Accuracy: 60.28%\n",
            "Epoch [4/10], Training Loss: 1.024, Validation Accuracy: 60.13%\n",
            "Epoch [5/10], Training Loss: 1.015, Validation Accuracy: 59.48%\n",
            "Epoch [6/10], Training Loss: 0.993, Validation Accuracy: 60.51%\n",
            "Epoch [7/10], Training Loss: 0.989, Validation Accuracy: 60.02%\n",
            "Epoch [8/10], Training Loss: 0.978, Validation Accuracy: 59.87%\n",
            "Epoch [9/10], Training Loss: 0.962, Validation Accuracy: 59.91%\n",
            "Epoch [10/10], Training Loss: 0.942, Validation Accuracy: 59.98%\n",
            "Epoch [1/10], Training Loss: 1.083, Validation Accuracy: 60.01%\n",
            "Epoch [2/10], Training Loss: 1.048, Validation Accuracy: 60.49%\n",
            "Epoch [3/10], Training Loss: 1.018, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 1.001, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.992, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.981, Validation Accuracy: 60.70%\n",
            "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.938, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.928, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.925, Validation Accuracy: 60.65%\n",
            "Epoch [1/10], Training Loss: 1.060, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 1.010, Validation Accuracy: 60.61%\n",
            "Epoch [3/10], Training Loss: 0.998, Validation Accuracy: 60.48%\n",
            "Epoch [4/10], Training Loss: 0.974, Validation Accuracy: 60.32%\n",
            "Epoch [5/10], Training Loss: 0.959, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.946, Validation Accuracy: 60.23%\n",
            "Epoch [7/10], Training Loss: 0.923, Validation Accuracy: 59.98%\n",
            "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.905, Validation Accuracy: 61.10%\n",
            "Epoch [10/10], Training Loss: 0.898, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 1.039, Validation Accuracy: 60.27%\n",
            "Epoch [2/10], Training Loss: 0.991, Validation Accuracy: 61.20%\n",
            "Epoch [3/10], Training Loss: 0.978, Validation Accuracy: 60.47%\n",
            "Epoch [4/10], Training Loss: 0.961, Validation Accuracy: 60.97%\n",
            "Epoch [5/10], Training Loss: 0.937, Validation Accuracy: 61.19%\n",
            "Epoch [6/10], Training Loss: 0.919, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.906, Validation Accuracy: 60.64%\n",
            "Epoch [8/10], Training Loss: 0.896, Validation Accuracy: 61.09%\n",
            "Epoch [9/10], Training Loss: 0.882, Validation Accuracy: 61.18%\n",
            "Epoch [10/10], Training Loss: 0.863, Validation Accuracy: 60.99%\n",
            "Epoch [1/10], Training Loss: 1.020, Validation Accuracy: 61.00%\n",
            "Epoch [2/10], Training Loss: 0.982, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.959, Validation Accuracy: 60.99%\n",
            "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.923, Validation Accuracy: 60.53%\n",
            "Epoch [6/10], Training Loss: 0.893, Validation Accuracy: 61.45%\n",
            "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 60.91%\n",
            "Epoch [8/10], Training Loss: 0.864, Validation Accuracy: 61.30%\n",
            "Epoch [9/10], Training Loss: 0.857, Validation Accuracy: 60.01%\n",
            "Epoch [10/10], Training Loss: 0.849, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 1.031, Validation Accuracy: 61.33%\n",
            "Epoch [2/10], Training Loss: 0.973, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.949, Validation Accuracy: 60.10%\n",
            "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 61.25%\n",
            "Epoch [5/10], Training Loss: 0.911, Validation Accuracy: 60.40%\n",
            "Epoch [6/10], Training Loss: 0.897, Validation Accuracy: 61.65%\n",
            "Epoch [7/10], Training Loss: 0.877, Validation Accuracy: 61.27%\n",
            "Epoch [8/10], Training Loss: 0.864, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.849, Validation Accuracy: 61.21%\n",
            "Epoch [10/10], Training Loss: 0.850, Validation Accuracy: 60.93%\n",
            "Epoch [1/10], Training Loss: 1.007, Validation Accuracy: 61.49%\n",
            "Epoch [2/10], Training Loss: 0.958, Validation Accuracy: 61.97%\n",
            "Epoch [3/10], Training Loss: 0.925, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.901, Validation Accuracy: 61.03%\n",
            "Epoch [5/10], Training Loss: 0.887, Validation Accuracy: 61.02%\n",
            "Epoch [6/10], Training Loss: 0.862, Validation Accuracy: 61.54%\n",
            "Epoch [7/10], Training Loss: 0.846, Validation Accuracy: 61.13%\n",
            "Epoch [8/10], Training Loss: 0.837, Validation Accuracy: 60.92%\n",
            "Epoch [9/10], Training Loss: 0.820, Validation Accuracy: 61.78%\n",
            "Epoch [10/10], Training Loss: 0.813, Validation Accuracy: 61.59%\n",
            "Epoch [1/10], Training Loss: 0.995, Validation Accuracy: 62.37%\n",
            "Epoch [2/10], Training Loss: 0.933, Validation Accuracy: 61.95%\n",
            "Epoch [3/10], Training Loss: 0.900, Validation Accuracy: 62.12%\n",
            "Epoch [4/10], Training Loss: 0.879, Validation Accuracy: 62.11%\n",
            "Epoch [5/10], Training Loss: 0.860, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.842, Validation Accuracy: 61.96%\n",
            "Epoch [7/10], Training Loss: 0.839, Validation Accuracy: 61.15%\n",
            "Epoch [8/10], Training Loss: 0.821, Validation Accuracy: 61.71%\n",
            "Epoch [9/10], Training Loss: 0.795, Validation Accuracy: 61.36%\n",
            "Epoch [10/10], Training Loss: 0.783, Validation Accuracy: 61.71%\n",
            "Epoch [1/10], Training Loss: 0.975, Validation Accuracy: 61.19%\n",
            "Epoch [2/10], Training Loss: 0.917, Validation Accuracy: 62.54%\n",
            "Epoch [3/10], Training Loss: 0.887, Validation Accuracy: 62.45%\n",
            "Epoch [4/10], Training Loss: 0.859, Validation Accuracy: 62.44%\n",
            "Epoch [5/10], Training Loss: 0.842, Validation Accuracy: 61.87%\n",
            "Epoch [6/10], Training Loss: 0.822, Validation Accuracy: 61.92%\n",
            "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 61.48%\n",
            "Epoch [8/10], Training Loss: 0.797, Validation Accuracy: 61.69%\n",
            "Epoch [9/10], Training Loss: 0.778, Validation Accuracy: 61.48%\n",
            "Epoch [10/10], Training Loss: 0.761, Validation Accuracy: 61.24%\n",
            "Epoch [1/10], Training Loss: 0.959, Validation Accuracy: 61.60%\n",
            "Epoch [2/10], Training Loss: 0.906, Validation Accuracy: 62.33%\n",
            "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 62.08%\n",
            "Epoch [4/10], Training Loss: 0.851, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.823, Validation Accuracy: 61.93%\n",
            "Epoch [6/10], Training Loss: 0.799, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.782, Validation Accuracy: 61.93%\n",
            "Epoch [8/10], Training Loss: 0.760, Validation Accuracy: 62.20%\n",
            "Epoch [9/10], Training Loss: 0.753, Validation Accuracy: 61.94%\n",
            "Epoch [10/10], Training Loss: 0.745, Validation Accuracy: 61.72%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 62.52%\n",
            "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 62.27%\n",
            "Epoch [3/10], Training Loss: 0.872, Validation Accuracy: 62.39%\n",
            "Epoch [4/10], Training Loss: 0.840, Validation Accuracy: 62.03%\n",
            "Epoch [5/10], Training Loss: 0.822, Validation Accuracy: 62.45%\n",
            "Epoch [6/10], Training Loss: 0.798, Validation Accuracy: 62.58%\n",
            "Epoch [7/10], Training Loss: 0.788, Validation Accuracy: 61.66%\n",
            "Epoch [8/10], Training Loss: 0.765, Validation Accuracy: 62.10%\n",
            "Epoch [9/10], Training Loss: 0.753, Validation Accuracy: 62.00%\n",
            "Epoch [10/10], Training Loss: 0.735, Validation Accuracy: 61.81%\n",
            "Epoch [1/10], Training Loss: 0.941, Validation Accuracy: 61.58%\n",
            "Epoch [2/10], Training Loss: 0.881, Validation Accuracy: 61.52%\n",
            "Epoch [3/10], Training Loss: 0.844, Validation Accuracy: 61.82%\n",
            "Epoch [4/10], Training Loss: 0.817, Validation Accuracy: 62.77%\n",
            "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 62.12%\n",
            "Epoch [6/10], Training Loss: 0.773, Validation Accuracy: 62.34%\n",
            "Epoch [7/10], Training Loss: 0.754, Validation Accuracy: 62.68%\n",
            "Epoch [8/10], Training Loss: 0.730, Validation Accuracy: 61.54%\n",
            "Epoch [9/10], Training Loss: 0.721, Validation Accuracy: 62.61%\n",
            "Epoch [10/10], Training Loss: 0.694, Validation Accuracy: 62.42%\n",
            "Epoch [1/10], Training Loss: 0.943, Validation Accuracy: 62.31%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 62.01%\n",
            "Epoch [3/10], Training Loss: 0.832, Validation Accuracy: 62.65%\n",
            "Epoch [4/10], Training Loss: 0.797, Validation Accuracy: 63.09%\n",
            "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 63.23%\n",
            "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 62.15%\n",
            "Epoch [7/10], Training Loss: 0.734, Validation Accuracy: 62.63%\n",
            "Epoch [8/10], Training Loss: 0.713, Validation Accuracy: 62.23%\n",
            "Epoch [9/10], Training Loss: 0.703, Validation Accuracy: 62.03%\n",
            "Epoch [10/10], Training Loss: 0.688, Validation Accuracy: 61.49%\n",
            "Epoch [1/10], Training Loss: 0.922, Validation Accuracy: 62.34%\n",
            "Epoch [2/10], Training Loss: 0.848, Validation Accuracy: 62.59%\n",
            "Epoch [3/10], Training Loss: 0.811, Validation Accuracy: 62.05%\n",
            "Epoch [4/10], Training Loss: 0.775, Validation Accuracy: 62.65%\n",
            "Epoch [5/10], Training Loss: 0.750, Validation Accuracy: 61.96%\n",
            "Epoch [6/10], Training Loss: 0.724, Validation Accuracy: 62.19%\n",
            "Epoch [7/10], Training Loss: 0.708, Validation Accuracy: 62.46%\n",
            "Epoch [8/10], Training Loss: 0.692, Validation Accuracy: 61.70%\n",
            "Epoch [9/10], Training Loss: 0.685, Validation Accuracy: 61.64%\n",
            "Epoch [10/10], Training Loss: 0.666, Validation Accuracy: 61.75%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 61.05%\n",
            "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 62.37%\n",
            "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 62.29%\n",
            "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 62.09%\n",
            "Epoch [5/10], Training Loss: 0.738, Validation Accuracy: 62.09%\n",
            "Epoch [6/10], Training Loss: 0.715, Validation Accuracy: 62.81%\n",
            "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 62.39%\n",
            "Epoch [8/10], Training Loss: 0.685, Validation Accuracy: 62.42%\n",
            "Epoch [9/10], Training Loss: 0.669, Validation Accuracy: 61.72%\n",
            "Epoch [10/10], Training Loss: 0.642, Validation Accuracy: 61.92%\n",
            "Epoch [1/10], Training Loss: 0.914, Validation Accuracy: 62.57%\n",
            "Epoch [2/10], Training Loss: 0.828, Validation Accuracy: 62.73%\n",
            "Epoch [3/10], Training Loss: 0.787, Validation Accuracy: 61.86%\n",
            "Epoch [4/10], Training Loss: 0.771, Validation Accuracy: 62.39%\n",
            "Epoch [5/10], Training Loss: 0.737, Validation Accuracy: 62.47%\n",
            "Epoch [6/10], Training Loss: 0.713, Validation Accuracy: 62.21%\n",
            "Epoch [7/10], Training Loss: 0.689, Validation Accuracy: 62.14%\n",
            "Epoch [8/10], Training Loss: 0.673, Validation Accuracy: 62.59%\n",
            "Epoch [9/10], Training Loss: 0.650, Validation Accuracy: 62.52%\n",
            "Epoch [10/10], Training Loss: 0.632, Validation Accuracy: 61.68%\n",
            "Epoch [1/10], Training Loss: 0.885, Validation Accuracy: 62.00%\n",
            "Epoch [2/10], Training Loss: 0.812, Validation Accuracy: 61.37%\n",
            "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 62.38%\n",
            "Epoch [4/10], Training Loss: 0.733, Validation Accuracy: 61.84%\n",
            "Epoch [5/10], Training Loss: 0.697, Validation Accuracy: 62.49%\n",
            "Epoch [6/10], Training Loss: 0.673, Validation Accuracy: 62.29%\n",
            "Epoch [7/10], Training Loss: 0.656, Validation Accuracy: 62.77%\n",
            "Epoch [8/10], Training Loss: 0.638, Validation Accuracy: 62.31%\n",
            "Epoch [9/10], Training Loss: 0.620, Validation Accuracy: 62.34%\n",
            "Epoch [10/10], Training Loss: 0.604, Validation Accuracy: 62.38%\n",
            "Epoch [1/10], Training Loss: 0.899, Validation Accuracy: 62.45%\n",
            "Epoch [2/10], Training Loss: 0.802, Validation Accuracy: 62.95%\n",
            "Epoch [3/10], Training Loss: 0.755, Validation Accuracy: 61.90%\n",
            "Epoch [4/10], Training Loss: 0.724, Validation Accuracy: 62.52%\n",
            "Epoch [5/10], Training Loss: 0.695, Validation Accuracy: 61.99%\n",
            "Epoch [6/10], Training Loss: 0.683, Validation Accuracy: 61.74%\n",
            "Epoch [7/10], Training Loss: 0.646, Validation Accuracy: 62.41%\n",
            "Epoch [8/10], Training Loss: 0.629, Validation Accuracy: 62.20%\n",
            "Epoch [9/10], Training Loss: 0.601, Validation Accuracy: 61.55%\n",
            "Epoch [10/10], Training Loss: 0.583, Validation Accuracy: 62.40%\n",
            "Epoch [1/10], Training Loss: 0.862, Validation Accuracy: 61.60%\n",
            "Epoch [2/10], Training Loss: 0.779, Validation Accuracy: 61.81%\n",
            "Epoch [3/10], Training Loss: 0.730, Validation Accuracy: 62.52%\n",
            "Epoch [4/10], Training Loss: 0.691, Validation Accuracy: 62.06%\n",
            "Epoch [5/10], Training Loss: 0.663, Validation Accuracy: 62.74%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 62.65%\n",
            "Epoch [7/10], Training Loss: 0.615, Validation Accuracy: 61.97%\n",
            "Epoch [8/10], Training Loss: 0.605, Validation Accuracy: 61.83%\n",
            "Epoch [9/10], Training Loss: 0.578, Validation Accuracy: 62.11%\n",
            "Epoch [10/10], Training Loss: 0.562, Validation Accuracy: 61.95%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 62.17%\n",
            "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 62.16%\n",
            "Epoch [3/10], Training Loss: 0.717, Validation Accuracy: 62.24%\n",
            "Epoch [4/10], Training Loss: 0.679, Validation Accuracy: 62.16%\n",
            "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 62.30%\n",
            "Epoch [6/10], Training Loss: 0.616, Validation Accuracy: 61.55%\n",
            "Epoch [7/10], Training Loss: 0.599, Validation Accuracy: 61.99%\n",
            "Epoch [8/10], Training Loss: 0.586, Validation Accuracy: 62.03%\n",
            "Epoch [9/10], Training Loss: 0.561, Validation Accuracy: 61.91%\n",
            "Epoch [10/10], Training Loss: 0.544, Validation Accuracy: 61.82%\n",
            "Epoch [1/10], Training Loss: 0.868, Validation Accuracy: 62.54%\n",
            "Epoch [2/10], Training Loss: 0.771, Validation Accuracy: 62.59%\n",
            "Epoch [3/10], Training Loss: 0.725, Validation Accuracy: 63.09%\n",
            "Epoch [4/10], Training Loss: 0.689, Validation Accuracy: 61.79%\n",
            "Epoch [5/10], Training Loss: 0.653, Validation Accuracy: 62.88%\n",
            "Epoch [6/10], Training Loss: 0.621, Validation Accuracy: 62.33%\n",
            "Epoch [7/10], Training Loss: 0.593, Validation Accuracy: 62.47%\n",
            "Epoch [8/10], Training Loss: 0.586, Validation Accuracy: 62.12%\n",
            "Epoch [9/10], Training Loss: 0.553, Validation Accuracy: 62.22%\n",
            "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 61.85%\n",
            "Epoch [1/10], Training Loss: 0.843, Validation Accuracy: 61.92%\n",
            "Epoch [2/10], Training Loss: 0.747, Validation Accuracy: 62.10%\n",
            "Epoch [3/10], Training Loss: 0.688, Validation Accuracy: 61.44%\n",
            "Epoch [4/10], Training Loss: 0.670, Validation Accuracy: 62.89%\n",
            "Epoch [5/10], Training Loss: 0.624, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.599, Validation Accuracy: 62.22%\n",
            "Epoch [7/10], Training Loss: 0.565, Validation Accuracy: 61.82%\n",
            "Epoch [8/10], Training Loss: 0.551, Validation Accuracy: 62.17%\n",
            "Epoch [9/10], Training Loss: 0.542, Validation Accuracy: 62.02%\n",
            "Epoch [10/10], Training Loss: 0.510, Validation Accuracy: 61.75%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 62.01%\n",
            "Epoch [2/10], Training Loss: 0.744, Validation Accuracy: 62.53%\n",
            "Epoch [3/10], Training Loss: 0.693, Validation Accuracy: 61.68%\n",
            "Epoch [4/10], Training Loss: 0.647, Validation Accuracy: 62.72%\n",
            "Epoch [5/10], Training Loss: 0.609, Validation Accuracy: 60.79%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 62.35%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 61.92%\n",
            "Epoch [8/10], Training Loss: 0.536, Validation Accuracy: 61.83%\n",
            "Epoch [9/10], Training Loss: 0.518, Validation Accuracy: 61.93%\n",
            "Epoch [10/10], Training Loss: 0.489, Validation Accuracy: 62.02%\n",
            "Epoch [1/10], Training Loss: 0.818, Validation Accuracy: 61.69%\n",
            "Epoch [2/10], Training Loss: 0.703, Validation Accuracy: 62.05%\n",
            "Epoch [3/10], Training Loss: 0.657, Validation Accuracy: 61.88%\n",
            "Epoch [4/10], Training Loss: 0.614, Validation Accuracy: 61.98%\n",
            "Epoch [5/10], Training Loss: 0.577, Validation Accuracy: 62.10%\n",
            "Epoch [6/10], Training Loss: 0.553, Validation Accuracy: 61.86%\n",
            "Epoch [7/10], Training Loss: 0.528, Validation Accuracy: 61.93%\n",
            "Epoch [8/10], Training Loss: 0.506, Validation Accuracy: 61.20%\n",
            "Epoch [9/10], Training Loss: 0.484, Validation Accuracy: 61.30%\n",
            "Epoch [10/10], Training Loss: 0.459, Validation Accuracy: 61.81%\n",
            "Epoch [1/10], Training Loss: 0.817, Validation Accuracy: 61.21%\n",
            "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 61.63%\n",
            "Epoch [3/10], Training Loss: 0.642, Validation Accuracy: 61.36%\n",
            "Epoch [4/10], Training Loss: 0.605, Validation Accuracy: 61.56%\n",
            "Epoch [5/10], Training Loss: 0.568, Validation Accuracy: 61.35%\n",
            "Epoch [6/10], Training Loss: 0.542, Validation Accuracy: 62.09%\n",
            "Epoch [7/10], Training Loss: 0.511, Validation Accuracy: 61.57%\n",
            "Epoch [8/10], Training Loss: 0.493, Validation Accuracy: 61.48%\n",
            "Epoch [9/10], Training Loss: 0.472, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.457, Validation Accuracy: 61.45%\n",
            "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 61.50%\n",
            "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 61.69%\n",
            "Epoch [3/10], Training Loss: 0.649, Validation Accuracy: 62.41%\n",
            "Epoch [4/10], Training Loss: 0.608, Validation Accuracy: 62.22%\n",
            "Epoch [5/10], Training Loss: 0.574, Validation Accuracy: 61.89%\n",
            "Epoch [6/10], Training Loss: 0.550, Validation Accuracy: 62.33%\n",
            "Epoch [7/10], Training Loss: 0.513, Validation Accuracy: 61.94%\n",
            "Epoch [8/10], Training Loss: 0.495, Validation Accuracy: 61.73%\n",
            "Epoch [9/10], Training Loss: 0.471, Validation Accuracy: 61.58%\n",
            "Epoch [10/10], Training Loss: 0.462, Validation Accuracy: 61.19%\n",
            "Epoch [1/10], Training Loss: 0.794, Validation Accuracy: 61.26%\n",
            "Epoch [2/10], Training Loss: 0.678, Validation Accuracy: 62.13%\n",
            "Epoch [3/10], Training Loss: 0.627, Validation Accuracy: 61.77%\n",
            "Epoch [4/10], Training Loss: 0.573, Validation Accuracy: 61.74%\n",
            "Epoch [5/10], Training Loss: 0.543, Validation Accuracy: 61.06%\n",
            "Epoch [6/10], Training Loss: 0.517, Validation Accuracy: 61.32%\n",
            "Epoch [7/10], Training Loss: 0.484, Validation Accuracy: 61.71%\n",
            "Epoch [8/10], Training Loss: 0.468, Validation Accuracy: 61.19%\n",
            "Epoch [9/10], Training Loss: 0.445, Validation Accuracy: 61.48%\n",
            "Epoch [10/10], Training Loss: 0.422, Validation Accuracy: 61.68%\n",
            "Epoch [1/10], Training Loss: 0.822, Validation Accuracy: 62.00%\n",
            "Epoch [2/10], Training Loss: 0.686, Validation Accuracy: 61.67%\n",
            "Epoch [3/10], Training Loss: 0.610, Validation Accuracy: 61.65%\n",
            "Epoch [4/10], Training Loss: 0.565, Validation Accuracy: 61.73%\n",
            "Epoch [5/10], Training Loss: 0.545, Validation Accuracy: 62.35%\n",
            "Epoch [6/10], Training Loss: 0.503, Validation Accuracy: 61.29%\n",
            "Epoch [7/10], Training Loss: 0.472, Validation Accuracy: 62.17%\n",
            "Epoch [8/10], Training Loss: 0.450, Validation Accuracy: 60.85%\n",
            "Epoch [9/10], Training Loss: 0.430, Validation Accuracy: 61.77%\n",
            "Epoch [10/10], Training Loss: 0.401, Validation Accuracy: 61.80%\n",
            "Epoch [1/10], Training Loss: 0.774, Validation Accuracy: 61.14%\n",
            "Epoch [2/10], Training Loss: 0.669, Validation Accuracy: 60.98%\n",
            "Epoch [3/10], Training Loss: 0.595, Validation Accuracy: 62.07%\n",
            "Epoch [4/10], Training Loss: 0.534, Validation Accuracy: 61.77%\n",
            "Epoch [5/10], Training Loss: 0.497, Validation Accuracy: 61.37%\n",
            "Epoch [6/10], Training Loss: 0.462, Validation Accuracy: 61.52%\n",
            "Epoch [7/10], Training Loss: 0.442, Validation Accuracy: 61.83%\n",
            "Epoch [8/10], Training Loss: 0.411, Validation Accuracy: 61.21%\n",
            "Epoch [9/10], Training Loss: 0.395, Validation Accuracy: 61.63%\n",
            "Epoch [10/10], Training Loss: 0.371, Validation Accuracy: 61.30%\n",
            "Epoch [1/10], Training Loss: 0.769, Validation Accuracy: 60.96%\n",
            "Epoch [2/10], Training Loss: 0.632, Validation Accuracy: 61.30%\n",
            "Epoch [3/10], Training Loss: 0.573, Validation Accuracy: 60.91%\n",
            "Epoch [4/10], Training Loss: 0.519, Validation Accuracy: 61.53%\n",
            "Epoch [5/10], Training Loss: 0.481, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.456, Validation Accuracy: 61.60%\n",
            "Epoch [7/10], Training Loss: 0.427, Validation Accuracy: 61.44%\n",
            "Epoch [8/10], Training Loss: 0.412, Validation Accuracy: 61.66%\n",
            "Epoch [9/10], Training Loss: 0.394, Validation Accuracy: 60.65%\n",
            "Epoch [10/10], Training Loss: 0.365, Validation Accuracy: 61.51%\n",
            "Confusion Matrix:\n",
            "[[688  32  72  21  31   8  18   5  90  35]\n",
            " [ 39 730  21  21  10   4   8  12  48 107]\n",
            " [ 77  14 536  80 114  65  56  30  15  13]\n",
            " [ 31  18  88 425  80 179  92  38  17  32]\n",
            " [ 33   5 119  60 552  61  62  86  16   6]\n",
            " [ 23   6  84 181  61 510  39  75  13   8]\n",
            " [  9  10  70  72  73  32 707   9  11   7]\n",
            " [ 20   6  64  47  81  68  12 674   4  24]\n",
            " [ 88  44  19  26  15   5   9   7 753  34]\n",
            " [ 50 152  15  19  16  14  18  18  63 635]]\n",
            "Test Accuracy: 62.10%\n",
            "True Positives (TP): [688 730 536 425 552 510 707 674 753 635]\n",
            "False Positives (FP): [370 287 552 527 481 436 314 280 277 266]\n",
            "True Negatives (TN): [8630 8713 8448 8473 8519 8564 8686 8720 8723 8734]\n",
            "False Negatives (FN): [312 270 464 575 448 490 293 326 247 365]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.65028355 0.71779744 0.49264706 0.44642857 0.53436592 0.53911205\n",
            " 0.69245837 0.70649895 0.73106796 0.70477248]\n",
            "Recall: [0.688 0.73  0.536 0.425 0.552 0.51  0.707 0.674 0.753 0.635]\n",
            "F1 Score: [0.6686103  0.7238473  0.51340996 0.43545082 0.54303984 0.52415211\n",
            " 0.69965364 0.68986694 0.74187192 0.66806944]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE,  distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average =  augmented_data_truncated\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}