{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Kl-FedDis-Research-/blob/main/ordinary_truncated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNXA5i6ve3Wz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from typing import Dict\n",
        "from scipy.stats import truncnorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp2zIBwYfDJX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVN9tba-fG27"
      },
      "outputs": [],
      "source": [
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJLVsmDtfKj4"
      },
      "outputs": [],
      "source": [
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlz4UTFXfQNp",
        "outputId": "ebe0ad6e-c882-4c25-e29b-14e722536777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 77306790.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raWvvQKufUjz"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arC0NiXAfYXH"
      },
      "outputs": [],
      "source": [
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4g2A2W6fcNd"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "453JmsekfhkR"
      },
      "outputs": [],
      "source": [
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJq5CiZQflxQ"
      },
      "outputs": [],
      "source": [
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gOSb3rRfpxk"
      },
      "outputs": [],
      "source": [
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ry_R_Myftfz"
      },
      "outputs": [],
      "source": [
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXS1j6PEfxMR"
      },
      "outputs": [],
      "source": [
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xtNAWdRf237"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both normal and truncated normal distributions\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    # Generate augmented data from normal distribution\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2S5RwKHf7Gd"
      },
      "outputs": [],
      "source": [
        "# Define the federated training logic\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs9eycMOf-_5"
      },
      "outputs": [],
      "source": [
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ev0rP0gDCT"
      },
      "outputs": [],
      "source": [
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0a2npibgGZW",
        "outputId": "bf37920a-7f41-412a-d6f8-e70374d51f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.91%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.95%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.01%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 10.32%\n",
            "Epoch [5/10], Training Loss: 2.299, Validation Accuracy: 11.35%\n",
            "Epoch [6/10], Training Loss: 2.298, Validation Accuracy: 13.96%\n",
            "Epoch [7/10], Training Loss: 2.296, Validation Accuracy: 14.94%\n",
            "Epoch [8/10], Training Loss: 2.294, Validation Accuracy: 14.61%\n",
            "Epoch [9/10], Training Loss: 2.290, Validation Accuracy: 14.43%\n",
            "Epoch [10/10], Training Loss: 2.286, Validation Accuracy: 14.36%\n",
            "Epoch [1/10], Training Loss: 2.280, Validation Accuracy: 14.73%\n",
            "Epoch [2/10], Training Loss: 2.270, Validation Accuracy: 13.47%\n",
            "Epoch [3/10], Training Loss: 2.256, Validation Accuracy: 12.90%\n",
            "Epoch [4/10], Training Loss: 2.240, Validation Accuracy: 13.76%\n",
            "Epoch [5/10], Training Loss: 2.225, Validation Accuracy: 14.83%\n",
            "Epoch [6/10], Training Loss: 2.209, Validation Accuracy: 17.45%\n",
            "Epoch [7/10], Training Loss: 2.193, Validation Accuracy: 20.39%\n",
            "Epoch [8/10], Training Loss: 2.176, Validation Accuracy: 21.49%\n",
            "Epoch [9/10], Training Loss: 2.159, Validation Accuracy: 22.28%\n",
            "Epoch [10/10], Training Loss: 2.141, Validation Accuracy: 22.26%\n",
            "Epoch [1/10], Training Loss: 2.132, Validation Accuracy: 23.27%\n",
            "Epoch [2/10], Training Loss: 2.115, Validation Accuracy: 24.04%\n",
            "Epoch [3/10], Training Loss: 2.097, Validation Accuracy: 24.15%\n",
            "Epoch [4/10], Training Loss: 2.081, Validation Accuracy: 24.52%\n",
            "Epoch [5/10], Training Loss: 2.064, Validation Accuracy: 25.74%\n",
            "Epoch [6/10], Training Loss: 2.048, Validation Accuracy: 26.24%\n",
            "Epoch [7/10], Training Loss: 2.034, Validation Accuracy: 27.28%\n",
            "Epoch [8/10], Training Loss: 2.021, Validation Accuracy: 27.28%\n",
            "Epoch [9/10], Training Loss: 2.014, Validation Accuracy: 27.69%\n",
            "Epoch [10/10], Training Loss: 2.002, Validation Accuracy: 28.16%\n",
            "Epoch [1/10], Training Loss: 2.015, Validation Accuracy: 28.74%\n",
            "Epoch [2/10], Training Loss: 2.006, Validation Accuracy: 28.87%\n",
            "Epoch [3/10], Training Loss: 1.997, Validation Accuracy: 29.11%\n",
            "Epoch [4/10], Training Loss: 1.990, Validation Accuracy: 29.21%\n",
            "Epoch [5/10], Training Loss: 1.984, Validation Accuracy: 29.63%\n",
            "Epoch [6/10], Training Loss: 1.974, Validation Accuracy: 29.73%\n",
            "Epoch [7/10], Training Loss: 1.968, Validation Accuracy: 30.00%\n",
            "Epoch [8/10], Training Loss: 1.959, Validation Accuracy: 30.35%\n",
            "Epoch [9/10], Training Loss: 1.952, Validation Accuracy: 30.38%\n",
            "Epoch [10/10], Training Loss: 1.943, Validation Accuracy: 30.63%\n",
            "Epoch [1/10], Training Loss: 1.934, Validation Accuracy: 30.64%\n",
            "Epoch [2/10], Training Loss: 1.924, Validation Accuracy: 30.24%\n",
            "Epoch [3/10], Training Loss: 1.916, Validation Accuracy: 30.22%\n",
            "Epoch [4/10], Training Loss: 1.910, Validation Accuracy: 31.49%\n",
            "Epoch [5/10], Training Loss: 1.898, Validation Accuracy: 30.74%\n",
            "Epoch [6/10], Training Loss: 1.890, Validation Accuracy: 31.36%\n",
            "Epoch [7/10], Training Loss: 1.883, Validation Accuracy: 32.43%\n",
            "Epoch [8/10], Training Loss: 1.870, Validation Accuracy: 32.85%\n",
            "Epoch [9/10], Training Loss: 1.861, Validation Accuracy: 32.62%\n",
            "Epoch [10/10], Training Loss: 1.853, Validation Accuracy: 33.41%\n",
            "Epoch [1/10], Training Loss: 1.848, Validation Accuracy: 34.36%\n",
            "Epoch [2/10], Training Loss: 1.830, Validation Accuracy: 35.23%\n",
            "Epoch [3/10], Training Loss: 1.812, Validation Accuracy: 35.81%\n",
            "Epoch [4/10], Training Loss: 1.798, Validation Accuracy: 35.77%\n",
            "Epoch [5/10], Training Loss: 1.781, Validation Accuracy: 36.13%\n",
            "Epoch [6/10], Training Loss: 1.763, Validation Accuracy: 37.29%\n",
            "Epoch [7/10], Training Loss: 1.745, Validation Accuracy: 38.11%\n",
            "Epoch [8/10], Training Loss: 1.720, Validation Accuracy: 38.29%\n",
            "Epoch [9/10], Training Loss: 1.704, Validation Accuracy: 38.63%\n",
            "Epoch [10/10], Training Loss: 1.688, Validation Accuracy: 39.31%\n",
            "Epoch [1/10], Training Loss: 1.688, Validation Accuracy: 39.39%\n",
            "Epoch [2/10], Training Loss: 1.672, Validation Accuracy: 40.06%\n",
            "Epoch [3/10], Training Loss: 1.652, Validation Accuracy: 40.30%\n",
            "Epoch [4/10], Training Loss: 1.636, Validation Accuracy: 41.02%\n",
            "Epoch [5/10], Training Loss: 1.625, Validation Accuracy: 41.04%\n",
            "Epoch [6/10], Training Loss: 1.607, Validation Accuracy: 41.73%\n",
            "Epoch [7/10], Training Loss: 1.594, Validation Accuracy: 42.45%\n",
            "Epoch [8/10], Training Loss: 1.578, Validation Accuracy: 42.53%\n",
            "Epoch [9/10], Training Loss: 1.567, Validation Accuracy: 42.67%\n",
            "Epoch [10/10], Training Loss: 1.553, Validation Accuracy: 42.95%\n",
            "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 43.05%\n",
            "Epoch [2/10], Training Loss: 1.572, Validation Accuracy: 43.48%\n",
            "Epoch [3/10], Training Loss: 1.550, Validation Accuracy: 44.71%\n",
            "Epoch [4/10], Training Loss: 1.536, Validation Accuracy: 42.63%\n",
            "Epoch [5/10], Training Loss: 1.526, Validation Accuracy: 44.53%\n",
            "Epoch [6/10], Training Loss: 1.511, Validation Accuracy: 45.72%\n",
            "Epoch [7/10], Training Loss: 1.501, Validation Accuracy: 45.51%\n",
            "Epoch [8/10], Training Loss: 1.487, Validation Accuracy: 45.29%\n",
            "Epoch [9/10], Training Loss: 1.481, Validation Accuracy: 46.20%\n",
            "Epoch [10/10], Training Loss: 1.468, Validation Accuracy: 45.93%\n",
            "Epoch [1/10], Training Loss: 1.507, Validation Accuracy: 47.68%\n",
            "Epoch [2/10], Training Loss: 1.484, Validation Accuracy: 47.06%\n",
            "Epoch [3/10], Training Loss: 1.478, Validation Accuracy: 47.85%\n",
            "Epoch [4/10], Training Loss: 1.464, Validation Accuracy: 47.19%\n",
            "Epoch [5/10], Training Loss: 1.454, Validation Accuracy: 48.03%\n",
            "Epoch [6/10], Training Loss: 1.442, Validation Accuracy: 48.31%\n",
            "Epoch [7/10], Training Loss: 1.439, Validation Accuracy: 48.54%\n",
            "Epoch [8/10], Training Loss: 1.422, Validation Accuracy: 48.43%\n",
            "Epoch [9/10], Training Loss: 1.421, Validation Accuracy: 47.38%\n",
            "Epoch [10/10], Training Loss: 1.412, Validation Accuracy: 48.43%\n",
            "Epoch [1/10], Training Loss: 1.457, Validation Accuracy: 49.06%\n",
            "Epoch [2/10], Training Loss: 1.444, Validation Accuracy: 48.82%\n",
            "Epoch [3/10], Training Loss: 1.436, Validation Accuracy: 49.75%\n",
            "Epoch [4/10], Training Loss: 1.418, Validation Accuracy: 49.48%\n",
            "Epoch [5/10], Training Loss: 1.408, Validation Accuracy: 49.59%\n",
            "Epoch [6/10], Training Loss: 1.398, Validation Accuracy: 49.91%\n",
            "Epoch [7/10], Training Loss: 1.386, Validation Accuracy: 50.26%\n",
            "Epoch [8/10], Training Loss: 1.382, Validation Accuracy: 50.17%\n",
            "Epoch [9/10], Training Loss: 1.372, Validation Accuracy: 50.59%\n",
            "Epoch [10/10], Training Loss: 1.364, Validation Accuracy: 49.57%\n",
            "Epoch [1/10], Training Loss: 1.374, Validation Accuracy: 50.21%\n",
            "Epoch [2/10], Training Loss: 1.359, Validation Accuracy: 51.17%\n",
            "Epoch [3/10], Training Loss: 1.335, Validation Accuracy: 51.09%\n",
            "Epoch [4/10], Training Loss: 1.335, Validation Accuracy: 51.55%\n",
            "Epoch [5/10], Training Loss: 1.322, Validation Accuracy: 51.92%\n",
            "Epoch [6/10], Training Loss: 1.319, Validation Accuracy: 51.71%\n",
            "Epoch [7/10], Training Loss: 1.302, Validation Accuracy: 51.11%\n",
            "Epoch [8/10], Training Loss: 1.296, Validation Accuracy: 51.53%\n",
            "Epoch [9/10], Training Loss: 1.292, Validation Accuracy: 52.28%\n",
            "Epoch [10/10], Training Loss: 1.284, Validation Accuracy: 51.92%\n",
            "Epoch [1/10], Training Loss: 1.348, Validation Accuracy: 51.60%\n",
            "Epoch [2/10], Training Loss: 1.328, Validation Accuracy: 53.05%\n",
            "Epoch [3/10], Training Loss: 1.312, Validation Accuracy: 52.89%\n",
            "Epoch [4/10], Training Loss: 1.307, Validation Accuracy: 52.60%\n",
            "Epoch [5/10], Training Loss: 1.299, Validation Accuracy: 52.02%\n",
            "Epoch [6/10], Training Loss: 1.295, Validation Accuracy: 52.30%\n",
            "Epoch [7/10], Training Loss: 1.281, Validation Accuracy: 52.50%\n",
            "Epoch [8/10], Training Loss: 1.272, Validation Accuracy: 52.14%\n",
            "Epoch [9/10], Training Loss: 1.268, Validation Accuracy: 53.26%\n",
            "Epoch [10/10], Training Loss: 1.262, Validation Accuracy: 51.93%\n",
            "Epoch [1/10], Training Loss: 1.328, Validation Accuracy: 51.59%\n",
            "Epoch [2/10], Training Loss: 1.318, Validation Accuracy: 52.85%\n",
            "Epoch [3/10], Training Loss: 1.291, Validation Accuracy: 54.03%\n",
            "Epoch [4/10], Training Loss: 1.285, Validation Accuracy: 53.53%\n",
            "Epoch [5/10], Training Loss: 1.283, Validation Accuracy: 53.57%\n",
            "Epoch [6/10], Training Loss: 1.268, Validation Accuracy: 53.54%\n",
            "Epoch [7/10], Training Loss: 1.255, Validation Accuracy: 53.80%\n",
            "Epoch [8/10], Training Loss: 1.246, Validation Accuracy: 53.84%\n",
            "Epoch [9/10], Training Loss: 1.246, Validation Accuracy: 53.67%\n",
            "Epoch [10/10], Training Loss: 1.240, Validation Accuracy: 53.73%\n",
            "Epoch [1/10], Training Loss: 1.317, Validation Accuracy: 54.46%\n",
            "Epoch [2/10], Training Loss: 1.285, Validation Accuracy: 53.80%\n",
            "Epoch [3/10], Training Loss: 1.280, Validation Accuracy: 54.27%\n",
            "Epoch [4/10], Training Loss: 1.262, Validation Accuracy: 53.47%\n",
            "Epoch [5/10], Training Loss: 1.254, Validation Accuracy: 54.54%\n",
            "Epoch [6/10], Training Loss: 1.240, Validation Accuracy: 53.77%\n",
            "Epoch [7/10], Training Loss: 1.231, Validation Accuracy: 54.77%\n",
            "Epoch [8/10], Training Loss: 1.223, Validation Accuracy: 54.37%\n",
            "Epoch [9/10], Training Loss: 1.221, Validation Accuracy: 55.06%\n",
            "Epoch [10/10], Training Loss: 1.207, Validation Accuracy: 55.25%\n",
            "Epoch [1/10], Training Loss: 1.282, Validation Accuracy: 55.42%\n",
            "Epoch [2/10], Training Loss: 1.254, Validation Accuracy: 55.36%\n",
            "Epoch [3/10], Training Loss: 1.248, Validation Accuracy: 55.60%\n",
            "Epoch [4/10], Training Loss: 1.230, Validation Accuracy: 55.58%\n",
            "Epoch [5/10], Training Loss: 1.221, Validation Accuracy: 56.08%\n",
            "Epoch [6/10], Training Loss: 1.204, Validation Accuracy: 56.16%\n",
            "Epoch [7/10], Training Loss: 1.198, Validation Accuracy: 55.80%\n",
            "Epoch [8/10], Training Loss: 1.193, Validation Accuracy: 55.85%\n",
            "Epoch [9/10], Training Loss: 1.183, Validation Accuracy: 55.81%\n",
            "Epoch [10/10], Training Loss: 1.174, Validation Accuracy: 55.62%\n",
            "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 56.12%\n",
            "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 56.56%\n",
            "Epoch [3/10], Training Loss: 1.178, Validation Accuracy: 56.39%\n",
            "Epoch [4/10], Training Loss: 1.165, Validation Accuracy: 56.79%\n",
            "Epoch [5/10], Training Loss: 1.147, Validation Accuracy: 55.96%\n",
            "Epoch [6/10], Training Loss: 1.137, Validation Accuracy: 55.65%\n",
            "Epoch [7/10], Training Loss: 1.136, Validation Accuracy: 56.87%\n",
            "Epoch [8/10], Training Loss: 1.129, Validation Accuracy: 56.70%\n",
            "Epoch [9/10], Training Loss: 1.117, Validation Accuracy: 57.08%\n",
            "Epoch [10/10], Training Loss: 1.107, Validation Accuracy: 57.06%\n",
            "Epoch [1/10], Training Loss: 1.215, Validation Accuracy: 56.64%\n",
            "Epoch [2/10], Training Loss: 1.185, Validation Accuracy: 56.58%\n",
            "Epoch [3/10], Training Loss: 1.174, Validation Accuracy: 55.72%\n",
            "Epoch [4/10], Training Loss: 1.162, Validation Accuracy: 57.05%\n",
            "Epoch [5/10], Training Loss: 1.143, Validation Accuracy: 56.85%\n",
            "Epoch [6/10], Training Loss: 1.129, Validation Accuracy: 57.03%\n",
            "Epoch [7/10], Training Loss: 1.124, Validation Accuracy: 57.02%\n",
            "Epoch [8/10], Training Loss: 1.113, Validation Accuracy: 57.15%\n",
            "Epoch [9/10], Training Loss: 1.101, Validation Accuracy: 56.67%\n",
            "Epoch [10/10], Training Loss: 1.092, Validation Accuracy: 57.87%\n",
            "Epoch [1/10], Training Loss: 1.198, Validation Accuracy: 57.55%\n",
            "Epoch [2/10], Training Loss: 1.171, Validation Accuracy: 56.83%\n",
            "Epoch [3/10], Training Loss: 1.158, Validation Accuracy: 57.52%\n",
            "Epoch [4/10], Training Loss: 1.143, Validation Accuracy: 57.32%\n",
            "Epoch [5/10], Training Loss: 1.124, Validation Accuracy: 58.22%\n",
            "Epoch [6/10], Training Loss: 1.118, Validation Accuracy: 57.70%\n",
            "Epoch [7/10], Training Loss: 1.103, Validation Accuracy: 57.66%\n",
            "Epoch [8/10], Training Loss: 1.097, Validation Accuracy: 57.58%\n",
            "Epoch [9/10], Training Loss: 1.087, Validation Accuracy: 57.88%\n",
            "Epoch [10/10], Training Loss: 1.073, Validation Accuracy: 57.60%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 58.76%\n",
            "Epoch [2/10], Training Loss: 1.157, Validation Accuracy: 58.27%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 57.97%\n",
            "Epoch [4/10], Training Loss: 1.122, Validation Accuracy: 58.54%\n",
            "Epoch [5/10], Training Loss: 1.110, Validation Accuracy: 58.41%\n",
            "Epoch [6/10], Training Loss: 1.093, Validation Accuracy: 58.03%\n",
            "Epoch [7/10], Training Loss: 1.090, Validation Accuracy: 58.26%\n",
            "Epoch [8/10], Training Loss: 1.071, Validation Accuracy: 58.60%\n",
            "Epoch [9/10], Training Loss: 1.066, Validation Accuracy: 58.53%\n",
            "Epoch [10/10], Training Loss: 1.052, Validation Accuracy: 58.26%\n",
            "Epoch [1/10], Training Loss: 1.167, Validation Accuracy: 58.65%\n",
            "Epoch [2/10], Training Loss: 1.135, Validation Accuracy: 58.99%\n",
            "Epoch [3/10], Training Loss: 1.107, Validation Accuracy: 58.69%\n",
            "Epoch [4/10], Training Loss: 1.100, Validation Accuracy: 59.13%\n",
            "Epoch [5/10], Training Loss: 1.083, Validation Accuracy: 58.89%\n",
            "Epoch [6/10], Training Loss: 1.072, Validation Accuracy: 58.88%\n",
            "Epoch [7/10], Training Loss: 1.061, Validation Accuracy: 58.18%\n",
            "Epoch [8/10], Training Loss: 1.043, Validation Accuracy: 59.24%\n",
            "Epoch [9/10], Training Loss: 1.038, Validation Accuracy: 58.96%\n",
            "Epoch [10/10], Training Loss: 1.031, Validation Accuracy: 59.44%\n",
            "Epoch [1/10], Training Loss: 1.109, Validation Accuracy: 59.35%\n",
            "Epoch [2/10], Training Loss: 1.074, Validation Accuracy: 59.58%\n",
            "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 59.33%\n",
            "Epoch [4/10], Training Loss: 1.048, Validation Accuracy: 59.77%\n",
            "Epoch [5/10], Training Loss: 1.025, Validation Accuracy: 59.70%\n",
            "Epoch [6/10], Training Loss: 1.017, Validation Accuracy: 59.68%\n",
            "Epoch [7/10], Training Loss: 0.995, Validation Accuracy: 59.85%\n",
            "Epoch [8/10], Training Loss: 0.991, Validation Accuracy: 59.92%\n",
            "Epoch [9/10], Training Loss: 0.985, Validation Accuracy: 60.09%\n",
            "Epoch [10/10], Training Loss: 0.980, Validation Accuracy: 58.70%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 60.28%\n",
            "Epoch [2/10], Training Loss: 1.075, Validation Accuracy: 59.92%\n",
            "Epoch [3/10], Training Loss: 1.059, Validation Accuracy: 59.44%\n",
            "Epoch [4/10], Training Loss: 1.039, Validation Accuracy: 60.20%\n",
            "Epoch [5/10], Training Loss: 1.022, Validation Accuracy: 60.00%\n",
            "Epoch [6/10], Training Loss: 1.011, Validation Accuracy: 59.09%\n",
            "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 60.06%\n",
            "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.27%\n",
            "Epoch [9/10], Training Loss: 0.969, Validation Accuracy: 59.67%\n",
            "Epoch [10/10], Training Loss: 0.960, Validation Accuracy: 59.29%\n",
            "Epoch [1/10], Training Loss: 1.109, Validation Accuracy: 59.45%\n",
            "Epoch [2/10], Training Loss: 1.070, Validation Accuracy: 59.61%\n",
            "Epoch [3/10], Training Loss: 1.047, Validation Accuracy: 59.32%\n",
            "Epoch [4/10], Training Loss: 1.026, Validation Accuracy: 60.00%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.07%\n",
            "Epoch [6/10], Training Loss: 1.006, Validation Accuracy: 60.15%\n",
            "Epoch [7/10], Training Loss: 0.987, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.974, Validation Accuracy: 59.77%\n",
            "Epoch [9/10], Training Loss: 0.957, Validation Accuracy: 59.60%\n",
            "Epoch [10/10], Training Loss: 0.947, Validation Accuracy: 59.48%\n",
            "Epoch [1/10], Training Loss: 1.098, Validation Accuracy: 59.24%\n",
            "Epoch [2/10], Training Loss: 1.066, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 1.032, Validation Accuracy: 60.14%\n",
            "Epoch [4/10], Training Loss: 1.012, Validation Accuracy: 60.29%\n",
            "Epoch [5/10], Training Loss: 1.001, Validation Accuracy: 60.13%\n",
            "Epoch [6/10], Training Loss: 0.978, Validation Accuracy: 60.25%\n",
            "Epoch [7/10], Training Loss: 0.967, Validation Accuracy: 60.38%\n",
            "Epoch [8/10], Training Loss: 0.955, Validation Accuracy: 60.56%\n",
            "Epoch [9/10], Training Loss: 0.945, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.928, Validation Accuracy: 60.23%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 1.039, Validation Accuracy: 60.88%\n",
            "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.989, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.985, Validation Accuracy: 60.66%\n",
            "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 61.13%\n",
            "Epoch [7/10], Training Loss: 0.940, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.934, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.924, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.910, Validation Accuracy: 60.68%\n",
            "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.998, Validation Accuracy: 61.38%\n",
            "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.84%\n",
            "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.928, Validation Accuracy: 61.54%\n",
            "Epoch [6/10], Training Loss: 0.906, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.895, Validation Accuracy: 61.56%\n",
            "Epoch [8/10], Training Loss: 0.879, Validation Accuracy: 61.55%\n",
            "Epoch [9/10], Training Loss: 0.872, Validation Accuracy: 60.20%\n",
            "Epoch [10/10], Training Loss: 0.859, Validation Accuracy: 60.88%\n",
            "Epoch [1/10], Training Loss: 1.033, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 61.41%\n",
            "Epoch [3/10], Training Loss: 0.963, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.942, Validation Accuracy: 61.86%\n",
            "Epoch [5/10], Training Loss: 0.926, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.910, Validation Accuracy: 60.94%\n",
            "Epoch [7/10], Training Loss: 0.895, Validation Accuracy: 60.61%\n",
            "Epoch [8/10], Training Loss: 0.875, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.858, Validation Accuracy: 60.84%\n",
            "Epoch [10/10], Training Loss: 0.847, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 1.034, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 61.25%\n",
            "Epoch [3/10], Training Loss: 0.954, Validation Accuracy: 60.72%\n",
            "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 60.97%\n",
            "Epoch [5/10], Training Loss: 0.906, Validation Accuracy: 60.49%\n",
            "Epoch [6/10], Training Loss: 0.899, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.876, Validation Accuracy: 61.02%\n",
            "Epoch [8/10], Training Loss: 0.860, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.854, Validation Accuracy: 61.19%\n",
            "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 60.34%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.00%\n",
            "Epoch [2/10], Training Loss: 0.981, Validation Accuracy: 61.51%\n",
            "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 61.67%\n",
            "Epoch [4/10], Training Loss: 0.920, Validation Accuracy: 61.79%\n",
            "Epoch [5/10], Training Loss: 0.899, Validation Accuracy: 61.90%\n",
            "Epoch [6/10], Training Loss: 0.878, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.864, Validation Accuracy: 62.23%\n",
            "Epoch [8/10], Training Loss: 0.846, Validation Accuracy: 61.90%\n",
            "Epoch [9/10], Training Loss: 0.845, Validation Accuracy: 61.70%\n",
            "Epoch [10/10], Training Loss: 0.819, Validation Accuracy: 61.63%\n",
            "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 61.31%\n",
            "Epoch [2/10], Training Loss: 0.959, Validation Accuracy: 62.28%\n",
            "Epoch [3/10], Training Loss: 0.933, Validation Accuracy: 61.91%\n",
            "Epoch [4/10], Training Loss: 0.906, Validation Accuracy: 61.51%\n",
            "Epoch [5/10], Training Loss: 0.889, Validation Accuracy: 62.04%\n",
            "Epoch [6/10], Training Loss: 0.867, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 62.00%\n",
            "Epoch [8/10], Training Loss: 0.840, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.812, Validation Accuracy: 61.20%\n",
            "Epoch [10/10], Training Loss: 0.807, Validation Accuracy: 61.26%\n",
            "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 61.98%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.85%\n",
            "Epoch [3/10], Training Loss: 0.873, Validation Accuracy: 60.89%\n",
            "Epoch [4/10], Training Loss: 0.858, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.826, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.810, Validation Accuracy: 61.73%\n",
            "Epoch [7/10], Training Loss: 0.798, Validation Accuracy: 62.28%\n",
            "Epoch [8/10], Training Loss: 0.777, Validation Accuracy: 62.21%\n",
            "Epoch [9/10], Training Loss: 0.775, Validation Accuracy: 62.52%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 62.26%\n",
            "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 62.16%\n",
            "Epoch [2/10], Training Loss: 0.915, Validation Accuracy: 62.74%\n",
            "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 61.77%\n",
            "Epoch [4/10], Training Loss: 0.858, Validation Accuracy: 62.06%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.816, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.796, Validation Accuracy: 62.02%\n",
            "Epoch [8/10], Training Loss: 0.779, Validation Accuracy: 61.44%\n",
            "Epoch [9/10], Training Loss: 0.767, Validation Accuracy: 62.20%\n",
            "Epoch [10/10], Training Loss: 0.750, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.910, Validation Accuracy: 62.14%\n",
            "Epoch [3/10], Training Loss: 0.870, Validation Accuracy: 62.33%\n",
            "Epoch [4/10], Training Loss: 0.838, Validation Accuracy: 62.37%\n",
            "Epoch [5/10], Training Loss: 0.820, Validation Accuracy: 62.52%\n",
            "Epoch [6/10], Training Loss: 0.800, Validation Accuracy: 62.01%\n",
            "Epoch [7/10], Training Loss: 0.789, Validation Accuracy: 61.27%\n",
            "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 62.44%\n",
            "Epoch [9/10], Training Loss: 0.752, Validation Accuracy: 61.61%\n",
            "Epoch [10/10], Training Loss: 0.731, Validation Accuracy: 61.65%\n",
            "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 62.04%\n",
            "Epoch [2/10], Training Loss: 0.901, Validation Accuracy: 62.20%\n",
            "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.837, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.811, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.791, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.777, Validation Accuracy: 62.32%\n",
            "Epoch [8/10], Training Loss: 0.758, Validation Accuracy: 61.53%\n",
            "Epoch [9/10], Training Loss: 0.740, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 61.99%\n",
            "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 62.09%\n",
            "Epoch [2/10], Training Loss: 0.885, Validation Accuracy: 62.82%\n",
            "Epoch [3/10], Training Loss: 0.847, Validation Accuracy: 62.46%\n",
            "Epoch [4/10], Training Loss: 0.823, Validation Accuracy: 62.84%\n",
            "Epoch [5/10], Training Loss: 0.793, Validation Accuracy: 61.62%\n",
            "Epoch [6/10], Training Loss: 0.774, Validation Accuracy: 61.93%\n",
            "Epoch [7/10], Training Loss: 0.751, Validation Accuracy: 62.64%\n",
            "Epoch [8/10], Training Loss: 0.738, Validation Accuracy: 61.95%\n",
            "Epoch [9/10], Training Loss: 0.722, Validation Accuracy: 62.13%\n",
            "Epoch [10/10], Training Loss: 0.705, Validation Accuracy: 61.52%\n",
            "Epoch [1/10], Training Loss: 0.915, Validation Accuracy: 61.74%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 62.53%\n",
            "Epoch [3/10], Training Loss: 0.805, Validation Accuracy: 62.69%\n",
            "Epoch [4/10], Training Loss: 0.779, Validation Accuracy: 61.89%\n",
            "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 61.99%\n",
            "Epoch [6/10], Training Loss: 0.730, Validation Accuracy: 62.89%\n",
            "Epoch [7/10], Training Loss: 0.703, Validation Accuracy: 61.87%\n",
            "Epoch [8/10], Training Loss: 0.686, Validation Accuracy: 62.58%\n",
            "Epoch [9/10], Training Loss: 0.674, Validation Accuracy: 62.19%\n",
            "Epoch [10/10], Training Loss: 0.664, Validation Accuracy: 62.05%\n",
            "Epoch [1/10], Training Loss: 0.928, Validation Accuracy: 62.19%\n",
            "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 62.14%\n",
            "Epoch [3/10], Training Loss: 0.812, Validation Accuracy: 62.60%\n",
            "Epoch [4/10], Training Loss: 0.786, Validation Accuracy: 63.20%\n",
            "Epoch [5/10], Training Loss: 0.754, Validation Accuracy: 61.83%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 62.64%\n",
            "Epoch [7/10], Training Loss: 0.714, Validation Accuracy: 62.91%\n",
            "Epoch [8/10], Training Loss: 0.693, Validation Accuracy: 63.33%\n",
            "Epoch [9/10], Training Loss: 0.674, Validation Accuracy: 62.51%\n",
            "Epoch [10/10], Training Loss: 0.650, Validation Accuracy: 62.10%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 62.32%\n",
            "Epoch [3/10], Training Loss: 0.793, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.767, Validation Accuracy: 62.18%\n",
            "Epoch [5/10], Training Loss: 0.736, Validation Accuracy: 62.21%\n",
            "Epoch [6/10], Training Loss: 0.717, Validation Accuracy: 62.18%\n",
            "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 62.30%\n",
            "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 61.80%\n",
            "Epoch [9/10], Training Loss: 0.652, Validation Accuracy: 61.45%\n",
            "Epoch [10/10], Training Loss: 0.634, Validation Accuracy: 61.57%\n",
            "Epoch [1/10], Training Loss: 0.929, Validation Accuracy: 62.19%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.794, Validation Accuracy: 62.42%\n",
            "Epoch [4/10], Training Loss: 0.763, Validation Accuracy: 62.86%\n",
            "Epoch [5/10], Training Loss: 0.721, Validation Accuracy: 62.95%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 62.44%\n",
            "Epoch [7/10], Training Loss: 0.695, Validation Accuracy: 62.19%\n",
            "Epoch [8/10], Training Loss: 0.668, Validation Accuracy: 62.22%\n",
            "Epoch [9/10], Training Loss: 0.645, Validation Accuracy: 61.78%\n",
            "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 62.84%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 61.60%\n",
            "Epoch [2/10], Training Loss: 0.818, Validation Accuracy: 62.24%\n",
            "Epoch [3/10], Training Loss: 0.785, Validation Accuracy: 63.32%\n",
            "Epoch [4/10], Training Loss: 0.736, Validation Accuracy: 62.70%\n",
            "Epoch [5/10], Training Loss: 0.713, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.687, Validation Accuracy: 62.86%\n",
            "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 62.69%\n",
            "Epoch [8/10], Training Loss: 0.640, Validation Accuracy: 61.98%\n",
            "Epoch [9/10], Training Loss: 0.617, Validation Accuracy: 62.13%\n",
            "Epoch [10/10], Training Loss: 0.605, Validation Accuracy: 62.12%\n",
            "Epoch [1/10], Training Loss: 0.861, Validation Accuracy: 62.02%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 62.81%\n",
            "Epoch [3/10], Training Loss: 0.729, Validation Accuracy: 62.01%\n",
            "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 62.33%\n",
            "Epoch [5/10], Training Loss: 0.669, Validation Accuracy: 62.71%\n",
            "Epoch [6/10], Training Loss: 0.639, Validation Accuracy: 62.75%\n",
            "Epoch [7/10], Training Loss: 0.617, Validation Accuracy: 62.32%\n",
            "Epoch [8/10], Training Loss: 0.595, Validation Accuracy: 62.41%\n",
            "Epoch [9/10], Training Loss: 0.572, Validation Accuracy: 62.53%\n",
            "Epoch [10/10], Training Loss: 0.557, Validation Accuracy: 62.21%\n",
            "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 62.22%\n",
            "Epoch [2/10], Training Loss: 0.789, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.735, Validation Accuracy: 61.93%\n",
            "Epoch [4/10], Training Loss: 0.701, Validation Accuracy: 62.12%\n",
            "Epoch [5/10], Training Loss: 0.679, Validation Accuracy: 63.07%\n",
            "Epoch [6/10], Training Loss: 0.637, Validation Accuracy: 62.44%\n",
            "Epoch [7/10], Training Loss: 0.630, Validation Accuracy: 63.07%\n",
            "Epoch [8/10], Training Loss: 0.594, Validation Accuracy: 62.42%\n",
            "Epoch [9/10], Training Loss: 0.580, Validation Accuracy: 62.57%\n",
            "Epoch [10/10], Training Loss: 0.563, Validation Accuracy: 61.58%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 62.41%\n",
            "Epoch [2/10], Training Loss: 0.766, Validation Accuracy: 62.82%\n",
            "Epoch [3/10], Training Loss: 0.717, Validation Accuracy: 62.56%\n",
            "Epoch [4/10], Training Loss: 0.671, Validation Accuracy: 62.30%\n",
            "Epoch [5/10], Training Loss: 0.650, Validation Accuracy: 61.87%\n",
            "Epoch [6/10], Training Loss: 0.624, Validation Accuracy: 62.53%\n",
            "Epoch [7/10], Training Loss: 0.600, Validation Accuracy: 61.69%\n",
            "Epoch [8/10], Training Loss: 0.581, Validation Accuracy: 61.90%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.53%\n",
            "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 61.92%\n",
            "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 61.70%\n",
            "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 62.65%\n",
            "Epoch [3/10], Training Loss: 0.715, Validation Accuracy: 62.28%\n",
            "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 62.57%\n",
            "Epoch [5/10], Training Loss: 0.647, Validation Accuracy: 62.85%\n",
            "Epoch [6/10], Training Loss: 0.619, Validation Accuracy: 61.80%\n",
            "Epoch [7/10], Training Loss: 0.596, Validation Accuracy: 62.33%\n",
            "Epoch [8/10], Training Loss: 0.574, Validation Accuracy: 62.34%\n",
            "Epoch [9/10], Training Loss: 0.553, Validation Accuracy: 62.32%\n",
            "Epoch [10/10], Training Loss: 0.532, Validation Accuracy: 61.68%\n",
            "Epoch [1/10], Training Loss: 0.870, Validation Accuracy: 61.97%\n",
            "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 62.62%\n",
            "Epoch [3/10], Training Loss: 0.700, Validation Accuracy: 63.46%\n",
            "Epoch [4/10], Training Loss: 0.648, Validation Accuracy: 62.02%\n",
            "Epoch [5/10], Training Loss: 0.633, Validation Accuracy: 61.69%\n",
            "Epoch [6/10], Training Loss: 0.598, Validation Accuracy: 62.44%\n",
            "Epoch [7/10], Training Loss: 0.572, Validation Accuracy: 62.45%\n",
            "Epoch [8/10], Training Loss: 0.546, Validation Accuracy: 61.93%\n",
            "Epoch [9/10], Training Loss: 0.532, Validation Accuracy: 61.30%\n",
            "Epoch [10/10], Training Loss: 0.515, Validation Accuracy: 62.02%\n",
            "Epoch [1/10], Training Loss: 0.819, Validation Accuracy: 61.72%\n",
            "Epoch [2/10], Training Loss: 0.724, Validation Accuracy: 62.80%\n",
            "Epoch [3/10], Training Loss: 0.663, Validation Accuracy: 62.63%\n",
            "Epoch [4/10], Training Loss: 0.617, Validation Accuracy: 63.03%\n",
            "Epoch [5/10], Training Loss: 0.581, Validation Accuracy: 62.54%\n",
            "Epoch [6/10], Training Loss: 0.564, Validation Accuracy: 62.97%\n",
            "Epoch [7/10], Training Loss: 0.536, Validation Accuracy: 62.51%\n",
            "Epoch [8/10], Training Loss: 0.503, Validation Accuracy: 62.05%\n",
            "Epoch [9/10], Training Loss: 0.495, Validation Accuracy: 62.21%\n",
            "Epoch [10/10], Training Loss: 0.468, Validation Accuracy: 61.65%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 60.78%\n",
            "Epoch [2/10], Training Loss: 0.736, Validation Accuracy: 62.44%\n",
            "Epoch [3/10], Training Loss: 0.669, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.642, Validation Accuracy: 62.30%\n",
            "Epoch [5/10], Training Loss: 0.600, Validation Accuracy: 62.08%\n",
            "Epoch [6/10], Training Loss: 0.568, Validation Accuracy: 61.80%\n",
            "Epoch [7/10], Training Loss: 0.534, Validation Accuracy: 62.03%\n",
            "Epoch [8/10], Training Loss: 0.518, Validation Accuracy: 61.84%\n",
            "Epoch [9/10], Training Loss: 0.487, Validation Accuracy: 61.15%\n",
            "Epoch [10/10], Training Loss: 0.474, Validation Accuracy: 61.66%\n",
            "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 61.80%\n",
            "Epoch [2/10], Training Loss: 0.714, Validation Accuracy: 61.88%\n",
            "Epoch [3/10], Training Loss: 0.649, Validation Accuracy: 62.59%\n",
            "Epoch [4/10], Training Loss: 0.606, Validation Accuracy: 61.87%\n",
            "Epoch [5/10], Training Loss: 0.569, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.545, Validation Accuracy: 61.46%\n",
            "Epoch [7/10], Training Loss: 0.522, Validation Accuracy: 61.71%\n",
            "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 62.18%\n",
            "Epoch [9/10], Training Loss: 0.474, Validation Accuracy: 61.62%\n",
            "Epoch [10/10], Training Loss: 0.454, Validation Accuracy: 61.61%\n",
            "Epoch [1/10], Training Loss: 0.840, Validation Accuracy: 62.01%\n",
            "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 62.27%\n",
            "Epoch [3/10], Training Loss: 0.644, Validation Accuracy: 61.39%\n",
            "Epoch [4/10], Training Loss: 0.611, Validation Accuracy: 62.04%\n",
            "Epoch [5/10], Training Loss: 0.560, Validation Accuracy: 61.63%\n",
            "Epoch [6/10], Training Loss: 0.547, Validation Accuracy: 62.63%\n",
            "Epoch [7/10], Training Loss: 0.513, Validation Accuracy: 62.16%\n",
            "Epoch [8/10], Training Loss: 0.479, Validation Accuracy: 62.27%\n",
            "Epoch [9/10], Training Loss: 0.463, Validation Accuracy: 62.02%\n",
            "Epoch [10/10], Training Loss: 0.438, Validation Accuracy: 62.21%\n",
            "Epoch [1/10], Training Loss: 0.817, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 62.28%\n",
            "Epoch [3/10], Training Loss: 0.637, Validation Accuracy: 61.75%\n",
            "Epoch [4/10], Training Loss: 0.577, Validation Accuracy: 62.42%\n",
            "Epoch [5/10], Training Loss: 0.546, Validation Accuracy: 62.08%\n",
            "Epoch [6/10], Training Loss: 0.518, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.487, Validation Accuracy: 61.50%\n",
            "Epoch [8/10], Training Loss: 0.464, Validation Accuracy: 61.47%\n",
            "Epoch [9/10], Training Loss: 0.442, Validation Accuracy: 62.15%\n",
            "Epoch [10/10], Training Loss: 0.421, Validation Accuracy: 61.33%\n",
            "Test Accuracy: 60.45%\n"
          ]
        }
      ],
      "source": [
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq2rryi8d6y6",
        "outputId": "d05dcb92-9245-4756-f420-707b4eb2f212"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43758282.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.302, Validation Accuracy: 9.65%\n",
            "Epoch [2/10], Training Loss: 2.301, Validation Accuracy: 9.72%\n",
            "Epoch [3/10], Training Loss: 2.300, Validation Accuracy: 9.99%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 10.53%\n",
            "Epoch [5/10], Training Loss: 2.297, Validation Accuracy: 11.09%\n",
            "Epoch [6/10], Training Loss: 2.295, Validation Accuracy: 12.15%\n",
            "Epoch [7/10], Training Loss: 2.293, Validation Accuracy: 12.91%\n",
            "Epoch [8/10], Training Loss: 2.290, Validation Accuracy: 14.11%\n",
            "Epoch [9/10], Training Loss: 2.286, Validation Accuracy: 15.58%\n",
            "Epoch [10/10], Training Loss: 2.281, Validation Accuracy: 16.29%\n",
            "Epoch [1/10], Training Loss: 2.275, Validation Accuracy: 16.97%\n",
            "Epoch [2/10], Training Loss: 2.265, Validation Accuracy: 19.59%\n",
            "Epoch [3/10], Training Loss: 2.251, Validation Accuracy: 20.96%\n",
            "Epoch [4/10], Training Loss: 2.230, Validation Accuracy: 20.48%\n",
            "Epoch [5/10], Training Loss: 2.203, Validation Accuracy: 20.64%\n",
            "Epoch [6/10], Training Loss: 2.166, Validation Accuracy: 21.06%\n",
            "Epoch [7/10], Training Loss: 2.123, Validation Accuracy: 22.06%\n",
            "Epoch [8/10], Training Loss: 2.088, Validation Accuracy: 23.46%\n",
            "Epoch [9/10], Training Loss: 2.057, Validation Accuracy: 24.85%\n",
            "Epoch [10/10], Training Loss: 2.031, Validation Accuracy: 27.06%\n",
            "Epoch [1/10], Training Loss: 2.020, Validation Accuracy: 27.46%\n",
            "Epoch [2/10], Training Loss: 2.002, Validation Accuracy: 28.43%\n",
            "Epoch [3/10], Training Loss: 1.983, Validation Accuracy: 28.91%\n",
            "Epoch [4/10], Training Loss: 1.967, Validation Accuracy: 29.96%\n",
            "Epoch [5/10], Training Loss: 1.948, Validation Accuracy: 30.19%\n",
            "Epoch [6/10], Training Loss: 1.928, Validation Accuracy: 31.15%\n",
            "Epoch [7/10], Training Loss: 1.912, Validation Accuracy: 31.49%\n",
            "Epoch [8/10], Training Loss: 1.897, Validation Accuracy: 31.84%\n",
            "Epoch [9/10], Training Loss: 1.880, Validation Accuracy: 33.01%\n",
            "Epoch [10/10], Training Loss: 1.869, Validation Accuracy: 33.31%\n",
            "Epoch [1/10], Training Loss: 1.865, Validation Accuracy: 32.68%\n",
            "Epoch [2/10], Training Loss: 1.851, Validation Accuracy: 34.10%\n",
            "Epoch [3/10], Training Loss: 1.835, Validation Accuracy: 34.41%\n",
            "Epoch [4/10], Training Loss: 1.822, Validation Accuracy: 33.65%\n",
            "Epoch [5/10], Training Loss: 1.806, Validation Accuracy: 34.98%\n",
            "Epoch [6/10], Training Loss: 1.792, Validation Accuracy: 35.97%\n",
            "Epoch [7/10], Training Loss: 1.776, Validation Accuracy: 36.18%\n",
            "Epoch [8/10], Training Loss: 1.762, Validation Accuracy: 35.61%\n",
            "Epoch [9/10], Training Loss: 1.747, Validation Accuracy: 36.83%\n",
            "Epoch [10/10], Training Loss: 1.731, Validation Accuracy: 36.36%\n",
            "Epoch [1/10], Training Loss: 1.736, Validation Accuracy: 37.58%\n",
            "Epoch [2/10], Training Loss: 1.719, Validation Accuracy: 38.09%\n",
            "Epoch [3/10], Training Loss: 1.707, Validation Accuracy: 38.32%\n",
            "Epoch [4/10], Training Loss: 1.696, Validation Accuracy: 38.69%\n",
            "Epoch [5/10], Training Loss: 1.685, Validation Accuracy: 38.91%\n",
            "Epoch [6/10], Training Loss: 1.673, Validation Accuracy: 39.31%\n",
            "Epoch [7/10], Training Loss: 1.662, Validation Accuracy: 39.59%\n",
            "Epoch [8/10], Training Loss: 1.648, Validation Accuracy: 39.36%\n",
            "Epoch [9/10], Training Loss: 1.648, Validation Accuracy: 40.28%\n",
            "Epoch [10/10], Training Loss: 1.628, Validation Accuracy: 39.35%\n",
            "Epoch [1/10], Training Loss: 1.639, Validation Accuracy: 41.04%\n",
            "Epoch [2/10], Training Loss: 1.624, Validation Accuracy: 40.83%\n",
            "Epoch [3/10], Training Loss: 1.616, Validation Accuracy: 41.26%\n",
            "Epoch [4/10], Training Loss: 1.605, Validation Accuracy: 41.38%\n",
            "Epoch [5/10], Training Loss: 1.595, Validation Accuracy: 42.07%\n",
            "Epoch [6/10], Training Loss: 1.587, Validation Accuracy: 42.00%\n",
            "Epoch [7/10], Training Loss: 1.575, Validation Accuracy: 41.79%\n",
            "Epoch [8/10], Training Loss: 1.568, Validation Accuracy: 41.98%\n",
            "Epoch [9/10], Training Loss: 1.553, Validation Accuracy: 42.78%\n",
            "Epoch [10/10], Training Loss: 1.544, Validation Accuracy: 42.13%\n",
            "Epoch [1/10], Training Loss: 1.573, Validation Accuracy: 43.33%\n",
            "Epoch [2/10], Training Loss: 1.556, Validation Accuracy: 43.97%\n",
            "Epoch [3/10], Training Loss: 1.545, Validation Accuracy: 43.27%\n",
            "Epoch [4/10], Training Loss: 1.537, Validation Accuracy: 44.94%\n",
            "Epoch [5/10], Training Loss: 1.526, Validation Accuracy: 44.13%\n",
            "Epoch [6/10], Training Loss: 1.516, Validation Accuracy: 45.28%\n",
            "Epoch [7/10], Training Loss: 1.503, Validation Accuracy: 45.46%\n",
            "Epoch [8/10], Training Loss: 1.498, Validation Accuracy: 45.07%\n",
            "Epoch [9/10], Training Loss: 1.484, Validation Accuracy: 44.97%\n",
            "Epoch [10/10], Training Loss: 1.482, Validation Accuracy: 45.27%\n",
            "Epoch [1/10], Training Loss: 1.508, Validation Accuracy: 46.60%\n",
            "Epoch [2/10], Training Loss: 1.486, Validation Accuracy: 46.94%\n",
            "Epoch [3/10], Training Loss: 1.477, Validation Accuracy: 46.27%\n",
            "Epoch [4/10], Training Loss: 1.474, Validation Accuracy: 47.17%\n",
            "Epoch [5/10], Training Loss: 1.464, Validation Accuracy: 47.20%\n",
            "Epoch [6/10], Training Loss: 1.451, Validation Accuracy: 47.48%\n",
            "Epoch [7/10], Training Loss: 1.440, Validation Accuracy: 47.21%\n",
            "Epoch [8/10], Training Loss: 1.433, Validation Accuracy: 47.16%\n",
            "Epoch [9/10], Training Loss: 1.423, Validation Accuracy: 47.92%\n",
            "Epoch [10/10], Training Loss: 1.413, Validation Accuracy: 47.82%\n",
            "Epoch [1/10], Training Loss: 1.447, Validation Accuracy: 48.27%\n",
            "Epoch [2/10], Training Loss: 1.426, Validation Accuracy: 48.11%\n",
            "Epoch [3/10], Training Loss: 1.416, Validation Accuracy: 48.06%\n",
            "Epoch [4/10], Training Loss: 1.405, Validation Accuracy: 47.73%\n",
            "Epoch [5/10], Training Loss: 1.402, Validation Accuracy: 48.28%\n",
            "Epoch [6/10], Training Loss: 1.387, Validation Accuracy: 48.30%\n",
            "Epoch [7/10], Training Loss: 1.382, Validation Accuracy: 49.37%\n",
            "Epoch [8/10], Training Loss: 1.372, Validation Accuracy: 48.91%\n",
            "Epoch [9/10], Training Loss: 1.364, Validation Accuracy: 49.06%\n",
            "Epoch [10/10], Training Loss: 1.356, Validation Accuracy: 49.25%\n",
            "Epoch [1/10], Training Loss: 1.413, Validation Accuracy: 48.93%\n",
            "Epoch [2/10], Training Loss: 1.392, Validation Accuracy: 49.74%\n",
            "Epoch [3/10], Training Loss: 1.383, Validation Accuracy: 49.70%\n",
            "Epoch [4/10], Training Loss: 1.373, Validation Accuracy: 49.69%\n",
            "Epoch [5/10], Training Loss: 1.363, Validation Accuracy: 48.81%\n",
            "Epoch [6/10], Training Loss: 1.352, Validation Accuracy: 49.43%\n",
            "Epoch [7/10], Training Loss: 1.345, Validation Accuracy: 48.99%\n",
            "Epoch [8/10], Training Loss: 1.340, Validation Accuracy: 49.84%\n",
            "Epoch [9/10], Training Loss: 1.326, Validation Accuracy: 50.29%\n",
            "Epoch [10/10], Training Loss: 1.327, Validation Accuracy: 49.53%\n",
            "Epoch [1/10], Training Loss: 1.380, Validation Accuracy: 50.18%\n",
            "Epoch [2/10], Training Loss: 1.356, Validation Accuracy: 50.52%\n",
            "Epoch [3/10], Training Loss: 1.344, Validation Accuracy: 50.58%\n",
            "Epoch [4/10], Training Loss: 1.330, Validation Accuracy: 50.80%\n",
            "Epoch [5/10], Training Loss: 1.325, Validation Accuracy: 51.28%\n",
            "Epoch [6/10], Training Loss: 1.311, Validation Accuracy: 50.94%\n",
            "Epoch [7/10], Training Loss: 1.307, Validation Accuracy: 51.00%\n",
            "Epoch [8/10], Training Loss: 1.293, Validation Accuracy: 50.60%\n",
            "Epoch [9/10], Training Loss: 1.295, Validation Accuracy: 51.47%\n",
            "Epoch [10/10], Training Loss: 1.280, Validation Accuracy: 51.58%\n",
            "Epoch [1/10], Training Loss: 1.338, Validation Accuracy: 52.23%\n",
            "Epoch [2/10], Training Loss: 1.324, Validation Accuracy: 51.29%\n",
            "Epoch [3/10], Training Loss: 1.313, Validation Accuracy: 52.52%\n",
            "Epoch [4/10], Training Loss: 1.304, Validation Accuracy: 52.00%\n",
            "Epoch [5/10], Training Loss: 1.291, Validation Accuracy: 52.04%\n",
            "Epoch [6/10], Training Loss: 1.280, Validation Accuracy: 52.78%\n",
            "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 52.07%\n",
            "Epoch [8/10], Training Loss: 1.269, Validation Accuracy: 52.58%\n",
            "Epoch [9/10], Training Loss: 1.262, Validation Accuracy: 52.15%\n",
            "Epoch [10/10], Training Loss: 1.246, Validation Accuracy: 52.18%\n",
            "Epoch [1/10], Training Loss: 1.305, Validation Accuracy: 53.37%\n",
            "Epoch [2/10], Training Loss: 1.285, Validation Accuracy: 53.04%\n",
            "Epoch [3/10], Training Loss: 1.273, Validation Accuracy: 53.16%\n",
            "Epoch [4/10], Training Loss: 1.259, Validation Accuracy: 53.66%\n",
            "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 53.26%\n",
            "Epoch [6/10], Training Loss: 1.239, Validation Accuracy: 53.11%\n",
            "Epoch [7/10], Training Loss: 1.230, Validation Accuracy: 53.84%\n",
            "Epoch [8/10], Training Loss: 1.228, Validation Accuracy: 53.29%\n",
            "Epoch [9/10], Training Loss: 1.216, Validation Accuracy: 53.92%\n",
            "Epoch [10/10], Training Loss: 1.204, Validation Accuracy: 53.05%\n",
            "Epoch [1/10], Training Loss: 1.279, Validation Accuracy: 53.26%\n",
            "Epoch [2/10], Training Loss: 1.256, Validation Accuracy: 54.23%\n",
            "Epoch [3/10], Training Loss: 1.231, Validation Accuracy: 53.36%\n",
            "Epoch [4/10], Training Loss: 1.220, Validation Accuracy: 54.48%\n",
            "Epoch [5/10], Training Loss: 1.210, Validation Accuracy: 53.55%\n",
            "Epoch [6/10], Training Loss: 1.200, Validation Accuracy: 54.20%\n",
            "Epoch [7/10], Training Loss: 1.190, Validation Accuracy: 54.21%\n",
            "Epoch [8/10], Training Loss: 1.194, Validation Accuracy: 54.25%\n",
            "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 54.02%\n",
            "Epoch [10/10], Training Loss: 1.159, Validation Accuracy: 54.46%\n",
            "Epoch [1/10], Training Loss: 1.253, Validation Accuracy: 54.78%\n",
            "Epoch [2/10], Training Loss: 1.230, Validation Accuracy: 54.96%\n",
            "Epoch [3/10], Training Loss: 1.209, Validation Accuracy: 54.55%\n",
            "Epoch [4/10], Training Loss: 1.203, Validation Accuracy: 53.36%\n",
            "Epoch [5/10], Training Loss: 1.195, Validation Accuracy: 54.76%\n",
            "Epoch [6/10], Training Loss: 1.170, Validation Accuracy: 55.04%\n",
            "Epoch [7/10], Training Loss: 1.158, Validation Accuracy: 54.21%\n",
            "Epoch [8/10], Training Loss: 1.162, Validation Accuracy: 54.30%\n",
            "Epoch [9/10], Training Loss: 1.145, Validation Accuracy: 54.42%\n",
            "Epoch [10/10], Training Loss: 1.135, Validation Accuracy: 53.80%\n",
            "Epoch [1/10], Training Loss: 1.227, Validation Accuracy: 55.64%\n",
            "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 55.67%\n",
            "Epoch [3/10], Training Loss: 1.182, Validation Accuracy: 56.61%\n",
            "Epoch [4/10], Training Loss: 1.170, Validation Accuracy: 56.60%\n",
            "Epoch [5/10], Training Loss: 1.154, Validation Accuracy: 56.44%\n",
            "Epoch [6/10], Training Loss: 1.141, Validation Accuracy: 56.56%\n",
            "Epoch [7/10], Training Loss: 1.124, Validation Accuracy: 55.45%\n",
            "Epoch [8/10], Training Loss: 1.121, Validation Accuracy: 56.26%\n",
            "Epoch [9/10], Training Loss: 1.106, Validation Accuracy: 56.11%\n",
            "Epoch [10/10], Training Loss: 1.107, Validation Accuracy: 55.40%\n",
            "Epoch [1/10], Training Loss: 1.208, Validation Accuracy: 56.75%\n",
            "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 55.52%\n",
            "Epoch [3/10], Training Loss: 1.169, Validation Accuracy: 56.40%\n",
            "Epoch [4/10], Training Loss: 1.149, Validation Accuracy: 56.51%\n",
            "Epoch [5/10], Training Loss: 1.129, Validation Accuracy: 56.61%\n",
            "Epoch [6/10], Training Loss: 1.129, Validation Accuracy: 55.53%\n",
            "Epoch [7/10], Training Loss: 1.117, Validation Accuracy: 56.29%\n",
            "Epoch [8/10], Training Loss: 1.104, Validation Accuracy: 56.82%\n",
            "Epoch [9/10], Training Loss: 1.093, Validation Accuracy: 56.93%\n",
            "Epoch [10/10], Training Loss: 1.083, Validation Accuracy: 57.18%\n",
            "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 57.39%\n",
            "Epoch [2/10], Training Loss: 1.139, Validation Accuracy: 57.96%\n",
            "Epoch [3/10], Training Loss: 1.128, Validation Accuracy: 57.39%\n",
            "Epoch [4/10], Training Loss: 1.114, Validation Accuracy: 57.70%\n",
            "Epoch [5/10], Training Loss: 1.099, Validation Accuracy: 57.86%\n",
            "Epoch [6/10], Training Loss: 1.089, Validation Accuracy: 58.02%\n",
            "Epoch [7/10], Training Loss: 1.074, Validation Accuracy: 57.12%\n",
            "Epoch [8/10], Training Loss: 1.070, Validation Accuracy: 56.72%\n",
            "Epoch [9/10], Training Loss: 1.061, Validation Accuracy: 56.92%\n",
            "Epoch [10/10], Training Loss: 1.045, Validation Accuracy: 57.34%\n",
            "Epoch [1/10], Training Loss: 1.149, Validation Accuracy: 56.76%\n",
            "Epoch [2/10], Training Loss: 1.116, Validation Accuracy: 58.08%\n",
            "Epoch [3/10], Training Loss: 1.110, Validation Accuracy: 57.29%\n",
            "Epoch [4/10], Training Loss: 1.095, Validation Accuracy: 56.76%\n",
            "Epoch [5/10], Training Loss: 1.073, Validation Accuracy: 58.35%\n",
            "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.15%\n",
            "Epoch [7/10], Training Loss: 1.049, Validation Accuracy: 57.56%\n",
            "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 56.66%\n",
            "Epoch [9/10], Training Loss: 1.023, Validation Accuracy: 58.04%\n",
            "Epoch [10/10], Training Loss: 1.008, Validation Accuracy: 57.81%\n",
            "Epoch [1/10], Training Loss: 1.136, Validation Accuracy: 58.21%\n",
            "Epoch [2/10], Training Loss: 1.103, Validation Accuracy: 58.41%\n",
            "Epoch [3/10], Training Loss: 1.083, Validation Accuracy: 58.51%\n",
            "Epoch [4/10], Training Loss: 1.065, Validation Accuracy: 57.82%\n",
            "Epoch [5/10], Training Loss: 1.049, Validation Accuracy: 58.73%\n",
            "Epoch [6/10], Training Loss: 1.036, Validation Accuracy: 59.10%\n",
            "Epoch [7/10], Training Loss: 1.017, Validation Accuracy: 58.10%\n",
            "Epoch [8/10], Training Loss: 1.014, Validation Accuracy: 58.57%\n",
            "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 58.00%\n",
            "Epoch [10/10], Training Loss: 0.994, Validation Accuracy: 58.37%\n",
            "Epoch [1/10], Training Loss: 1.116, Validation Accuracy: 58.84%\n",
            "Epoch [2/10], Training Loss: 1.087, Validation Accuracy: 59.51%\n",
            "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 59.44%\n",
            "Epoch [4/10], Training Loss: 1.047, Validation Accuracy: 59.23%\n",
            "Epoch [5/10], Training Loss: 1.028, Validation Accuracy: 59.30%\n",
            "Epoch [6/10], Training Loss: 1.011, Validation Accuracy: 59.28%\n",
            "Epoch [7/10], Training Loss: 1.008, Validation Accuracy: 59.24%\n",
            "Epoch [8/10], Training Loss: 1.000, Validation Accuracy: 58.72%\n",
            "Epoch [9/10], Training Loss: 0.981, Validation Accuracy: 58.46%\n",
            "Epoch [10/10], Training Loss: 0.972, Validation Accuracy: 59.08%\n",
            "Epoch [1/10], Training Loss: 1.104, Validation Accuracy: 59.30%\n",
            "Epoch [2/10], Training Loss: 1.073, Validation Accuracy: 59.92%\n",
            "Epoch [3/10], Training Loss: 1.057, Validation Accuracy: 59.25%\n",
            "Epoch [4/10], Training Loss: 1.035, Validation Accuracy: 59.12%\n",
            "Epoch [5/10], Training Loss: 1.020, Validation Accuracy: 59.53%\n",
            "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 59.65%\n",
            "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 59.22%\n",
            "Epoch [8/10], Training Loss: 0.980, Validation Accuracy: 59.36%\n",
            "Epoch [9/10], Training Loss: 0.965, Validation Accuracy: 59.32%\n",
            "Epoch [10/10], Training Loss: 0.955, Validation Accuracy: 59.73%\n",
            "Epoch [1/10], Training Loss: 1.067, Validation Accuracy: 60.06%\n",
            "Epoch [2/10], Training Loss: 1.038, Validation Accuracy: 60.30%\n",
            "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 59.78%\n",
            "Epoch [4/10], Training Loss: 1.000, Validation Accuracy: 59.94%\n",
            "Epoch [5/10], Training Loss: 0.983, Validation Accuracy: 59.53%\n",
            "Epoch [6/10], Training Loss: 0.972, Validation Accuracy: 59.63%\n",
            "Epoch [7/10], Training Loss: 0.959, Validation Accuracy: 60.03%\n",
            "Epoch [8/10], Training Loss: 0.949, Validation Accuracy: 59.79%\n",
            "Epoch [9/10], Training Loss: 0.932, Validation Accuracy: 59.75%\n",
            "Epoch [10/10], Training Loss: 0.922, Validation Accuracy: 60.07%\n",
            "Epoch [1/10], Training Loss: 1.068, Validation Accuracy: 58.22%\n",
            "Epoch [2/10], Training Loss: 1.029, Validation Accuracy: 59.66%\n",
            "Epoch [3/10], Training Loss: 1.000, Validation Accuracy: 60.26%\n",
            "Epoch [4/10], Training Loss: 0.980, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.03%\n",
            "Epoch [6/10], Training Loss: 0.948, Validation Accuracy: 58.87%\n",
            "Epoch [7/10], Training Loss: 0.939, Validation Accuracy: 59.67%\n",
            "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 59.86%\n",
            "Epoch [9/10], Training Loss: 0.914, Validation Accuracy: 60.17%\n",
            "Epoch [10/10], Training Loss: 0.895, Validation Accuracy: 60.09%\n",
            "Epoch [1/10], Training Loss: 1.046, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 58.70%\n",
            "Epoch [3/10], Training Loss: 0.991, Validation Accuracy: 60.30%\n",
            "Epoch [4/10], Training Loss: 0.960, Validation Accuracy: 60.36%\n",
            "Epoch [5/10], Training Loss: 0.951, Validation Accuracy: 60.19%\n",
            "Epoch [6/10], Training Loss: 0.939, Validation Accuracy: 60.19%\n",
            "Epoch [7/10], Training Loss: 0.920, Validation Accuracy: 60.44%\n",
            "Epoch [8/10], Training Loss: 0.904, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 60.86%\n",
            "Epoch [10/10], Training Loss: 0.890, Validation Accuracy: 60.43%\n",
            "Epoch [1/10], Training Loss: 1.042, Validation Accuracy: 61.05%\n",
            "Epoch [2/10], Training Loss: 0.999, Validation Accuracy: 60.01%\n",
            "Epoch [3/10], Training Loss: 0.969, Validation Accuracy: 60.66%\n",
            "Epoch [4/10], Training Loss: 0.951, Validation Accuracy: 61.15%\n",
            "Epoch [5/10], Training Loss: 0.935, Validation Accuracy: 60.88%\n",
            "Epoch [6/10], Training Loss: 0.925, Validation Accuracy: 60.61%\n",
            "Epoch [7/10], Training Loss: 0.898, Validation Accuracy: 60.50%\n",
            "Epoch [8/10], Training Loss: 0.894, Validation Accuracy: 60.82%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 60.23%\n",
            "Epoch [10/10], Training Loss: 0.868, Validation Accuracy: 60.84%\n",
            "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 61.02%\n",
            "Epoch [2/10], Training Loss: 0.986, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.99%\n",
            "Epoch [4/10], Training Loss: 0.932, Validation Accuracy: 61.56%\n",
            "Epoch [5/10], Training Loss: 0.917, Validation Accuracy: 61.18%\n",
            "Epoch [6/10], Training Loss: 0.903, Validation Accuracy: 61.07%\n",
            "Epoch [7/10], Training Loss: 0.886, Validation Accuracy: 60.96%\n",
            "Epoch [8/10], Training Loss: 0.866, Validation Accuracy: 61.23%\n",
            "Epoch [9/10], Training Loss: 0.848, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 60.94%\n",
            "Epoch [1/10], Training Loss: 1.007, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.965, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.934, Validation Accuracy: 61.75%\n",
            "Epoch [4/10], Training Loss: 0.905, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.893, Validation Accuracy: 61.19%\n",
            "Epoch [6/10], Training Loss: 0.876, Validation Accuracy: 61.53%\n",
            "Epoch [7/10], Training Loss: 0.861, Validation Accuracy: 61.08%\n",
            "Epoch [8/10], Training Loss: 0.849, Validation Accuracy: 61.62%\n",
            "Epoch [9/10], Training Loss: 0.833, Validation Accuracy: 61.17%\n",
            "Epoch [10/10], Training Loss: 0.817, Validation Accuracy: 61.46%\n",
            "Epoch [1/10], Training Loss: 0.984, Validation Accuracy: 61.31%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 61.52%\n",
            "Epoch [4/10], Training Loss: 0.887, Validation Accuracy: 61.49%\n",
            "Epoch [5/10], Training Loss: 0.871, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.855, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.837, Validation Accuracy: 61.64%\n",
            "Epoch [8/10], Training Loss: 0.820, Validation Accuracy: 61.08%\n",
            "Epoch [9/10], Training Loss: 0.809, Validation Accuracy: 61.51%\n",
            "Epoch [10/10], Training Loss: 0.796, Validation Accuracy: 61.53%\n",
            "Epoch [1/10], Training Loss: 0.992, Validation Accuracy: 61.19%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.96%\n",
            "Epoch [3/10], Training Loss: 0.900, Validation Accuracy: 61.89%\n",
            "Epoch [4/10], Training Loss: 0.873, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.848, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.843, Validation Accuracy: 60.63%\n",
            "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.86%\n",
            "Epoch [8/10], Training Loss: 0.803, Validation Accuracy: 61.07%\n",
            "Epoch [9/10], Training Loss: 0.789, Validation Accuracy: 61.63%\n",
            "Epoch [10/10], Training Loss: 0.782, Validation Accuracy: 61.71%\n",
            "Epoch [1/10], Training Loss: 0.976, Validation Accuracy: 61.88%\n",
            "Epoch [2/10], Training Loss: 0.915, Validation Accuracy: 62.13%\n",
            "Epoch [3/10], Training Loss: 0.891, Validation Accuracy: 62.07%\n",
            "Epoch [4/10], Training Loss: 0.868, Validation Accuracy: 61.67%\n",
            "Epoch [5/10], Training Loss: 0.844, Validation Accuracy: 61.94%\n",
            "Epoch [6/10], Training Loss: 0.830, Validation Accuracy: 61.81%\n",
            "Epoch [7/10], Training Loss: 0.817, Validation Accuracy: 61.68%\n",
            "Epoch [8/10], Training Loss: 0.796, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.784, Validation Accuracy: 61.92%\n",
            "Epoch [10/10], Training Loss: 0.784, Validation Accuracy: 60.92%\n",
            "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 62.38%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 62.08%\n",
            "Epoch [3/10], Training Loss: 0.871, Validation Accuracy: 61.94%\n",
            "Epoch [4/10], Training Loss: 0.848, Validation Accuracy: 61.75%\n",
            "Epoch [5/10], Training Loss: 0.824, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.797, Validation Accuracy: 62.54%\n",
            "Epoch [7/10], Training Loss: 0.782, Validation Accuracy: 62.46%\n",
            "Epoch [8/10], Training Loss: 0.768, Validation Accuracy: 62.27%\n",
            "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 62.26%\n",
            "Epoch [10/10], Training Loss: 0.730, Validation Accuracy: 61.57%\n",
            "Epoch [1/10], Training Loss: 0.948, Validation Accuracy: 62.37%\n",
            "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.850, Validation Accuracy: 61.35%\n",
            "Epoch [4/10], Training Loss: 0.833, Validation Accuracy: 61.88%\n",
            "Epoch [5/10], Training Loss: 0.809, Validation Accuracy: 62.12%\n",
            "Epoch [6/10], Training Loss: 0.788, Validation Accuracy: 62.57%\n",
            "Epoch [7/10], Training Loss: 0.772, Validation Accuracy: 62.52%\n",
            "Epoch [8/10], Training Loss: 0.754, Validation Accuracy: 61.99%\n",
            "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 62.04%\n",
            "Epoch [10/10], Training Loss: 0.724, Validation Accuracy: 61.86%\n",
            "Epoch [1/10], Training Loss: 0.928, Validation Accuracy: 61.95%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 62.03%\n",
            "Epoch [3/10], Training Loss: 0.833, Validation Accuracy: 62.37%\n",
            "Epoch [4/10], Training Loss: 0.813, Validation Accuracy: 62.62%\n",
            "Epoch [5/10], Training Loss: 0.783, Validation Accuracy: 61.91%\n",
            "Epoch [6/10], Training Loss: 0.768, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.746, Validation Accuracy: 62.77%\n",
            "Epoch [8/10], Training Loss: 0.731, Validation Accuracy: 62.27%\n",
            "Epoch [9/10], Training Loss: 0.706, Validation Accuracy: 62.72%\n",
            "Epoch [10/10], Training Loss: 0.704, Validation Accuracy: 61.55%\n",
            "Epoch [1/10], Training Loss: 0.926, Validation Accuracy: 62.24%\n",
            "Epoch [2/10], Training Loss: 0.859, Validation Accuracy: 61.45%\n",
            "Epoch [3/10], Training Loss: 0.816, Validation Accuracy: 62.71%\n",
            "Epoch [4/10], Training Loss: 0.792, Validation Accuracy: 62.24%\n",
            "Epoch [5/10], Training Loss: 0.772, Validation Accuracy: 62.08%\n",
            "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 62.62%\n",
            "Epoch [7/10], Training Loss: 0.732, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.720, Validation Accuracy: 62.03%\n",
            "Epoch [9/10], Training Loss: 0.700, Validation Accuracy: 61.96%\n",
            "Epoch [10/10], Training Loss: 0.680, Validation Accuracy: 62.29%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 62.44%\n",
            "Epoch [2/10], Training Loss: 0.853, Validation Accuracy: 61.82%\n",
            "Epoch [3/10], Training Loss: 0.820, Validation Accuracy: 62.24%\n",
            "Epoch [4/10], Training Loss: 0.782, Validation Accuracy: 62.53%\n",
            "Epoch [5/10], Training Loss: 0.761, Validation Accuracy: 62.82%\n",
            "Epoch [6/10], Training Loss: 0.749, Validation Accuracy: 62.40%\n",
            "Epoch [7/10], Training Loss: 0.729, Validation Accuracy: 62.65%\n",
            "Epoch [8/10], Training Loss: 0.707, Validation Accuracy: 62.41%\n",
            "Epoch [9/10], Training Loss: 0.691, Validation Accuracy: 62.49%\n",
            "Epoch [10/10], Training Loss: 0.667, Validation Accuracy: 62.99%\n",
            "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 63.02%\n",
            "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 62.59%\n",
            "Epoch [3/10], Training Loss: 0.797, Validation Accuracy: 62.71%\n",
            "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 63.44%\n",
            "Epoch [5/10], Training Loss: 0.728, Validation Accuracy: 62.98%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 62.90%\n",
            "Epoch [7/10], Training Loss: 0.687, Validation Accuracy: 63.08%\n",
            "Epoch [8/10], Training Loss: 0.663, Validation Accuracy: 62.41%\n",
            "Epoch [9/10], Training Loss: 0.654, Validation Accuracy: 62.70%\n",
            "Epoch [10/10], Training Loss: 0.639, Validation Accuracy: 61.65%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 62.00%\n",
            "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 62.58%\n",
            "Epoch [3/10], Training Loss: 0.775, Validation Accuracy: 62.46%\n",
            "Epoch [4/10], Training Loss: 0.750, Validation Accuracy: 62.39%\n",
            "Epoch [5/10], Training Loss: 0.724, Validation Accuracy: 62.35%\n",
            "Epoch [6/10], Training Loss: 0.699, Validation Accuracy: 62.62%\n",
            "Epoch [7/10], Training Loss: 0.677, Validation Accuracy: 62.63%\n",
            "Epoch [8/10], Training Loss: 0.661, Validation Accuracy: 63.19%\n",
            "Epoch [9/10], Training Loss: 0.631, Validation Accuracy: 62.78%\n",
            "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 62.04%\n",
            "Epoch [1/10], Training Loss: 0.887, Validation Accuracy: 61.19%\n",
            "Epoch [2/10], Training Loss: 0.812, Validation Accuracy: 62.81%\n",
            "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 62.67%\n",
            "Epoch [4/10], Training Loss: 0.726, Validation Accuracy: 62.79%\n",
            "Epoch [5/10], Training Loss: 0.701, Validation Accuracy: 62.49%\n",
            "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 62.94%\n",
            "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 62.55%\n",
            "Epoch [8/10], Training Loss: 0.629, Validation Accuracy: 62.49%\n",
            "Epoch [9/10], Training Loss: 0.619, Validation Accuracy: 62.80%\n",
            "Epoch [10/10], Training Loss: 0.607, Validation Accuracy: 62.28%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.54%\n",
            "Epoch [2/10], Training Loss: 0.802, Validation Accuracy: 62.33%\n",
            "Epoch [3/10], Training Loss: 0.746, Validation Accuracy: 62.69%\n",
            "Epoch [4/10], Training Loss: 0.718, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.693, Validation Accuracy: 62.84%\n",
            "Epoch [6/10], Training Loss: 0.664, Validation Accuracy: 61.88%\n",
            "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 62.23%\n",
            "Epoch [8/10], Training Loss: 0.616, Validation Accuracy: 62.87%\n",
            "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 62.40%\n",
            "Epoch [10/10], Training Loss: 0.601, Validation Accuracy: 61.45%\n",
            "Epoch [1/10], Training Loss: 0.874, Validation Accuracy: 62.34%\n",
            "Epoch [2/10], Training Loss: 0.791, Validation Accuracy: 62.28%\n",
            "Epoch [3/10], Training Loss: 0.745, Validation Accuracy: 63.27%\n",
            "Epoch [4/10], Training Loss: 0.713, Validation Accuracy: 63.31%\n",
            "Epoch [5/10], Training Loss: 0.684, Validation Accuracy: 62.79%\n",
            "Epoch [6/10], Training Loss: 0.655, Validation Accuracy: 62.99%\n",
            "Epoch [7/10], Training Loss: 0.636, Validation Accuracy: 62.70%\n",
            "Epoch [8/10], Training Loss: 0.613, Validation Accuracy: 61.98%\n",
            "Epoch [9/10], Training Loss: 0.603, Validation Accuracy: 62.41%\n",
            "Epoch [10/10], Training Loss: 0.582, Validation Accuracy: 62.69%\n",
            "Epoch [1/10], Training Loss: 0.863, Validation Accuracy: 63.13%\n",
            "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 63.02%\n",
            "Epoch [3/10], Training Loss: 0.708, Validation Accuracy: 63.64%\n",
            "Epoch [4/10], Training Loss: 0.684, Validation Accuracy: 62.23%\n",
            "Epoch [5/10], Training Loss: 0.656, Validation Accuracy: 62.80%\n",
            "Epoch [6/10], Training Loss: 0.624, Validation Accuracy: 63.19%\n",
            "Epoch [7/10], Training Loss: 0.608, Validation Accuracy: 62.38%\n",
            "Epoch [8/10], Training Loss: 0.576, Validation Accuracy: 63.21%\n",
            "Epoch [9/10], Training Loss: 0.561, Validation Accuracy: 62.47%\n",
            "Epoch [10/10], Training Loss: 0.540, Validation Accuracy: 63.02%\n",
            "Epoch [1/10], Training Loss: 0.843, Validation Accuracy: 62.64%\n",
            "Epoch [2/10], Training Loss: 0.754, Validation Accuracy: 63.33%\n",
            "Epoch [3/10], Training Loss: 0.697, Validation Accuracy: 62.39%\n",
            "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 63.34%\n",
            "Epoch [5/10], Training Loss: 0.637, Validation Accuracy: 62.04%\n",
            "Epoch [6/10], Training Loss: 0.607, Validation Accuracy: 63.05%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 62.75%\n",
            "Epoch [8/10], Training Loss: 0.567, Validation Accuracy: 62.76%\n",
            "Epoch [9/10], Training Loss: 0.553, Validation Accuracy: 62.30%\n",
            "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 62.54%\n",
            "Epoch [1/10], Training Loss: 0.852, Validation Accuracy: 61.45%\n",
            "Epoch [2/10], Training Loss: 0.753, Validation Accuracy: 62.70%\n",
            "Epoch [3/10], Training Loss: 0.690, Validation Accuracy: 62.71%\n",
            "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 62.89%\n",
            "Epoch [5/10], Training Loss: 0.621, Validation Accuracy: 62.91%\n",
            "Epoch [6/10], Training Loss: 0.593, Validation Accuracy: 62.95%\n",
            "Epoch [7/10], Training Loss: 0.571, Validation Accuracy: 62.50%\n",
            "Epoch [8/10], Training Loss: 0.549, Validation Accuracy: 62.74%\n",
            "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 62.73%\n",
            "Epoch [10/10], Training Loss: 0.514, Validation Accuracy: 61.72%\n",
            "Epoch [1/10], Training Loss: 0.826, Validation Accuracy: 62.88%\n",
            "Epoch [2/10], Training Loss: 0.722, Validation Accuracy: 61.72%\n",
            "Epoch [3/10], Training Loss: 0.671, Validation Accuracy: 62.66%\n",
            "Epoch [4/10], Training Loss: 0.635, Validation Accuracy: 62.83%\n",
            "Epoch [5/10], Training Loss: 0.599, Validation Accuracy: 61.92%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 62.69%\n",
            "Epoch [7/10], Training Loss: 0.556, Validation Accuracy: 62.14%\n",
            "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 62.60%\n",
            "Epoch [9/10], Training Loss: 0.506, Validation Accuracy: 62.09%\n",
            "Epoch [10/10], Training Loss: 0.495, Validation Accuracy: 62.12%\n",
            "Epoch [1/10], Training Loss: 0.828, Validation Accuracy: 61.96%\n",
            "Epoch [2/10], Training Loss: 0.742, Validation Accuracy: 62.92%\n",
            "Epoch [3/10], Training Loss: 0.671, Validation Accuracy: 62.43%\n",
            "Epoch [4/10], Training Loss: 0.639, Validation Accuracy: 63.06%\n",
            "Epoch [5/10], Training Loss: 0.610, Validation Accuracy: 63.48%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 62.87%\n",
            "Epoch [7/10], Training Loss: 0.565, Validation Accuracy: 62.87%\n",
            "Epoch [8/10], Training Loss: 0.538, Validation Accuracy: 63.04%\n",
            "Epoch [9/10], Training Loss: 0.521, Validation Accuracy: 63.10%\n",
            "Epoch [10/10], Training Loss: 0.497, Validation Accuracy: 62.88%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 62.43%\n",
            "Epoch [2/10], Training Loss: 0.717, Validation Accuracy: 63.10%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 63.42%\n",
            "Epoch [4/10], Training Loss: 0.602, Validation Accuracy: 62.68%\n",
            "Epoch [5/10], Training Loss: 0.575, Validation Accuracy: 62.68%\n",
            "Epoch [6/10], Training Loss: 0.549, Validation Accuracy: 61.72%\n",
            "Epoch [7/10], Training Loss: 0.526, Validation Accuracy: 62.96%\n",
            "Epoch [8/10], Training Loss: 0.504, Validation Accuracy: 62.72%\n",
            "Epoch [9/10], Training Loss: 0.490, Validation Accuracy: 62.31%\n",
            "Epoch [10/10], Training Loss: 0.462, Validation Accuracy: 63.42%\n",
            "Epoch [1/10], Training Loss: 0.809, Validation Accuracy: 62.49%\n",
            "Epoch [2/10], Training Loss: 0.689, Validation Accuracy: 62.23%\n",
            "Epoch [3/10], Training Loss: 0.631, Validation Accuracy: 63.21%\n",
            "Epoch [4/10], Training Loss: 0.597, Validation Accuracy: 63.10%\n",
            "Epoch [5/10], Training Loss: 0.557, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.535, Validation Accuracy: 62.66%\n",
            "Epoch [7/10], Training Loss: 0.512, Validation Accuracy: 61.56%\n",
            "Epoch [8/10], Training Loss: 0.488, Validation Accuracy: 62.85%\n",
            "Epoch [9/10], Training Loss: 0.465, Validation Accuracy: 62.05%\n",
            "Epoch [10/10], Training Loss: 0.444, Validation Accuracy: 62.21%\n",
            "Epoch [1/10], Training Loss: 0.804, Validation Accuracy: 62.04%\n",
            "Epoch [2/10], Training Loss: 0.688, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.630, Validation Accuracy: 62.26%\n",
            "Epoch [4/10], Training Loss: 0.580, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.550, Validation Accuracy: 62.04%\n",
            "Epoch [6/10], Training Loss: 0.533, Validation Accuracy: 62.39%\n",
            "Epoch [7/10], Training Loss: 0.494, Validation Accuracy: 62.28%\n",
            "Epoch [8/10], Training Loss: 0.466, Validation Accuracy: 62.51%\n",
            "Epoch [9/10], Training Loss: 0.450, Validation Accuracy: 62.01%\n",
            "Epoch [10/10], Training Loss: 0.437, Validation Accuracy: 62.61%\n",
            "Epoch [1/10], Training Loss: 0.796, Validation Accuracy: 61.25%\n",
            "Epoch [2/10], Training Loss: 0.674, Validation Accuracy: 62.05%\n",
            "Epoch [3/10], Training Loss: 0.607, Validation Accuracy: 62.85%\n",
            "Epoch [4/10], Training Loss: 0.562, Validation Accuracy: 62.50%\n",
            "Epoch [5/10], Training Loss: 0.526, Validation Accuracy: 62.43%\n",
            "Epoch [6/10], Training Loss: 0.497, Validation Accuracy: 62.46%\n",
            "Epoch [7/10], Training Loss: 0.470, Validation Accuracy: 62.12%\n",
            "Epoch [8/10], Training Loss: 0.453, Validation Accuracy: 62.23%\n",
            "Epoch [9/10], Training Loss: 0.434, Validation Accuracy: 61.99%\n",
            "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 62.12%\n",
            "Confusion Matrix:\n",
            "[[597  32  77  22  39  22  17  31 100  63]\n",
            " [ 14 760  19   6   6  13  18  10  33 121]\n",
            " [ 50  18 482  58 104 115  85  61  15  12]\n",
            " [ 15  16  77 329  61 282 108  68  21  23]\n",
            " [ 18   9  76  47 528  85  95 119  14   9]\n",
            " [  7   6  52 136  43 620  33  87   9   7]\n",
            " [  2  17  40  46  51  49 740  28  13  14]\n",
            " [ 10   4  31  37  52 100  13 733   1  19]\n",
            " [ 68  46  20  13  10  15  13  19 736  60]\n",
            " [ 23 117   8  15  15  20  19  44  53 686]]\n",
            "Test Accuracy: 62.11%\n",
            "True Positives (TP): [597 760 482 329 528 620 740 733 736 686]\n",
            "False Positives (FP): [207 265 400 380 381 701 401 467 259 328]\n",
            "True Negatives (TN): [8793 8735 8600 8620 8619 8299 8599 8533 8741 8672]\n",
            "False Negatives (FN): [403 240 518 671 472 380 260 267 264 314]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.74253731 0.74146341 0.54648526 0.46403385 0.58085809 0.46934141\n",
            " 0.6485539  0.61083333 0.73969849 0.6765286 ]\n",
            "Recall: [0.597 0.76  0.482 0.329 0.528 0.62  0.74  0.733 0.736 0.686]\n",
            "F1 Score: [0.66186253 0.75061728 0.51222104 0.38502048 0.5531692  0.53425248\n",
            " 0.69126576 0.66636364 0.73784461 0.68123138]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both normal and truncated normal distributions\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    # Generate augmented data from normal distribution\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.302, Validation Accuracy: 9.65%\n",
        "Epoch [2/10], Training Loss: 2.301, Validation Accuracy: 9.72%\n",
        "Epoch [3/10], Training Loss: 2.300, Validation Accuracy: 9.99%\n",
        "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 10.53%\n",
        "Epoch [5/10], Training Loss: 2.297, Validation Accuracy: 11.09%\n",
        "Epoch [6/10], Training Loss: 2.295, Validation Accuracy: 12.15%\n",
        "Epoch [7/10], Training Loss: 2.293, Validation Accuracy: 12.91%\n",
        "Epoch [8/10], Training Loss: 2.290, Validation Accuracy: 14.11%\n",
        "Epoch [9/10], Training Loss: 2.286, Validation Accuracy: 15.58%\n",
        "Epoch [10/10], Training Loss: 2.281, Validation Accuracy: 16.29%\n",
        "Epoch [1/10], Training Loss: 2.275, Validation Accuracy: 16.97%\n",
        "Epoch [2/10], Training Loss: 2.265, Validation Accuracy: 19.59%\n",
        "Epoch [3/10], Training Loss: 2.251, Validation Accuracy: 20.96%\n",
        "Epoch [4/10], Training Loss: 2.230, Validation Accuracy: 20.48%\n",
        "Epoch [5/10], Training Loss: 2.203, Validation Accuracy: 20.64%\n",
        "Epoch [6/10], Training Loss: 2.166, Validation Accuracy: 21.06%\n",
        "Epoch [7/10], Training Loss: 2.123, Validation Accuracy: 22.06%\n",
        "Epoch [8/10], Training Loss: 2.088, Validation Accuracy: 23.46%\n",
        "Epoch [9/10], Training Loss: 2.057, Validation Accuracy: 24.85%\n",
        "Epoch [10/10], Training Loss: 2.031, Validation Accuracy: 27.06%\n",
        "Epoch [1/10], Training Loss: 2.020, Validation Accuracy: 27.46%\n",
        "Epoch [2/10], Training Loss: 2.002, Validation Accuracy: 28.43%\n",
        "Epoch [3/10], Training Loss: 1.983, Validation Accuracy: 28.91%\n",
        "Epoch [4/10], Training Loss: 1.967, Validation Accuracy: 29.96%\n",
        "Epoch [5/10], Training Loss: 1.948, Validation Accuracy: 30.19%\n",
        "Epoch [6/10], Training Loss: 1.928, Validation Accuracy: 31.15%\n",
        "Epoch [7/10], Training Loss: 1.912, Validation Accuracy: 31.49%\n",
        "Epoch [8/10], Training Loss: 1.897, Validation Accuracy: 31.84%\n",
        "Epoch [9/10], Training Loss: 1.880, Validation Accuracy: 33.01%\n",
        "Epoch [10/10], Training Loss: 1.869, Validation Accuracy: 33.31%\n",
        "Epoch [1/10], Training Loss: 1.865, Validation Accuracy: 32.68%\n",
        "Epoch [2/10], Training Loss: 1.851, Validation Accuracy: 34.10%\n",
        "Epoch [3/10], Training Loss: 1.835, Validation Accuracy: 34.41%\n",
        "Epoch [4/10], Training Loss: 1.822, Validation Accuracy: 33.65%\n",
        "Epoch [5/10], Training Loss: 1.806, Validation Accuracy: 34.98%\n",
        "Epoch [6/10], Training Loss: 1.792, Validation Accuracy: 35.97%\n",
        "Epoch [7/10], Training Loss: 1.776, Validation Accuracy: 36.18%\n",
        "Epoch [8/10], Training Loss: 1.762, Validation Accuracy: 35.61%\n",
        "Epoch [9/10], Training Loss: 1.747, Validation Accuracy: 36.83%\n",
        "Epoch [10/10], Training Loss: 1.731, Validation Accuracy: 36.36%\n",
        "Epoch [1/10], Training Loss: 1.736, Validation Accuracy: 37.58%\n",
        "Epoch [2/10], Training Loss: 1.719, Validation Accuracy: 38.09%\n",
        "Epoch [3/10], Training Loss: 1.707, Validation Accuracy: 38.32%\n",
        "Epoch [4/10], Training Loss: 1.696, Validation Accuracy: 38.69%\n",
        "Epoch [5/10], Training Loss: 1.685, Validation Accuracy: 38.91%\n",
        "Epoch [6/10], Training Loss: 1.673, Validation Accuracy: 39.31%\n",
        "Epoch [7/10], Training Loss: 1.662, Validation Accuracy: 39.59%\n",
        "Epoch [8/10], Training Loss: 1.648, Validation Accuracy: 39.36%\n",
        "Epoch [9/10], Training Loss: 1.648, Validation Accuracy: 40.28%\n",
        "Epoch [10/10], Training Loss: 1.628, Validation Accuracy: 39.35%\n",
        "Epoch [1/10], Training Loss: 1.639, Validation Accuracy: 41.04%\n",
        "Epoch [2/10], Training Loss: 1.624, Validation Accuracy: 40.83%\n",
        "Epoch [3/10], Training Loss: 1.616, Validation Accuracy: 41.26%\n",
        "Epoch [4/10], Training Loss: 1.605, Validation Accuracy: 41.38%\n",
        "Epoch [5/10], Training Loss: 1.595, Validation Accuracy: 42.07%\n",
        "Epoch [6/10], Training Loss: 1.587, Validation Accuracy: 42.00%\n",
        "Epoch [7/10], Training Loss: 1.575, Validation Accuracy: 41.79%\n",
        "Epoch [8/10], Training Loss: 1.568, Validation Accuracy: 41.98%\n",
        "Epoch [9/10], Training Loss: 1.553, Validation Accuracy: 42.78%\n",
        "Epoch [10/10], Training Loss: 1.544, Validation Accuracy: 42.13%\n",
        "Epoch [1/10], Training Loss: 1.573, Validation Accuracy: 43.33%\n",
        "Epoch [2/10], Training Loss: 1.556, Validation Accuracy: 43.97%\n",
        "Epoch [3/10], Training Loss: 1.545, Validation Accuracy: 43.27%\n",
        "Epoch [4/10], Training Loss: 1.537, Validation Accuracy: 44.94%\n",
        "Epoch [5/10], Training Loss: 1.526, Validation Accuracy: 44.13%\n",
        "Epoch [6/10], Training Loss: 1.516, Validation Accuracy: 45.28%\n",
        "Epoch [7/10], Training Loss: 1.503, Validation Accuracy: 45.46%\n",
        "Epoch [8/10], Training Loss: 1.498, Validation Accuracy: 45.07%\n",
        "Epoch [9/10], Training Loss: 1.484, Validation Accuracy: 44.97%\n",
        "Epoch [10/10], Training Loss: 1.482, Validation Accuracy: 45.27%\n",
        "Epoch [1/10], Training Loss: 1.508, Validation Accuracy: 46.60%\n",
        "Epoch [2/10], Training Loss: 1.486, Validation Accuracy: 46.94%\n",
        "Epoch [3/10], Training Loss: 1.477, Validation Accuracy: 46.27%\n",
        "Epoch [4/10], Training Loss: 1.474, Validation Accuracy: 47.17%\n",
        "Epoch [5/10], Training Loss: 1.464, Validation Accuracy: 47.20%\n",
        "Epoch [6/10], Training Loss: 1.451, Validation Accuracy: 47.48%\n",
        "Epoch [7/10], Training Loss: 1.440, Validation Accuracy: 47.21%\n",
        "Epoch [8/10], Training Loss: 1.433, Validation Accuracy: 47.16%\n",
        "Epoch [9/10], Training Loss: 1.423, Validation Accuracy: 47.92%\n",
        "Epoch [10/10], Training Loss: 1.413, Validation Accuracy: 47.82%\n",
        "Epoch [1/10], Training Loss: 1.447, Validation Accuracy: 48.27%\n",
        "Epoch [2/10], Training Loss: 1.426, Validation Accuracy: 48.11%\n",
        "Epoch [3/10], Training Loss: 1.416, Validation Accuracy: 48.06%\n",
        "Epoch [4/10], Training Loss: 1.405, Validation Accuracy: 47.73%\n",
        "Epoch [5/10], Training Loss: 1.402, Validation Accuracy: 48.28%\n",
        "Epoch [6/10], Training Loss: 1.387, Validation Accuracy: 48.30%\n",
        "Epoch [7/10], Training Loss: 1.382, Validation Accuracy: 49.37%\n",
        "Epoch [8/10], Training Loss: 1.372, Validation Accuracy: 48.91%\n",
        "Epoch [9/10], Training Loss: 1.364, Validation Accuracy: 49.06%\n",
        "Epoch [10/10], Training Loss: 1.356, Validation Accuracy: 49.25%\n",
        "Epoch [1/10], Training Loss: 1.413, Validation Accuracy: 48.93%\n",
        "Epoch [2/10], Training Loss: 1.392, Validation Accuracy: 49.74%\n",
        "Epoch [3/10], Training Loss: 1.383, Validation Accuracy: 49.70%\n",
        "Epoch [4/10], Training Loss: 1.373, Validation Accuracy: 49.69%\n",
        "Epoch [5/10], Training Loss: 1.363, Validation Accuracy: 48.81%\n",
        "Epoch [6/10], Training Loss: 1.352, Validation Accuracy: 49.43%\n",
        "Epoch [7/10], Training Loss: 1.345, Validation Accuracy: 48.99%\n",
        "Epoch [8/10], Training Loss: 1.340, Validation Accuracy: 49.84%\n",
        "Epoch [9/10], Training Loss: 1.326, Validation Accuracy: 50.29%\n",
        "Epoch [10/10], Training Loss: 1.327, Validation Accuracy: 49.53%\n",
        "Epoch [1/10], Training Loss: 1.380, Validation Accuracy: 50.18%\n",
        "Epoch [2/10], Training Loss: 1.356, Validation Accuracy: 50.52%\n",
        "Epoch [3/10], Training Loss: 1.344, Validation Accuracy: 50.58%\n",
        "Epoch [4/10], Training Loss: 1.330, Validation Accuracy: 50.80%\n",
        "Epoch [5/10], Training Loss: 1.325, Validation Accuracy: 51.28%\n",
        "Epoch [6/10], Training Loss: 1.311, Validation Accuracy: 50.94%\n",
        "Epoch [7/10], Training Loss: 1.307, Validation Accuracy: 51.00%\n",
        "Epoch [8/10], Training Loss: 1.293, Validation Accuracy: 50.60%\n",
        "Epoch [9/10], Training Loss: 1.295, Validation Accuracy: 51.47%\n",
        "Epoch [10/10], Training Loss: 1.280, Validation Accuracy: 51.58%\n",
        "Epoch [1/10], Training Loss: 1.338, Validation Accuracy: 52.23%\n",
        "Epoch [2/10], Training Loss: 1.324, Validation Accuracy: 51.29%\n",
        "Epoch [3/10], Training Loss: 1.313, Validation Accuracy: 52.52%\n",
        "Epoch [4/10], Training Loss: 1.304, Validation Accuracy: 52.00%\n",
        "Epoch [5/10], Training Loss: 1.291, Validation Accuracy: 52.04%\n",
        "Epoch [6/10], Training Loss: 1.280, Validation Accuracy: 52.78%\n",
        "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 52.07%\n",
        "Epoch [8/10], Training Loss: 1.269, Validation Accuracy: 52.58%\n",
        "Epoch [9/10], Training Loss: 1.262, Validation Accuracy: 52.15%\n",
        "Epoch [10/10], Training Loss: 1.246, Validation Accuracy: 52.18%\n",
        "Epoch [1/10], Training Loss: 1.305, Validation Accuracy: 53.37%\n",
        "Epoch [2/10], Training Loss: 1.285, Validation Accuracy: 53.04%\n",
        "Epoch [3/10], Training Loss: 1.273, Validation Accuracy: 53.16%\n",
        "Epoch [4/10], Training Loss: 1.259, Validation Accuracy: 53.66%\n",
        "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 53.26%\n",
        "Epoch [6/10], Training Loss: 1.239, Validation Accuracy: 53.11%\n",
        "Epoch [7/10], Training Loss: 1.230, Validation Accuracy: 53.84%\n",
        "Epoch [8/10], Training Loss: 1.228, Validation Accuracy: 53.29%\n",
        "Epoch [9/10], Training Loss: 1.216, Validation Accuracy: 53.92%\n",
        "Epoch [10/10], Training Loss: 1.204, Validation Accuracy: 53.05%\n",
        "Epoch [1/10], Training Loss: 1.279, Validation Accuracy: 53.26%\n",
        "Epoch [2/10], Training Loss: 1.256, Validation Accuracy: 54.23%\n",
        "Epoch [3/10], Training Loss: 1.231, Validation Accuracy: 53.36%\n",
        "Epoch [4/10], Training Loss: 1.220, Validation Accuracy: 54.48%\n",
        "Epoch [5/10], Training Loss: 1.210, Validation Accuracy: 53.55%\n",
        "Epoch [6/10], Training Loss: 1.200, Validation Accuracy: 54.20%\n",
        "Epoch [7/10], Training Loss: 1.190, Validation Accuracy: 54.21%\n",
        "Epoch [8/10], Training Loss: 1.194, Validation Accuracy: 54.25%\n",
        "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 54.02%\n",
        "Epoch [10/10], Training Loss: 1.159, Validation Accuracy: 54.46%\n",
        "Epoch [1/10], Training Loss: 1.253, Validation Accuracy: 54.78%\n",
        "Epoch [2/10], Training Loss: 1.230, Validation Accuracy: 54.96%\n",
        "Epoch [3/10], Training Loss: 1.209, Validation Accuracy: 54.55%\n",
        "Epoch [4/10], Training Loss: 1.203, Validation Accuracy: 53.36%\n",
        "Epoch [5/10], Training Loss: 1.195, Validation Accuracy: 54.76%\n",
        "Epoch [6/10], Training Loss: 1.170, Validation Accuracy: 55.04%\n",
        "Epoch [7/10], Training Loss: 1.158, Validation Accuracy: 54.21%\n",
        "Epoch [8/10], Training Loss: 1.162, Validation Accuracy: 54.30%\n",
        "Epoch [9/10], Training Loss: 1.145, Validation Accuracy: 54.42%\n",
        "Epoch [10/10], Training Loss: 1.135, Validation Accuracy: 53.80%\n",
        "Epoch [1/10], Training Loss: 1.227, Validation Accuracy: 55.64%\n",
        "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 55.67%\n",
        "Epoch [3/10], Training Loss: 1.182, Validation Accuracy: 56.61%\n",
        "Epoch [4/10], Training Loss: 1.170, Validation Accuracy: 56.60%\n",
        "Epoch [5/10], Training Loss: 1.154, Validation Accuracy: 56.44%\n",
        "Epoch [6/10], Training Loss: 1.141, Validation Accuracy: 56.56%\n",
        "Epoch [7/10], Training Loss: 1.124, Validation Accuracy: 55.45%\n",
        "Epoch [8/10], Training Loss: 1.121, Validation Accuracy: 56.26%\n",
        "Epoch [9/10], Training Loss: 1.106, Validation Accuracy: 56.11%\n",
        "Epoch [10/10], Training Loss: 1.107, Validation Accuracy: 55.40%\n",
        "Epoch [1/10], Training Loss: 1.208, Validation Accuracy: 56.75%\n",
        "Epoch [2/10], Training Loss: 1.176, Validation Accuracy: 55.52%\n",
        "Epoch [3/10], Training Loss: 1.169, Validation Accuracy: 56.40%\n",
        "Epoch [4/10], Training Loss: 1.149, Validation Accuracy: 56.51%\n",
        "Epoch [5/10], Training Loss: 1.129, Validation Accuracy: 56.61%\n",
        "Epoch [6/10], Training Loss: 1.129, Validation Accuracy: 55.53%\n",
        "Epoch [7/10], Training Loss: 1.117, Validation Accuracy: 56.29%\n",
        "Epoch [8/10], Training Loss: 1.104, Validation Accuracy: 56.82%\n",
        "Epoch [9/10], Training Loss: 1.093, Validation Accuracy: 56.93%\n",
        "Epoch [10/10], Training Loss: 1.083, Validation Accuracy: 57.18%\n",
        "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 57.39%\n",
        "Epoch [2/10], Training Loss: 1.139, Validation Accuracy: 57.96%\n",
        "Epoch [3/10], Training Loss: 1.128, Validation Accuracy: 57.39%\n",
        "Epoch [4/10], Training Loss: 1.114, Validation Accuracy: 57.70%\n",
        "Epoch [5/10], Training Loss: 1.099, Validation Accuracy: 57.86%\n",
        "Epoch [6/10], Training Loss: 1.089, Validation Accuracy: 58.02%\n",
        "Epoch [7/10], Training Loss: 1.074, Validation Accuracy: 57.12%\n",
        "Epoch [8/10], Training Loss: 1.070, Validation Accuracy: 56.72%\n",
        "Epoch [9/10], Training Loss: 1.061, Validation Accuracy: 56.92%\n",
        "Epoch [10/10], Training Loss: 1.045, Validation Accuracy: 57.34%\n",
        "Epoch [1/10], Training Loss: 1.149, Validation Accuracy: 56.76%\n",
        "Epoch [2/10], Training Loss: 1.116, Validation Accuracy: 58.08%\n",
        "Epoch [3/10], Training Loss: 1.110, Validation Accuracy: 57.29%\n",
        "Epoch [4/10], Training Loss: 1.095, Validation Accuracy: 56.76%\n",
        "Epoch [5/10], Training Loss: 1.073, Validation Accuracy: 58.35%\n",
        "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.15%\n",
        "Epoch [7/10], Training Loss: 1.049, Validation Accuracy: 57.56%\n",
        "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 56.66%\n",
        "Epoch [9/10], Training Loss: 1.023, Validation Accuracy: 58.04%\n",
        "Epoch [10/10], Training Loss: 1.008, Validation Accuracy: 57.81%\n",
        "Epoch [1/10], Training Loss: 1.136, Validation Accuracy: 58.21%\n",
        "Epoch [2/10], Training Loss: 1.103, Validation Accuracy: 58.41%\n",
        "Epoch [3/10], Training Loss: 1.083, Validation Accuracy: 58.51%\n",
        "Epoch [4/10], Training Loss: 1.065, Validation Accuracy: 57.82%\n",
        "Epoch [5/10], Training Loss: 1.049, Validation Accuracy: 58.73%\n",
        "Epoch [6/10], Training Loss: 1.036, Validation Accuracy: 59.10%\n",
        "Epoch [7/10], Training Loss: 1.017, Validation Accuracy: 58.10%\n",
        "Epoch [8/10], Training Loss: 1.014, Validation Accuracy: 58.57%\n",
        "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 58.00%\n",
        "Epoch [10/10], Training Loss: 0.994, Validation Accuracy: 58.37%\n",
        "Epoch [1/10], Training Loss: 1.116, Validation Accuracy: 58.84%\n",
        "Epoch [2/10], Training Loss: 1.087, Validation Accuracy: 59.51%\n",
        "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 59.44%\n",
        "Epoch [4/10], Training Loss: 1.047, Validation Accuracy: 59.23%\n",
        "Epoch [5/10], Training Loss: 1.028, Validation Accuracy: 59.30%\n",
        "Epoch [6/10], Training Loss: 1.011, Validation Accuracy: 59.28%\n",
        "Epoch [7/10], Training Loss: 1.008, Validation Accuracy: 59.24%\n",
        "Epoch [8/10], Training Loss: 1.000, Validation Accuracy: 58.72%\n",
        "Epoch [9/10], Training Loss: 0.981, Validation Accuracy: 58.46%\n",
        "Epoch [10/10], Training Loss: 0.972, Validation Accuracy: 59.08%\n",
        "Epoch [1/10], Training Loss: 1.104, Validation Accuracy: 59.30%\n",
        "Epoch [2/10], Training Loss: 1.073, Validation Accuracy: 59.92%\n",
        "Epoch [3/10], Training Loss: 1.057, Validation Accuracy: 59.25%\n",
        "Epoch [4/10], Training Loss: 1.035, Validation Accuracy: 59.12%\n",
        "Epoch [5/10], Training Loss: 1.020, Validation Accuracy: 59.53%\n",
        "Epoch [6/10], Training Loss: 1.007, Validation Accuracy: 59.65%\n",
        "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 59.22%\n",
        "Epoch [8/10], Training Loss: 0.980, Validation Accuracy: 59.36%\n",
        "Epoch [9/10], Training Loss: 0.965, Validation Accuracy: 59.32%\n",
        "Epoch [10/10], Training Loss: 0.955, Validation Accuracy: 59.73%\n",
        "Epoch [1/10], Training Loss: 1.067, Validation Accuracy: 60.06%\n",
        "Epoch [2/10], Training Loss: 1.038, Validation Accuracy: 60.30%\n",
        "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 59.78%\n",
        "Epoch [4/10], Training Loss: 1.000, Validation Accuracy: 59.94%\n",
        "Epoch [5/10], Training Loss: 0.983, Validation Accuracy: 59.53%\n",
        "Epoch [6/10], Training Loss: 0.972, Validation Accuracy: 59.63%\n",
        "Epoch [7/10], Training Loss: 0.959, Validation Accuracy: 60.03%\n",
        "Epoch [8/10], Training Loss: 0.949, Validation Accuracy: 59.79%\n",
        "Epoch [9/10], Training Loss: 0.932, Validation Accuracy: 59.75%\n",
        "Epoch [10/10], Training Loss: 0.922, Validation Accuracy: 60.07%\n",
        "Epoch [1/10], Training Loss: 1.068, Validation Accuracy: 58.22%\n",
        "Epoch [2/10], Training Loss: 1.029, Validation Accuracy: 59.66%\n",
        "Epoch [3/10], Training Loss: 1.000, Validation Accuracy: 60.26%\n",
        "Epoch [4/10], Training Loss: 0.980, Validation Accuracy: 60.31%\n",
        "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.03%\n",
        "Epoch [6/10], Training Loss: 0.948, Validation Accuracy: 58.87%\n",
        "Epoch [7/10], Training Loss: 0.939, Validation Accuracy: 59.67%\n",
        "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 59.86%\n",
        "Epoch [9/10], Training Loss: 0.914, Validation Accuracy: 60.17%\n",
        "Epoch [10/10], Training Loss: 0.895, Validation Accuracy: 60.09%\n",
        "Epoch [1/10], Training Loss: 1.046, Validation Accuracy: 60.58%\n",
        "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 58.70%\n",
        "Epoch [3/10], Training Loss: 0.991, Validation Accuracy: 60.30%\n",
        "Epoch [4/10], Training Loss: 0.960, Validation Accuracy: 60.36%\n",
        "Epoch [5/10], Training Loss: 0.951, Validation Accuracy: 60.19%\n",
        "Epoch [6/10], Training Loss: 0.939, Validation Accuracy: 60.19%\n",
        "Epoch [7/10], Training Loss: 0.920, Validation Accuracy: 60.44%\n",
        "Epoch [8/10], Training Loss: 0.904, Validation Accuracy: 60.63%\n",
        "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 60.86%\n",
        "Epoch [10/10], Training Loss: 0.890, Validation Accuracy: 60.43%\n",
        "Epoch [1/10], Training Loss: 1.042, Validation Accuracy: 61.05%\n",
        "Epoch [2/10], Training Loss: 0.999, Validation Accuracy: 60.01%\n",
        "Epoch [3/10], Training Loss: 0.969, Validation Accuracy: 60.66%\n",
        "Epoch [4/10], Training Loss: 0.951, Validation Accuracy: 61.15%\n",
        "Epoch [5/10], Training Loss: 0.935, Validation Accuracy: 60.88%\n",
        "Epoch [6/10], Training Loss: 0.925, Validation Accuracy: 60.61%\n",
        "Epoch [7/10], Training Loss: 0.898, Validation Accuracy: 60.50%\n",
        "Epoch [8/10], Training Loss: 0.894, Validation Accuracy: 60.82%\n",
        "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 60.23%\n",
        "Epoch [10/10], Training Loss: 0.868, Validation Accuracy: 60.84%\n",
        "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 61.02%\n",
        "Epoch [2/10], Training Loss: 0.986, Validation Accuracy: 61.76%\n",
        "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.99%\n",
        "Epoch [4/10], Training Loss: 0.932, Validation Accuracy: 61.56%\n",
        "Epoch [5/10], Training Loss: 0.917, Validation Accuracy: 61.18%\n",
        "Epoch [6/10], Training Loss: 0.903, Validation Accuracy: 61.07%\n",
        "Epoch [7/10], Training Loss: 0.886, Validation Accuracy: 60.96%\n",
        "Epoch [8/10], Training Loss: 0.866, Validation Accuracy: 61.23%\n",
        "Epoch [9/10], Training Loss: 0.848, Validation Accuracy: 61.04%\n",
        "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 60.94%\n",
        "Epoch [1/10], Training Loss: 1.007, Validation Accuracy: 61.79%\n",
        "Epoch [2/10], Training Loss: 0.965, Validation Accuracy: 61.94%\n",
        "Epoch [3/10], Training Loss: 0.934, Validation Accuracy: 61.75%\n",
        "Epoch [4/10], Training Loss: 0.905, Validation Accuracy: 61.20%\n",
        "Epoch [5/10], Training Loss: 0.893, Validation Accuracy: 61.19%\n",
        "Epoch [6/10], Training Loss: 0.876, Validation Accuracy: 61.53%\n",
        "Epoch [7/10], Training Loss: 0.861, Validation Accuracy: 61.08%\n",
        "Epoch [8/10], Training Loss: 0.849, Validation Accuracy: 61.62%\n",
        "Epoch [9/10], Training Loss: 0.833, Validation Accuracy: 61.17%\n",
        "Epoch [10/10], Training Loss: 0.817, Validation Accuracy: 61.46%\n",
        "Epoch [1/10], Training Loss: 0.984, Validation Accuracy: 61.31%\n",
        "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 61.33%\n",
        "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 61.52%\n",
        "Epoch [4/10], Training Loss: 0.887, Validation Accuracy: 61.49%\n",
        "Epoch [5/10], Training Loss: 0.871, Validation Accuracy: 61.80%\n",
        "Epoch [6/10], Training Loss: 0.855, Validation Accuracy: 61.20%\n",
        "Epoch [7/10], Training Loss: 0.837, Validation Accuracy: 61.64%\n",
        "Epoch [8/10], Training Loss: 0.820, Validation Accuracy: 61.08%\n",
        "Epoch [9/10], Training Loss: 0.809, Validation Accuracy: 61.51%\n",
        "Epoch [10/10], Training Loss: 0.796, Validation Accuracy: 61.53%\n",
        "Epoch [1/10], Training Loss: 0.992, Validation Accuracy: 61.19%\n",
        "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.96%\n",
        "Epoch [3/10], Training Loss: 0.900, Validation Accuracy: 61.89%\n",
        "Epoch [4/10], Training Loss: 0.873, Validation Accuracy: 61.31%\n",
        "Epoch [5/10], Training Loss: 0.848, Validation Accuracy: 61.80%\n",
        "Epoch [6/10], Training Loss: 0.843, Validation Accuracy: 60.63%\n",
        "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.86%\n",
        "Epoch [8/10], Training Loss: 0.803, Validation Accuracy: 61.07%\n",
        "Epoch [9/10], Training Loss: 0.789, Validation Accuracy: 61.63%\n",
        "Epoch [10/10], Training Loss: 0.782, Validation Accuracy: 61.71%\n",
        "Epoch [1/10], Training Loss: 0.976, Validation Accuracy: 61.88%\n",
        "Epoch [2/10], Training Loss: 0.915, Validation Accuracy: 62.13%\n",
        "Epoch [3/10], Training Loss: 0.891, Validation Accuracy: 62.07%\n",
        "Epoch [4/10], Training Loss: 0.868, Validation Accuracy: 61.67%\n",
        "Epoch [5/10], Training Loss: 0.844, Validation Accuracy: 61.94%\n",
        "Epoch [6/10], Training Loss: 0.830, Validation Accuracy: 61.81%\n",
        "Epoch [7/10], Training Loss: 0.817, Validation Accuracy: 61.68%\n",
        "Epoch [8/10], Training Loss: 0.796, Validation Accuracy: 61.41%\n",
        "Epoch [9/10], Training Loss: 0.784, Validation Accuracy: 61.92%\n",
        "Epoch [10/10], Training Loss: 0.784, Validation Accuracy: 60.92%\n",
        "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 62.38%\n",
        "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 62.08%\n",
        "Epoch [3/10], Training Loss: 0.871, Validation Accuracy: 61.94%\n",
        "Epoch [4/10], Training Loss: 0.848, Validation Accuracy: 61.75%\n",
        "Epoch [5/10], Training Loss: 0.824, Validation Accuracy: 62.29%\n",
        "Epoch [6/10], Training Loss: 0.797, Validation Accuracy: 62.54%\n",
        "Epoch [7/10], Training Loss: 0.782, Validation Accuracy: 62.46%\n",
        "Epoch [8/10], Training Loss: 0.768, Validation Accuracy: 62.27%\n",
        "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 62.26%\n",
        "Epoch [10/10], Training Loss: 0.730, Validation Accuracy: 61.57%\n",
        "Epoch [1/10], Training Loss: 0.948, Validation Accuracy: 62.37%\n",
        "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.94%\n",
        "Epoch [3/10], Training Loss: 0.850, Validation Accuracy: 61.35%\n",
        "Epoch [4/10], Training Loss: 0.833, Validation Accuracy: 61.88%\n",
        "Epoch [5/10], Training Loss: 0.809, Validation Accuracy: 62.12%\n",
        "Epoch [6/10], Training Loss: 0.788, Validation Accuracy: 62.57%\n",
        "Epoch [7/10], Training Loss: 0.772, Validation Accuracy: 62.52%\n",
        "Epoch [8/10], Training Loss: 0.754, Validation Accuracy: 61.99%\n",
        "Epoch [9/10], Training Loss: 0.743, Validation Accuracy: 62.04%\n",
        "Epoch [10/10], Training Loss: 0.724, Validation Accuracy: 61.86%\n",
        "Epoch [1/10], Training Loss: 0.928, Validation Accuracy: 61.95%\n",
        "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 62.03%\n",
        "Epoch [3/10], Training Loss: 0.833, Validation Accuracy: 62.37%\n",
        "Epoch [4/10], Training Loss: 0.813, Validation Accuracy: 62.62%\n",
        "Epoch [5/10], Training Loss: 0.783, Validation Accuracy: 61.91%\n",
        "Epoch [6/10], Training Loss: 0.768, Validation Accuracy: 61.50%\n",
        "Epoch [7/10], Training Loss: 0.746, Validation Accuracy: 62.77%\n",
        "Epoch [8/10], Training Loss: 0.731, Validation Accuracy: 62.27%\n",
        "Epoch [9/10], Training Loss: 0.706, Validation Accuracy: 62.72%\n",
        "Epoch [10/10], Training Loss: 0.704, Validation Accuracy: 61.55%\n",
        "Epoch [1/10], Training Loss: 0.926, Validation Accuracy: 62.24%\n",
        "Epoch [2/10], Training Loss: 0.859, Validation Accuracy: 61.45%\n",
        "Epoch [3/10], Training Loss: 0.816, Validation Accuracy: 62.71%\n",
        "Epoch [4/10], Training Loss: 0.792, Validation Accuracy: 62.24%\n",
        "Epoch [5/10], Training Loss: 0.772, Validation Accuracy: 62.08%\n",
        "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 62.62%\n",
        "Epoch [7/10], Training Loss: 0.732, Validation Accuracy: 61.65%\n",
        "Epoch [8/10], Training Loss: 0.720, Validation Accuracy: 62.03%\n",
        "Epoch [9/10], Training Loss: 0.700, Validation Accuracy: 61.96%\n",
        "Epoch [10/10], Training Loss: 0.680, Validation Accuracy: 62.29%\n",
        "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 62.44%\n",
        "Epoch [2/10], Training Loss: 0.853, Validation Accuracy: 61.82%\n",
        "Epoch [3/10], Training Loss: 0.820, Validation Accuracy: 62.24%\n",
        "Epoch [4/10], Training Loss: 0.782, Validation Accuracy: 62.53%\n",
        "Epoch [5/10], Training Loss: 0.761, Validation Accuracy: 62.82%\n",
        "Epoch [6/10], Training Loss: 0.749, Validation Accuracy: 62.40%\n",
        "Epoch [7/10], Training Loss: 0.729, Validation Accuracy: 62.65%\n",
        "Epoch [8/10], Training Loss: 0.707, Validation Accuracy: 62.41%\n",
        "Epoch [9/10], Training Loss: 0.691, Validation Accuracy: 62.49%\n",
        "Epoch [10/10], Training Loss: 0.667, Validation Accuracy: 62.99%\n",
        "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 63.02%\n",
        "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 62.59%\n",
        "Epoch [3/10], Training Loss: 0.797, Validation Accuracy: 62.71%\n",
        "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 63.44%\n",
        "Epoch [5/10], Training Loss: 0.728, Validation Accuracy: 62.98%\n",
        "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 62.90%\n",
        "Epoch [7/10], Training Loss: 0.687, Validation Accuracy: 63.08%\n",
        "Epoch [8/10], Training Loss: 0.663, Validation Accuracy: 62.41%\n",
        "Epoch [9/10], Training Loss: 0.654, Validation Accuracy: 62.70%\n",
        "Epoch [10/10], Training Loss: 0.639, Validation Accuracy: 61.65%\n",
        "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 62.00%\n",
        "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 62.58%\n",
        "Epoch [3/10], Training Loss: 0.775, Validation Accuracy: 62.46%\n",
        "Epoch [4/10], Training Loss: 0.750, Validation Accuracy: 62.39%\n",
        "Epoch [5/10], Training Loss: 0.724, Validation Accuracy: 62.35%\n",
        "Epoch [6/10], Training Loss: 0.699, Validation Accuracy: 62.62%\n",
        "Epoch [7/10], Training Loss: 0.677, Validation Accuracy: 62.63%\n",
        "Epoch [8/10], Training Loss: 0.661, Validation Accuracy: 63.19%\n",
        "Epoch [9/10], Training Loss: 0.631, Validation Accuracy: 62.78%\n",
        "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 62.04%\n",
        "Epoch [1/10], Training Loss: 0.887, Validation Accuracy: 61.19%\n",
        "Epoch [2/10], Training Loss: 0.812, Validation Accuracy: 62.81%\n",
        "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 62.67%\n",
        "Epoch [4/10], Training Loss: 0.726, Validation Accuracy: 62.79%\n",
        "Epoch [5/10], Training Loss: 0.701, Validation Accuracy: 62.49%\n",
        "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 62.94%\n",
        "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 62.55%\n",
        "Epoch [8/10], Training Loss: 0.629, Validation Accuracy: 62.49%\n",
        "Epoch [9/10], Training Loss: 0.619, Validation Accuracy: 62.80%\n",
        "Epoch [10/10], Training Loss: 0.607, Validation Accuracy: 62.28%\n",
        "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.54%\n",
        "Epoch [2/10], Training Loss: 0.802, Validation Accuracy: 62.33%\n",
        "Epoch [3/10], Training Loss: 0.746, Validation Accuracy: 62.69%\n",
        "Epoch [4/10], Training Loss: 0.718, Validation Accuracy: 62.61%\n",
        "Epoch [5/10], Training Loss: 0.693, Validation Accuracy: 62.84%\n",
        "Epoch [6/10], Training Loss: 0.664, Validation Accuracy: 61.88%\n",
        "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 62.23%\n",
        "Epoch [8/10], Training Loss: 0.616, Validation Accuracy: 62.87%\n",
        "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 62.40%\n",
        "Epoch [10/10], Training Loss: 0.601, Validation Accuracy: 61.45%\n",
        "Epoch [1/10], Training Loss: 0.874, Validation Accuracy: 62.34%\n",
        "Epoch [2/10], Training Loss: 0.791, Validation Accuracy: 62.28%\n",
        "Epoch [3/10], Training Loss: 0.745, Validation Accuracy: 63.27%\n",
        "Epoch [4/10], Training Loss: 0.713, Validation Accuracy: 63.31%\n",
        "Epoch [5/10], Training Loss: 0.684, Validation Accuracy: 62.79%\n",
        "Epoch [6/10], Training Loss: 0.655, Validation Accuracy: 62.99%\n",
        "Epoch [7/10], Training Loss: 0.636, Validation Accuracy: 62.70%\n",
        "Epoch [8/10], Training Loss: 0.613, Validation Accuracy: 61.98%\n",
        "Epoch [9/10], Training Loss: 0.603, Validation Accuracy: 62.41%\n",
        "Epoch [10/10], Training Loss: 0.582, Validation Accuracy: 62.69%\n",
        "Epoch [1/10], Training Loss: 0.863, Validation Accuracy: 63.13%\n",
        "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 63.02%\n",
        "Epoch [3/10], Training Loss: 0.708, Validation Accuracy: 63.64%\n",
        "Epoch [4/10], Training Loss: 0.684, Validation Accuracy: 62.23%\n",
        "Epoch [5/10], Training Loss: 0.656, Validation Accuracy: 62.80%\n",
        "Epoch [6/10], Training Loss: 0.624, Validation Accuracy: 63.19%\n",
        "Epoch [7/10], Training Loss: 0.608, Validation Accuracy: 62.38%\n",
        "Epoch [8/10], Training Loss: 0.576, Validation Accuracy: 63.21%\n",
        "Epoch [9/10], Training Loss: 0.561, Validation Accuracy: 62.47%\n",
        "Epoch [10/10], Training Loss: 0.540, Validation Accuracy: 63.02%\n",
        "Epoch [1/10], Training Loss: 0.843, Validation Accuracy: 62.64%\n",
        "Epoch [2/10], Training Loss: 0.754, Validation Accuracy: 63.33%\n",
        "Epoch [3/10], Training Loss: 0.697, Validation Accuracy: 62.39%\n",
        "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 63.34%\n",
        "Epoch [5/10], Training Loss: 0.637, Validation Accuracy: 62.04%\n",
        "Epoch [6/10], Training Loss: 0.607, Validation Accuracy: 63.05%\n",
        "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 62.75%\n",
        "Epoch [8/10], Training Loss: 0.567, Validation Accuracy: 62.76%\n",
        "Epoch [9/10], Training Loss: 0.553, Validation Accuracy: 62.30%\n",
        "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 62.54%\n",
        "Epoch [1/10], Training Loss: 0.852, Validation Accuracy: 61.45%\n",
        "Epoch [2/10], Training Loss: 0.753, Validation Accuracy: 62.70%\n",
        "Epoch [3/10], Training Loss: 0.690, Validation Accuracy: 62.71%\n",
        "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 62.89%\n",
        "Epoch [5/10], Training Loss: 0.621, Validation Accuracy: 62.91%\n",
        "Epoch [6/10], Training Loss: 0.593, Validation Accuracy: 62.95%\n",
        "Epoch [7/10], Training Loss: 0.571, Validation Accuracy: 62.50%\n",
        "Epoch [8/10], Training Loss: 0.549, Validation Accuracy: 62.74%\n",
        "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 62.73%\n",
        "Epoch [10/10], Training Loss: 0.514, Validation Accuracy: 61.72%\n",
        "Epoch [1/10], Training Loss: 0.826, Validation Accuracy: 62.88%\n",
        "Epoch [2/10], Training Loss: 0.722, Validation Accuracy: 61.72%\n",
        "Epoch [3/10], Training Loss: 0.671, Validation Accuracy: 62.66%\n",
        "Epoch [4/10], Training Loss: 0.635, Validation Accuracy: 62.83%\n",
        "Epoch [5/10], Training Loss: 0.599, Validation Accuracy: 61.92%\n",
        "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 62.69%\n",
        "Epoch [7/10], Training Loss: 0.556, Validation Accuracy: 62.14%\n",
        "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 62.60%\n",
        "Epoch [9/10], Training Loss: 0.506, Validation Accuracy: 62.09%\n",
        "Epoch [10/10], Training Loss: 0.495, Validation Accuracy: 62.12%\n",
        "Epoch [1/10], Training Loss: 0.828, Validation Accuracy: 61.96%\n",
        "Epoch [2/10], Training Loss: 0.742, Validation Accuracy: 62.92%\n",
        "Epoch [3/10], Training Loss: 0.671, Validation Accuracy: 62.43%\n",
        "Epoch [4/10], Training Loss: 0.639, Validation Accuracy: 63.06%\n",
        "Epoch [5/10], Training Loss: 0.610, Validation Accuracy: 63.48%\n",
        "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 62.87%\n",
        "Epoch [7/10], Training Loss: 0.565, Validation Accuracy: 62.87%\n",
        "Epoch [8/10], Training Loss: 0.538, Validation Accuracy: 63.04%\n",
        "Epoch [9/10], Training Loss: 0.521, Validation Accuracy: 63.10%\n",
        "Epoch [10/10], Training Loss: 0.497, Validation Accuracy: 62.88%\n",
        "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 62.43%\n",
        "Epoch [2/10], Training Loss: 0.717, Validation Accuracy: 63.10%\n",
        "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 63.42%\n",
        "Epoch [4/10], Training Loss: 0.602, Validation Accuracy: 62.68%\n",
        "Epoch [5/10], Training Loss: 0.575, Validation Accuracy: 62.68%\n",
        "Epoch [6/10], Training Loss: 0.549, Validation Accuracy: 61.72%\n",
        "Epoch [7/10], Training Loss: 0.526, Validation Accuracy: 62.96%\n",
        "Epoch [8/10], Training Loss: 0.504, Validation Accuracy: 62.72%\n",
        "Epoch [9/10], Training Loss: 0.490, Validation Accuracy: 62.31%\n",
        "Epoch [10/10], Training Loss: 0.462, Validation Accuracy: 63.42%\n",
        "Epoch [1/10], Training Loss: 0.809, Validation Accuracy: 62.49%\n",
        "Epoch [2/10], Training Loss: 0.689, Validation Accuracy: 62.23%\n",
        "Epoch [3/10], Training Loss: 0.631, Validation Accuracy: 63.21%\n",
        "Epoch [4/10], Training Loss: 0.597, Validation Accuracy: 63.10%\n",
        "Epoch [5/10], Training Loss: 0.557, Validation Accuracy: 62.29%\n",
        "Epoch [6/10], Training Loss: 0.535, Validation Accuracy: 62.66%\n",
        "Epoch [7/10], Training Loss: 0.512, Validation Accuracy: 61.56%\n",
        "Epoch [8/10], Training Loss: 0.488, Validation Accuracy: 62.85%\n",
        "Epoch [9/10], Training Loss: 0.465, Validation Accuracy: 62.05%\n",
        "Epoch [10/10], Training Loss: 0.444, Validation Accuracy: 62.21%\n",
        "Epoch [1/10], Training Loss: 0.804, Validation Accuracy: 62.04%\n",
        "Epoch [2/10], Training Loss: 0.688, Validation Accuracy: 61.94%\n",
        "Epoch [3/10], Training Loss: 0.630, Validation Accuracy: 62.26%\n",
        "Epoch [4/10], Training Loss: 0.580, Validation Accuracy: 62.46%\n",
        "Epoch [5/10], Training Loss: 0.550, Validation Accuracy: 62.04%\n",
        "Epoch [6/10], Training Loss: 0.533, Validation Accuracy: 62.39%\n",
        "Epoch [7/10], Training Loss: 0.494, Validation Accuracy: 62.28%\n",
        "Epoch [8/10], Training Loss: 0.466, Validation Accuracy: 62.51%\n",
        "Epoch [9/10], Training Loss: 0.450, Validation Accuracy: 62.01%\n",
        "Epoch [10/10], Training Loss: 0.437, Validation Accuracy: 62.61%\n",
        "Epoch [1/10], Training Loss: 0.796, Validation Accuracy: 61.25%\n",
        "Epoch [2/10], Training Loss: 0.674, Validation Accuracy: 62.05%\n",
        "Epoch [3/10], Training Loss: 0.607, Validation Accuracy: 62.85%\n",
        "Epoch [4/10], Training Loss: 0.562, Validation Accuracy: 62.50%\n",
        "Epoch [5/10], Training Loss: 0.526, Validation Accuracy: 62.43%\n",
        "Epoch [6/10], Training Loss: 0.497, Validation Accuracy: 62.46%\n",
        "Epoch [7/10], Training Loss: 0.470, Validation Accuracy: 62.12%\n",
        "Epoch [8/10], Training Loss: 0.453, Validation Accuracy: 62.23%\n",
        "Epoch [9/10], Training Loss: 0.434, Validation Accuracy: 61.99%\n",
        "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 62.12%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF5C8LbKsiiS",
        "outputId": "6032c018-ae81-407a-a267-662d5feabb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [9.65, 9.72, 9.99, 10.53, 11.09, 12.15, 12.91, 14.11, 15.58, 16.29, 16.97, 19.59, 20.96, 20.48, 20.64, 21.06, 22.06, 23.46, 24.85, 27.06, 27.46, 28.43, 28.91, 29.96, 30.19, 31.15, 31.49, 31.84, 33.01, 33.31, 32.68, 34.1, 34.41, 33.65, 34.98, 35.97, 36.18, 35.61, 36.83, 36.36, 37.58, 38.09, 38.32, 38.69, 38.91, 39.31, 39.59, 39.36, 40.28, 39.35, 41.04, 40.83, 41.26, 41.38, 42.07, 42.0, 41.79, 41.98, 42.78, 42.13, 43.33, 43.97, 43.27, 44.94, 44.13, 45.28, 45.46, 45.07, 44.97, 45.27, 46.6, 46.94, 46.27, 47.17, 47.2, 47.48, 47.21, 47.16, 47.92, 47.82, 48.27, 48.11, 48.06, 47.73, 48.28, 48.3, 49.37, 48.91, 49.06, 49.25, 48.93, 49.74, 49.7, 49.69, 48.81, 49.43, 48.99, 49.84, 50.29, 49.53, 50.18, 50.52, 50.58, 50.8, 51.28, 50.94, 51.0, 50.6, 51.47, 51.58, 52.23, 51.29, 52.52, 52.0, 52.04, 52.78, 52.07, 52.58, 52.15, 52.18, 53.37, 53.04, 53.16, 53.66, 53.26, 53.11, 53.84, 53.29, 53.92, 53.05, 53.26, 54.23, 53.36, 54.48, 53.55, 54.2, 54.21, 54.25, 54.02, 54.46, 54.78, 54.96, 54.55, 53.36, 54.76, 55.04, 54.21, 54.3, 54.42, 53.8, 55.64, 55.67, 56.61, 56.6, 56.44, 56.56, 55.45, 56.26, 56.11, 55.4, 56.75, 55.52, 56.4, 56.51, 56.61, 55.53, 56.29, 56.82, 56.93, 57.18, 57.39, 57.96, 57.39, 57.7, 57.86, 58.02, 57.12, 56.72, 56.92, 57.34, 56.76, 58.08, 57.29, 56.76, 58.35, 58.15, 57.56, 56.66, 58.04, 57.81, 58.21, 58.41, 58.51, 57.82, 58.73, 59.1, 58.1, 58.57, 58.0, 58.37, 58.84, 59.51, 59.44, 59.23, 59.3, 59.28, 59.24, 58.72, 58.46, 59.08, 59.3, 59.92, 59.25, 59.12, 59.53, 59.65, 59.22, 59.36, 59.32, 59.73, 60.06, 60.3, 59.78, 59.94, 59.53, 59.63, 60.03, 59.79, 59.75, 60.07, 58.22, 59.66, 60.26, 60.31, 60.03, 58.87, 59.67, 59.86, 60.17, 60.09, 60.58, 58.7, 60.3, 60.36, 60.19, 60.19, 60.44, 60.63, 60.86, 60.43, 61.05, 60.01, 60.66, 61.15, 60.88, 60.61, 60.5, 60.82, 60.23, 60.84, 61.02, 61.76, 60.99, 61.56, 61.18, 61.07, 60.96, 61.23, 61.04, 60.94, 61.79, 61.94, 61.75, 61.2, 61.19, 61.53, 61.08, 61.62, 61.17, 61.46, 61.31, 61.33, 61.52, 61.49, 61.8, 61.2, 61.64, 61.08, 61.51, 61.53, 61.19, 60.96, 61.89, 61.31, 61.8, 60.63, 61.86, 61.07, 61.63, 61.71, 61.88, 62.13, 62.07, 61.67, 61.94, 61.81, 61.68, 61.41, 61.92, 60.92, 62.38, 62.08, 61.94, 61.75, 62.29, 62.54, 62.46, 62.27, 62.26, 61.57, 62.37, 61.94, 61.35, 61.88, 62.12, 62.57, 62.52, 61.99, 62.04, 61.86, 61.95, 62.03, 62.37, 62.62, 61.91, 61.5, 62.77, 62.27, 62.72, 61.55, 62.24, 61.45, 62.71, 62.24, 62.08, 62.62, 61.65, 62.03, 61.96, 62.29, 62.44, 61.82, 62.24, 62.53, 62.82, 62.4, 62.65, 62.41, 62.49, 62.99, 63.02, 62.59, 62.71, 63.44, 62.98, 62.9, 63.08, 62.41, 62.7, 61.65, 62.0, 62.58, 62.46, 62.39, 62.35, 62.62, 62.63, 63.19, 62.78, 62.04, 61.19, 62.81, 62.67, 62.79, 62.49, 62.94, 62.55, 62.49, 62.8, 62.28, 61.54, 62.33, 62.69, 62.61, 62.84, 61.88, 62.23, 62.87, 62.4, 61.45, 62.34, 62.28, 63.27, 63.31, 62.79, 62.99, 62.7, 61.98, 62.41, 62.69, 63.13, 63.02, 63.64, 62.23, 62.8, 63.19, 62.38, 63.21, 62.47, 63.02, 62.64, 63.33, 62.39, 63.34, 62.04, 63.05, 62.75, 62.76, 62.3, 62.54, 61.45, 62.7, 62.71, 62.89, 62.91, 62.95, 62.5, 62.74, 62.73, 61.72, 62.88, 61.72, 62.66, 62.83, 61.92, 62.69, 62.14, 62.6, 62.09, 62.12, 61.96, 62.92, 62.43, 63.06, 63.48, 62.87, 62.87, 63.04, 63.1, 62.88, 62.43, 63.1, 63.42, 62.68, 62.68, 61.72, 62.96, 62.72, 62.31, 63.42, 62.49, 62.23, 63.21, 63.1, 62.29, 62.66, 61.56, 62.85, 62.05, 62.21, 62.04, 61.94, 62.26, 62.46, 62.04, 62.39, 62.28, 62.51, 62.01, 62.61, 61.25, 62.05, 62.85, 62.5, 62.43, 62.46, 62.12, 62.23, 61.99, 62.12]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KpAaAZsWi6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N803YpEbWjLV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from typing import Dict\n",
        "from scipy.stats import truncnorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1uHX6glWjLW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcUOlXqgWjLW"
      },
      "outputs": [],
      "source": [
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tUIK8e7WjLW"
      },
      "outputs": [],
      "source": [
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe0ad6e-c882-4c25-e29b-14e722536777",
        "id": "y2G8-e-pWjLX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 77306790.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBb_-d0eWjLY"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZpQPxdkWjLY"
      },
      "outputs": [],
      "source": [
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHA-NIUKWjLY"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VcQc-xqWjLZ"
      },
      "outputs": [],
      "source": [
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvLJuxZeWjLZ"
      },
      "outputs": [],
      "source": [
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQXgQ546WjLZ"
      },
      "outputs": [],
      "source": [
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3XxJZkxWjLZ"
      },
      "outputs": [],
      "source": [
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09FEni4wWjLa"
      },
      "outputs": [],
      "source": [
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b79vGWSYWjLa"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both normal and truncated normal distributions\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    # Generate augmented data from normal distribution\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqpTNZwPWjLa"
      },
      "outputs": [],
      "source": [
        "# Define the federated training logic\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rls-XNCaWjLa"
      },
      "outputs": [],
      "source": [
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sFK2oTdWjLa"
      },
      "outputs": [],
      "source": [
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf37920a-7f41-412a-d6f8-e70374d51f52",
        "id": "yOHVHk6-WjLa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.91%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.95%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.01%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 10.32%\n",
            "Epoch [5/10], Training Loss: 2.299, Validation Accuracy: 11.35%\n",
            "Epoch [6/10], Training Loss: 2.298, Validation Accuracy: 13.96%\n",
            "Epoch [7/10], Training Loss: 2.296, Validation Accuracy: 14.94%\n",
            "Epoch [8/10], Training Loss: 2.294, Validation Accuracy: 14.61%\n",
            "Epoch [9/10], Training Loss: 2.290, Validation Accuracy: 14.43%\n",
            "Epoch [10/10], Training Loss: 2.286, Validation Accuracy: 14.36%\n",
            "Epoch [1/10], Training Loss: 2.280, Validation Accuracy: 14.73%\n",
            "Epoch [2/10], Training Loss: 2.270, Validation Accuracy: 13.47%\n",
            "Epoch [3/10], Training Loss: 2.256, Validation Accuracy: 12.90%\n",
            "Epoch [4/10], Training Loss: 2.240, Validation Accuracy: 13.76%\n",
            "Epoch [5/10], Training Loss: 2.225, Validation Accuracy: 14.83%\n",
            "Epoch [6/10], Training Loss: 2.209, Validation Accuracy: 17.45%\n",
            "Epoch [7/10], Training Loss: 2.193, Validation Accuracy: 20.39%\n",
            "Epoch [8/10], Training Loss: 2.176, Validation Accuracy: 21.49%\n",
            "Epoch [9/10], Training Loss: 2.159, Validation Accuracy: 22.28%\n",
            "Epoch [10/10], Training Loss: 2.141, Validation Accuracy: 22.26%\n",
            "Epoch [1/10], Training Loss: 2.132, Validation Accuracy: 23.27%\n",
            "Epoch [2/10], Training Loss: 2.115, Validation Accuracy: 24.04%\n",
            "Epoch [3/10], Training Loss: 2.097, Validation Accuracy: 24.15%\n",
            "Epoch [4/10], Training Loss: 2.081, Validation Accuracy: 24.52%\n",
            "Epoch [5/10], Training Loss: 2.064, Validation Accuracy: 25.74%\n",
            "Epoch [6/10], Training Loss: 2.048, Validation Accuracy: 26.24%\n",
            "Epoch [7/10], Training Loss: 2.034, Validation Accuracy: 27.28%\n",
            "Epoch [8/10], Training Loss: 2.021, Validation Accuracy: 27.28%\n",
            "Epoch [9/10], Training Loss: 2.014, Validation Accuracy: 27.69%\n",
            "Epoch [10/10], Training Loss: 2.002, Validation Accuracy: 28.16%\n",
            "Epoch [1/10], Training Loss: 2.015, Validation Accuracy: 28.74%\n",
            "Epoch [2/10], Training Loss: 2.006, Validation Accuracy: 28.87%\n",
            "Epoch [3/10], Training Loss: 1.997, Validation Accuracy: 29.11%\n",
            "Epoch [4/10], Training Loss: 1.990, Validation Accuracy: 29.21%\n",
            "Epoch [5/10], Training Loss: 1.984, Validation Accuracy: 29.63%\n",
            "Epoch [6/10], Training Loss: 1.974, Validation Accuracy: 29.73%\n",
            "Epoch [7/10], Training Loss: 1.968, Validation Accuracy: 30.00%\n",
            "Epoch [8/10], Training Loss: 1.959, Validation Accuracy: 30.35%\n",
            "Epoch [9/10], Training Loss: 1.952, Validation Accuracy: 30.38%\n",
            "Epoch [10/10], Training Loss: 1.943, Validation Accuracy: 30.63%\n",
            "Epoch [1/10], Training Loss: 1.934, Validation Accuracy: 30.64%\n",
            "Epoch [2/10], Training Loss: 1.924, Validation Accuracy: 30.24%\n",
            "Epoch [3/10], Training Loss: 1.916, Validation Accuracy: 30.22%\n",
            "Epoch [4/10], Training Loss: 1.910, Validation Accuracy: 31.49%\n",
            "Epoch [5/10], Training Loss: 1.898, Validation Accuracy: 30.74%\n",
            "Epoch [6/10], Training Loss: 1.890, Validation Accuracy: 31.36%\n",
            "Epoch [7/10], Training Loss: 1.883, Validation Accuracy: 32.43%\n",
            "Epoch [8/10], Training Loss: 1.870, Validation Accuracy: 32.85%\n",
            "Epoch [9/10], Training Loss: 1.861, Validation Accuracy: 32.62%\n",
            "Epoch [10/10], Training Loss: 1.853, Validation Accuracy: 33.41%\n",
            "Epoch [1/10], Training Loss: 1.848, Validation Accuracy: 34.36%\n",
            "Epoch [2/10], Training Loss: 1.830, Validation Accuracy: 35.23%\n",
            "Epoch [3/10], Training Loss: 1.812, Validation Accuracy: 35.81%\n",
            "Epoch [4/10], Training Loss: 1.798, Validation Accuracy: 35.77%\n",
            "Epoch [5/10], Training Loss: 1.781, Validation Accuracy: 36.13%\n",
            "Epoch [6/10], Training Loss: 1.763, Validation Accuracy: 37.29%\n",
            "Epoch [7/10], Training Loss: 1.745, Validation Accuracy: 38.11%\n",
            "Epoch [8/10], Training Loss: 1.720, Validation Accuracy: 38.29%\n",
            "Epoch [9/10], Training Loss: 1.704, Validation Accuracy: 38.63%\n",
            "Epoch [10/10], Training Loss: 1.688, Validation Accuracy: 39.31%\n",
            "Epoch [1/10], Training Loss: 1.688, Validation Accuracy: 39.39%\n",
            "Epoch [2/10], Training Loss: 1.672, Validation Accuracy: 40.06%\n",
            "Epoch [3/10], Training Loss: 1.652, Validation Accuracy: 40.30%\n",
            "Epoch [4/10], Training Loss: 1.636, Validation Accuracy: 41.02%\n",
            "Epoch [5/10], Training Loss: 1.625, Validation Accuracy: 41.04%\n",
            "Epoch [6/10], Training Loss: 1.607, Validation Accuracy: 41.73%\n",
            "Epoch [7/10], Training Loss: 1.594, Validation Accuracy: 42.45%\n",
            "Epoch [8/10], Training Loss: 1.578, Validation Accuracy: 42.53%\n",
            "Epoch [9/10], Training Loss: 1.567, Validation Accuracy: 42.67%\n",
            "Epoch [10/10], Training Loss: 1.553, Validation Accuracy: 42.95%\n",
            "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 43.05%\n",
            "Epoch [2/10], Training Loss: 1.572, Validation Accuracy: 43.48%\n",
            "Epoch [3/10], Training Loss: 1.550, Validation Accuracy: 44.71%\n",
            "Epoch [4/10], Training Loss: 1.536, Validation Accuracy: 42.63%\n",
            "Epoch [5/10], Training Loss: 1.526, Validation Accuracy: 44.53%\n",
            "Epoch [6/10], Training Loss: 1.511, Validation Accuracy: 45.72%\n",
            "Epoch [7/10], Training Loss: 1.501, Validation Accuracy: 45.51%\n",
            "Epoch [8/10], Training Loss: 1.487, Validation Accuracy: 45.29%\n",
            "Epoch [9/10], Training Loss: 1.481, Validation Accuracy: 46.20%\n",
            "Epoch [10/10], Training Loss: 1.468, Validation Accuracy: 45.93%\n",
            "Epoch [1/10], Training Loss: 1.507, Validation Accuracy: 47.68%\n",
            "Epoch [2/10], Training Loss: 1.484, Validation Accuracy: 47.06%\n",
            "Epoch [3/10], Training Loss: 1.478, Validation Accuracy: 47.85%\n",
            "Epoch [4/10], Training Loss: 1.464, Validation Accuracy: 47.19%\n",
            "Epoch [5/10], Training Loss: 1.454, Validation Accuracy: 48.03%\n",
            "Epoch [6/10], Training Loss: 1.442, Validation Accuracy: 48.31%\n",
            "Epoch [7/10], Training Loss: 1.439, Validation Accuracy: 48.54%\n",
            "Epoch [8/10], Training Loss: 1.422, Validation Accuracy: 48.43%\n",
            "Epoch [9/10], Training Loss: 1.421, Validation Accuracy: 47.38%\n",
            "Epoch [10/10], Training Loss: 1.412, Validation Accuracy: 48.43%\n",
            "Epoch [1/10], Training Loss: 1.457, Validation Accuracy: 49.06%\n",
            "Epoch [2/10], Training Loss: 1.444, Validation Accuracy: 48.82%\n",
            "Epoch [3/10], Training Loss: 1.436, Validation Accuracy: 49.75%\n",
            "Epoch [4/10], Training Loss: 1.418, Validation Accuracy: 49.48%\n",
            "Epoch [5/10], Training Loss: 1.408, Validation Accuracy: 49.59%\n",
            "Epoch [6/10], Training Loss: 1.398, Validation Accuracy: 49.91%\n",
            "Epoch [7/10], Training Loss: 1.386, Validation Accuracy: 50.26%\n",
            "Epoch [8/10], Training Loss: 1.382, Validation Accuracy: 50.17%\n",
            "Epoch [9/10], Training Loss: 1.372, Validation Accuracy: 50.59%\n",
            "Epoch [10/10], Training Loss: 1.364, Validation Accuracy: 49.57%\n",
            "Epoch [1/10], Training Loss: 1.374, Validation Accuracy: 50.21%\n",
            "Epoch [2/10], Training Loss: 1.359, Validation Accuracy: 51.17%\n",
            "Epoch [3/10], Training Loss: 1.335, Validation Accuracy: 51.09%\n",
            "Epoch [4/10], Training Loss: 1.335, Validation Accuracy: 51.55%\n",
            "Epoch [5/10], Training Loss: 1.322, Validation Accuracy: 51.92%\n",
            "Epoch [6/10], Training Loss: 1.319, Validation Accuracy: 51.71%\n",
            "Epoch [7/10], Training Loss: 1.302, Validation Accuracy: 51.11%\n",
            "Epoch [8/10], Training Loss: 1.296, Validation Accuracy: 51.53%\n",
            "Epoch [9/10], Training Loss: 1.292, Validation Accuracy: 52.28%\n",
            "Epoch [10/10], Training Loss: 1.284, Validation Accuracy: 51.92%\n",
            "Epoch [1/10], Training Loss: 1.348, Validation Accuracy: 51.60%\n",
            "Epoch [2/10], Training Loss: 1.328, Validation Accuracy: 53.05%\n",
            "Epoch [3/10], Training Loss: 1.312, Validation Accuracy: 52.89%\n",
            "Epoch [4/10], Training Loss: 1.307, Validation Accuracy: 52.60%\n",
            "Epoch [5/10], Training Loss: 1.299, Validation Accuracy: 52.02%\n",
            "Epoch [6/10], Training Loss: 1.295, Validation Accuracy: 52.30%\n",
            "Epoch [7/10], Training Loss: 1.281, Validation Accuracy: 52.50%\n",
            "Epoch [8/10], Training Loss: 1.272, Validation Accuracy: 52.14%\n",
            "Epoch [9/10], Training Loss: 1.268, Validation Accuracy: 53.26%\n",
            "Epoch [10/10], Training Loss: 1.262, Validation Accuracy: 51.93%\n",
            "Epoch [1/10], Training Loss: 1.328, Validation Accuracy: 51.59%\n",
            "Epoch [2/10], Training Loss: 1.318, Validation Accuracy: 52.85%\n",
            "Epoch [3/10], Training Loss: 1.291, Validation Accuracy: 54.03%\n",
            "Epoch [4/10], Training Loss: 1.285, Validation Accuracy: 53.53%\n",
            "Epoch [5/10], Training Loss: 1.283, Validation Accuracy: 53.57%\n",
            "Epoch [6/10], Training Loss: 1.268, Validation Accuracy: 53.54%\n",
            "Epoch [7/10], Training Loss: 1.255, Validation Accuracy: 53.80%\n",
            "Epoch [8/10], Training Loss: 1.246, Validation Accuracy: 53.84%\n",
            "Epoch [9/10], Training Loss: 1.246, Validation Accuracy: 53.67%\n",
            "Epoch [10/10], Training Loss: 1.240, Validation Accuracy: 53.73%\n",
            "Epoch [1/10], Training Loss: 1.317, Validation Accuracy: 54.46%\n",
            "Epoch [2/10], Training Loss: 1.285, Validation Accuracy: 53.80%\n",
            "Epoch [3/10], Training Loss: 1.280, Validation Accuracy: 54.27%\n",
            "Epoch [4/10], Training Loss: 1.262, Validation Accuracy: 53.47%\n",
            "Epoch [5/10], Training Loss: 1.254, Validation Accuracy: 54.54%\n",
            "Epoch [6/10], Training Loss: 1.240, Validation Accuracy: 53.77%\n",
            "Epoch [7/10], Training Loss: 1.231, Validation Accuracy: 54.77%\n",
            "Epoch [8/10], Training Loss: 1.223, Validation Accuracy: 54.37%\n",
            "Epoch [9/10], Training Loss: 1.221, Validation Accuracy: 55.06%\n",
            "Epoch [10/10], Training Loss: 1.207, Validation Accuracy: 55.25%\n",
            "Epoch [1/10], Training Loss: 1.282, Validation Accuracy: 55.42%\n",
            "Epoch [2/10], Training Loss: 1.254, Validation Accuracy: 55.36%\n",
            "Epoch [3/10], Training Loss: 1.248, Validation Accuracy: 55.60%\n",
            "Epoch [4/10], Training Loss: 1.230, Validation Accuracy: 55.58%\n",
            "Epoch [5/10], Training Loss: 1.221, Validation Accuracy: 56.08%\n",
            "Epoch [6/10], Training Loss: 1.204, Validation Accuracy: 56.16%\n",
            "Epoch [7/10], Training Loss: 1.198, Validation Accuracy: 55.80%\n",
            "Epoch [8/10], Training Loss: 1.193, Validation Accuracy: 55.85%\n",
            "Epoch [9/10], Training Loss: 1.183, Validation Accuracy: 55.81%\n",
            "Epoch [10/10], Training Loss: 1.174, Validation Accuracy: 55.62%\n",
            "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 56.12%\n",
            "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 56.56%\n",
            "Epoch [3/10], Training Loss: 1.178, Validation Accuracy: 56.39%\n",
            "Epoch [4/10], Training Loss: 1.165, Validation Accuracy: 56.79%\n",
            "Epoch [5/10], Training Loss: 1.147, Validation Accuracy: 55.96%\n",
            "Epoch [6/10], Training Loss: 1.137, Validation Accuracy: 55.65%\n",
            "Epoch [7/10], Training Loss: 1.136, Validation Accuracy: 56.87%\n",
            "Epoch [8/10], Training Loss: 1.129, Validation Accuracy: 56.70%\n",
            "Epoch [9/10], Training Loss: 1.117, Validation Accuracy: 57.08%\n",
            "Epoch [10/10], Training Loss: 1.107, Validation Accuracy: 57.06%\n",
            "Epoch [1/10], Training Loss: 1.215, Validation Accuracy: 56.64%\n",
            "Epoch [2/10], Training Loss: 1.185, Validation Accuracy: 56.58%\n",
            "Epoch [3/10], Training Loss: 1.174, Validation Accuracy: 55.72%\n",
            "Epoch [4/10], Training Loss: 1.162, Validation Accuracy: 57.05%\n",
            "Epoch [5/10], Training Loss: 1.143, Validation Accuracy: 56.85%\n",
            "Epoch [6/10], Training Loss: 1.129, Validation Accuracy: 57.03%\n",
            "Epoch [7/10], Training Loss: 1.124, Validation Accuracy: 57.02%\n",
            "Epoch [8/10], Training Loss: 1.113, Validation Accuracy: 57.15%\n",
            "Epoch [9/10], Training Loss: 1.101, Validation Accuracy: 56.67%\n",
            "Epoch [10/10], Training Loss: 1.092, Validation Accuracy: 57.87%\n",
            "Epoch [1/10], Training Loss: 1.198, Validation Accuracy: 57.55%\n",
            "Epoch [2/10], Training Loss: 1.171, Validation Accuracy: 56.83%\n",
            "Epoch [3/10], Training Loss: 1.158, Validation Accuracy: 57.52%\n",
            "Epoch [4/10], Training Loss: 1.143, Validation Accuracy: 57.32%\n",
            "Epoch [5/10], Training Loss: 1.124, Validation Accuracy: 58.22%\n",
            "Epoch [6/10], Training Loss: 1.118, Validation Accuracy: 57.70%\n",
            "Epoch [7/10], Training Loss: 1.103, Validation Accuracy: 57.66%\n",
            "Epoch [8/10], Training Loss: 1.097, Validation Accuracy: 57.58%\n",
            "Epoch [9/10], Training Loss: 1.087, Validation Accuracy: 57.88%\n",
            "Epoch [10/10], Training Loss: 1.073, Validation Accuracy: 57.60%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 58.76%\n",
            "Epoch [2/10], Training Loss: 1.157, Validation Accuracy: 58.27%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 57.97%\n",
            "Epoch [4/10], Training Loss: 1.122, Validation Accuracy: 58.54%\n",
            "Epoch [5/10], Training Loss: 1.110, Validation Accuracy: 58.41%\n",
            "Epoch [6/10], Training Loss: 1.093, Validation Accuracy: 58.03%\n",
            "Epoch [7/10], Training Loss: 1.090, Validation Accuracy: 58.26%\n",
            "Epoch [8/10], Training Loss: 1.071, Validation Accuracy: 58.60%\n",
            "Epoch [9/10], Training Loss: 1.066, Validation Accuracy: 58.53%\n",
            "Epoch [10/10], Training Loss: 1.052, Validation Accuracy: 58.26%\n",
            "Epoch [1/10], Training Loss: 1.167, Validation Accuracy: 58.65%\n",
            "Epoch [2/10], Training Loss: 1.135, Validation Accuracy: 58.99%\n",
            "Epoch [3/10], Training Loss: 1.107, Validation Accuracy: 58.69%\n",
            "Epoch [4/10], Training Loss: 1.100, Validation Accuracy: 59.13%\n",
            "Epoch [5/10], Training Loss: 1.083, Validation Accuracy: 58.89%\n",
            "Epoch [6/10], Training Loss: 1.072, Validation Accuracy: 58.88%\n",
            "Epoch [7/10], Training Loss: 1.061, Validation Accuracy: 58.18%\n",
            "Epoch [8/10], Training Loss: 1.043, Validation Accuracy: 59.24%\n",
            "Epoch [9/10], Training Loss: 1.038, Validation Accuracy: 58.96%\n",
            "Epoch [10/10], Training Loss: 1.031, Validation Accuracy: 59.44%\n",
            "Epoch [1/10], Training Loss: 1.109, Validation Accuracy: 59.35%\n",
            "Epoch [2/10], Training Loss: 1.074, Validation Accuracy: 59.58%\n",
            "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 59.33%\n",
            "Epoch [4/10], Training Loss: 1.048, Validation Accuracy: 59.77%\n",
            "Epoch [5/10], Training Loss: 1.025, Validation Accuracy: 59.70%\n",
            "Epoch [6/10], Training Loss: 1.017, Validation Accuracy: 59.68%\n",
            "Epoch [7/10], Training Loss: 0.995, Validation Accuracy: 59.85%\n",
            "Epoch [8/10], Training Loss: 0.991, Validation Accuracy: 59.92%\n",
            "Epoch [9/10], Training Loss: 0.985, Validation Accuracy: 60.09%\n",
            "Epoch [10/10], Training Loss: 0.980, Validation Accuracy: 58.70%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 60.28%\n",
            "Epoch [2/10], Training Loss: 1.075, Validation Accuracy: 59.92%\n",
            "Epoch [3/10], Training Loss: 1.059, Validation Accuracy: 59.44%\n",
            "Epoch [4/10], Training Loss: 1.039, Validation Accuracy: 60.20%\n",
            "Epoch [5/10], Training Loss: 1.022, Validation Accuracy: 60.00%\n",
            "Epoch [6/10], Training Loss: 1.011, Validation Accuracy: 59.09%\n",
            "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 60.06%\n",
            "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.27%\n",
            "Epoch [9/10], Training Loss: 0.969, Validation Accuracy: 59.67%\n",
            "Epoch [10/10], Training Loss: 0.960, Validation Accuracy: 59.29%\n",
            "Epoch [1/10], Training Loss: 1.109, Validation Accuracy: 59.45%\n",
            "Epoch [2/10], Training Loss: 1.070, Validation Accuracy: 59.61%\n",
            "Epoch [3/10], Training Loss: 1.047, Validation Accuracy: 59.32%\n",
            "Epoch [4/10], Training Loss: 1.026, Validation Accuracy: 60.00%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.07%\n",
            "Epoch [6/10], Training Loss: 1.006, Validation Accuracy: 60.15%\n",
            "Epoch [7/10], Training Loss: 0.987, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.974, Validation Accuracy: 59.77%\n",
            "Epoch [9/10], Training Loss: 0.957, Validation Accuracy: 59.60%\n",
            "Epoch [10/10], Training Loss: 0.947, Validation Accuracy: 59.48%\n",
            "Epoch [1/10], Training Loss: 1.098, Validation Accuracy: 59.24%\n",
            "Epoch [2/10], Training Loss: 1.066, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 1.032, Validation Accuracy: 60.14%\n",
            "Epoch [4/10], Training Loss: 1.012, Validation Accuracy: 60.29%\n",
            "Epoch [5/10], Training Loss: 1.001, Validation Accuracy: 60.13%\n",
            "Epoch [6/10], Training Loss: 0.978, Validation Accuracy: 60.25%\n",
            "Epoch [7/10], Training Loss: 0.967, Validation Accuracy: 60.38%\n",
            "Epoch [8/10], Training Loss: 0.955, Validation Accuracy: 60.56%\n",
            "Epoch [9/10], Training Loss: 0.945, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.928, Validation Accuracy: 60.23%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 1.039, Validation Accuracy: 60.88%\n",
            "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.989, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.985, Validation Accuracy: 60.66%\n",
            "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 61.13%\n",
            "Epoch [7/10], Training Loss: 0.940, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.934, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.924, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.910, Validation Accuracy: 60.68%\n",
            "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.998, Validation Accuracy: 61.38%\n",
            "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.84%\n",
            "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.928, Validation Accuracy: 61.54%\n",
            "Epoch [6/10], Training Loss: 0.906, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.895, Validation Accuracy: 61.56%\n",
            "Epoch [8/10], Training Loss: 0.879, Validation Accuracy: 61.55%\n",
            "Epoch [9/10], Training Loss: 0.872, Validation Accuracy: 60.20%\n",
            "Epoch [10/10], Training Loss: 0.859, Validation Accuracy: 60.88%\n",
            "Epoch [1/10], Training Loss: 1.033, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 61.41%\n",
            "Epoch [3/10], Training Loss: 0.963, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.942, Validation Accuracy: 61.86%\n",
            "Epoch [5/10], Training Loss: 0.926, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.910, Validation Accuracy: 60.94%\n",
            "Epoch [7/10], Training Loss: 0.895, Validation Accuracy: 60.61%\n",
            "Epoch [8/10], Training Loss: 0.875, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.858, Validation Accuracy: 60.84%\n",
            "Epoch [10/10], Training Loss: 0.847, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 1.034, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 61.25%\n",
            "Epoch [3/10], Training Loss: 0.954, Validation Accuracy: 60.72%\n",
            "Epoch [4/10], Training Loss: 0.933, Validation Accuracy: 60.97%\n",
            "Epoch [5/10], Training Loss: 0.906, Validation Accuracy: 60.49%\n",
            "Epoch [6/10], Training Loss: 0.899, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.876, Validation Accuracy: 61.02%\n",
            "Epoch [8/10], Training Loss: 0.860, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.854, Validation Accuracy: 61.19%\n",
            "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 60.34%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.00%\n",
            "Epoch [2/10], Training Loss: 0.981, Validation Accuracy: 61.51%\n",
            "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 61.67%\n",
            "Epoch [4/10], Training Loss: 0.920, Validation Accuracy: 61.79%\n",
            "Epoch [5/10], Training Loss: 0.899, Validation Accuracy: 61.90%\n",
            "Epoch [6/10], Training Loss: 0.878, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.864, Validation Accuracy: 62.23%\n",
            "Epoch [8/10], Training Loss: 0.846, Validation Accuracy: 61.90%\n",
            "Epoch [9/10], Training Loss: 0.845, Validation Accuracy: 61.70%\n",
            "Epoch [10/10], Training Loss: 0.819, Validation Accuracy: 61.63%\n",
            "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 61.31%\n",
            "Epoch [2/10], Training Loss: 0.959, Validation Accuracy: 62.28%\n",
            "Epoch [3/10], Training Loss: 0.933, Validation Accuracy: 61.91%\n",
            "Epoch [4/10], Training Loss: 0.906, Validation Accuracy: 61.51%\n",
            "Epoch [5/10], Training Loss: 0.889, Validation Accuracy: 62.04%\n",
            "Epoch [6/10], Training Loss: 0.867, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 62.00%\n",
            "Epoch [8/10], Training Loss: 0.840, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.812, Validation Accuracy: 61.20%\n",
            "Epoch [10/10], Training Loss: 0.807, Validation Accuracy: 61.26%\n",
            "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 61.98%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.85%\n",
            "Epoch [3/10], Training Loss: 0.873, Validation Accuracy: 60.89%\n",
            "Epoch [4/10], Training Loss: 0.858, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.826, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.810, Validation Accuracy: 61.73%\n",
            "Epoch [7/10], Training Loss: 0.798, Validation Accuracy: 62.28%\n",
            "Epoch [8/10], Training Loss: 0.777, Validation Accuracy: 62.21%\n",
            "Epoch [9/10], Training Loss: 0.775, Validation Accuracy: 62.52%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 62.26%\n",
            "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 62.16%\n",
            "Epoch [2/10], Training Loss: 0.915, Validation Accuracy: 62.74%\n",
            "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 61.77%\n",
            "Epoch [4/10], Training Loss: 0.858, Validation Accuracy: 62.06%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.816, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.796, Validation Accuracy: 62.02%\n",
            "Epoch [8/10], Training Loss: 0.779, Validation Accuracy: 61.44%\n",
            "Epoch [9/10], Training Loss: 0.767, Validation Accuracy: 62.20%\n",
            "Epoch [10/10], Training Loss: 0.750, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.910, Validation Accuracy: 62.14%\n",
            "Epoch [3/10], Training Loss: 0.870, Validation Accuracy: 62.33%\n",
            "Epoch [4/10], Training Loss: 0.838, Validation Accuracy: 62.37%\n",
            "Epoch [5/10], Training Loss: 0.820, Validation Accuracy: 62.52%\n",
            "Epoch [6/10], Training Loss: 0.800, Validation Accuracy: 62.01%\n",
            "Epoch [7/10], Training Loss: 0.789, Validation Accuracy: 61.27%\n",
            "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 62.44%\n",
            "Epoch [9/10], Training Loss: 0.752, Validation Accuracy: 61.61%\n",
            "Epoch [10/10], Training Loss: 0.731, Validation Accuracy: 61.65%\n",
            "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 62.04%\n",
            "Epoch [2/10], Training Loss: 0.901, Validation Accuracy: 62.20%\n",
            "Epoch [3/10], Training Loss: 0.863, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.837, Validation Accuracy: 62.61%\n",
            "Epoch [5/10], Training Loss: 0.811, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.791, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.777, Validation Accuracy: 62.32%\n",
            "Epoch [8/10], Training Loss: 0.758, Validation Accuracy: 61.53%\n",
            "Epoch [9/10], Training Loss: 0.740, Validation Accuracy: 61.71%\n",
            "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 61.99%\n",
            "Epoch [1/10], Training Loss: 0.956, Validation Accuracy: 62.09%\n",
            "Epoch [2/10], Training Loss: 0.885, Validation Accuracy: 62.82%\n",
            "Epoch [3/10], Training Loss: 0.847, Validation Accuracy: 62.46%\n",
            "Epoch [4/10], Training Loss: 0.823, Validation Accuracy: 62.84%\n",
            "Epoch [5/10], Training Loss: 0.793, Validation Accuracy: 61.62%\n",
            "Epoch [6/10], Training Loss: 0.774, Validation Accuracy: 61.93%\n",
            "Epoch [7/10], Training Loss: 0.751, Validation Accuracy: 62.64%\n",
            "Epoch [8/10], Training Loss: 0.738, Validation Accuracy: 61.95%\n",
            "Epoch [9/10], Training Loss: 0.722, Validation Accuracy: 62.13%\n",
            "Epoch [10/10], Training Loss: 0.705, Validation Accuracy: 61.52%\n",
            "Epoch [1/10], Training Loss: 0.915, Validation Accuracy: 61.74%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 62.53%\n",
            "Epoch [3/10], Training Loss: 0.805, Validation Accuracy: 62.69%\n",
            "Epoch [4/10], Training Loss: 0.779, Validation Accuracy: 61.89%\n",
            "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 61.99%\n",
            "Epoch [6/10], Training Loss: 0.730, Validation Accuracy: 62.89%\n",
            "Epoch [7/10], Training Loss: 0.703, Validation Accuracy: 61.87%\n",
            "Epoch [8/10], Training Loss: 0.686, Validation Accuracy: 62.58%\n",
            "Epoch [9/10], Training Loss: 0.674, Validation Accuracy: 62.19%\n",
            "Epoch [10/10], Training Loss: 0.664, Validation Accuracy: 62.05%\n",
            "Epoch [1/10], Training Loss: 0.928, Validation Accuracy: 62.19%\n",
            "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 62.14%\n",
            "Epoch [3/10], Training Loss: 0.812, Validation Accuracy: 62.60%\n",
            "Epoch [4/10], Training Loss: 0.786, Validation Accuracy: 63.20%\n",
            "Epoch [5/10], Training Loss: 0.754, Validation Accuracy: 61.83%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 62.64%\n",
            "Epoch [7/10], Training Loss: 0.714, Validation Accuracy: 62.91%\n",
            "Epoch [8/10], Training Loss: 0.693, Validation Accuracy: 63.33%\n",
            "Epoch [9/10], Training Loss: 0.674, Validation Accuracy: 62.51%\n",
            "Epoch [10/10], Training Loss: 0.650, Validation Accuracy: 62.10%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 62.32%\n",
            "Epoch [3/10], Training Loss: 0.793, Validation Accuracy: 62.20%\n",
            "Epoch [4/10], Training Loss: 0.767, Validation Accuracy: 62.18%\n",
            "Epoch [5/10], Training Loss: 0.736, Validation Accuracy: 62.21%\n",
            "Epoch [6/10], Training Loss: 0.717, Validation Accuracy: 62.18%\n",
            "Epoch [7/10], Training Loss: 0.694, Validation Accuracy: 62.30%\n",
            "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 61.80%\n",
            "Epoch [9/10], Training Loss: 0.652, Validation Accuracy: 61.45%\n",
            "Epoch [10/10], Training Loss: 0.634, Validation Accuracy: 61.57%\n",
            "Epoch [1/10], Training Loss: 0.929, Validation Accuracy: 62.19%\n",
            "Epoch [2/10], Training Loss: 0.836, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.794, Validation Accuracy: 62.42%\n",
            "Epoch [4/10], Training Loss: 0.763, Validation Accuracy: 62.86%\n",
            "Epoch [5/10], Training Loss: 0.721, Validation Accuracy: 62.95%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 62.44%\n",
            "Epoch [7/10], Training Loss: 0.695, Validation Accuracy: 62.19%\n",
            "Epoch [8/10], Training Loss: 0.668, Validation Accuracy: 62.22%\n",
            "Epoch [9/10], Training Loss: 0.645, Validation Accuracy: 61.78%\n",
            "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 62.84%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 61.60%\n",
            "Epoch [2/10], Training Loss: 0.818, Validation Accuracy: 62.24%\n",
            "Epoch [3/10], Training Loss: 0.785, Validation Accuracy: 63.32%\n",
            "Epoch [4/10], Training Loss: 0.736, Validation Accuracy: 62.70%\n",
            "Epoch [5/10], Training Loss: 0.713, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.687, Validation Accuracy: 62.86%\n",
            "Epoch [7/10], Training Loss: 0.664, Validation Accuracy: 62.69%\n",
            "Epoch [8/10], Training Loss: 0.640, Validation Accuracy: 61.98%\n",
            "Epoch [9/10], Training Loss: 0.617, Validation Accuracy: 62.13%\n",
            "Epoch [10/10], Training Loss: 0.605, Validation Accuracy: 62.12%\n",
            "Epoch [1/10], Training Loss: 0.861, Validation Accuracy: 62.02%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 62.81%\n",
            "Epoch [3/10], Training Loss: 0.729, Validation Accuracy: 62.01%\n",
            "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 62.33%\n",
            "Epoch [5/10], Training Loss: 0.669, Validation Accuracy: 62.71%\n",
            "Epoch [6/10], Training Loss: 0.639, Validation Accuracy: 62.75%\n",
            "Epoch [7/10], Training Loss: 0.617, Validation Accuracy: 62.32%\n",
            "Epoch [8/10], Training Loss: 0.595, Validation Accuracy: 62.41%\n",
            "Epoch [9/10], Training Loss: 0.572, Validation Accuracy: 62.53%\n",
            "Epoch [10/10], Training Loss: 0.557, Validation Accuracy: 62.21%\n",
            "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 62.22%\n",
            "Epoch [2/10], Training Loss: 0.789, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Training Loss: 0.735, Validation Accuracy: 61.93%\n",
            "Epoch [4/10], Training Loss: 0.701, Validation Accuracy: 62.12%\n",
            "Epoch [5/10], Training Loss: 0.679, Validation Accuracy: 63.07%\n",
            "Epoch [6/10], Training Loss: 0.637, Validation Accuracy: 62.44%\n",
            "Epoch [7/10], Training Loss: 0.630, Validation Accuracy: 63.07%\n",
            "Epoch [8/10], Training Loss: 0.594, Validation Accuracy: 62.42%\n",
            "Epoch [9/10], Training Loss: 0.580, Validation Accuracy: 62.57%\n",
            "Epoch [10/10], Training Loss: 0.563, Validation Accuracy: 61.58%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 62.41%\n",
            "Epoch [2/10], Training Loss: 0.766, Validation Accuracy: 62.82%\n",
            "Epoch [3/10], Training Loss: 0.717, Validation Accuracy: 62.56%\n",
            "Epoch [4/10], Training Loss: 0.671, Validation Accuracy: 62.30%\n",
            "Epoch [5/10], Training Loss: 0.650, Validation Accuracy: 61.87%\n",
            "Epoch [6/10], Training Loss: 0.624, Validation Accuracy: 62.53%\n",
            "Epoch [7/10], Training Loss: 0.600, Validation Accuracy: 61.69%\n",
            "Epoch [8/10], Training Loss: 0.581, Validation Accuracy: 61.90%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.53%\n",
            "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 61.92%\n",
            "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 61.70%\n",
            "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 62.65%\n",
            "Epoch [3/10], Training Loss: 0.715, Validation Accuracy: 62.28%\n",
            "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 62.57%\n",
            "Epoch [5/10], Training Loss: 0.647, Validation Accuracy: 62.85%\n",
            "Epoch [6/10], Training Loss: 0.619, Validation Accuracy: 61.80%\n",
            "Epoch [7/10], Training Loss: 0.596, Validation Accuracy: 62.33%\n",
            "Epoch [8/10], Training Loss: 0.574, Validation Accuracy: 62.34%\n",
            "Epoch [9/10], Training Loss: 0.553, Validation Accuracy: 62.32%\n",
            "Epoch [10/10], Training Loss: 0.532, Validation Accuracy: 61.68%\n",
            "Epoch [1/10], Training Loss: 0.870, Validation Accuracy: 61.97%\n",
            "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 62.62%\n",
            "Epoch [3/10], Training Loss: 0.700, Validation Accuracy: 63.46%\n",
            "Epoch [4/10], Training Loss: 0.648, Validation Accuracy: 62.02%\n",
            "Epoch [5/10], Training Loss: 0.633, Validation Accuracy: 61.69%\n",
            "Epoch [6/10], Training Loss: 0.598, Validation Accuracy: 62.44%\n",
            "Epoch [7/10], Training Loss: 0.572, Validation Accuracy: 62.45%\n",
            "Epoch [8/10], Training Loss: 0.546, Validation Accuracy: 61.93%\n",
            "Epoch [9/10], Training Loss: 0.532, Validation Accuracy: 61.30%\n",
            "Epoch [10/10], Training Loss: 0.515, Validation Accuracy: 62.02%\n",
            "Epoch [1/10], Training Loss: 0.819, Validation Accuracy: 61.72%\n",
            "Epoch [2/10], Training Loss: 0.724, Validation Accuracy: 62.80%\n",
            "Epoch [3/10], Training Loss: 0.663, Validation Accuracy: 62.63%\n",
            "Epoch [4/10], Training Loss: 0.617, Validation Accuracy: 63.03%\n",
            "Epoch [5/10], Training Loss: 0.581, Validation Accuracy: 62.54%\n",
            "Epoch [6/10], Training Loss: 0.564, Validation Accuracy: 62.97%\n",
            "Epoch [7/10], Training Loss: 0.536, Validation Accuracy: 62.51%\n",
            "Epoch [8/10], Training Loss: 0.503, Validation Accuracy: 62.05%\n",
            "Epoch [9/10], Training Loss: 0.495, Validation Accuracy: 62.21%\n",
            "Epoch [10/10], Training Loss: 0.468, Validation Accuracy: 61.65%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 60.78%\n",
            "Epoch [2/10], Training Loss: 0.736, Validation Accuracy: 62.44%\n",
            "Epoch [3/10], Training Loss: 0.669, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.642, Validation Accuracy: 62.30%\n",
            "Epoch [5/10], Training Loss: 0.600, Validation Accuracy: 62.08%\n",
            "Epoch [6/10], Training Loss: 0.568, Validation Accuracy: 61.80%\n",
            "Epoch [7/10], Training Loss: 0.534, Validation Accuracy: 62.03%\n",
            "Epoch [8/10], Training Loss: 0.518, Validation Accuracy: 61.84%\n",
            "Epoch [9/10], Training Loss: 0.487, Validation Accuracy: 61.15%\n",
            "Epoch [10/10], Training Loss: 0.474, Validation Accuracy: 61.66%\n",
            "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 61.80%\n",
            "Epoch [2/10], Training Loss: 0.714, Validation Accuracy: 61.88%\n",
            "Epoch [3/10], Training Loss: 0.649, Validation Accuracy: 62.59%\n",
            "Epoch [4/10], Training Loss: 0.606, Validation Accuracy: 61.87%\n",
            "Epoch [5/10], Training Loss: 0.569, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.545, Validation Accuracy: 61.46%\n",
            "Epoch [7/10], Training Loss: 0.522, Validation Accuracy: 61.71%\n",
            "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 62.18%\n",
            "Epoch [9/10], Training Loss: 0.474, Validation Accuracy: 61.62%\n",
            "Epoch [10/10], Training Loss: 0.454, Validation Accuracy: 61.61%\n",
            "Epoch [1/10], Training Loss: 0.840, Validation Accuracy: 62.01%\n",
            "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 62.27%\n",
            "Epoch [3/10], Training Loss: 0.644, Validation Accuracy: 61.39%\n",
            "Epoch [4/10], Training Loss: 0.611, Validation Accuracy: 62.04%\n",
            "Epoch [5/10], Training Loss: 0.560, Validation Accuracy: 61.63%\n",
            "Epoch [6/10], Training Loss: 0.547, Validation Accuracy: 62.63%\n",
            "Epoch [7/10], Training Loss: 0.513, Validation Accuracy: 62.16%\n",
            "Epoch [8/10], Training Loss: 0.479, Validation Accuracy: 62.27%\n",
            "Epoch [9/10], Training Loss: 0.463, Validation Accuracy: 62.02%\n",
            "Epoch [10/10], Training Loss: 0.438, Validation Accuracy: 62.21%\n",
            "Epoch [1/10], Training Loss: 0.817, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 62.28%\n",
            "Epoch [3/10], Training Loss: 0.637, Validation Accuracy: 61.75%\n",
            "Epoch [4/10], Training Loss: 0.577, Validation Accuracy: 62.42%\n",
            "Epoch [5/10], Training Loss: 0.546, Validation Accuracy: 62.08%\n",
            "Epoch [6/10], Training Loss: 0.518, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.487, Validation Accuracy: 61.50%\n",
            "Epoch [8/10], Training Loss: 0.464, Validation Accuracy: 61.47%\n",
            "Epoch [9/10], Training Loss: 0.442, Validation Accuracy: 62.15%\n",
            "Epoch [10/10], Training Loss: 0.421, Validation Accuracy: 61.33%\n",
            "Test Accuracy: 60.45%\n"
          ]
        }
      ],
      "source": [
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from flwr.datasets import emnist\n",
        "# Load and preprocess the CFEMNIST dataset\n",
        "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load EMNIST in a federated way\n",
        "def load_federated_emnist(num_clients: int):\n",
        "    # Download Federated EMNIST dataset\n",
        "    train, test = emnist.load_data()\n",
        "    train_clients, train_data = train\n",
        "    test_clients, test_data = test\n",
        "\n",
        "    # Map the data to PyTorch DataLoader for each client\n",
        "    train_loaders = {\n",
        "        client_id: DataLoader(\n",
        "            [(transform(image), label) for image, label in data],\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        for client_id, data in train_data.items()\n",
        "    }\n",
        "\n",
        "    test_loaders = {\n",
        "        client_id: DataLoader(\n",
        "            [(transform(image), label) for image, label in data],\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "        )\n",
        "        for client_id, data in test_data.items()\n",
        "    }\n",
        "\n",
        "    return train_loaders, test_loaders\n",
        "\n",
        "# Specify the number of clients\n",
        "num_clients = 10\n",
        "train_loaders, test_loaders = load_federated_emnist(num_clients)\n",
        "\n",
        "def federated_train(global_model, train_loaders, num_clients, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        client_models = {}\n",
        "        for client_id, train_loader in train_loaders.items():\n",
        "            # Clone the global model for this client\n",
        "            client_model = deepcopy(global_model)\n",
        "            optimizer = torch.optim.Adam(client_model.parameters(), lr=1e-3)\n",
        "\n",
        "            # Train on client's local data\n",
        "            client_model.train()\n",
        "            for images, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = client_model(images)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Save the updated client model\n",
        "            client_models[client_id] = client_model.state_dict()\n",
        "\n",
        "        # Aggregate client models to update the global model\n",
        "        global_model.load_state_dict(aggregate(client_models))\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
        "\n",
        "def aggregate(client_models):\n",
        "    \"\"\"Aggregate model weights from all clients (simple average).\"\"\"\n",
        "    global_model_state = deepcopy(list(client_models.values())[0])\n",
        "    for key in global_model_state.keys():\n",
        "        global_model_state[key] = torch.stack([model[key] for model in client_models.values()], dim=0).mean(dim=0)\n",
        "    return global_model_state\n",
        "def evaluate_federated(global_model, test_loaders):\n",
        "    global_model.eval()\n",
        "    overall_accuracy = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for client_id, test_loader in test_loaders.items():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                outputs = global_model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        overall_accuracy += accuracy * total\n",
        "        total_samples += total\n",
        "        print(f\"Client {client_id} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Weighted average accuracy across clients\n",
        "    overall_accuracy /= total_samples\n",
        "    print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
        "    return overall_accuracy\n",
        "if __name__ == \"__main__\":\n",
        "    # Load federated EMNIST dataset\n",
        "    num_clients = 10\n",
        "    train_loaders, test_loaders = load_federated_emnist(num_clients)\n",
        "\n",
        "    # Initialize global model (e.g., VAE or classifier)\n",
        "    global_model = Net()  # Replace with VAE if needed\n",
        "\n",
        "    # Train the model using federated learning\n",
        "    federated_train(global_model, train_loaders, num_clients, epochs=10)\n",
        "\n",
        "    # Evaluate the global model\n",
        "    evaluate_federated(global_model, test_loaders)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Snmbj8OzZpde",
        "outputId": "a0e36910-792e-4ab3-ba26-4e9c5ed6fc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'flwr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-00e61281a6f1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0memnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load and preprocess the CFEMNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flwr'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import EMNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "\n",
        "# Load the EMNIST dataset\n",
        "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
        "full_dataset = EMNIST(root=\"./data\", split=\"balanced\", train=True, download=True, transform=transform)\n",
        "test_dataset = EMNIST(root=\"./data\", split=\"balanced\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into federated subsets\n",
        "def create_federated_datasets(dataset, num_clients):\n",
        "    data_per_client = len(dataset) // num_clients\n",
        "    subsets = random_split(dataset, [data_per_client] * num_clients)\n",
        "    return {\n",
        "        f\"client_{i}\": DataLoader(subsets[i], batch_size=32, shuffle=True)\n",
        "        for i in range(num_clients)\n",
        "    }\n",
        "\n",
        "# Create federated training and testing datasets\n",
        "num_clients = 10\n",
        "train_loaders = create_federated_datasets(full_dataset, num_clients)\n",
        "test_loaders = create_federated_datasets(test_dataset, num_clients)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cODYSQ3Obted",
        "outputId": "f4106f47-b4a2-455c-ac60-c1f643d37df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:15<00:00, 35.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Confusion matrix from your data\n",
        "confusion_matrix = np.array([\n",
        "    [597, 32, 77, 22, 39, 22, 17, 31, 100, 63],\n",
        "    [14, 760, 19, 6, 6, 13, 18, 10, 33, 121],\n",
        "    [50, 18, 482, 58, 104, 115, 85, 61, 15, 12],\n",
        "    [15, 16, 77, 329, 61, 282, 108, 68, 21, 23],\n",
        "    [18, 9, 76, 47, 528, 85, 95, 119, 14, 9],\n",
        "    [7, 6, 52, 136, 43, 620, 33, 87, 9, 7],\n",
        "    [2, 17, 40, 46, 51, 49, 740, 28, 13, 14],\n",
        "    [10, 4, 31, 37, 52, 100, 13, 733, 1, 19],\n",
        "    [68, 46, 20, 13, 10, 15, 13, 19, 736, 60],\n",
        "    [23, 117, 8, 15, 15, 20, 19, 44, 53, 686]\n",
        "])\n",
        "\n",
        "# Function to calculate errors for each class\n",
        "def calculate_errors(conf_matrix):\n",
        "    class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "    errors_per_class = {}\n",
        "    num_classes = conf_matrix.shape[0]\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        TP = conf_matrix[i, i]  # True Positives\n",
        "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives (wrongly predicted as this class)\n",
        "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives (missed this class)\n",
        "        TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives (correctly classified all other classes)\n",
        "\n",
        "        errors_per_class[class_names[i]] = {\n",
        "            'TP': TP,\n",
        "            'FP': FP,\n",
        "            'FN': FN,\n",
        "            'TN': TN,\n",
        "            'Total Errors': FP + FN  # Total errors = False Positives + False Negatives\n",
        "        }\n",
        "\n",
        "    return errors_per_class\n",
        "\n",
        "# Calculate errors for each class\n",
        "errors = calculate_errors(confusion_matrix)\n",
        "\n",
        "# Display the errors for each class\n",
        "for class_name, error_info in errors.items():\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  True Positives (TP): {error_info['TP']}\")\n",
        "    print(f\"  False Positives (FP): {error_info['FP']}\")\n",
        "    print(f\"  False Negatives (FN): {error_info['FN']}\")\n",
        "    print(f\"  True Negatives (TN): {error_info['TN']}\")\n",
        "    print(f\"  Total Errors (FP + FN): {error_info['Total Errors']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8siW1wNFdb_",
        "outputId": "5eb38581-fba5-495e-bd4a-4ec77c22af84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Airplane:\n",
            "  True Positives (TP): 597\n",
            "  False Positives (FP): 207\n",
            "  False Negatives (FN): 403\n",
            "  True Negatives (TN): 8793\n",
            "  Total Errors (FP + FN): 610\n",
            "\n",
            "Automobile:\n",
            "  True Positives (TP): 760\n",
            "  False Positives (FP): 265\n",
            "  False Negatives (FN): 240\n",
            "  True Negatives (TN): 8735\n",
            "  Total Errors (FP + FN): 505\n",
            "\n",
            "Bird:\n",
            "  True Positives (TP): 482\n",
            "  False Positives (FP): 400\n",
            "  False Negatives (FN): 518\n",
            "  True Negatives (TN): 8600\n",
            "  Total Errors (FP + FN): 918\n",
            "\n",
            "Cat:\n",
            "  True Positives (TP): 329\n",
            "  False Positives (FP): 380\n",
            "  False Negatives (FN): 671\n",
            "  True Negatives (TN): 8620\n",
            "  Total Errors (FP + FN): 1051\n",
            "\n",
            "Deer:\n",
            "  True Positives (TP): 528\n",
            "  False Positives (FP): 381\n",
            "  False Negatives (FN): 472\n",
            "  True Negatives (TN): 8619\n",
            "  Total Errors (FP + FN): 853\n",
            "\n",
            "Dog:\n",
            "  True Positives (TP): 620\n",
            "  False Positives (FP): 701\n",
            "  False Negatives (FN): 380\n",
            "  True Negatives (TN): 8299\n",
            "  Total Errors (FP + FN): 1081\n",
            "\n",
            "Frog:\n",
            "  True Positives (TP): 740\n",
            "  False Positives (FP): 401\n",
            "  False Negatives (FN): 260\n",
            "  True Negatives (TN): 8599\n",
            "  Total Errors (FP + FN): 661\n",
            "\n",
            "Horse:\n",
            "  True Positives (TP): 733\n",
            "  False Positives (FP): 467\n",
            "  False Negatives (FN): 267\n",
            "  True Negatives (TN): 8533\n",
            "  Total Errors (FP + FN): 734\n",
            "\n",
            "Ship:\n",
            "  True Positives (TP): 736\n",
            "  False Positives (FP): 259\n",
            "  False Negatives (FN): 264\n",
            "  True Negatives (TN): 8741\n",
            "  Total Errors (FP + FN): 523\n",
            "\n",
            "Truck:\n",
            "  True Positives (TP): 686\n",
            "  False Positives (FP): 328\n",
            "  False Negatives (FN): 314\n",
            "  True Negatives (TN): 8672\n",
            "  Total Errors (FP + FN): 642\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Your confusion matrix\n",
        "confusion_matrix = np.array([[\n",
        "    [597, 32, 77, 22, 39, 22, 17, 31, 100, 63],\n",
        "    [14, 760, 19, 6, 6, 13, 18, 10, 33, 121],\n",
        "    [50, 18, 482, 58, 104, 115, 85, 61, 15, 12],\n",
        "    [15, 16, 77, 329, 61, 282, 108, 68, 21, 23],\n",
        "    [18, 9, 76, 47, 528, 85, 95, 119, 14, 9],\n",
        "    [7, 6, 52, 136, 43, 620, 33, 87, 9, 7],\n",
        "    [2, 17, 40, 46, 51, 49, 740, 28, 13, 14],\n",
        "    [10, 4, 31, 37, 52, 100, 13, 733, 1, 19],\n",
        "    [68, 46, 20, 13, 10, 15, 13, 19, 736, 60],\n",
        "    [23, 117, 8, 15, 15, 20, 19, 44, 53, 686]\n",
        "]])\n",
        "\n",
        "# Calculate errors for each class\n",
        "errors_per_class = {}\n",
        "for i in range(confusion_matrix.shape[0]):\n",
        "    # False Positives (sum of column i excluding diagonal element)\n",
        "    FP = np.sum(confusion_matrix[:, i]) - confusion_matrix[i, i]\n",
        "    # False Negatives (sum of row i excluding diagonal element)\n",
        "    FN = np.sum(confusion_matrix[i, :]) - confusion_matrix[i, i]\n",
        "    # Total errors for class i\n",
        "    total_errors = FP + FN\n",
        "    errors_per_class[i] = total_errors\n",
        "\n",
        "# Print errors for each class\n",
        "for class_label, error_count in errors_per_class.items():\n",
        "    print(f\"Class {class_label}: {error_count} errors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMxTyb6BF32_",
        "outputId": "c7239d15-977d-4785-a144-e4e7387245c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: [ 9806 10936 10846 10956 10922 10956 10966 10938 10800 10874] errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP66dBwLIw3_",
        "outputId": "e7cdb86f-db21-4dfe-ea2f-cd74501411a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6008 6065 6074 5856 6147 5986 6008 5962 5904 5990]\n",
            "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 10.17%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 10.24%\n",
            "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 10.16%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 10.14%\n",
            "Epoch [5/10], Training Loss: 2.295, Validation Accuracy: 10.12%\n",
            "Epoch [6/10], Training Loss: 2.291, Validation Accuracy: 10.24%\n",
            "Epoch [7/10], Training Loss: 2.284, Validation Accuracy: 11.09%\n",
            "Epoch [8/10], Training Loss: 2.274, Validation Accuracy: 12.03%\n",
            "Epoch [9/10], Training Loss: 2.259, Validation Accuracy: 12.78%\n",
            "Epoch [10/10], Training Loss: 2.243, Validation Accuracy: 14.08%\n",
            "Epoch [1/10], Training Loss: 2.232, Validation Accuracy: 15.69%\n",
            "Epoch [2/10], Training Loss: 2.217, Validation Accuracy: 17.64%\n",
            "Epoch [3/10], Training Loss: 2.201, Validation Accuracy: 19.24%\n",
            "Epoch [4/10], Training Loss: 2.184, Validation Accuracy: 19.88%\n",
            "Epoch [5/10], Training Loss: 2.168, Validation Accuracy: 20.73%\n",
            "Epoch [6/10], Training Loss: 2.152, Validation Accuracy: 22.25%\n",
            "Epoch [7/10], Training Loss: 2.137, Validation Accuracy: 21.60%\n",
            "Epoch [8/10], Training Loss: 2.121, Validation Accuracy: 22.45%\n",
            "Epoch [9/10], Training Loss: 2.105, Validation Accuracy: 23.39%\n",
            "Epoch [10/10], Training Loss: 2.089, Validation Accuracy: 24.34%\n",
            "Epoch [1/10], Training Loss: 2.082, Validation Accuracy: 27.71%\n",
            "Epoch [2/10], Training Loss: 2.066, Validation Accuracy: 28.13%\n",
            "Epoch [3/10], Training Loss: 2.048, Validation Accuracy: 27.49%\n",
            "Epoch [4/10], Training Loss: 2.028, Validation Accuracy: 28.12%\n",
            "Epoch [5/10], Training Loss: 2.006, Validation Accuracy: 28.80%\n",
            "Epoch [6/10], Training Loss: 1.984, Validation Accuracy: 29.36%\n",
            "Epoch [7/10], Training Loss: 1.962, Validation Accuracy: 29.68%\n",
            "Epoch [8/10], Training Loss: 1.939, Validation Accuracy: 30.34%\n",
            "Epoch [9/10], Training Loss: 1.920, Validation Accuracy: 30.88%\n",
            "Epoch [10/10], Training Loss: 1.902, Validation Accuracy: 31.75%\n",
            "Epoch [1/10], Training Loss: 1.890, Validation Accuracy: 32.49%\n",
            "Epoch [2/10], Training Loss: 1.877, Validation Accuracy: 32.30%\n",
            "Epoch [3/10], Training Loss: 1.863, Validation Accuracy: 32.76%\n",
            "Epoch [4/10], Training Loss: 1.845, Validation Accuracy: 33.83%\n",
            "Epoch [5/10], Training Loss: 1.830, Validation Accuracy: 34.41%\n",
            "Epoch [6/10], Training Loss: 1.815, Validation Accuracy: 34.35%\n",
            "Epoch [7/10], Training Loss: 1.794, Validation Accuracy: 35.59%\n",
            "Epoch [8/10], Training Loss: 1.779, Validation Accuracy: 36.21%\n",
            "Epoch [9/10], Training Loss: 1.759, Validation Accuracy: 36.27%\n",
            "Epoch [10/10], Training Loss: 1.745, Validation Accuracy: 37.12%\n",
            "Epoch [1/10], Training Loss: 1.734, Validation Accuracy: 37.81%\n",
            "Epoch [2/10], Training Loss: 1.715, Validation Accuracy: 38.31%\n",
            "Epoch [3/10], Training Loss: 1.697, Validation Accuracy: 37.80%\n",
            "Epoch [4/10], Training Loss: 1.679, Validation Accuracy: 38.92%\n",
            "Epoch [5/10], Training Loss: 1.661, Validation Accuracy: 38.53%\n",
            "Epoch [6/10], Training Loss: 1.648, Validation Accuracy: 38.86%\n",
            "Epoch [7/10], Training Loss: 1.634, Validation Accuracy: 39.55%\n",
            "Epoch [8/10], Training Loss: 1.623, Validation Accuracy: 39.75%\n",
            "Epoch [9/10], Training Loss: 1.607, Validation Accuracy: 40.67%\n",
            "Epoch [10/10], Training Loss: 1.594, Validation Accuracy: 40.43%\n",
            "Epoch [1/10], Training Loss: 1.639, Validation Accuracy: 40.93%\n",
            "Epoch [2/10], Training Loss: 1.618, Validation Accuracy: 41.43%\n",
            "Epoch [3/10], Training Loss: 1.608, Validation Accuracy: 41.32%\n",
            "Epoch [4/10], Training Loss: 1.596, Validation Accuracy: 41.16%\n",
            "Epoch [5/10], Training Loss: 1.586, Validation Accuracy: 41.67%\n",
            "Epoch [6/10], Training Loss: 1.580, Validation Accuracy: 42.35%\n",
            "Epoch [7/10], Training Loss: 1.570, Validation Accuracy: 42.56%\n",
            "Epoch [8/10], Training Loss: 1.561, Validation Accuracy: 42.47%\n",
            "Epoch [9/10], Training Loss: 1.554, Validation Accuracy: 42.86%\n",
            "Epoch [10/10], Training Loss: 1.543, Validation Accuracy: 42.42%\n",
            "Epoch [1/10], Training Loss: 1.550, Validation Accuracy: 43.98%\n",
            "Epoch [2/10], Training Loss: 1.539, Validation Accuracy: 43.37%\n",
            "Epoch [3/10], Training Loss: 1.531, Validation Accuracy: 43.02%\n",
            "Epoch [4/10], Training Loss: 1.519, Validation Accuracy: 44.81%\n",
            "Epoch [5/10], Training Loss: 1.506, Validation Accuracy: 44.49%\n",
            "Epoch [6/10], Training Loss: 1.501, Validation Accuracy: 44.85%\n",
            "Epoch [7/10], Training Loss: 1.493, Validation Accuracy: 44.86%\n",
            "Epoch [8/10], Training Loss: 1.486, Validation Accuracy: 44.82%\n",
            "Epoch [9/10], Training Loss: 1.473, Validation Accuracy: 45.09%\n",
            "Epoch [10/10], Training Loss: 1.469, Validation Accuracy: 45.14%\n",
            "Epoch [1/10], Training Loss: 1.506, Validation Accuracy: 45.68%\n",
            "Epoch [2/10], Training Loss: 1.497, Validation Accuracy: 46.31%\n",
            "Epoch [3/10], Training Loss: 1.485, Validation Accuracy: 44.64%\n",
            "Epoch [4/10], Training Loss: 1.481, Validation Accuracy: 45.50%\n",
            "Epoch [5/10], Training Loss: 1.468, Validation Accuracy: 46.86%\n",
            "Epoch [6/10], Training Loss: 1.461, Validation Accuracy: 46.66%\n",
            "Epoch [7/10], Training Loss: 1.457, Validation Accuracy: 46.80%\n",
            "Epoch [8/10], Training Loss: 1.442, Validation Accuracy: 46.50%\n",
            "Epoch [9/10], Training Loss: 1.445, Validation Accuracy: 47.59%\n",
            "Epoch [10/10], Training Loss: 1.429, Validation Accuracy: 47.65%\n",
            "Epoch [1/10], Training Loss: 1.452, Validation Accuracy: 46.99%\n",
            "Epoch [2/10], Training Loss: 1.434, Validation Accuracy: 47.80%\n",
            "Epoch [3/10], Training Loss: 1.425, Validation Accuracy: 47.92%\n",
            "Epoch [4/10], Training Loss: 1.417, Validation Accuracy: 48.09%\n",
            "Epoch [5/10], Training Loss: 1.408, Validation Accuracy: 47.34%\n",
            "Epoch [6/10], Training Loss: 1.395, Validation Accuracy: 47.69%\n",
            "Epoch [7/10], Training Loss: 1.398, Validation Accuracy: 47.07%\n",
            "Epoch [8/10], Training Loss: 1.382, Validation Accuracy: 48.60%\n",
            "Epoch [9/10], Training Loss: 1.385, Validation Accuracy: 46.95%\n",
            "Epoch [10/10], Training Loss: 1.379, Validation Accuracy: 48.71%\n",
            "Epoch [1/10], Training Loss: 1.395, Validation Accuracy: 47.52%\n",
            "Epoch [2/10], Training Loss: 1.381, Validation Accuracy: 48.75%\n",
            "Epoch [3/10], Training Loss: 1.370, Validation Accuracy: 48.72%\n",
            "Epoch [4/10], Training Loss: 1.358, Validation Accuracy: 50.37%\n",
            "Epoch [5/10], Training Loss: 1.344, Validation Accuracy: 49.66%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 50.00%\n",
            "Epoch [7/10], Training Loss: 1.328, Validation Accuracy: 49.62%\n",
            "Epoch [8/10], Training Loss: 1.330, Validation Accuracy: 49.95%\n",
            "Epoch [9/10], Training Loss: 1.311, Validation Accuracy: 49.79%\n",
            "Epoch [10/10], Training Loss: 1.313, Validation Accuracy: 49.17%\n",
            "Epoch [1/10], Training Loss: 1.401, Validation Accuracy: 50.98%\n",
            "Epoch [2/10], Training Loss: 1.384, Validation Accuracy: 51.37%\n",
            "Epoch [3/10], Training Loss: 1.361, Validation Accuracy: 51.11%\n",
            "Epoch [4/10], Training Loss: 1.352, Validation Accuracy: 51.07%\n",
            "Epoch [5/10], Training Loss: 1.350, Validation Accuracy: 50.90%\n",
            "Epoch [6/10], Training Loss: 1.338, Validation Accuracy: 51.97%\n",
            "Epoch [7/10], Training Loss: 1.331, Validation Accuracy: 50.99%\n",
            "Epoch [8/10], Training Loss: 1.322, Validation Accuracy: 51.29%\n",
            "Epoch [9/10], Training Loss: 1.313, Validation Accuracy: 51.66%\n",
            "Epoch [10/10], Training Loss: 1.319, Validation Accuracy: 49.10%\n",
            "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 51.42%\n",
            "Epoch [2/10], Training Loss: 1.325, Validation Accuracy: 52.03%\n",
            "Epoch [3/10], Training Loss: 1.305, Validation Accuracy: 52.39%\n",
            "Epoch [4/10], Training Loss: 1.295, Validation Accuracy: 52.85%\n",
            "Epoch [5/10], Training Loss: 1.286, Validation Accuracy: 52.78%\n",
            "Epoch [6/10], Training Loss: 1.284, Validation Accuracy: 52.30%\n",
            "Epoch [7/10], Training Loss: 1.277, Validation Accuracy: 51.87%\n",
            "Epoch [8/10], Training Loss: 1.267, Validation Accuracy: 50.89%\n",
            "Epoch [9/10], Training Loss: 1.251, Validation Accuracy: 52.01%\n",
            "Epoch [10/10], Training Loss: 1.249, Validation Accuracy: 52.60%\n",
            "Epoch [1/10], Training Loss: 1.326, Validation Accuracy: 52.29%\n",
            "Epoch [2/10], Training Loss: 1.317, Validation Accuracy: 52.48%\n",
            "Epoch [3/10], Training Loss: 1.302, Validation Accuracy: 52.77%\n",
            "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 52.62%\n",
            "Epoch [5/10], Training Loss: 1.284, Validation Accuracy: 52.61%\n",
            "Epoch [6/10], Training Loss: 1.271, Validation Accuracy: 52.93%\n",
            "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 53.08%\n",
            "Epoch [8/10], Training Loss: 1.257, Validation Accuracy: 53.41%\n",
            "Epoch [9/10], Training Loss: 1.248, Validation Accuracy: 52.70%\n",
            "Epoch [10/10], Training Loss: 1.238, Validation Accuracy: 53.77%\n",
            "Epoch [1/10], Training Loss: 1.300, Validation Accuracy: 52.18%\n",
            "Epoch [2/10], Training Loss: 1.274, Validation Accuracy: 53.35%\n",
            "Epoch [3/10], Training Loss: 1.256, Validation Accuracy: 53.93%\n",
            "Epoch [4/10], Training Loss: 1.247, Validation Accuracy: 52.25%\n",
            "Epoch [5/10], Training Loss: 1.235, Validation Accuracy: 53.51%\n",
            "Epoch [6/10], Training Loss: 1.226, Validation Accuracy: 53.24%\n",
            "Epoch [7/10], Training Loss: 1.227, Validation Accuracy: 54.00%\n",
            "Epoch [8/10], Training Loss: 1.207, Validation Accuracy: 54.61%\n",
            "Epoch [9/10], Training Loss: 1.200, Validation Accuracy: 53.91%\n",
            "Epoch [10/10], Training Loss: 1.193, Validation Accuracy: 54.12%\n",
            "Epoch [1/10], Training Loss: 1.245, Validation Accuracy: 54.70%\n",
            "Epoch [2/10], Training Loss: 1.223, Validation Accuracy: 54.88%\n",
            "Epoch [3/10], Training Loss: 1.212, Validation Accuracy: 54.29%\n",
            "Epoch [4/10], Training Loss: 1.194, Validation Accuracy: 55.00%\n",
            "Epoch [5/10], Training Loss: 1.189, Validation Accuracy: 54.32%\n",
            "Epoch [6/10], Training Loss: 1.173, Validation Accuracy: 53.69%\n",
            "Epoch [7/10], Training Loss: 1.165, Validation Accuracy: 53.73%\n",
            "Epoch [8/10], Training Loss: 1.155, Validation Accuracy: 54.49%\n",
            "Epoch [9/10], Training Loss: 1.145, Validation Accuracy: 55.24%\n",
            "Epoch [10/10], Training Loss: 1.142, Validation Accuracy: 54.82%\n",
            "Epoch [1/10], Training Loss: 1.268, Validation Accuracy: 55.58%\n",
            "Epoch [2/10], Training Loss: 1.238, Validation Accuracy: 55.25%\n",
            "Epoch [3/10], Training Loss: 1.217, Validation Accuracy: 55.81%\n",
            "Epoch [4/10], Training Loss: 1.204, Validation Accuracy: 55.75%\n",
            "Epoch [5/10], Training Loss: 1.190, Validation Accuracy: 55.15%\n",
            "Epoch [6/10], Training Loss: 1.183, Validation Accuracy: 56.25%\n",
            "Epoch [7/10], Training Loss: 1.171, Validation Accuracy: 55.15%\n",
            "Epoch [8/10], Training Loss: 1.162, Validation Accuracy: 55.55%\n",
            "Epoch [9/10], Training Loss: 1.154, Validation Accuracy: 55.53%\n",
            "Epoch [10/10], Training Loss: 1.146, Validation Accuracy: 56.14%\n",
            "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 55.82%\n",
            "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 56.40%\n",
            "Epoch [3/10], Training Loss: 1.173, Validation Accuracy: 56.66%\n",
            "Epoch [4/10], Training Loss: 1.151, Validation Accuracy: 55.91%\n",
            "Epoch [5/10], Training Loss: 1.143, Validation Accuracy: 56.48%\n",
            "Epoch [6/10], Training Loss: 1.139, Validation Accuracy: 56.37%\n",
            "Epoch [7/10], Training Loss: 1.126, Validation Accuracy: 56.08%\n",
            "Epoch [8/10], Training Loss: 1.111, Validation Accuracy: 56.79%\n",
            "Epoch [9/10], Training Loss: 1.107, Validation Accuracy: 56.20%\n",
            "Epoch [10/10], Training Loss: 1.091, Validation Accuracy: 56.60%\n",
            "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 56.96%\n",
            "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 56.07%\n",
            "Epoch [3/10], Training Loss: 1.170, Validation Accuracy: 57.41%\n",
            "Epoch [4/10], Training Loss: 1.152, Validation Accuracy: 56.71%\n",
            "Epoch [5/10], Training Loss: 1.142, Validation Accuracy: 56.14%\n",
            "Epoch [6/10], Training Loss: 1.135, Validation Accuracy: 57.20%\n",
            "Epoch [7/10], Training Loss: 1.119, Validation Accuracy: 57.25%\n",
            "Epoch [8/10], Training Loss: 1.108, Validation Accuracy: 57.28%\n",
            "Epoch [9/10], Training Loss: 1.101, Validation Accuracy: 57.02%\n",
            "Epoch [10/10], Training Loss: 1.086, Validation Accuracy: 57.43%\n",
            "Epoch [1/10], Training Loss: 1.189, Validation Accuracy: 57.45%\n",
            "Epoch [2/10], Training Loss: 1.158, Validation Accuracy: 58.15%\n",
            "Epoch [3/10], Training Loss: 1.143, Validation Accuracy: 58.04%\n",
            "Epoch [4/10], Training Loss: 1.120, Validation Accuracy: 57.35%\n",
            "Epoch [5/10], Training Loss: 1.109, Validation Accuracy: 57.40%\n",
            "Epoch [6/10], Training Loss: 1.096, Validation Accuracy: 57.20%\n",
            "Epoch [7/10], Training Loss: 1.082, Validation Accuracy: 56.95%\n",
            "Epoch [8/10], Training Loss: 1.069, Validation Accuracy: 56.99%\n",
            "Epoch [9/10], Training Loss: 1.069, Validation Accuracy: 57.48%\n",
            "Epoch [10/10], Training Loss: 1.064, Validation Accuracy: 57.74%\n",
            "Epoch [1/10], Training Loss: 1.149, Validation Accuracy: 57.60%\n",
            "Epoch [2/10], Training Loss: 1.109, Validation Accuracy: 57.95%\n",
            "Epoch [3/10], Training Loss: 1.086, Validation Accuracy: 58.11%\n",
            "Epoch [4/10], Training Loss: 1.082, Validation Accuracy: 58.06%\n",
            "Epoch [5/10], Training Loss: 1.063, Validation Accuracy: 57.10%\n",
            "Epoch [6/10], Training Loss: 1.052, Validation Accuracy: 57.33%\n",
            "Epoch [7/10], Training Loss: 1.036, Validation Accuracy: 58.53%\n",
            "Epoch [8/10], Training Loss: 1.021, Validation Accuracy: 58.43%\n",
            "Epoch [9/10], Training Loss: 1.012, Validation Accuracy: 57.59%\n",
            "Epoch [10/10], Training Loss: 1.001, Validation Accuracy: 57.31%\n",
            "Epoch [1/10], Training Loss: 1.161, Validation Accuracy: 58.03%\n",
            "Epoch [2/10], Training Loss: 1.128, Validation Accuracy: 58.59%\n",
            "Epoch [3/10], Training Loss: 1.106, Validation Accuracy: 58.09%\n",
            "Epoch [4/10], Training Loss: 1.083, Validation Accuracy: 58.70%\n",
            "Epoch [5/10], Training Loss: 1.070, Validation Accuracy: 58.12%\n",
            "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.97%\n",
            "Epoch [7/10], Training Loss: 1.049, Validation Accuracy: 58.49%\n",
            "Epoch [8/10], Training Loss: 1.035, Validation Accuracy: 58.37%\n",
            "Epoch [9/10], Training Loss: 1.028, Validation Accuracy: 58.39%\n",
            "Epoch [10/10], Training Loss: 1.016, Validation Accuracy: 58.40%\n",
            "Epoch [1/10], Training Loss: 1.121, Validation Accuracy: 58.49%\n",
            "Epoch [2/10], Training Loss: 1.085, Validation Accuracy: 58.50%\n",
            "Epoch [3/10], Training Loss: 1.059, Validation Accuracy: 58.46%\n",
            "Epoch [4/10], Training Loss: 1.053, Validation Accuracy: 58.79%\n",
            "Epoch [5/10], Training Loss: 1.025, Validation Accuracy: 59.09%\n",
            "Epoch [6/10], Training Loss: 1.018, Validation Accuracy: 58.97%\n",
            "Epoch [7/10], Training Loss: 0.996, Validation Accuracy: 58.70%\n",
            "Epoch [8/10], Training Loss: 0.987, Validation Accuracy: 58.42%\n",
            "Epoch [9/10], Training Loss: 0.972, Validation Accuracy: 58.51%\n",
            "Epoch [10/10], Training Loss: 0.963, Validation Accuracy: 58.35%\n",
            "Epoch [1/10], Training Loss: 1.132, Validation Accuracy: 57.56%\n",
            "Epoch [2/10], Training Loss: 1.095, Validation Accuracy: 59.33%\n",
            "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 58.23%\n",
            "Epoch [4/10], Training Loss: 1.038, Validation Accuracy: 58.99%\n",
            "Epoch [5/10], Training Loss: 1.024, Validation Accuracy: 59.32%\n",
            "Epoch [6/10], Training Loss: 1.019, Validation Accuracy: 59.17%\n",
            "Epoch [7/10], Training Loss: 1.000, Validation Accuracy: 58.38%\n",
            "Epoch [8/10], Training Loss: 0.992, Validation Accuracy: 59.06%\n",
            "Epoch [9/10], Training Loss: 0.979, Validation Accuracy: 59.38%\n",
            "Epoch [10/10], Training Loss: 0.968, Validation Accuracy: 59.12%\n",
            "Epoch [1/10], Training Loss: 1.108, Validation Accuracy: 59.91%\n",
            "Epoch [2/10], Training Loss: 1.058, Validation Accuracy: 59.51%\n",
            "Epoch [3/10], Training Loss: 1.048, Validation Accuracy: 59.60%\n",
            "Epoch [4/10], Training Loss: 1.010, Validation Accuracy: 59.51%\n",
            "Epoch [5/10], Training Loss: 0.998, Validation Accuracy: 59.51%\n",
            "Epoch [6/10], Training Loss: 0.990, Validation Accuracy: 59.49%\n",
            "Epoch [7/10], Training Loss: 0.970, Validation Accuracy: 59.00%\n",
            "Epoch [8/10], Training Loss: 0.951, Validation Accuracy: 59.56%\n",
            "Epoch [9/10], Training Loss: 0.939, Validation Accuracy: 59.66%\n",
            "Epoch [10/10], Training Loss: 0.933, Validation Accuracy: 58.87%\n",
            "Epoch [1/10], Training Loss: 1.054, Validation Accuracy: 59.66%\n",
            "Epoch [2/10], Training Loss: 1.020, Validation Accuracy: 59.64%\n",
            "Epoch [3/10], Training Loss: 0.989, Validation Accuracy: 59.59%\n",
            "Epoch [4/10], Training Loss: 0.967, Validation Accuracy: 60.24%\n",
            "Epoch [5/10], Training Loss: 0.941, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.930, Validation Accuracy: 59.69%\n",
            "Epoch [7/10], Training Loss: 0.919, Validation Accuracy: 59.87%\n",
            "Epoch [8/10], Training Loss: 0.900, Validation Accuracy: 58.85%\n",
            "Epoch [9/10], Training Loss: 0.889, Validation Accuracy: 59.52%\n",
            "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 59.45%\n",
            "Epoch [1/10], Training Loss: 1.083, Validation Accuracy: 59.64%\n",
            "Epoch [2/10], Training Loss: 1.044, Validation Accuracy: 59.64%\n",
            "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 59.82%\n",
            "Epoch [4/10], Training Loss: 0.996, Validation Accuracy: 59.09%\n",
            "Epoch [5/10], Training Loss: 0.975, Validation Accuracy: 59.20%\n",
            "Epoch [6/10], Training Loss: 0.960, Validation Accuracy: 60.20%\n",
            "Epoch [7/10], Training Loss: 0.945, Validation Accuracy: 59.93%\n",
            "Epoch [8/10], Training Loss: 0.926, Validation Accuracy: 59.52%\n",
            "Epoch [9/10], Training Loss: 0.917, Validation Accuracy: 59.53%\n",
            "Epoch [10/10], Training Loss: 0.901, Validation Accuracy: 59.75%\n",
            "Epoch [1/10], Training Loss: 1.039, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.989, Validation Accuracy: 60.13%\n",
            "Epoch [3/10], Training Loss: 0.964, Validation Accuracy: 59.62%\n",
            "Epoch [4/10], Training Loss: 0.941, Validation Accuracy: 60.02%\n",
            "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 59.46%\n",
            "Epoch [6/10], Training Loss: 0.908, Validation Accuracy: 59.33%\n",
            "Epoch [7/10], Training Loss: 0.899, Validation Accuracy: 59.47%\n",
            "Epoch [8/10], Training Loss: 0.880, Validation Accuracy: 60.00%\n",
            "Epoch [9/10], Training Loss: 0.855, Validation Accuracy: 59.58%\n",
            "Epoch [10/10], Training Loss: 0.843, Validation Accuracy: 59.94%\n",
            "Epoch [1/10], Training Loss: 1.058, Validation Accuracy: 59.44%\n",
            "Epoch [2/10], Training Loss: 1.006, Validation Accuracy: 60.68%\n",
            "Epoch [3/10], Training Loss: 0.969, Validation Accuracy: 60.58%\n",
            "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 60.16%\n",
            "Epoch [5/10], Training Loss: 0.931, Validation Accuracy: 59.64%\n",
            "Epoch [6/10], Training Loss: 0.914, Validation Accuracy: 60.08%\n",
            "Epoch [7/10], Training Loss: 0.888, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.879, Validation Accuracy: 60.26%\n",
            "Epoch [9/10], Training Loss: 0.876, Validation Accuracy: 60.08%\n",
            "Epoch [10/10], Training Loss: 0.847, Validation Accuracy: 60.45%\n",
            "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 60.23%\n",
            "Epoch [2/10], Training Loss: 0.982, Validation Accuracy: 60.01%\n",
            "Epoch [3/10], Training Loss: 0.944, Validation Accuracy: 60.26%\n",
            "Epoch [4/10], Training Loss: 0.928, Validation Accuracy: 60.41%\n",
            "Epoch [5/10], Training Loss: 0.910, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.888, Validation Accuracy: 59.89%\n",
            "Epoch [7/10], Training Loss: 0.864, Validation Accuracy: 60.42%\n",
            "Epoch [8/10], Training Loss: 0.855, Validation Accuracy: 59.98%\n",
            "Epoch [9/10], Training Loss: 0.841, Validation Accuracy: 60.05%\n",
            "Epoch [10/10], Training Loss: 0.824, Validation Accuracy: 59.50%\n",
            "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 59.41%\n",
            "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 60.51%\n",
            "Epoch [3/10], Training Loss: 0.899, Validation Accuracy: 60.24%\n",
            "Epoch [4/10], Training Loss: 0.875, Validation Accuracy: 59.89%\n",
            "Epoch [5/10], Training Loss: 0.851, Validation Accuracy: 60.46%\n",
            "Epoch [6/10], Training Loss: 0.834, Validation Accuracy: 60.45%\n",
            "Epoch [7/10], Training Loss: 0.817, Validation Accuracy: 59.87%\n",
            "Epoch [8/10], Training Loss: 0.794, Validation Accuracy: 59.05%\n",
            "Epoch [9/10], Training Loss: 0.781, Validation Accuracy: 60.17%\n",
            "Epoch [10/10], Training Loss: 0.767, Validation Accuracy: 60.05%\n",
            "Epoch [1/10], Training Loss: 1.030, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.966, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.924, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.907, Validation Accuracy: 59.31%\n",
            "Epoch [5/10], Training Loss: 0.881, Validation Accuracy: 59.63%\n",
            "Epoch [6/10], Training Loss: 0.859, Validation Accuracy: 59.81%\n",
            "Epoch [7/10], Training Loss: 0.842, Validation Accuracy: 59.72%\n",
            "Epoch [8/10], Training Loss: 0.822, Validation Accuracy: 60.25%\n",
            "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 60.20%\n",
            "Epoch [10/10], Training Loss: 0.793, Validation Accuracy: 60.43%\n",
            "Epoch [1/10], Training Loss: 0.977, Validation Accuracy: 60.45%\n",
            "Epoch [2/10], Training Loss: 0.913, Validation Accuracy: 61.05%\n",
            "Epoch [3/10], Training Loss: 0.877, Validation Accuracy: 60.51%\n",
            "Epoch [4/10], Training Loss: 0.846, Validation Accuracy: 60.68%\n",
            "Epoch [5/10], Training Loss: 0.836, Validation Accuracy: 60.13%\n",
            "Epoch [6/10], Training Loss: 0.799, Validation Accuracy: 60.87%\n",
            "Epoch [7/10], Training Loss: 0.795, Validation Accuracy: 60.48%\n",
            "Epoch [8/10], Training Loss: 0.768, Validation Accuracy: 59.92%\n",
            "Epoch [9/10], Training Loss: 0.756, Validation Accuracy: 59.54%\n",
            "Epoch [10/10], Training Loss: 0.741, Validation Accuracy: 60.07%\n",
            "Epoch [1/10], Training Loss: 1.000, Validation Accuracy: 59.77%\n",
            "Epoch [2/10], Training Loss: 0.930, Validation Accuracy: 60.16%\n",
            "Epoch [3/10], Training Loss: 0.893, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.864, Validation Accuracy: 61.28%\n",
            "Epoch [5/10], Training Loss: 0.834, Validation Accuracy: 60.79%\n",
            "Epoch [6/10], Training Loss: 0.808, Validation Accuracy: 60.95%\n",
            "Epoch [7/10], Training Loss: 0.797, Validation Accuracy: 61.02%\n",
            "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.763, Validation Accuracy: 60.78%\n",
            "Epoch [10/10], Training Loss: 0.740, Validation Accuracy: 60.62%\n",
            "Epoch [1/10], Training Loss: 0.979, Validation Accuracy: 60.17%\n",
            "Epoch [2/10], Training Loss: 0.907, Validation Accuracy: 60.89%\n",
            "Epoch [3/10], Training Loss: 0.875, Validation Accuracy: 61.28%\n",
            "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 60.99%\n",
            "Epoch [5/10], Training Loss: 0.816, Validation Accuracy: 60.89%\n",
            "Epoch [6/10], Training Loss: 0.795, Validation Accuracy: 61.01%\n",
            "Epoch [7/10], Training Loss: 0.776, Validation Accuracy: 61.29%\n",
            "Epoch [8/10], Training Loss: 0.752, Validation Accuracy: 60.45%\n",
            "Epoch [9/10], Training Loss: 0.735, Validation Accuracy: 60.28%\n",
            "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 59.78%\n",
            "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 60.89%\n",
            "Epoch [2/10], Training Loss: 0.875, Validation Accuracy: 61.15%\n",
            "Epoch [3/10], Training Loss: 0.821, Validation Accuracy: 60.44%\n",
            "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.15%\n",
            "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.724, Validation Accuracy: 60.19%\n",
            "Epoch [8/10], Training Loss: 0.697, Validation Accuracy: 60.69%\n",
            "Epoch [9/10], Training Loss: 0.673, Validation Accuracy: 60.23%\n",
            "Epoch [10/10], Training Loss: 0.661, Validation Accuracy: 60.16%\n",
            "Epoch [1/10], Training Loss: 0.975, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.899, Validation Accuracy: 60.32%\n",
            "Epoch [3/10], Training Loss: 0.854, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.820, Validation Accuracy: 60.97%\n",
            "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.768, Validation Accuracy: 60.92%\n",
            "Epoch [7/10], Training Loss: 0.750, Validation Accuracy: 60.07%\n",
            "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 60.51%\n",
            "Epoch [9/10], Training Loss: 0.707, Validation Accuracy: 59.95%\n",
            "Epoch [10/10], Training Loss: 0.696, Validation Accuracy: 60.05%\n",
            "Epoch [1/10], Training Loss: 0.924, Validation Accuracy: 59.99%\n",
            "Epoch [2/10], Training Loss: 0.862, Validation Accuracy: 60.82%\n",
            "Epoch [3/10], Training Loss: 0.804, Validation Accuracy: 60.53%\n",
            "Epoch [4/10], Training Loss: 0.771, Validation Accuracy: 60.60%\n",
            "Epoch [5/10], Training Loss: 0.744, Validation Accuracy: 60.34%\n",
            "Epoch [6/10], Training Loss: 0.717, Validation Accuracy: 60.87%\n",
            "Epoch [7/10], Training Loss: 0.695, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.654, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.634, Validation Accuracy: 60.18%\n",
            "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 0.851, Validation Accuracy: 60.88%\n",
            "Epoch [3/10], Training Loss: 0.805, Validation Accuracy: 60.58%\n",
            "Epoch [4/10], Training Loss: 0.770, Validation Accuracy: 61.33%\n",
            "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 60.35%\n",
            "Epoch [6/10], Training Loss: 0.715, Validation Accuracy: 60.46%\n",
            "Epoch [7/10], Training Loss: 0.688, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.665, Validation Accuracy: 61.09%\n",
            "Epoch [9/10], Training Loss: 0.652, Validation Accuracy: 61.31%\n",
            "Epoch [10/10], Training Loss: 0.629, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 61.01%\n",
            "Epoch [2/10], Training Loss: 0.837, Validation Accuracy: 61.52%\n",
            "Epoch [3/10], Training Loss: 0.783, Validation Accuracy: 61.39%\n",
            "Epoch [4/10], Training Loss: 0.755, Validation Accuracy: 60.52%\n",
            "Epoch [5/10], Training Loss: 0.723, Validation Accuracy: 61.21%\n",
            "Epoch [6/10], Training Loss: 0.703, Validation Accuracy: 61.30%\n",
            "Epoch [7/10], Training Loss: 0.673, Validation Accuracy: 61.16%\n",
            "Epoch [8/10], Training Loss: 0.650, Validation Accuracy: 60.22%\n",
            "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 60.57%\n",
            "Epoch [10/10], Training Loss: 0.613, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 60.27%\n",
            "Epoch [2/10], Training Loss: 0.799, Validation Accuracy: 60.16%\n",
            "Epoch [3/10], Training Loss: 0.742, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.696, Validation Accuracy: 60.50%\n",
            "Epoch [5/10], Training Loss: 0.686, Validation Accuracy: 60.48%\n",
            "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 60.93%\n",
            "Epoch [7/10], Training Loss: 0.620, Validation Accuracy: 60.92%\n",
            "Epoch [8/10], Training Loss: 0.601, Validation Accuracy: 60.26%\n",
            "Epoch [9/10], Training Loss: 0.592, Validation Accuracy: 59.47%\n",
            "Epoch [10/10], Training Loss: 0.557, Validation Accuracy: 59.89%\n",
            "Epoch [1/10], Training Loss: 0.940, Validation Accuracy: 60.66%\n",
            "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.784, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.732, Validation Accuracy: 61.23%\n",
            "Epoch [5/10], Training Loss: 0.701, Validation Accuracy: 61.02%\n",
            "Epoch [6/10], Training Loss: 0.677, Validation Accuracy: 60.75%\n",
            "Epoch [7/10], Training Loss: 0.647, Validation Accuracy: 60.71%\n",
            "Epoch [8/10], Training Loss: 0.627, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.607, Validation Accuracy: 60.59%\n",
            "Epoch [10/10], Training Loss: 0.590, Validation Accuracy: 60.99%\n",
            "Epoch [1/10], Training Loss: 0.871, Validation Accuracy: 61.10%\n",
            "Epoch [2/10], Training Loss: 0.777, Validation Accuracy: 61.34%\n",
            "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 61.01%\n",
            "Epoch [4/10], Training Loss: 0.686, Validation Accuracy: 60.64%\n",
            "Epoch [5/10], Training Loss: 0.652, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.605, Validation Accuracy: 60.79%\n",
            "Epoch [8/10], Training Loss: 0.578, Validation Accuracy: 60.60%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 60.78%\n",
            "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 60.82%\n",
            "Epoch [2/10], Training Loss: 0.783, Validation Accuracy: 61.00%\n",
            "Epoch [3/10], Training Loss: 0.736, Validation Accuracy: 60.98%\n",
            "Epoch [4/10], Training Loss: 0.691, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.649, Validation Accuracy: 61.91%\n",
            "Epoch [6/10], Training Loss: 0.623, Validation Accuracy: 61.32%\n",
            "Epoch [7/10], Training Loss: 0.603, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.573, Validation Accuracy: 61.27%\n",
            "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 60.88%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 61.79%\n",
            "Epoch [2/10], Training Loss: 0.769, Validation Accuracy: 61.70%\n",
            "Epoch [3/10], Training Loss: 0.707, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.665, Validation Accuracy: 60.32%\n",
            "Epoch [5/10], Training Loss: 0.634, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.603, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.02%\n",
            "Epoch [8/10], Training Loss: 0.555, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.530, Validation Accuracy: 60.82%\n",
            "Epoch [10/10], Training Loss: 0.518, Validation Accuracy: 60.19%\n",
            "Epoch [1/10], Training Loss: 0.856, Validation Accuracy: 60.07%\n",
            "Epoch [2/10], Training Loss: 0.739, Validation Accuracy: 60.81%\n",
            "Epoch [3/10], Training Loss: 0.668, Validation Accuracy: 60.41%\n",
            "Epoch [4/10], Training Loss: 0.625, Validation Accuracy: 61.07%\n",
            "Epoch [5/10], Training Loss: 0.588, Validation Accuracy: 60.78%\n",
            "Epoch [6/10], Training Loss: 0.555, Validation Accuracy: 60.13%\n",
            "Epoch [7/10], Training Loss: 0.524, Validation Accuracy: 60.87%\n",
            "Epoch [8/10], Training Loss: 0.505, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.485, Validation Accuracy: 60.60%\n",
            "Epoch [10/10], Training Loss: 0.463, Validation Accuracy: 59.93%\n",
            "Epoch [1/10], Training Loss: 0.914, Validation Accuracy: 60.92%\n",
            "Epoch [2/10], Training Loss: 0.782, Validation Accuracy: 61.35%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 61.00%\n",
            "Epoch [4/10], Training Loss: 0.647, Validation Accuracy: 60.66%\n",
            "Epoch [5/10], Training Loss: 0.613, Validation Accuracy: 61.26%\n",
            "Epoch [6/10], Training Loss: 0.597, Validation Accuracy: 60.98%\n",
            "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 61.01%\n",
            "Epoch [8/10], Training Loss: 0.529, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.504, Validation Accuracy: 60.75%\n",
            "Epoch [10/10], Training Loss: 0.486, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.719, Validation Accuracy: 60.36%\n",
            "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 60.84%\n",
            "Epoch [4/10], Training Loss: 0.608, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.573, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.538, Validation Accuracy: 60.81%\n",
            "Epoch [7/10], Training Loss: 0.508, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.490, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 60.15%\n",
            "Epoch [10/10], Training Loss: 0.435, Validation Accuracy: 60.76%\n",
            "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 60.50%\n",
            "Epoch [2/10], Training Loss: 0.705, Validation Accuracy: 60.87%\n",
            "Epoch [3/10], Training Loss: 0.643, Validation Accuracy: 61.42%\n",
            "Epoch [4/10], Training Loss: 0.599, Validation Accuracy: 59.88%\n",
            "Epoch [5/10], Training Loss: 0.565, Validation Accuracy: 60.83%\n",
            "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 61.29%\n",
            "Epoch [7/10], Training Loss: 0.501, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.478, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 60.17%\n",
            "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 61.18%\n",
            "Epoch [1/10], Training Loss: 0.840, Validation Accuracy: 61.18%\n",
            "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 61.01%\n",
            "Epoch [3/10], Training Loss: 0.629, Validation Accuracy: 61.28%\n",
            "Epoch [4/10], Training Loss: 0.582, Validation Accuracy: 61.35%\n",
            "Epoch [5/10], Training Loss: 0.538, Validation Accuracy: 60.65%\n",
            "Epoch [6/10], Training Loss: 0.509, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.484, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.457, Validation Accuracy: 59.92%\n",
            "Epoch [9/10], Training Loss: 0.439, Validation Accuracy: 60.74%\n",
            "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 60.59%\n",
            "Epoch [1/10], Training Loss: 0.828, Validation Accuracy: 59.25%\n",
            "Epoch [2/10], Training Loss: 0.673, Validation Accuracy: 60.12%\n",
            "Epoch [3/10], Training Loss: 0.597, Validation Accuracy: 60.68%\n",
            "Epoch [4/10], Training Loss: 0.550, Validation Accuracy: 60.16%\n",
            "Epoch [5/10], Training Loss: 0.503, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.472, Validation Accuracy: 60.43%\n",
            "Epoch [7/10], Training Loss: 0.444, Validation Accuracy: 60.29%\n",
            "Epoch [8/10], Training Loss: 0.419, Validation Accuracy: 59.90%\n",
            "Epoch [9/10], Training Loss: 0.392, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.374, Validation Accuracy: 60.34%\n",
            "Confusion Matrix:\n",
            "[[686  29  61  20  26  12  17   9 102  38]\n",
            " [ 49 716  14   9   4   7  22  12  45 122]\n",
            " [ 78  13 500  79  99  73  68  48  26  16]\n",
            " [ 34  17 105 372  65 194 114  56  26  17]\n",
            " [ 40   6 117  60 500  52  90 113  16   6]\n",
            " [ 15   6  90 182  74 471  52  87  13  10]\n",
            " [ 11  12  71  76  47  28 709  18  17  11]\n",
            " [ 25   8  45  49  68  68  16 687  10  24]\n",
            " [ 99  56  22  14  13  10  15   3 733  35]\n",
            " [ 63 152  19  24  14  14  21  36  48 609]]\n",
            "Test Accuracy: 59.83%\n",
            "True Positives (TP): [686 716 500 372 500 471 709 687 733 609]\n",
            "False Positives (FP): [414 299 544 513 410 458 415 382 303 279]\n",
            "True Negatives (TN): [8586 8701 8456 8487 8590 8542 8585 8618 8697 8721]\n",
            "False Negatives (FN): [314 284 500 628 500 529 291 313 267 391]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.62363636 0.70541872 0.4789272  0.42033898 0.54945055 0.50699677\n",
            " 0.63078292 0.64265669 0.70752896 0.68581081]\n",
            "Recall: [0.686 0.716 0.5   0.372 0.5   0.471 0.709 0.687 0.733 0.609]\n",
            "F1 Score: [0.65333333 0.71066998 0.48923679 0.39469496 0.52356021 0.48833593\n",
            " 0.66760829 0.66408893 0.72003929 0.64512712]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = ( augmented_data_normal + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMFSzAJYkag9",
        "outputId": "342fdd46-5d59-4c21-cc57-77e770a5d55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6049 5959 5932 6018 5974 6043 5971 6058 6001 5995]\n",
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.39%\n",
            "Epoch [2/10], Training Loss: 2.302, Validation Accuracy: 10.79%\n",
            "Epoch [3/10], Training Loss: 2.300, Validation Accuracy: 10.93%\n",
            "Epoch [4/10], Training Loss: 2.298, Validation Accuracy: 11.68%\n",
            "Epoch [5/10], Training Loss: 2.296, Validation Accuracy: 12.45%\n",
            "Epoch [6/10], Training Loss: 2.293, Validation Accuracy: 12.85%\n",
            "Epoch [7/10], Training Loss: 2.290, Validation Accuracy: 13.24%\n",
            "Epoch [8/10], Training Loss: 2.285, Validation Accuracy: 13.36%\n",
            "Epoch [9/10], Training Loss: 2.278, Validation Accuracy: 13.75%\n",
            "Epoch [10/10], Training Loss: 2.267, Validation Accuracy: 12.47%\n",
            "Epoch [1/10], Training Loss: 2.259, Validation Accuracy: 14.27%\n",
            "Epoch [2/10], Training Loss: 2.244, Validation Accuracy: 16.91%\n",
            "Epoch [3/10], Training Loss: 2.225, Validation Accuracy: 17.81%\n",
            "Epoch [4/10], Training Loss: 2.202, Validation Accuracy: 18.83%\n",
            "Epoch [5/10], Training Loss: 2.176, Validation Accuracy: 19.60%\n",
            "Epoch [6/10], Training Loss: 2.147, Validation Accuracy: 20.76%\n",
            "Epoch [7/10], Training Loss: 2.116, Validation Accuracy: 21.53%\n",
            "Epoch [8/10], Training Loss: 2.084, Validation Accuracy: 24.49%\n",
            "Epoch [9/10], Training Loss: 2.058, Validation Accuracy: 24.72%\n",
            "Epoch [10/10], Training Loss: 2.034, Validation Accuracy: 25.41%\n",
            "Epoch [1/10], Training Loss: 2.021, Validation Accuracy: 26.46%\n",
            "Epoch [2/10], Training Loss: 2.002, Validation Accuracy: 26.96%\n",
            "Epoch [3/10], Training Loss: 1.985, Validation Accuracy: 27.36%\n",
            "Epoch [4/10], Training Loss: 1.967, Validation Accuracy: 27.85%\n",
            "Epoch [5/10], Training Loss: 1.951, Validation Accuracy: 28.30%\n",
            "Epoch [6/10], Training Loss: 1.933, Validation Accuracy: 29.03%\n",
            "Epoch [7/10], Training Loss: 1.919, Validation Accuracy: 29.52%\n",
            "Epoch [8/10], Training Loss: 1.892, Validation Accuracy: 30.21%\n",
            "Epoch [9/10], Training Loss: 1.871, Validation Accuracy: 30.62%\n",
            "Epoch [10/10], Training Loss: 1.846, Validation Accuracy: 31.19%\n",
            "Epoch [1/10], Training Loss: 1.835, Validation Accuracy: 32.78%\n",
            "Epoch [2/10], Training Loss: 1.808, Validation Accuracy: 33.00%\n",
            "Epoch [3/10], Training Loss: 1.787, Validation Accuracy: 34.10%\n",
            "Epoch [4/10], Training Loss: 1.765, Validation Accuracy: 35.10%\n",
            "Epoch [5/10], Training Loss: 1.748, Validation Accuracy: 35.10%\n",
            "Epoch [6/10], Training Loss: 1.732, Validation Accuracy: 35.75%\n",
            "Epoch [7/10], Training Loss: 1.716, Validation Accuracy: 36.46%\n",
            "Epoch [8/10], Training Loss: 1.703, Validation Accuracy: 36.83%\n",
            "Epoch [9/10], Training Loss: 1.686, Validation Accuracy: 37.59%\n",
            "Epoch [10/10], Training Loss: 1.673, Validation Accuracy: 37.07%\n",
            "Epoch [1/10], Training Loss: 1.668, Validation Accuracy: 38.09%\n",
            "Epoch [2/10], Training Loss: 1.655, Validation Accuracy: 38.41%\n",
            "Epoch [3/10], Training Loss: 1.644, Validation Accuracy: 38.67%\n",
            "Epoch [4/10], Training Loss: 1.629, Validation Accuracy: 38.78%\n",
            "Epoch [5/10], Training Loss: 1.617, Validation Accuracy: 39.48%\n",
            "Epoch [6/10], Training Loss: 1.602, Validation Accuracy: 39.77%\n",
            "Epoch [7/10], Training Loss: 1.599, Validation Accuracy: 40.04%\n",
            "Epoch [8/10], Training Loss: 1.582, Validation Accuracy: 41.02%\n",
            "Epoch [9/10], Training Loss: 1.572, Validation Accuracy: 41.10%\n",
            "Epoch [10/10], Training Loss: 1.566, Validation Accuracy: 41.37%\n",
            "Epoch [1/10], Training Loss: 1.591, Validation Accuracy: 41.81%\n",
            "Epoch [2/10], Training Loss: 1.580, Validation Accuracy: 42.37%\n",
            "Epoch [3/10], Training Loss: 1.568, Validation Accuracy: 42.26%\n",
            "Epoch [4/10], Training Loss: 1.556, Validation Accuracy: 43.06%\n",
            "Epoch [5/10], Training Loss: 1.546, Validation Accuracy: 42.62%\n",
            "Epoch [6/10], Training Loss: 1.542, Validation Accuracy: 42.85%\n",
            "Epoch [7/10], Training Loss: 1.531, Validation Accuracy: 43.27%\n",
            "Epoch [8/10], Training Loss: 1.515, Validation Accuracy: 42.69%\n",
            "Epoch [9/10], Training Loss: 1.509, Validation Accuracy: 44.49%\n",
            "Epoch [10/10], Training Loss: 1.500, Validation Accuracy: 44.63%\n",
            "Epoch [1/10], Training Loss: 1.518, Validation Accuracy: 44.46%\n",
            "Epoch [2/10], Training Loss: 1.503, Validation Accuracy: 45.27%\n",
            "Epoch [3/10], Training Loss: 1.491, Validation Accuracy: 44.78%\n",
            "Epoch [4/10], Training Loss: 1.485, Validation Accuracy: 45.71%\n",
            "Epoch [5/10], Training Loss: 1.472, Validation Accuracy: 46.10%\n",
            "Epoch [6/10], Training Loss: 1.469, Validation Accuracy: 46.19%\n",
            "Epoch [7/10], Training Loss: 1.459, Validation Accuracy: 45.83%\n",
            "Epoch [8/10], Training Loss: 1.450, Validation Accuracy: 46.71%\n",
            "Epoch [9/10], Training Loss: 1.439, Validation Accuracy: 46.77%\n",
            "Epoch [10/10], Training Loss: 1.446, Validation Accuracy: 46.00%\n",
            "Epoch [1/10], Training Loss: 1.473, Validation Accuracy: 47.26%\n",
            "Epoch [2/10], Training Loss: 1.453, Validation Accuracy: 46.53%\n",
            "Epoch [3/10], Training Loss: 1.452, Validation Accuracy: 47.09%\n",
            "Epoch [4/10], Training Loss: 1.441, Validation Accuracy: 46.68%\n",
            "Epoch [5/10], Training Loss: 1.429, Validation Accuracy: 47.61%\n",
            "Epoch [6/10], Training Loss: 1.423, Validation Accuracy: 47.85%\n",
            "Epoch [7/10], Training Loss: 1.421, Validation Accuracy: 46.88%\n",
            "Epoch [8/10], Training Loss: 1.419, Validation Accuracy: 48.06%\n",
            "Epoch [9/10], Training Loss: 1.404, Validation Accuracy: 47.24%\n",
            "Epoch [10/10], Training Loss: 1.395, Validation Accuracy: 48.16%\n",
            "Epoch [1/10], Training Loss: 1.428, Validation Accuracy: 48.33%\n",
            "Epoch [2/10], Training Loss: 1.413, Validation Accuracy: 48.53%\n",
            "Epoch [3/10], Training Loss: 1.403, Validation Accuracy: 48.56%\n",
            "Epoch [4/10], Training Loss: 1.397, Validation Accuracy: 48.59%\n",
            "Epoch [5/10], Training Loss: 1.387, Validation Accuracy: 48.75%\n",
            "Epoch [6/10], Training Loss: 1.382, Validation Accuracy: 48.35%\n",
            "Epoch [7/10], Training Loss: 1.370, Validation Accuracy: 49.05%\n",
            "Epoch [8/10], Training Loss: 1.362, Validation Accuracy: 49.11%\n",
            "Epoch [9/10], Training Loss: 1.365, Validation Accuracy: 49.71%\n",
            "Epoch [10/10], Training Loss: 1.356, Validation Accuracy: 49.17%\n",
            "Epoch [1/10], Training Loss: 1.377, Validation Accuracy: 49.55%\n",
            "Epoch [2/10], Training Loss: 1.366, Validation Accuracy: 49.72%\n",
            "Epoch [3/10], Training Loss: 1.352, Validation Accuracy: 50.00%\n",
            "Epoch [4/10], Training Loss: 1.346, Validation Accuracy: 50.66%\n",
            "Epoch [5/10], Training Loss: 1.339, Validation Accuracy: 49.85%\n",
            "Epoch [6/10], Training Loss: 1.332, Validation Accuracy: 50.69%\n",
            "Epoch [7/10], Training Loss: 1.321, Validation Accuracy: 50.74%\n",
            "Epoch [8/10], Training Loss: 1.310, Validation Accuracy: 50.84%\n",
            "Epoch [9/10], Training Loss: 1.304, Validation Accuracy: 50.77%\n",
            "Epoch [10/10], Training Loss: 1.302, Validation Accuracy: 51.43%\n",
            "Epoch [1/10], Training Loss: 1.357, Validation Accuracy: 50.52%\n",
            "Epoch [2/10], Training Loss: 1.349, Validation Accuracy: 50.11%\n",
            "Epoch [3/10], Training Loss: 1.324, Validation Accuracy: 50.57%\n",
            "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 51.18%\n",
            "Epoch [5/10], Training Loss: 1.320, Validation Accuracy: 51.03%\n",
            "Epoch [6/10], Training Loss: 1.305, Validation Accuracy: 51.31%\n",
            "Epoch [7/10], Training Loss: 1.292, Validation Accuracy: 50.93%\n",
            "Epoch [8/10], Training Loss: 1.287, Validation Accuracy: 50.47%\n",
            "Epoch [9/10], Training Loss: 1.280, Validation Accuracy: 51.16%\n",
            "Epoch [10/10], Training Loss: 1.276, Validation Accuracy: 51.54%\n",
            "Epoch [1/10], Training Loss: 1.329, Validation Accuracy: 51.66%\n",
            "Epoch [2/10], Training Loss: 1.318, Validation Accuracy: 51.11%\n",
            "Epoch [3/10], Training Loss: 1.306, Validation Accuracy: 51.33%\n",
            "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 51.31%\n",
            "Epoch [5/10], Training Loss: 1.281, Validation Accuracy: 51.78%\n",
            "Epoch [6/10], Training Loss: 1.269, Validation Accuracy: 51.02%\n",
            "Epoch [7/10], Training Loss: 1.268, Validation Accuracy: 52.10%\n",
            "Epoch [8/10], Training Loss: 1.251, Validation Accuracy: 51.37%\n",
            "Epoch [9/10], Training Loss: 1.247, Validation Accuracy: 51.69%\n",
            "Epoch [10/10], Training Loss: 1.240, Validation Accuracy: 52.11%\n",
            "Epoch [1/10], Training Loss: 1.319, Validation Accuracy: 51.81%\n",
            "Epoch [2/10], Training Loss: 1.305, Validation Accuracy: 52.50%\n",
            "Epoch [3/10], Training Loss: 1.290, Validation Accuracy: 52.78%\n",
            "Epoch [4/10], Training Loss: 1.276, Validation Accuracy: 52.71%\n",
            "Epoch [5/10], Training Loss: 1.268, Validation Accuracy: 51.94%\n",
            "Epoch [6/10], Training Loss: 1.261, Validation Accuracy: 52.68%\n",
            "Epoch [7/10], Training Loss: 1.248, Validation Accuracy: 52.60%\n",
            "Epoch [8/10], Training Loss: 1.240, Validation Accuracy: 52.48%\n",
            "Epoch [9/10], Training Loss: 1.234, Validation Accuracy: 52.47%\n",
            "Epoch [10/10], Training Loss: 1.223, Validation Accuracy: 52.78%\n",
            "Epoch [1/10], Training Loss: 1.294, Validation Accuracy: 52.08%\n",
            "Epoch [2/10], Training Loss: 1.274, Validation Accuracy: 52.73%\n",
            "Epoch [3/10], Training Loss: 1.258, Validation Accuracy: 53.28%\n",
            "Epoch [4/10], Training Loss: 1.245, Validation Accuracy: 53.78%\n",
            "Epoch [5/10], Training Loss: 1.229, Validation Accuracy: 53.21%\n",
            "Epoch [6/10], Training Loss: 1.227, Validation Accuracy: 52.61%\n",
            "Epoch [7/10], Training Loss: 1.215, Validation Accuracy: 52.02%\n",
            "Epoch [8/10], Training Loss: 1.208, Validation Accuracy: 53.87%\n",
            "Epoch [9/10], Training Loss: 1.193, Validation Accuracy: 53.60%\n",
            "Epoch [10/10], Training Loss: 1.187, Validation Accuracy: 53.61%\n",
            "Epoch [1/10], Training Loss: 1.256, Validation Accuracy: 54.03%\n",
            "Epoch [2/10], Training Loss: 1.234, Validation Accuracy: 53.80%\n",
            "Epoch [3/10], Training Loss: 1.215, Validation Accuracy: 54.60%\n",
            "Epoch [4/10], Training Loss: 1.201, Validation Accuracy: 54.24%\n",
            "Epoch [5/10], Training Loss: 1.196, Validation Accuracy: 53.99%\n",
            "Epoch [6/10], Training Loss: 1.190, Validation Accuracy: 54.58%\n",
            "Epoch [7/10], Training Loss: 1.179, Validation Accuracy: 54.80%\n",
            "Epoch [8/10], Training Loss: 1.167, Validation Accuracy: 53.81%\n",
            "Epoch [9/10], Training Loss: 1.159, Validation Accuracy: 54.39%\n",
            "Epoch [10/10], Training Loss: 1.150, Validation Accuracy: 54.42%\n",
            "Epoch [1/10], Training Loss: 1.236, Validation Accuracy: 54.44%\n",
            "Epoch [2/10], Training Loss: 1.222, Validation Accuracy: 54.51%\n",
            "Epoch [3/10], Training Loss: 1.198, Validation Accuracy: 54.48%\n",
            "Epoch [4/10], Training Loss: 1.185, Validation Accuracy: 54.64%\n",
            "Epoch [5/10], Training Loss: 1.171, Validation Accuracy: 55.06%\n",
            "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 54.29%\n",
            "Epoch [7/10], Training Loss: 1.150, Validation Accuracy: 55.35%\n",
            "Epoch [8/10], Training Loss: 1.154, Validation Accuracy: 54.77%\n",
            "Epoch [9/10], Training Loss: 1.138, Validation Accuracy: 54.65%\n",
            "Epoch [10/10], Training Loss: 1.122, Validation Accuracy: 55.12%\n",
            "Epoch [1/10], Training Loss: 1.224, Validation Accuracy: 54.80%\n",
            "Epoch [2/10], Training Loss: 1.203, Validation Accuracy: 55.37%\n",
            "Epoch [3/10], Training Loss: 1.173, Validation Accuracy: 54.97%\n",
            "Epoch [4/10], Training Loss: 1.166, Validation Accuracy: 55.41%\n",
            "Epoch [5/10], Training Loss: 1.155, Validation Accuracy: 55.56%\n",
            "Epoch [6/10], Training Loss: 1.137, Validation Accuracy: 55.00%\n",
            "Epoch [7/10], Training Loss: 1.133, Validation Accuracy: 55.18%\n",
            "Epoch [8/10], Training Loss: 1.114, Validation Accuracy: 55.23%\n",
            "Epoch [9/10], Training Loss: 1.115, Validation Accuracy: 54.28%\n",
            "Epoch [10/10], Training Loss: 1.101, Validation Accuracy: 54.70%\n",
            "Epoch [1/10], Training Loss: 1.222, Validation Accuracy: 55.69%\n",
            "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 54.35%\n",
            "Epoch [3/10], Training Loss: 1.169, Validation Accuracy: 55.63%\n",
            "Epoch [4/10], Training Loss: 1.158, Validation Accuracy: 55.62%\n",
            "Epoch [5/10], Training Loss: 1.144, Validation Accuracy: 55.58%\n",
            "Epoch [6/10], Training Loss: 1.132, Validation Accuracy: 55.44%\n",
            "Epoch [7/10], Training Loss: 1.123, Validation Accuracy: 55.77%\n",
            "Epoch [8/10], Training Loss: 1.114, Validation Accuracy: 54.90%\n",
            "Epoch [9/10], Training Loss: 1.110, Validation Accuracy: 55.74%\n",
            "Epoch [10/10], Training Loss: 1.096, Validation Accuracy: 55.83%\n",
            "Epoch [1/10], Training Loss: 1.183, Validation Accuracy: 56.18%\n",
            "Epoch [2/10], Training Loss: 1.155, Validation Accuracy: 55.70%\n",
            "Epoch [3/10], Training Loss: 1.140, Validation Accuracy: 55.87%\n",
            "Epoch [4/10], Training Loss: 1.115, Validation Accuracy: 56.59%\n",
            "Epoch [5/10], Training Loss: 1.103, Validation Accuracy: 56.35%\n",
            "Epoch [6/10], Training Loss: 1.097, Validation Accuracy: 56.43%\n",
            "Epoch [7/10], Training Loss: 1.095, Validation Accuracy: 54.99%\n",
            "Epoch [8/10], Training Loss: 1.073, Validation Accuracy: 56.05%\n",
            "Epoch [9/10], Training Loss: 1.076, Validation Accuracy: 56.05%\n",
            "Epoch [10/10], Training Loss: 1.048, Validation Accuracy: 55.91%\n",
            "Epoch [1/10], Training Loss: 1.165, Validation Accuracy: 56.89%\n",
            "Epoch [2/10], Training Loss: 1.119, Validation Accuracy: 56.05%\n",
            "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 56.76%\n",
            "Epoch [4/10], Training Loss: 1.093, Validation Accuracy: 56.54%\n",
            "Epoch [5/10], Training Loss: 1.085, Validation Accuracy: 57.10%\n",
            "Epoch [6/10], Training Loss: 1.067, Validation Accuracy: 56.33%\n",
            "Epoch [7/10], Training Loss: 1.059, Validation Accuracy: 57.42%\n",
            "Epoch [8/10], Training Loss: 1.043, Validation Accuracy: 56.45%\n",
            "Epoch [9/10], Training Loss: 1.043, Validation Accuracy: 56.30%\n",
            "Epoch [10/10], Training Loss: 1.024, Validation Accuracy: 57.10%\n",
            "Epoch [1/10], Training Loss: 1.148, Validation Accuracy: 57.26%\n",
            "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 56.57%\n",
            "Epoch [3/10], Training Loss: 1.086, Validation Accuracy: 57.56%\n",
            "Epoch [4/10], Training Loss: 1.072, Validation Accuracy: 56.89%\n",
            "Epoch [5/10], Training Loss: 1.062, Validation Accuracy: 57.58%\n",
            "Epoch [6/10], Training Loss: 1.045, Validation Accuracy: 57.62%\n",
            "Epoch [7/10], Training Loss: 1.030, Validation Accuracy: 57.41%\n",
            "Epoch [8/10], Training Loss: 1.022, Validation Accuracy: 56.55%\n",
            "Epoch [9/10], Training Loss: 1.004, Validation Accuracy: 56.34%\n",
            "Epoch [10/10], Training Loss: 0.998, Validation Accuracy: 57.14%\n",
            "Epoch [1/10], Training Loss: 1.138, Validation Accuracy: 57.29%\n",
            "Epoch [2/10], Training Loss: 1.094, Validation Accuracy: 57.47%\n",
            "Epoch [3/10], Training Loss: 1.072, Validation Accuracy: 57.32%\n",
            "Epoch [4/10], Training Loss: 1.053, Validation Accuracy: 57.05%\n",
            "Epoch [5/10], Training Loss: 1.035, Validation Accuracy: 57.34%\n",
            "Epoch [6/10], Training Loss: 1.028, Validation Accuracy: 57.08%\n",
            "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 57.75%\n",
            "Epoch [8/10], Training Loss: 0.995, Validation Accuracy: 57.51%\n",
            "Epoch [9/10], Training Loss: 0.989, Validation Accuracy: 56.87%\n",
            "Epoch [10/10], Training Loss: 0.980, Validation Accuracy: 57.35%\n",
            "Epoch [1/10], Training Loss: 1.129, Validation Accuracy: 57.89%\n",
            "Epoch [2/10], Training Loss: 1.093, Validation Accuracy: 57.96%\n",
            "Epoch [3/10], Training Loss: 1.084, Validation Accuracy: 58.56%\n",
            "Epoch [4/10], Training Loss: 1.049, Validation Accuracy: 58.06%\n",
            "Epoch [5/10], Training Loss: 1.034, Validation Accuracy: 58.31%\n",
            "Epoch [6/10], Training Loss: 1.024, Validation Accuracy: 57.53%\n",
            "Epoch [7/10], Training Loss: 1.007, Validation Accuracy: 58.35%\n",
            "Epoch [8/10], Training Loss: 0.996, Validation Accuracy: 58.16%\n",
            "Epoch [9/10], Training Loss: 0.984, Validation Accuracy: 58.15%\n",
            "Epoch [10/10], Training Loss: 0.977, Validation Accuracy: 57.25%\n",
            "Epoch [1/10], Training Loss: 1.108, Validation Accuracy: 58.12%\n",
            "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 58.63%\n",
            "Epoch [3/10], Training Loss: 1.030, Validation Accuracy: 58.08%\n",
            "Epoch [4/10], Training Loss: 1.018, Validation Accuracy: 57.90%\n",
            "Epoch [5/10], Training Loss: 1.006, Validation Accuracy: 55.55%\n",
            "Epoch [6/10], Training Loss: 0.985, Validation Accuracy: 58.53%\n",
            "Epoch [7/10], Training Loss: 0.967, Validation Accuracy: 57.96%\n",
            "Epoch [8/10], Training Loss: 0.965, Validation Accuracy: 57.98%\n",
            "Epoch [9/10], Training Loss: 0.947, Validation Accuracy: 57.74%\n",
            "Epoch [10/10], Training Loss: 0.922, Validation Accuracy: 58.02%\n",
            "Epoch [1/10], Training Loss: 1.087, Validation Accuracy: 58.06%\n",
            "Epoch [2/10], Training Loss: 1.043, Validation Accuracy: 59.13%\n",
            "Epoch [3/10], Training Loss: 1.022, Validation Accuracy: 58.95%\n",
            "Epoch [4/10], Training Loss: 0.995, Validation Accuracy: 58.97%\n",
            "Epoch [5/10], Training Loss: 0.975, Validation Accuracy: 59.01%\n",
            "Epoch [6/10], Training Loss: 0.966, Validation Accuracy: 59.05%\n",
            "Epoch [7/10], Training Loss: 0.946, Validation Accuracy: 58.62%\n",
            "Epoch [8/10], Training Loss: 0.939, Validation Accuracy: 58.82%\n",
            "Epoch [9/10], Training Loss: 0.924, Validation Accuracy: 58.94%\n",
            "Epoch [10/10], Training Loss: 0.910, Validation Accuracy: 58.47%\n",
            "Epoch [1/10], Training Loss: 1.066, Validation Accuracy: 58.84%\n",
            "Epoch [2/10], Training Loss: 1.019, Validation Accuracy: 59.17%\n",
            "Epoch [3/10], Training Loss: 0.987, Validation Accuracy: 59.03%\n",
            "Epoch [4/10], Training Loss: 0.965, Validation Accuracy: 58.41%\n",
            "Epoch [5/10], Training Loss: 0.948, Validation Accuracy: 58.89%\n",
            "Epoch [6/10], Training Loss: 0.926, Validation Accuracy: 58.57%\n",
            "Epoch [7/10], Training Loss: 0.914, Validation Accuracy: 58.59%\n",
            "Epoch [8/10], Training Loss: 0.914, Validation Accuracy: 58.36%\n",
            "Epoch [9/10], Training Loss: 0.883, Validation Accuracy: 58.62%\n",
            "Epoch [10/10], Training Loss: 0.879, Validation Accuracy: 59.07%\n",
            "Epoch [1/10], Training Loss: 1.059, Validation Accuracy: 58.91%\n",
            "Epoch [2/10], Training Loss: 1.003, Validation Accuracy: 58.81%\n",
            "Epoch [3/10], Training Loss: 0.981, Validation Accuracy: 59.23%\n",
            "Epoch [4/10], Training Loss: 0.956, Validation Accuracy: 59.34%\n",
            "Epoch [5/10], Training Loss: 0.938, Validation Accuracy: 58.46%\n",
            "Epoch [6/10], Training Loss: 0.926, Validation Accuracy: 59.09%\n",
            "Epoch [7/10], Training Loss: 0.910, Validation Accuracy: 59.03%\n",
            "Epoch [8/10], Training Loss: 0.894, Validation Accuracy: 58.99%\n",
            "Epoch [9/10], Training Loss: 0.871, Validation Accuracy: 59.07%\n",
            "Epoch [10/10], Training Loss: 0.860, Validation Accuracy: 58.73%\n",
            "Epoch [1/10], Training Loss: 1.074, Validation Accuracy: 58.82%\n",
            "Epoch [2/10], Training Loss: 1.029, Validation Accuracy: 59.56%\n",
            "Epoch [3/10], Training Loss: 0.984, Validation Accuracy: 59.57%\n",
            "Epoch [4/10], Training Loss: 0.961, Validation Accuracy: 59.86%\n",
            "Epoch [5/10], Training Loss: 0.940, Validation Accuracy: 59.25%\n",
            "Epoch [6/10], Training Loss: 0.921, Validation Accuracy: 59.22%\n",
            "Epoch [7/10], Training Loss: 0.905, Validation Accuracy: 58.88%\n",
            "Epoch [8/10], Training Loss: 0.894, Validation Accuracy: 59.20%\n",
            "Epoch [9/10], Training Loss: 0.876, Validation Accuracy: 59.08%\n",
            "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 59.55%\n",
            "Epoch [1/10], Training Loss: 1.032, Validation Accuracy: 58.79%\n",
            "Epoch [2/10], Training Loss: 0.977, Validation Accuracy: 59.11%\n",
            "Epoch [3/10], Training Loss: 0.945, Validation Accuracy: 58.87%\n",
            "Epoch [4/10], Training Loss: 0.923, Validation Accuracy: 59.27%\n",
            "Epoch [5/10], Training Loss: 0.911, Validation Accuracy: 59.51%\n",
            "Epoch [6/10], Training Loss: 0.882, Validation Accuracy: 59.69%\n",
            "Epoch [7/10], Training Loss: 0.867, Validation Accuracy: 59.64%\n",
            "Epoch [8/10], Training Loss: 0.855, Validation Accuracy: 59.04%\n",
            "Epoch [9/10], Training Loss: 0.839, Validation Accuracy: 59.47%\n",
            "Epoch [10/10], Training Loss: 0.819, Validation Accuracy: 59.30%\n",
            "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 59.52%\n",
            "Epoch [2/10], Training Loss: 0.965, Validation Accuracy: 59.55%\n",
            "Epoch [3/10], Training Loss: 0.932, Validation Accuracy: 59.61%\n",
            "Epoch [4/10], Training Loss: 0.902, Validation Accuracy: 59.67%\n",
            "Epoch [5/10], Training Loss: 0.882, Validation Accuracy: 60.17%\n",
            "Epoch [6/10], Training Loss: 0.870, Validation Accuracy: 59.87%\n",
            "Epoch [7/10], Training Loss: 0.859, Validation Accuracy: 60.09%\n",
            "Epoch [8/10], Training Loss: 0.834, Validation Accuracy: 59.75%\n",
            "Epoch [9/10], Training Loss: 0.824, Validation Accuracy: 59.44%\n",
            "Epoch [10/10], Training Loss: 0.810, Validation Accuracy: 59.49%\n",
            "Epoch [1/10], Training Loss: 0.983, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.932, Validation Accuracy: 60.25%\n",
            "Epoch [3/10], Training Loss: 0.905, Validation Accuracy: 59.98%\n",
            "Epoch [4/10], Training Loss: 0.874, Validation Accuracy: 60.01%\n",
            "Epoch [5/10], Training Loss: 0.858, Validation Accuracy: 60.47%\n",
            "Epoch [6/10], Training Loss: 0.833, Validation Accuracy: 59.95%\n",
            "Epoch [7/10], Training Loss: 0.822, Validation Accuracy: 59.24%\n",
            "Epoch [8/10], Training Loss: 0.801, Validation Accuracy: 59.79%\n",
            "Epoch [9/10], Training Loss: 0.786, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.775, Validation Accuracy: 60.06%\n",
            "Epoch [1/10], Training Loss: 0.982, Validation Accuracy: 59.22%\n",
            "Epoch [2/10], Training Loss: 0.921, Validation Accuracy: 59.07%\n",
            "Epoch [3/10], Training Loss: 0.898, Validation Accuracy: 59.32%\n",
            "Epoch [4/10], Training Loss: 0.884, Validation Accuracy: 59.45%\n",
            "Epoch [5/10], Training Loss: 0.842, Validation Accuracy: 59.92%\n",
            "Epoch [6/10], Training Loss: 0.822, Validation Accuracy: 60.09%\n",
            "Epoch [7/10], Training Loss: 0.807, Validation Accuracy: 59.73%\n",
            "Epoch [8/10], Training Loss: 0.789, Validation Accuracy: 59.75%\n",
            "Epoch [9/10], Training Loss: 0.779, Validation Accuracy: 59.88%\n",
            "Epoch [10/10], Training Loss: 0.763, Validation Accuracy: 59.93%\n",
            "Epoch [1/10], Training Loss: 1.007, Validation Accuracy: 60.31%\n",
            "Epoch [2/10], Training Loss: 0.935, Validation Accuracy: 60.08%\n",
            "Epoch [3/10], Training Loss: 0.895, Validation Accuracy: 60.14%\n",
            "Epoch [4/10], Training Loss: 0.871, Validation Accuracy: 60.11%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 60.37%\n",
            "Epoch [6/10], Training Loss: 0.828, Validation Accuracy: 60.14%\n",
            "Epoch [7/10], Training Loss: 0.812, Validation Accuracy: 60.09%\n",
            "Epoch [8/10], Training Loss: 0.784, Validation Accuracy: 59.89%\n",
            "Epoch [9/10], Training Loss: 0.775, Validation Accuracy: 59.71%\n",
            "Epoch [10/10], Training Loss: 0.759, Validation Accuracy: 59.41%\n",
            "Epoch [1/10], Training Loss: 0.974, Validation Accuracy: 60.12%\n",
            "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 60.31%\n",
            "Epoch [3/10], Training Loss: 0.861, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.836, Validation Accuracy: 60.06%\n",
            "Epoch [5/10], Training Loss: 0.813, Validation Accuracy: 59.47%\n",
            "Epoch [6/10], Training Loss: 0.787, Validation Accuracy: 59.47%\n",
            "Epoch [7/10], Training Loss: 0.771, Validation Accuracy: 60.29%\n",
            "Epoch [8/10], Training Loss: 0.756, Validation Accuracy: 60.25%\n",
            "Epoch [9/10], Training Loss: 0.738, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.727, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 59.68%\n",
            "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.853, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 0.823, Validation Accuracy: 60.48%\n",
            "Epoch [5/10], Training Loss: 0.801, Validation Accuracy: 60.19%\n",
            "Epoch [6/10], Training Loss: 0.781, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.752, Validation Accuracy: 60.48%\n",
            "Epoch [8/10], Training Loss: 0.744, Validation Accuracy: 60.30%\n",
            "Epoch [9/10], Training Loss: 0.724, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.709, Validation Accuracy: 59.80%\n",
            "Epoch [1/10], Training Loss: 0.941, Validation Accuracy: 60.17%\n",
            "Epoch [2/10], Training Loss: 0.861, Validation Accuracy: 59.97%\n",
            "Epoch [3/10], Training Loss: 0.821, Validation Accuracy: 59.19%\n",
            "Epoch [4/10], Training Loss: 0.790, Validation Accuracy: 60.54%\n",
            "Epoch [5/10], Training Loss: 0.767, Validation Accuracy: 60.69%\n",
            "Epoch [6/10], Training Loss: 0.743, Validation Accuracy: 59.90%\n",
            "Epoch [7/10], Training Loss: 0.722, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.701, Validation Accuracy: 60.43%\n",
            "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 60.27%\n",
            "Epoch [10/10], Training Loss: 0.662, Validation Accuracy: 60.31%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 60.61%\n",
            "Epoch [2/10], Training Loss: 0.849, Validation Accuracy: 59.71%\n",
            "Epoch [3/10], Training Loss: 0.818, Validation Accuracy: 59.67%\n",
            "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 59.84%\n",
            "Epoch [5/10], Training Loss: 0.756, Validation Accuracy: 60.00%\n",
            "Epoch [6/10], Training Loss: 0.737, Validation Accuracy: 59.50%\n",
            "Epoch [7/10], Training Loss: 0.711, Validation Accuracy: 59.94%\n",
            "Epoch [8/10], Training Loss: 0.699, Validation Accuracy: 60.08%\n",
            "Epoch [9/10], Training Loss: 0.669, Validation Accuracy: 60.13%\n",
            "Epoch [10/10], Training Loss: 0.653, Validation Accuracy: 59.64%\n",
            "Epoch [1/10], Training Loss: 0.955, Validation Accuracy: 60.19%\n",
            "Epoch [2/10], Training Loss: 0.874, Validation Accuracy: 59.74%\n",
            "Epoch [3/10], Training Loss: 0.817, Validation Accuracy: 60.24%\n",
            "Epoch [4/10], Training Loss: 0.789, Validation Accuracy: 60.71%\n",
            "Epoch [5/10], Training Loss: 0.753, Validation Accuracy: 60.50%\n",
            "Epoch [6/10], Training Loss: 0.733, Validation Accuracy: 60.05%\n",
            "Epoch [7/10], Training Loss: 0.706, Validation Accuracy: 60.07%\n",
            "Epoch [8/10], Training Loss: 0.689, Validation Accuracy: 60.11%\n",
            "Epoch [9/10], Training Loss: 0.671, Validation Accuracy: 59.68%\n",
            "Epoch [10/10], Training Loss: 0.652, Validation Accuracy: 59.75%\n",
            "Epoch [1/10], Training Loss: 0.921, Validation Accuracy: 59.75%\n",
            "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 59.84%\n",
            "Epoch [3/10], Training Loss: 0.783, Validation Accuracy: 60.36%\n",
            "Epoch [4/10], Training Loss: 0.755, Validation Accuracy: 60.29%\n",
            "Epoch [5/10], Training Loss: 0.726, Validation Accuracy: 60.30%\n",
            "Epoch [6/10], Training Loss: 0.703, Validation Accuracy: 60.41%\n",
            "Epoch [7/10], Training Loss: 0.674, Validation Accuracy: 59.91%\n",
            "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 59.99%\n",
            "Epoch [9/10], Training Loss: 0.632, Validation Accuracy: 60.09%\n",
            "Epoch [10/10], Training Loss: 0.617, Validation Accuracy: 59.01%\n",
            "Epoch [1/10], Training Loss: 0.931, Validation Accuracy: 60.25%\n",
            "Epoch [2/10], Training Loss: 0.824, Validation Accuracy: 60.71%\n",
            "Epoch [3/10], Training Loss: 0.782, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.740, Validation Accuracy: 60.64%\n",
            "Epoch [5/10], Training Loss: 0.719, Validation Accuracy: 60.36%\n",
            "Epoch [6/10], Training Loss: 0.697, Validation Accuracy: 60.66%\n",
            "Epoch [7/10], Training Loss: 0.670, Validation Accuracy: 60.85%\n",
            "Epoch [8/10], Training Loss: 0.644, Validation Accuracy: 60.84%\n",
            "Epoch [9/10], Training Loss: 0.632, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.613, Validation Accuracy: 59.68%\n",
            "Epoch [1/10], Training Loss: 0.886, Validation Accuracy: 60.15%\n",
            "Epoch [2/10], Training Loss: 0.790, Validation Accuracy: 59.78%\n",
            "Epoch [3/10], Training Loss: 0.746, Validation Accuracy: 60.39%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.674, Validation Accuracy: 61.15%\n",
            "Epoch [6/10], Training Loss: 0.651, Validation Accuracy: 60.33%\n",
            "Epoch [7/10], Training Loss: 0.631, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.603, Validation Accuracy: 60.83%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 59.76%\n",
            "Epoch [10/10], Training Loss: 0.573, Validation Accuracy: 59.63%\n",
            "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 59.60%\n",
            "Epoch [2/10], Training Loss: 0.798, Validation Accuracy: 59.64%\n",
            "Epoch [3/10], Training Loss: 0.745, Validation Accuracy: 60.35%\n",
            "Epoch [4/10], Training Loss: 0.699, Validation Accuracy: 60.18%\n",
            "Epoch [5/10], Training Loss: 0.668, Validation Accuracy: 60.54%\n",
            "Epoch [6/10], Training Loss: 0.640, Validation Accuracy: 59.96%\n",
            "Epoch [7/10], Training Loss: 0.621, Validation Accuracy: 60.14%\n",
            "Epoch [8/10], Training Loss: 0.602, Validation Accuracy: 60.03%\n",
            "Epoch [9/10], Training Loss: 0.579, Validation Accuracy: 60.32%\n",
            "Epoch [10/10], Training Loss: 0.559, Validation Accuracy: 59.73%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 60.08%\n",
            "Epoch [2/10], Training Loss: 0.797, Validation Accuracy: 60.70%\n",
            "Epoch [3/10], Training Loss: 0.752, Validation Accuracy: 60.22%\n",
            "Epoch [4/10], Training Loss: 0.704, Validation Accuracy: 60.30%\n",
            "Epoch [5/10], Training Loss: 0.665, Validation Accuracy: 59.30%\n",
            "Epoch [6/10], Training Loss: 0.650, Validation Accuracy: 60.23%\n",
            "Epoch [7/10], Training Loss: 0.621, Validation Accuracy: 60.34%\n",
            "Epoch [8/10], Training Loss: 0.595, Validation Accuracy: 60.04%\n",
            "Epoch [9/10], Training Loss: 0.578, Validation Accuracy: 59.64%\n",
            "Epoch [10/10], Training Loss: 0.551, Validation Accuracy: 59.98%\n",
            "Epoch [1/10], Training Loss: 0.876, Validation Accuracy: 60.04%\n",
            "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 59.19%\n",
            "Epoch [3/10], Training Loss: 0.724, Validation Accuracy: 60.26%\n",
            "Epoch [4/10], Training Loss: 0.677, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.636, Validation Accuracy: 60.35%\n",
            "Epoch [6/10], Training Loss: 0.613, Validation Accuracy: 60.45%\n",
            "Epoch [7/10], Training Loss: 0.591, Validation Accuracy: 60.17%\n",
            "Epoch [8/10], Training Loss: 0.562, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 60.23%\n",
            "Epoch [10/10], Training Loss: 0.517, Validation Accuracy: 59.79%\n",
            "Epoch [1/10], Training Loss: 0.867, Validation Accuracy: 60.38%\n",
            "Epoch [2/10], Training Loss: 0.757, Validation Accuracy: 60.80%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 59.96%\n",
            "Epoch [4/10], Training Loss: 0.659, Validation Accuracy: 60.96%\n",
            "Epoch [5/10], Training Loss: 0.629, Validation Accuracy: 61.09%\n",
            "Epoch [6/10], Training Loss: 0.597, Validation Accuracy: 60.23%\n",
            "Epoch [7/10], Training Loss: 0.575, Validation Accuracy: 60.64%\n",
            "Epoch [8/10], Training Loss: 0.551, Validation Accuracy: 60.76%\n",
            "Epoch [9/10], Training Loss: 0.540, Validation Accuracy: 60.33%\n",
            "Epoch [10/10], Training Loss: 0.524, Validation Accuracy: 59.98%\n",
            "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 60.19%\n",
            "Epoch [2/10], Training Loss: 0.732, Validation Accuracy: 60.71%\n",
            "Epoch [3/10], Training Loss: 0.672, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.628, Validation Accuracy: 60.52%\n",
            "Epoch [5/10], Training Loss: 0.600, Validation Accuracy: 60.87%\n",
            "Epoch [6/10], Training Loss: 0.560, Validation Accuracy: 59.75%\n",
            "Epoch [7/10], Training Loss: 0.549, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.517, Validation Accuracy: 60.46%\n",
            "Epoch [9/10], Training Loss: 0.502, Validation Accuracy: 59.72%\n",
            "Epoch [10/10], Training Loss: 0.489, Validation Accuracy: 59.96%\n",
            "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 59.29%\n",
            "Epoch [2/10], Training Loss: 0.718, Validation Accuracy: 60.06%\n",
            "Epoch [3/10], Training Loss: 0.663, Validation Accuracy: 60.36%\n",
            "Epoch [4/10], Training Loss: 0.615, Validation Accuracy: 59.61%\n",
            "Epoch [5/10], Training Loss: 0.579, Validation Accuracy: 59.97%\n",
            "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 59.95%\n",
            "Epoch [7/10], Training Loss: 0.520, Validation Accuracy: 60.33%\n",
            "Epoch [8/10], Training Loss: 0.506, Validation Accuracy: 59.15%\n",
            "Epoch [9/10], Training Loss: 0.488, Validation Accuracy: 59.99%\n",
            "Epoch [10/10], Training Loss: 0.466, Validation Accuracy: 58.82%\n",
            "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 0.720, Validation Accuracy: 60.66%\n",
            "Epoch [3/10], Training Loss: 0.673, Validation Accuracy: 60.17%\n",
            "Epoch [4/10], Training Loss: 0.622, Validation Accuracy: 60.37%\n",
            "Epoch [5/10], Training Loss: 0.588, Validation Accuracy: 60.78%\n",
            "Epoch [6/10], Training Loss: 0.556, Validation Accuracy: 59.67%\n",
            "Epoch [7/10], Training Loss: 0.527, Validation Accuracy: 60.10%\n",
            "Epoch [8/10], Training Loss: 0.503, Validation Accuracy: 59.83%\n",
            "Epoch [9/10], Training Loss: 0.482, Validation Accuracy: 59.44%\n",
            "Epoch [10/10], Training Loss: 0.471, Validation Accuracy: 59.45%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 59.01%\n",
            "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.638, Validation Accuracy: 60.19%\n",
            "Epoch [4/10], Training Loss: 0.591, Validation Accuracy: 60.54%\n",
            "Epoch [5/10], Training Loss: 0.564, Validation Accuracy: 60.03%\n",
            "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 60.08%\n",
            "Epoch [7/10], Training Loss: 0.497, Validation Accuracy: 60.00%\n",
            "Epoch [8/10], Training Loss: 0.465, Validation Accuracy: 59.91%\n",
            "Epoch [9/10], Training Loss: 0.449, Validation Accuracy: 60.34%\n",
            "Epoch [10/10], Training Loss: 0.426, Validation Accuracy: 59.73%\n",
            "Epoch [1/10], Training Loss: 0.844, Validation Accuracy: 59.74%\n",
            "Epoch [2/10], Training Loss: 0.708, Validation Accuracy: 60.26%\n",
            "Epoch [3/10], Training Loss: 0.624, Validation Accuracy: 60.91%\n",
            "Epoch [4/10], Training Loss: 0.581, Validation Accuracy: 60.30%\n",
            "Epoch [5/10], Training Loss: 0.544, Validation Accuracy: 60.41%\n",
            "Epoch [6/10], Training Loss: 0.518, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.481, Validation Accuracy: 60.26%\n",
            "Epoch [8/10], Training Loss: 0.457, Validation Accuracy: 60.57%\n",
            "Epoch [9/10], Training Loss: 0.441, Validation Accuracy: 59.81%\n",
            "Epoch [10/10], Training Loss: 0.419, Validation Accuracy: 59.87%\n",
            "Confusion Matrix:\n",
            "[[683  30  42  28  24  12  11  11 112  47]\n",
            " [ 31 736   8  17   8  13  20  12  46 109]\n",
            " [ 94   7 424 108  80 141  72  40  22  12]\n",
            " [ 31  15  61 431  56 257  67  47  18  17]\n",
            " [ 41   7  89  84 501  76  91  81  23   7]\n",
            " [ 18   5  45 173  50 584  42  58  14  11]\n",
            " [  5  14  43  91  47  81 685  13   9  12]\n",
            " [ 38   7  31  61  43 100  18 669   6  27]\n",
            " [ 80  41  14  24   8  13   9   8 774  29]\n",
            " [ 58 144  14  30  10  23  19  28  55 619]]\n",
            "Test Accuracy: 61.06%\n",
            "True Positives (TP): [683 736 424 431 501 584 685 669 774 619]\n",
            "False Positives (FP): [396 270 347 616 326 716 349 298 305 271]\n",
            "True Negatives (TN): [8604 8730 8653 8384 8674 8284 8651 8702 8695 8729]\n",
            "False Negatives (FN): [317 264 576 569 499 416 315 331 226 381]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.63299351 0.73161034 0.54993515 0.41165234 0.60580411 0.44923077\n",
            " 0.66247582 0.6918304  0.71733086 0.69550562]\n",
            "Recall: [0.683 0.736 0.424 0.431 0.501 0.584 0.685 0.669 0.774 0.619]\n",
            "F1 Score: [0.65704666 0.7337986  0.47882552 0.42110405 0.54844007 0.50782609\n",
            " 0.67354966 0.68022369 0.74458874 0.65502646]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvNpwyo0kaU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yfTPTBqUoz2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 10.17%\n",
        "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 10.24%\n",
        "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 10.16%\n",
        "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 10.14%\n",
        "Epoch [5/10], Training Loss: 2.295, Validation Accuracy: 10.12%\n",
        "Epoch [6/10], Training Loss: 2.291, Validation Accuracy: 10.24%\n",
        "Epoch [7/10], Training Loss: 2.284, Validation Accuracy: 11.09%\n",
        "Epoch [8/10], Training Loss: 2.274, Validation Accuracy: 12.03%\n",
        "Epoch [9/10], Training Loss: 2.259, Validation Accuracy: 12.78%\n",
        "Epoch [10/10], Training Loss: 2.243, Validation Accuracy: 14.08%\n",
        "Epoch [1/10], Training Loss: 2.232, Validation Accuracy: 15.69%\n",
        "Epoch [2/10], Training Loss: 2.217, Validation Accuracy: 17.64%\n",
        "Epoch [3/10], Training Loss: 2.201, Validation Accuracy: 19.24%\n",
        "Epoch [4/10], Training Loss: 2.184, Validation Accuracy: 19.88%\n",
        "Epoch [5/10], Training Loss: 2.168, Validation Accuracy: 20.73%\n",
        "Epoch [6/10], Training Loss: 2.152, Validation Accuracy: 22.25%\n",
        "Epoch [7/10], Training Loss: 2.137, Validation Accuracy: 21.60%\n",
        "Epoch [8/10], Training Loss: 2.121, Validation Accuracy: 22.45%\n",
        "Epoch [9/10], Training Loss: 2.105, Validation Accuracy: 23.39%\n",
        "Epoch [10/10], Training Loss: 2.089, Validation Accuracy: 24.34%\n",
        "Epoch [1/10], Training Loss: 2.082, Validation Accuracy: 27.71%\n",
        "Epoch [2/10], Training Loss: 2.066, Validation Accuracy: 28.13%\n",
        "Epoch [3/10], Training Loss: 2.048, Validation Accuracy: 27.49%\n",
        "Epoch [4/10], Training Loss: 2.028, Validation Accuracy: 28.12%\n",
        "Epoch [5/10], Training Loss: 2.006, Validation Accuracy: 28.80%\n",
        "Epoch [6/10], Training Loss: 1.984, Validation Accuracy: 29.36%\n",
        "Epoch [7/10], Training Loss: 1.962, Validation Accuracy: 29.68%\n",
        "Epoch [8/10], Training Loss: 1.939, Validation Accuracy: 30.34%\n",
        "Epoch [9/10], Training Loss: 1.920, Validation Accuracy: 30.88%\n",
        "Epoch [10/10], Training Loss: 1.902, Validation Accuracy: 31.75%\n",
        "Epoch [1/10], Training Loss: 1.890, Validation Accuracy: 32.49%\n",
        "Epoch [2/10], Training Loss: 1.877, Validation Accuracy: 32.30%\n",
        "Epoch [3/10], Training Loss: 1.863, Validation Accuracy: 32.76%\n",
        "Epoch [4/10], Training Loss: 1.845, Validation Accuracy: 33.83%\n",
        "Epoch [5/10], Training Loss: 1.830, Validation Accuracy: 34.41%\n",
        "Epoch [6/10], Training Loss: 1.815, Validation Accuracy: 34.35%\n",
        "Epoch [7/10], Training Loss: 1.794, Validation Accuracy: 35.59%\n",
        "Epoch [8/10], Training Loss: 1.779, Validation Accuracy: 36.21%\n",
        "Epoch [9/10], Training Loss: 1.759, Validation Accuracy: 36.27%\n",
        "Epoch [10/10], Training Loss: 1.745, Validation Accuracy: 37.12%\n",
        "Epoch [1/10], Training Loss: 1.734, Validation Accuracy: 37.81%\n",
        "Epoch [2/10], Training Loss: 1.715, Validation Accuracy: 38.31%\n",
        "Epoch [3/10], Training Loss: 1.697, Validation Accuracy: 37.80%\n",
        "Epoch [4/10], Training Loss: 1.679, Validation Accuracy: 38.92%\n",
        "Epoch [5/10], Training Loss: 1.661, Validation Accuracy: 38.53%\n",
        "Epoch [6/10], Training Loss: 1.648, Validation Accuracy: 38.86%\n",
        "Epoch [7/10], Training Loss: 1.634, Validation Accuracy: 39.55%\n",
        "Epoch [8/10], Training Loss: 1.623, Validation Accuracy: 39.75%\n",
        "Epoch [9/10], Training Loss: 1.607, Validation Accuracy: 40.67%\n",
        "Epoch [10/10], Training Loss: 1.594, Validation Accuracy: 40.43%\n",
        "Epoch [1/10], Training Loss: 1.639, Validation Accuracy: 40.93%\n",
        "Epoch [2/10], Training Loss: 1.618, Validation Accuracy: 41.43%\n",
        "Epoch [3/10], Training Loss: 1.608, Validation Accuracy: 41.32%\n",
        "Epoch [4/10], Training Loss: 1.596, Validation Accuracy: 41.16%\n",
        "Epoch [5/10], Training Loss: 1.586, Validation Accuracy: 41.67%\n",
        "Epoch [6/10], Training Loss: 1.580, Validation Accuracy: 42.35%\n",
        "Epoch [7/10], Training Loss: 1.570, Validation Accuracy: 42.56%\n",
        "Epoch [8/10], Training Loss: 1.561, Validation Accuracy: 42.47%\n",
        "Epoch [9/10], Training Loss: 1.554, Validation Accuracy: 42.86%\n",
        "Epoch [10/10], Training Loss: 1.543, Validation Accuracy: 42.42%\n",
        "Epoch [1/10], Training Loss: 1.550, Validation Accuracy: 43.98%\n",
        "Epoch [2/10], Training Loss: 1.539, Validation Accuracy: 43.37%\n",
        "Epoch [3/10], Training Loss: 1.531, Validation Accuracy: 43.02%\n",
        "Epoch [4/10], Training Loss: 1.519, Validation Accuracy: 44.81%\n",
        "Epoch [5/10], Training Loss: 1.506, Validation Accuracy: 44.49%\n",
        "Epoch [6/10], Training Loss: 1.501, Validation Accuracy: 44.85%\n",
        "Epoch [7/10], Training Loss: 1.493, Validation Accuracy: 44.86%\n",
        "Epoch [8/10], Training Loss: 1.486, Validation Accuracy: 44.82%\n",
        "Epoch [9/10], Training Loss: 1.473, Validation Accuracy: 45.09%\n",
        "Epoch [10/10], Training Loss: 1.469, Validation Accuracy: 45.14%\n",
        "Epoch [1/10], Training Loss: 1.506, Validation Accuracy: 45.68%\n",
        "Epoch [2/10], Training Loss: 1.497, Validation Accuracy: 46.31%\n",
        "Epoch [3/10], Training Loss: 1.485, Validation Accuracy: 44.64%\n",
        "Epoch [4/10], Training Loss: 1.481, Validation Accuracy: 45.50%\n",
        "Epoch [5/10], Training Loss: 1.468, Validation Accuracy: 46.86%\n",
        "Epoch [6/10], Training Loss: 1.461, Validation Accuracy: 46.66%\n",
        "Epoch [7/10], Training Loss: 1.457, Validation Accuracy: 46.80%\n",
        "Epoch [8/10], Training Loss: 1.442, Validation Accuracy: 46.50%\n",
        "Epoch [9/10], Training Loss: 1.445, Validation Accuracy: 47.59%\n",
        "Epoch [10/10], Training Loss: 1.429, Validation Accuracy: 47.65%\n",
        "Epoch [1/10], Training Loss: 1.452, Validation Accuracy: 46.99%\n",
        "Epoch [2/10], Training Loss: 1.434, Validation Accuracy: 47.80%\n",
        "Epoch [3/10], Training Loss: 1.425, Validation Accuracy: 47.92%\n",
        "Epoch [4/10], Training Loss: 1.417, Validation Accuracy: 48.09%\n",
        "Epoch [5/10], Training Loss: 1.408, Validation Accuracy: 47.34%\n",
        "Epoch [6/10], Training Loss: 1.395, Validation Accuracy: 47.69%\n",
        "Epoch [7/10], Training Loss: 1.398, Validation Accuracy: 47.07%\n",
        "Epoch [8/10], Training Loss: 1.382, Validation Accuracy: 48.60%\n",
        "Epoch [9/10], Training Loss: 1.385, Validation Accuracy: 46.95%\n",
        "Epoch [10/10], Training Loss: 1.379, Validation Accuracy: 48.71%\n",
        "Epoch [1/10], Training Loss: 1.395, Validation Accuracy: 47.52%\n",
        "Epoch [2/10], Training Loss: 1.381, Validation Accuracy: 48.75%\n",
        "Epoch [3/10], Training Loss: 1.370, Validation Accuracy: 48.72%\n",
        "Epoch [4/10], Training Loss: 1.358, Validation Accuracy: 50.37%\n",
        "Epoch [5/10], Training Loss: 1.344, Validation Accuracy: 49.66%\n",
        "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 50.00%\n",
        "Epoch [7/10], Training Loss: 1.328, Validation Accuracy: 49.62%\n",
        "Epoch [8/10], Training Loss: 1.330, Validation Accuracy: 49.95%\n",
        "Epoch [9/10], Training Loss: 1.311, Validation Accuracy: 49.79%\n",
        "Epoch [10/10], Training Loss: 1.313, Validation Accuracy: 49.17%\n",
        "Epoch [1/10], Training Loss: 1.401, Validation Accuracy: 50.98%\n",
        "Epoch [2/10], Training Loss: 1.384, Validation Accuracy: 51.37%\n",
        "Epoch [3/10], Training Loss: 1.361, Validation Accuracy: 51.11%\n",
        "Epoch [4/10], Training Loss: 1.352, Validation Accuracy: 51.07%\n",
        "Epoch [5/10], Training Loss: 1.350, Validation Accuracy: 50.90%\n",
        "Epoch [6/10], Training Loss: 1.338, Validation Accuracy: 51.97%\n",
        "Epoch [7/10], Training Loss: 1.331, Validation Accuracy: 50.99%\n",
        "Epoch [8/10], Training Loss: 1.322, Validation Accuracy: 51.29%\n",
        "Epoch [9/10], Training Loss: 1.313, Validation Accuracy: 51.66%\n",
        "Epoch [10/10], Training Loss: 1.319, Validation Accuracy: 49.10%\n",
        "Epoch [1/10], Training Loss: 1.344, Validation Accuracy: 51.42%\n",
        "Epoch [2/10], Training Loss: 1.325, Validation Accuracy: 52.03%\n",
        "Epoch [3/10], Training Loss: 1.305, Validation Accuracy: 52.39%\n",
        "Epoch [4/10], Training Loss: 1.295, Validation Accuracy: 52.85%\n",
        "Epoch [5/10], Training Loss: 1.286, Validation Accuracy: 52.78%\n",
        "Epoch [6/10], Training Loss: 1.284, Validation Accuracy: 52.30%\n",
        "Epoch [7/10], Training Loss: 1.277, Validation Accuracy: 51.87%\n",
        "Epoch [8/10], Training Loss: 1.267, Validation Accuracy: 50.89%\n",
        "Epoch [9/10], Training Loss: 1.251, Validation Accuracy: 52.01%\n",
        "Epoch [10/10], Training Loss: 1.249, Validation Accuracy: 52.60%\n",
        "Epoch [1/10], Training Loss: 1.326, Validation Accuracy: 52.29%\n",
        "Epoch [2/10], Training Loss: 1.317, Validation Accuracy: 52.48%\n",
        "Epoch [3/10], Training Loss: 1.302, Validation Accuracy: 52.77%\n",
        "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 52.62%\n",
        "Epoch [5/10], Training Loss: 1.284, Validation Accuracy: 52.61%\n",
        "Epoch [6/10], Training Loss: 1.271, Validation Accuracy: 52.93%\n",
        "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 53.08%\n",
        "Epoch [8/10], Training Loss: 1.257, Validation Accuracy: 53.41%\n",
        "Epoch [9/10], Training Loss: 1.248, Validation Accuracy: 52.70%\n",
        "Epoch [10/10], Training Loss: 1.238, Validation Accuracy: 53.77%\n",
        "Epoch [1/10], Training Loss: 1.300, Validation Accuracy: 52.18%\n",
        "Epoch [2/10], Training Loss: 1.274, Validation Accuracy: 53.35%\n",
        "Epoch [3/10], Training Loss: 1.256, Validation Accuracy: 53.93%\n",
        "Epoch [4/10], Training Loss: 1.247, Validation Accuracy: 52.25%\n",
        "Epoch [5/10], Training Loss: 1.235, Validation Accuracy: 53.51%\n",
        "Epoch [6/10], Training Loss: 1.226, Validation Accuracy: 53.24%\n",
        "Epoch [7/10], Training Loss: 1.227, Validation Accuracy: 54.00%\n",
        "Epoch [8/10], Training Loss: 1.207, Validation Accuracy: 54.61%\n",
        "Epoch [9/10], Training Loss: 1.200, Validation Accuracy: 53.91%\n",
        "Epoch [10/10], Training Loss: 1.193, Validation Accuracy: 54.12%\n",
        "Epoch [1/10], Training Loss: 1.245, Validation Accuracy: 54.70%\n",
        "Epoch [2/10], Training Loss: 1.223, Validation Accuracy: 54.88%\n",
        "Epoch [3/10], Training Loss: 1.212, Validation Accuracy: 54.29%\n",
        "Epoch [4/10], Training Loss: 1.194, Validation Accuracy: 55.00%\n",
        "Epoch [5/10], Training Loss: 1.189, Validation Accuracy: 54.32%\n",
        "Epoch [6/10], Training Loss: 1.173, Validation Accuracy: 53.69%\n",
        "Epoch [7/10], Training Loss: 1.165, Validation Accuracy: 53.73%\n",
        "Epoch [8/10], Training Loss: 1.155, Validation Accuracy: 54.49%\n",
        "Epoch [9/10], Training Loss: 1.145, Validation Accuracy: 55.24%\n",
        "Epoch [10/10], Training Loss: 1.142, Validation Accuracy: 54.82%\n",
        "Epoch [1/10], Training Loss: 1.268, Validation Accuracy: 55.58%\n",
        "Epoch [2/10], Training Loss: 1.238, Validation Accuracy: 55.25%\n",
        "Epoch [3/10], Training Loss: 1.217, Validation Accuracy: 55.81%\n",
        "Epoch [4/10], Training Loss: 1.204, Validation Accuracy: 55.75%\n",
        "Epoch [5/10], Training Loss: 1.190, Validation Accuracy: 55.15%\n",
        "Epoch [6/10], Training Loss: 1.183, Validation Accuracy: 56.25%\n",
        "Epoch [7/10], Training Loss: 1.171, Validation Accuracy: 55.15%\n",
        "Epoch [8/10], Training Loss: 1.162, Validation Accuracy: 55.55%\n",
        "Epoch [9/10], Training Loss: 1.154, Validation Accuracy: 55.53%\n",
        "Epoch [10/10], Training Loss: 1.146, Validation Accuracy: 56.14%\n",
        "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 55.82%\n",
        "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 56.40%\n",
        "Epoch [3/10], Training Loss: 1.173, Validation Accuracy: 56.66%\n",
        "Epoch [4/10], Training Loss: 1.151, Validation Accuracy: 55.91%\n",
        "Epoch [5/10], Training Loss: 1.143, Validation Accuracy: 56.48%\n",
        "Epoch [6/10], Training Loss: 1.139, Validation Accuracy: 56.37%\n",
        "Epoch [7/10], Training Loss: 1.126, Validation Accuracy: 56.08%\n",
        "Epoch [8/10], Training Loss: 1.111, Validation Accuracy: 56.79%\n",
        "Epoch [9/10], Training Loss: 1.107, Validation Accuracy: 56.20%\n",
        "Epoch [10/10], Training Loss: 1.091, Validation Accuracy: 56.60%\n",
        "Epoch [1/10], Training Loss: 1.214, Validation Accuracy: 56.96%\n",
        "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 56.07%\n",
        "Epoch [3/10], Training Loss: 1.170, Validation Accuracy: 57.41%\n",
        "Epoch [4/10], Training Loss: 1.152, Validation Accuracy: 56.71%\n",
        "Epoch [5/10], Training Loss: 1.142, Validation Accuracy: 56.14%\n",
        "Epoch [6/10], Training Loss: 1.135, Validation Accuracy: 57.20%\n",
        "Epoch [7/10], Training Loss: 1.119, Validation Accuracy: 57.25%\n",
        "Epoch [8/10], Training Loss: 1.108, Validation Accuracy: 57.28%\n",
        "Epoch [9/10], Training Loss: 1.101, Validation Accuracy: 57.02%\n",
        "Epoch [10/10], Training Loss: 1.086, Validation Accuracy: 57.43%\n",
        "Epoch [1/10], Training Loss: 1.189, Validation Accuracy: 57.45%\n",
        "Epoch [2/10], Training Loss: 1.158, Validation Accuracy: 58.15%\n",
        "Epoch [3/10], Training Loss: 1.143, Validation Accuracy: 58.04%\n",
        "Epoch [4/10], Training Loss: 1.120, Validation Accuracy: 57.35%\n",
        "Epoch [5/10], Training Loss: 1.109, Validation Accuracy: 57.40%\n",
        "Epoch [6/10], Training Loss: 1.096, Validation Accuracy: 57.20%\n",
        "Epoch [7/10], Training Loss: 1.082, Validation Accuracy: 56.95%\n",
        "Epoch [8/10], Training Loss: 1.069, Validation Accuracy: 56.99%\n",
        "Epoch [9/10], Training Loss: 1.069, Validation Accuracy: 57.48%\n",
        "Epoch [10/10], Training Loss: 1.064, Validation Accuracy: 57.74%\n",
        "Epoch [1/10], Training Loss: 1.149, Validation Accuracy: 57.60%\n",
        "Epoch [2/10], Training Loss: 1.109, Validation Accuracy: 57.95%\n",
        "Epoch [3/10], Training Loss: 1.086, Validation Accuracy: 58.11%\n",
        "Epoch [4/10], Training Loss: 1.082, Validation Accuracy: 58.06%\n",
        "Epoch [5/10], Training Loss: 1.063, Validation Accuracy: 57.10%\n",
        "Epoch [6/10], Training Loss: 1.052, Validation Accuracy: 57.33%\n",
        "Epoch [7/10], Training Loss: 1.036, Validation Accuracy: 58.53%\n",
        "Epoch [8/10], Training Loss: 1.021, Validation Accuracy: 58.43%\n",
        "Epoch [9/10], Training Loss: 1.012, Validation Accuracy: 57.59%\n",
        "Epoch [10/10], Training Loss: 1.001, Validation Accuracy: 57.31%\n",
        "Epoch [1/10], Training Loss: 1.161, Validation Accuracy: 58.03%\n",
        "Epoch [2/10], Training Loss: 1.128, Validation Accuracy: 58.59%\n",
        "Epoch [3/10], Training Loss: 1.106, Validation Accuracy: 58.09%\n",
        "Epoch [4/10], Training Loss: 1.083, Validation Accuracy: 58.70%\n",
        "Epoch [5/10], Training Loss: 1.070, Validation Accuracy: 58.12%\n",
        "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.97%\n",
        "Epoch [7/10], Training Loss: 1.049, Validation Accuracy: 58.49%\n",
        "Epoch [8/10], Training Loss: 1.035, Validation Accuracy: 58.37%\n",
        "Epoch [9/10], Training Loss: 1.028, Validation Accuracy: 58.39%\n",
        "Epoch [10/10], Training Loss: 1.016, Validation Accuracy: 58.40%\n",
        "Epoch [1/10], Training Loss: 1.121, Validation Accuracy: 58.49%\n",
        "Epoch [2/10], Training Loss: 1.085, Validation Accuracy: 58.50%\n",
        "Epoch [3/10], Training Loss: 1.059, Validation Accuracy: 58.46%\n",
        "Epoch [4/10], Training Loss: 1.053, Validation Accuracy: 58.79%\n",
        "Epoch [5/10], Training Loss: 1.025, Validation Accuracy: 59.09%\n",
        "Epoch [6/10], Training Loss: 1.018, Validation Accuracy: 58.97%\n",
        "Epoch [7/10], Training Loss: 0.996, Validation Accuracy: 58.70%\n",
        "Epoch [8/10], Training Loss: 0.987, Validation Accuracy: 58.42%\n",
        "Epoch [9/10], Training Loss: 0.972, Validation Accuracy: 58.51%\n",
        "Epoch [10/10], Training Loss: 0.963, Validation Accuracy: 58.35%\n",
        "Epoch [1/10], Training Loss: 1.132, Validation Accuracy: 57.56%\n",
        "Epoch [2/10], Training Loss: 1.095, Validation Accuracy: 59.33%\n",
        "Epoch [3/10], Training Loss: 1.061, Validation Accuracy: 58.23%\n",
        "Epoch [4/10], Training Loss: 1.038, Validation Accuracy: 58.99%\n",
        "Epoch [5/10], Training Loss: 1.024, Validation Accuracy: 59.32%\n",
        "Epoch [6/10], Training Loss: 1.019, Validation Accuracy: 59.17%\n",
        "Epoch [7/10], Training Loss: 1.000, Validation Accuracy: 58.38%\n",
        "Epoch [8/10], Training Loss: 0.992, Validation Accuracy: 59.06%\n",
        "Epoch [9/10], Training Loss: 0.979, Validation Accuracy: 59.38%\n",
        "Epoch [10/10], Training Loss: 0.968, Validation Accuracy: 59.12%\n",
        "Epoch [1/10], Training Loss: 1.108, Validation Accuracy: 59.91%\n",
        "Epoch [2/10], Training Loss: 1.058, Validation Accuracy: 59.51%\n",
        "Epoch [3/10], Training Loss: 1.048, Validation Accuracy: 59.60%\n",
        "Epoch [4/10], Training Loss: 1.010, Validation Accuracy: 59.51%\n",
        "Epoch [5/10], Training Loss: 0.998, Validation Accuracy: 59.51%\n",
        "Epoch [6/10], Training Loss: 0.990, Validation Accuracy: 59.49%\n",
        "Epoch [7/10], Training Loss: 0.970, Validation Accuracy: 59.00%\n",
        "Epoch [8/10], Training Loss: 0.951, Validation Accuracy: 59.56%\n",
        "Epoch [9/10], Training Loss: 0.939, Validation Accuracy: 59.66%\n",
        "Epoch [10/10], Training Loss: 0.933, Validation Accuracy: 58.87%\n",
        "Epoch [1/10], Training Loss: 1.054, Validation Accuracy: 59.66%\n",
        "Epoch [2/10], Training Loss: 1.020, Validation Accuracy: 59.64%\n",
        "Epoch [3/10], Training Loss: 0.989, Validation Accuracy: 59.59%\n",
        "Epoch [4/10], Training Loss: 0.967, Validation Accuracy: 60.24%\n",
        "Epoch [5/10], Training Loss: 0.941, Validation Accuracy: 59.93%\n",
        "Epoch [6/10], Training Loss: 0.930, Validation Accuracy: 59.69%\n",
        "Epoch [7/10], Training Loss: 0.919, Validation Accuracy: 59.87%\n",
        "Epoch [8/10], Training Loss: 0.900, Validation Accuracy: 58.85%\n",
        "Epoch [9/10], Training Loss: 0.889, Validation Accuracy: 59.52%\n",
        "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 59.45%\n",
        "Epoch [1/10], Training Loss: 1.083, Validation Accuracy: 59.64%\n",
        "Epoch [2/10], Training Loss: 1.044, Validation Accuracy: 59.64%\n",
        "Epoch [3/10], Training Loss: 1.016, Validation Accuracy: 59.82%\n",
        "Epoch [4/10], Training Loss: 0.996, Validation Accuracy: 59.09%\n",
        "Epoch [5/10], Training Loss: 0.975, Validation Accuracy: 59.20%\n",
        "Epoch [6/10], Training Loss: 0.960, Validation Accuracy: 60.20%\n",
        "Epoch [7/10], Training Loss: 0.945, Validation Accuracy: 59.93%\n",
        "Epoch [8/10], Training Loss: 0.926, Validation Accuracy: 59.52%\n",
        "Epoch [9/10], Training Loss: 0.917, Validation Accuracy: 59.53%\n",
        "Epoch [10/10], Training Loss: 0.901, Validation Accuracy: 59.75%\n",
        "Epoch [1/10], Training Loss: 1.039, Validation Accuracy: 60.36%\n",
        "Epoch [2/10], Training Loss: 0.989, Validation Accuracy: 60.13%\n",
        "Epoch [3/10], Training Loss: 0.964, Validation Accuracy: 59.62%\n",
        "Epoch [4/10], Training Loss: 0.941, Validation Accuracy: 60.02%\n",
        "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 59.46%\n",
        "Epoch [6/10], Training Loss: 0.908, Validation Accuracy: 59.33%\n",
        "Epoch [7/10], Training Loss: 0.899, Validation Accuracy: 59.47%\n",
        "Epoch [8/10], Training Loss: 0.880, Validation Accuracy: 60.00%\n",
        "Epoch [9/10], Training Loss: 0.855, Validation Accuracy: 59.58%\n",
        "Epoch [10/10], Training Loss: 0.843, Validation Accuracy: 59.94%\n",
        "Epoch [1/10], Training Loss: 1.058, Validation Accuracy: 59.44%\n",
        "Epoch [2/10], Training Loss: 1.006, Validation Accuracy: 60.68%\n",
        "Epoch [3/10], Training Loss: 0.969, Validation Accuracy: 60.58%\n",
        "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 60.16%\n",
        "Epoch [5/10], Training Loss: 0.931, Validation Accuracy: 59.64%\n",
        "Epoch [6/10], Training Loss: 0.914, Validation Accuracy: 60.08%\n",
        "Epoch [7/10], Training Loss: 0.888, Validation Accuracy: 60.49%\n",
        "Epoch [8/10], Training Loss: 0.879, Validation Accuracy: 60.26%\n",
        "Epoch [9/10], Training Loss: 0.876, Validation Accuracy: 60.08%\n",
        "Epoch [10/10], Training Loss: 0.847, Validation Accuracy: 60.45%\n",
        "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 60.23%\n",
        "Epoch [2/10], Training Loss: 0.982, Validation Accuracy: 60.01%\n",
        "Epoch [3/10], Training Loss: 0.944, Validation Accuracy: 60.26%\n",
        "Epoch [4/10], Training Loss: 0.928, Validation Accuracy: 60.41%\n",
        "Epoch [5/10], Training Loss: 0.910, Validation Accuracy: 60.20%\n",
        "Epoch [6/10], Training Loss: 0.888, Validation Accuracy: 59.89%\n",
        "Epoch [7/10], Training Loss: 0.864, Validation Accuracy: 60.42%\n",
        "Epoch [8/10], Training Loss: 0.855, Validation Accuracy: 59.98%\n",
        "Epoch [9/10], Training Loss: 0.841, Validation Accuracy: 60.05%\n",
        "Epoch [10/10], Training Loss: 0.824, Validation Accuracy: 59.50%\n",
        "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 59.41%\n",
        "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 60.51%\n",
        "Epoch [3/10], Training Loss: 0.899, Validation Accuracy: 60.24%\n",
        "Epoch [4/10], Training Loss: 0.875, Validation Accuracy: 59.89%\n",
        "Epoch [5/10], Training Loss: 0.851, Validation Accuracy: 60.46%\n",
        "Epoch [6/10], Training Loss: 0.834, Validation Accuracy: 60.45%\n",
        "Epoch [7/10], Training Loss: 0.817, Validation Accuracy: 59.87%\n",
        "Epoch [8/10], Training Loss: 0.794, Validation Accuracy: 59.05%\n",
        "Epoch [9/10], Training Loss: 0.781, Validation Accuracy: 60.17%\n",
        "Epoch [10/10], Training Loss: 0.767, Validation Accuracy: 60.05%\n",
        "Epoch [1/10], Training Loss: 1.030, Validation Accuracy: 60.51%\n",
        "Epoch [2/10], Training Loss: 0.966, Validation Accuracy: 60.02%\n",
        "Epoch [3/10], Training Loss: 0.924, Validation Accuracy: 60.54%\n",
        "Epoch [4/10], Training Loss: 0.907, Validation Accuracy: 59.31%\n",
        "Epoch [5/10], Training Loss: 0.881, Validation Accuracy: 59.63%\n",
        "Epoch [6/10], Training Loss: 0.859, Validation Accuracy: 59.81%\n",
        "Epoch [7/10], Training Loss: 0.842, Validation Accuracy: 59.72%\n",
        "Epoch [8/10], Training Loss: 0.822, Validation Accuracy: 60.25%\n",
        "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 60.20%\n",
        "Epoch [10/10], Training Loss: 0.793, Validation Accuracy: 60.43%\n",
        "Epoch [1/10], Training Loss: 0.977, Validation Accuracy: 60.45%\n",
        "Epoch [2/10], Training Loss: 0.913, Validation Accuracy: 61.05%\n",
        "Epoch [3/10], Training Loss: 0.877, Validation Accuracy: 60.51%\n",
        "Epoch [4/10], Training Loss: 0.846, Validation Accuracy: 60.68%\n",
        "Epoch [5/10], Training Loss: 0.836, Validation Accuracy: 60.13%\n",
        "Epoch [6/10], Training Loss: 0.799, Validation Accuracy: 60.87%\n",
        "Epoch [7/10], Training Loss: 0.795, Validation Accuracy: 60.48%\n",
        "Epoch [8/10], Training Loss: 0.768, Validation Accuracy: 59.92%\n",
        "Epoch [9/10], Training Loss: 0.756, Validation Accuracy: 59.54%\n",
        "Epoch [10/10], Training Loss: 0.741, Validation Accuracy: 60.07%\n",
        "Epoch [1/10], Training Loss: 1.000, Validation Accuracy: 59.77%\n",
        "Epoch [2/10], Training Loss: 0.930, Validation Accuracy: 60.16%\n",
        "Epoch [3/10], Training Loss: 0.893, Validation Accuracy: 60.95%\n",
        "Epoch [4/10], Training Loss: 0.864, Validation Accuracy: 61.28%\n",
        "Epoch [5/10], Training Loss: 0.834, Validation Accuracy: 60.79%\n",
        "Epoch [6/10], Training Loss: 0.808, Validation Accuracy: 60.95%\n",
        "Epoch [7/10], Training Loss: 0.797, Validation Accuracy: 61.02%\n",
        "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.99%\n",
        "Epoch [9/10], Training Loss: 0.763, Validation Accuracy: 60.78%\n",
        "Epoch [10/10], Training Loss: 0.740, Validation Accuracy: 60.62%\n",
        "Epoch [1/10], Training Loss: 0.979, Validation Accuracy: 60.17%\n",
        "Epoch [2/10], Training Loss: 0.907, Validation Accuracy: 60.89%\n",
        "Epoch [3/10], Training Loss: 0.875, Validation Accuracy: 61.28%\n",
        "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 60.99%\n",
        "Epoch [5/10], Training Loss: 0.816, Validation Accuracy: 60.89%\n",
        "Epoch [6/10], Training Loss: 0.795, Validation Accuracy: 61.01%\n",
        "Epoch [7/10], Training Loss: 0.776, Validation Accuracy: 61.29%\n",
        "Epoch [8/10], Training Loss: 0.752, Validation Accuracy: 60.45%\n",
        "Epoch [9/10], Training Loss: 0.735, Validation Accuracy: 60.28%\n",
        "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 59.78%\n",
        "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 60.89%\n",
        "Epoch [2/10], Training Loss: 0.875, Validation Accuracy: 61.15%\n",
        "Epoch [3/10], Training Loss: 0.821, Validation Accuracy: 60.44%\n",
        "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.15%\n",
        "Epoch [5/10], Training Loss: 0.769, Validation Accuracy: 61.36%\n",
        "Epoch [6/10], Training Loss: 0.738, Validation Accuracy: 60.91%\n",
        "Epoch [7/10], Training Loss: 0.724, Validation Accuracy: 60.19%\n",
        "Epoch [8/10], Training Loss: 0.697, Validation Accuracy: 60.69%\n",
        "Epoch [9/10], Training Loss: 0.673, Validation Accuracy: 60.23%\n",
        "Epoch [10/10], Training Loss: 0.661, Validation Accuracy: 60.16%\n",
        "Epoch [1/10], Training Loss: 0.975, Validation Accuracy: 60.62%\n",
        "Epoch [2/10], Training Loss: 0.899, Validation Accuracy: 60.32%\n",
        "Epoch [3/10], Training Loss: 0.854, Validation Accuracy: 61.46%\n",
        "Epoch [4/10], Training Loss: 0.820, Validation Accuracy: 60.97%\n",
        "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 60.20%\n",
        "Epoch [6/10], Training Loss: 0.768, Validation Accuracy: 60.92%\n",
        "Epoch [7/10], Training Loss: 0.750, Validation Accuracy: 60.07%\n",
        "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 60.51%\n",
        "Epoch [9/10], Training Loss: 0.707, Validation Accuracy: 59.95%\n",
        "Epoch [10/10], Training Loss: 0.696, Validation Accuracy: 60.05%\n",
        "Epoch [1/10], Training Loss: 0.924, Validation Accuracy: 59.99%\n",
        "Epoch [2/10], Training Loss: 0.862, Validation Accuracy: 60.82%\n",
        "Epoch [3/10], Training Loss: 0.804, Validation Accuracy: 60.53%\n",
        "Epoch [4/10], Training Loss: 0.771, Validation Accuracy: 60.60%\n",
        "Epoch [5/10], Training Loss: 0.744, Validation Accuracy: 60.34%\n",
        "Epoch [6/10], Training Loss: 0.717, Validation Accuracy: 60.87%\n",
        "Epoch [7/10], Training Loss: 0.695, Validation Accuracy: 60.95%\n",
        "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 60.52%\n",
        "Epoch [9/10], Training Loss: 0.654, Validation Accuracy: 60.11%\n",
        "Epoch [10/10], Training Loss: 0.634, Validation Accuracy: 60.18%\n",
        "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 60.37%\n",
        "Epoch [2/10], Training Loss: 0.851, Validation Accuracy: 60.88%\n",
        "Epoch [3/10], Training Loss: 0.805, Validation Accuracy: 60.58%\n",
        "Epoch [4/10], Training Loss: 0.770, Validation Accuracy: 61.33%\n",
        "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 60.35%\n",
        "Epoch [6/10], Training Loss: 0.715, Validation Accuracy: 60.46%\n",
        "Epoch [7/10], Training Loss: 0.688, Validation Accuracy: 61.35%\n",
        "Epoch [8/10], Training Loss: 0.665, Validation Accuracy: 61.09%\n",
        "Epoch [9/10], Training Loss: 0.652, Validation Accuracy: 61.31%\n",
        "Epoch [10/10], Training Loss: 0.629, Validation Accuracy: 61.21%\n",
        "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 61.01%\n",
        "Epoch [2/10], Training Loss: 0.837, Validation Accuracy: 61.52%\n",
        "Epoch [3/10], Training Loss: 0.783, Validation Accuracy: 61.39%\n",
        "Epoch [4/10], Training Loss: 0.755, Validation Accuracy: 60.52%\n",
        "Epoch [5/10], Training Loss: 0.723, Validation Accuracy: 61.21%\n",
        "Epoch [6/10], Training Loss: 0.703, Validation Accuracy: 61.30%\n",
        "Epoch [7/10], Training Loss: 0.673, Validation Accuracy: 61.16%\n",
        "Epoch [8/10], Training Loss: 0.650, Validation Accuracy: 60.22%\n",
        "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 60.57%\n",
        "Epoch [10/10], Training Loss: 0.613, Validation Accuracy: 60.86%\n",
        "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 60.27%\n",
        "Epoch [2/10], Training Loss: 0.799, Validation Accuracy: 60.16%\n",
        "Epoch [3/10], Training Loss: 0.742, Validation Accuracy: 61.33%\n",
        "Epoch [4/10], Training Loss: 0.696, Validation Accuracy: 60.50%\n",
        "Epoch [5/10], Training Loss: 0.686, Validation Accuracy: 60.48%\n",
        "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 60.93%\n",
        "Epoch [7/10], Training Loss: 0.620, Validation Accuracy: 60.92%\n",
        "Epoch [8/10], Training Loss: 0.601, Validation Accuracy: 60.26%\n",
        "Epoch [9/10], Training Loss: 0.592, Validation Accuracy: 59.47%\n",
        "Epoch [10/10], Training Loss: 0.557, Validation Accuracy: 59.89%\n",
        "Epoch [1/10], Training Loss: 0.940, Validation Accuracy: 60.66%\n",
        "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 60.76%\n",
        "Epoch [3/10], Training Loss: 0.784, Validation Accuracy: 61.05%\n",
        "Epoch [4/10], Training Loss: 0.732, Validation Accuracy: 61.23%\n",
        "Epoch [5/10], Training Loss: 0.701, Validation Accuracy: 61.02%\n",
        "Epoch [6/10], Training Loss: 0.677, Validation Accuracy: 60.75%\n",
        "Epoch [7/10], Training Loss: 0.647, Validation Accuracy: 60.71%\n",
        "Epoch [8/10], Training Loss: 0.627, Validation Accuracy: 61.12%\n",
        "Epoch [9/10], Training Loss: 0.607, Validation Accuracy: 60.59%\n",
        "Epoch [10/10], Training Loss: 0.590, Validation Accuracy: 60.99%\n",
        "Epoch [1/10], Training Loss: 0.871, Validation Accuracy: 61.10%\n",
        "Epoch [2/10], Training Loss: 0.777, Validation Accuracy: 61.34%\n",
        "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 61.01%\n",
        "Epoch [4/10], Training Loss: 0.686, Validation Accuracy: 60.64%\n",
        "Epoch [5/10], Training Loss: 0.652, Validation Accuracy: 60.99%\n",
        "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 60.59%\n",
        "Epoch [7/10], Training Loss: 0.605, Validation Accuracy: 60.79%\n",
        "Epoch [8/10], Training Loss: 0.578, Validation Accuracy: 60.60%\n",
        "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 60.78%\n",
        "Epoch [10/10], Training Loss: 0.541, Validation Accuracy: 60.75%\n",
        "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 60.82%\n",
        "Epoch [2/10], Training Loss: 0.783, Validation Accuracy: 61.00%\n",
        "Epoch [3/10], Training Loss: 0.736, Validation Accuracy: 60.98%\n",
        "Epoch [4/10], Training Loss: 0.691, Validation Accuracy: 61.54%\n",
        "Epoch [5/10], Training Loss: 0.649, Validation Accuracy: 61.91%\n",
        "Epoch [6/10], Training Loss: 0.623, Validation Accuracy: 61.32%\n",
        "Epoch [7/10], Training Loss: 0.603, Validation Accuracy: 61.24%\n",
        "Epoch [8/10], Training Loss: 0.573, Validation Accuracy: 61.27%\n",
        "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 61.04%\n",
        "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 60.88%\n",
        "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 61.79%\n",
        "Epoch [2/10], Training Loss: 0.769, Validation Accuracy: 61.70%\n",
        "Epoch [3/10], Training Loss: 0.707, Validation Accuracy: 61.22%\n",
        "Epoch [4/10], Training Loss: 0.665, Validation Accuracy: 60.32%\n",
        "Epoch [5/10], Training Loss: 0.634, Validation Accuracy: 61.22%\n",
        "Epoch [6/10], Training Loss: 0.603, Validation Accuracy: 61.16%\n",
        "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.02%\n",
        "Epoch [8/10], Training Loss: 0.555, Validation Accuracy: 61.05%\n",
        "Epoch [9/10], Training Loss: 0.530, Validation Accuracy: 60.82%\n",
        "Epoch [10/10], Training Loss: 0.518, Validation Accuracy: 60.19%\n",
        "Epoch [1/10], Training Loss: 0.856, Validation Accuracy: 60.07%\n",
        "Epoch [2/10], Training Loss: 0.739, Validation Accuracy: 60.81%\n",
        "Epoch [3/10], Training Loss: 0.668, Validation Accuracy: 60.41%\n",
        "Epoch [4/10], Training Loss: 0.625, Validation Accuracy: 61.07%\n",
        "Epoch [5/10], Training Loss: 0.588, Validation Accuracy: 60.78%\n",
        "Epoch [6/10], Training Loss: 0.555, Validation Accuracy: 60.13%\n",
        "Epoch [7/10], Training Loss: 0.524, Validation Accuracy: 60.87%\n",
        "Epoch [8/10], Training Loss: 0.505, Validation Accuracy: 61.03%\n",
        "Epoch [9/10], Training Loss: 0.485, Validation Accuracy: 60.60%\n",
        "Epoch [10/10], Training Loss: 0.463, Validation Accuracy: 59.93%\n",
        "Epoch [1/10], Training Loss: 0.914, Validation Accuracy: 60.92%\n",
        "Epoch [2/10], Training Loss: 0.782, Validation Accuracy: 61.35%\n",
        "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 61.00%\n",
        "Epoch [4/10], Training Loss: 0.647, Validation Accuracy: 60.66%\n",
        "Epoch [5/10], Training Loss: 0.613, Validation Accuracy: 61.26%\n",
        "Epoch [6/10], Training Loss: 0.597, Validation Accuracy: 60.98%\n",
        "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 61.01%\n",
        "Epoch [8/10], Training Loss: 0.529, Validation Accuracy: 60.61%\n",
        "Epoch [9/10], Training Loss: 0.504, Validation Accuracy: 60.75%\n",
        "Epoch [10/10], Training Loss: 0.486, Validation Accuracy: 60.77%\n",
        "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 60.59%\n",
        "Epoch [2/10], Training Loss: 0.719, Validation Accuracy: 60.36%\n",
        "Epoch [3/10], Training Loss: 0.645, Validation Accuracy: 60.84%\n",
        "Epoch [4/10], Training Loss: 0.608, Validation Accuracy: 60.31%\n",
        "Epoch [5/10], Training Loss: 0.573, Validation Accuracy: 61.11%\n",
        "Epoch [6/10], Training Loss: 0.538, Validation Accuracy: 60.81%\n",
        "Epoch [7/10], Training Loss: 0.508, Validation Accuracy: 60.49%\n",
        "Epoch [8/10], Training Loss: 0.490, Validation Accuracy: 60.93%\n",
        "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 60.15%\n",
        "Epoch [10/10], Training Loss: 0.435, Validation Accuracy: 60.76%\n",
        "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 60.50%\n",
        "Epoch [2/10], Training Loss: 0.705, Validation Accuracy: 60.87%\n",
        "Epoch [3/10], Training Loss: 0.643, Validation Accuracy: 61.42%\n",
        "Epoch [4/10], Training Loss: 0.599, Validation Accuracy: 59.88%\n",
        "Epoch [5/10], Training Loss: 0.565, Validation Accuracy: 60.83%\n",
        "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 61.29%\n",
        "Epoch [7/10], Training Loss: 0.501, Validation Accuracy: 61.10%\n",
        "Epoch [8/10], Training Loss: 0.478, Validation Accuracy: 60.61%\n",
        "Epoch [9/10], Training Loss: 0.459, Validation Accuracy: 60.17%\n",
        "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 61.18%\n",
        "Epoch [1/10], Training Loss: 0.840, Validation Accuracy: 61.18%\n",
        "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 61.01%\n",
        "Epoch [3/10], Training Loss: 0.629, Validation Accuracy: 61.28%\n",
        "Epoch [4/10], Training Loss: 0.582, Validation Accuracy: 61.35%\n",
        "Epoch [5/10], Training Loss: 0.538, Validation Accuracy: 60.65%\n",
        "Epoch [6/10], Training Loss: 0.509, Validation Accuracy: 60.96%\n",
        "Epoch [7/10], Training Loss: 0.484, Validation Accuracy: 60.31%\n",
        "Epoch [8/10], Training Loss: 0.457, Validation Accuracy: 59.92%\n",
        "Epoch [9/10], Training Loss: 0.439, Validation Accuracy: 60.74%\n",
        "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 60.59%\n",
        "Epoch [1/10], Training Loss: 0.828, Validation Accuracy: 59.25%\n",
        "Epoch [2/10], Training Loss: 0.673, Validation Accuracy: 60.12%\n",
        "Epoch [3/10], Training Loss: 0.597, Validation Accuracy: 60.68%\n",
        "Epoch [4/10], Training Loss: 0.550, Validation Accuracy: 60.16%\n",
        "Epoch [5/10], Training Loss: 0.503, Validation Accuracy: 60.91%\n",
        "Epoch [6/10], Training Loss: 0.472, Validation Accuracy: 60.43%\n",
        "Epoch [7/10], Training Loss: 0.444, Validation Accuracy: 60.29%\n",
        "Epoch [8/10], Training Loss: 0.419, Validation Accuracy: 59.90%\n",
        "Epoch [9/10], Training Loss: 0.392, Validation Accuracy: 60.46%\n",
        "Epoch [10/10], Training Loss: 0.374, Validation Accuracy: 60.34%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b604da8-b4af-46dd-dd6a-63f1eb4ed0da",
        "id": "wPdKra2qnYGr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [10.17, 10.24, 10.16, 10.14, 10.12, 10.24, 11.09, 12.03, 12.78, 14.08, 15.69, 17.64, 19.24, 19.88, 20.73, 22.25, 21.6, 22.45, 23.39, 24.34, 27.71, 28.13, 27.49, 28.12, 28.8, 29.36, 29.68, 30.34, 30.88, 31.75, 32.49, 32.3, 32.76, 33.83, 34.41, 34.35, 35.59, 36.21, 36.27, 37.12, 37.81, 38.31, 37.8, 38.92, 38.53, 38.86, 39.55, 39.75, 40.67, 40.43, 40.93, 41.43, 41.32, 41.16, 41.67, 42.35, 42.56, 42.47, 42.86, 42.42, 43.98, 43.37, 43.02, 44.81, 44.49, 44.85, 44.86, 44.82, 45.09, 45.14, 45.68, 46.31, 44.64, 45.5, 46.86, 46.66, 46.8, 46.5, 47.59, 47.65, 46.99, 47.8, 47.92, 48.09, 47.34, 47.69, 47.07, 48.6, 46.95, 48.71, 47.52, 48.75, 48.72, 50.37, 49.66, 50.0, 49.62, 49.95, 49.79, 49.17, 50.98, 51.37, 51.11, 51.07, 50.9, 51.97, 50.99, 51.29, 51.66, 49.1, 51.42, 52.03, 52.39, 52.85, 52.78, 52.3, 51.87, 50.89, 52.01, 52.6, 52.29, 52.48, 52.77, 52.62, 52.61, 52.93, 53.08, 53.41, 52.7, 53.77, 52.18, 53.35, 53.93, 52.25, 53.51, 53.24, 54.0, 54.61, 53.91, 54.12, 54.7, 54.88, 54.29, 55.0, 54.32, 53.69, 53.73, 54.49, 55.24, 54.82, 55.58, 55.25, 55.81, 55.75, 55.15, 56.25, 55.15, 55.55, 55.53, 56.14, 55.82, 56.4, 56.66, 55.91, 56.48, 56.37, 56.08, 56.79, 56.2, 56.6, 56.96, 56.07, 57.41, 56.71, 56.14, 57.2, 57.25, 57.28, 57.02, 57.43, 57.45, 58.15, 58.04, 57.35, 57.4, 57.2, 56.95, 56.99, 57.48, 57.74, 57.6, 57.95, 58.11, 58.06, 57.1, 57.33, 58.53, 58.43, 57.59, 57.31, 58.03, 58.59, 58.09, 58.7, 58.12, 58.97, 58.49, 58.37, 58.39, 58.4, 58.49, 58.5, 58.46, 58.79, 59.09, 58.97, 58.7, 58.42, 58.51, 58.35, 57.56, 59.33, 58.23, 58.99, 59.32, 59.17, 58.38, 59.06, 59.38, 59.12, 59.91, 59.51, 59.6, 59.51, 59.51, 59.49, 59.0, 59.56, 59.66, 58.87, 59.66, 59.64, 59.59, 60.24, 59.93, 59.69, 59.87, 58.85, 59.52, 59.45, 59.64, 59.64, 59.82, 59.09, 59.2, 60.2, 59.93, 59.52, 59.53, 59.75, 60.36, 60.13, 59.62, 60.02, 59.46, 59.33, 59.47, 60.0, 59.58, 59.94, 59.44, 60.68, 60.58, 60.16, 59.64, 60.08, 60.49, 60.26, 60.08, 60.45, 60.23, 60.01, 60.26, 60.41, 60.2, 59.89, 60.42, 59.98, 60.05, 59.5, 59.41, 60.51, 60.24, 59.89, 60.46, 60.45, 59.87, 59.05, 60.17, 60.05, 60.51, 60.02, 60.54, 59.31, 59.63, 59.81, 59.72, 60.25, 60.2, 60.43, 60.45, 61.05, 60.51, 60.68, 60.13, 60.87, 60.48, 59.92, 59.54, 60.07, 59.77, 60.16, 60.95, 61.28, 60.79, 60.95, 61.02, 60.99, 60.78, 60.62, 60.17, 60.89, 61.28, 60.99, 60.89, 61.01, 61.29, 60.45, 60.28, 59.78, 60.89, 61.15, 60.44, 61.15, 61.36, 60.91, 60.19, 60.69, 60.23, 60.16, 60.62, 60.32, 61.46, 60.97, 60.2, 60.92, 60.07, 60.51, 59.95, 60.05, 59.99, 60.82, 60.53, 60.6, 60.34, 60.87, 60.95, 60.52, 60.11, 60.18, 60.37, 60.88, 60.58, 61.33, 60.35, 60.46, 61.35, 61.09, 61.31, 61.21, 61.01, 61.52, 61.39, 60.52, 61.21, 61.3, 61.16, 60.22, 60.57, 60.86, 60.27, 60.16, 61.33, 60.5, 60.48, 60.93, 60.92, 60.26, 59.47, 59.89, 60.66, 60.76, 61.05, 61.23, 61.02, 60.75, 60.71, 61.12, 60.59, 60.99, 61.1, 61.34, 61.01, 60.64, 60.99, 60.59, 60.79, 60.6, 60.78, 60.75, 60.82, 61.0, 60.98, 61.54, 61.91, 61.32, 61.24, 61.27, 61.04, 60.88, 61.79, 61.7, 61.22, 60.32, 61.22, 61.16, 61.02, 61.05, 60.82, 60.19, 60.07, 60.81, 60.41, 61.07, 60.78, 60.13, 60.87, 61.03, 60.6, 59.93, 60.92, 61.35, 61.0, 60.66, 61.26, 60.98, 61.01, 60.61, 60.75, 60.77, 60.59, 60.36, 60.84, 60.31, 61.11, 60.81, 60.49, 60.93, 60.15, 60.76, 60.5, 60.87, 61.42, 59.88, 60.83, 61.29, 61.1, 60.61, 60.17, 61.18, 61.18, 61.01, 61.28, 61.35, 60.65, 60.96, 60.31, 59.92, 60.74, 60.59, 59.25, 60.12, 60.68, 60.16, 60.91, 60.43, 60.29, 59.9, 60.46, 60.34]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}