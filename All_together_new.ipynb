{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkvNinVLB/UWQ/eP5w+fxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Kl-FedDis-Research-/blob/main/All_together_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POoPiG9nbdPT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict, distribution_info_normal: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_uniform + augmented_data_normal + augmented_data_truncated) / 3\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"], other_distribution_info[\"normal\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrx6WM19OYBZ",
        "outputId": "e2a2b301-ce97-4ead-9525-62c00dda3636"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 40.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6001 6128 5977 5963 5967 5904 5991 5966 6076 6027]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 11.58%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 12.26%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 12.43%\n",
            "Epoch [4/10], Training Loss: 2.300, Validation Accuracy: 12.17%\n",
            "Epoch [5/10], Training Loss: 2.299, Validation Accuracy: 12.06%\n",
            "Epoch [6/10], Training Loss: 2.298, Validation Accuracy: 12.76%\n",
            "Epoch [7/10], Training Loss: 2.296, Validation Accuracy: 13.54%\n",
            "Epoch [8/10], Training Loss: 2.294, Validation Accuracy: 15.84%\n",
            "Epoch [9/10], Training Loss: 2.291, Validation Accuracy: 17.75%\n",
            "Epoch [10/10], Training Loss: 2.287, Validation Accuracy: 19.03%\n",
            "Epoch [1/10], Training Loss: 2.284, Validation Accuracy: 19.92%\n",
            "Epoch [2/10], Training Loss: 2.277, Validation Accuracy: 19.85%\n",
            "Epoch [3/10], Training Loss: 2.266, Validation Accuracy: 19.45%\n",
            "Epoch [4/10], Training Loss: 2.250, Validation Accuracy: 19.28%\n",
            "Epoch [5/10], Training Loss: 2.228, Validation Accuracy: 19.23%\n",
            "Epoch [6/10], Training Loss: 2.201, Validation Accuracy: 19.46%\n",
            "Epoch [7/10], Training Loss: 2.170, Validation Accuracy: 21.07%\n",
            "Epoch [8/10], Training Loss: 2.139, Validation Accuracy: 23.50%\n",
            "Epoch [9/10], Training Loss: 2.111, Validation Accuracy: 24.26%\n",
            "Epoch [10/10], Training Loss: 2.085, Validation Accuracy: 25.54%\n",
            "Epoch [1/10], Training Loss: 2.064, Validation Accuracy: 25.83%\n",
            "Epoch [2/10], Training Loss: 2.044, Validation Accuracy: 26.45%\n",
            "Epoch [3/10], Training Loss: 2.023, Validation Accuracy: 27.67%\n",
            "Epoch [4/10], Training Loss: 2.004, Validation Accuracy: 28.55%\n",
            "Epoch [5/10], Training Loss: 1.988, Validation Accuracy: 29.14%\n",
            "Epoch [6/10], Training Loss: 1.968, Validation Accuracy: 30.12%\n",
            "Epoch [7/10], Training Loss: 1.952, Validation Accuracy: 31.06%\n",
            "Epoch [8/10], Training Loss: 1.932, Validation Accuracy: 31.35%\n",
            "Epoch [9/10], Training Loss: 1.913, Validation Accuracy: 32.18%\n",
            "Epoch [10/10], Training Loss: 1.893, Validation Accuracy: 32.63%\n",
            "Epoch [1/10], Training Loss: 1.893, Validation Accuracy: 32.57%\n",
            "Epoch [2/10], Training Loss: 1.865, Validation Accuracy: 33.14%\n",
            "Epoch [3/10], Training Loss: 1.844, Validation Accuracy: 34.15%\n",
            "Epoch [4/10], Training Loss: 1.819, Validation Accuracy: 34.40%\n",
            "Epoch [5/10], Training Loss: 1.791, Validation Accuracy: 35.67%\n",
            "Epoch [6/10], Training Loss: 1.770, Validation Accuracy: 36.69%\n",
            "Epoch [7/10], Training Loss: 1.745, Validation Accuracy: 37.14%\n",
            "Epoch [8/10], Training Loss: 1.724, Validation Accuracy: 37.35%\n",
            "Epoch [9/10], Training Loss: 1.708, Validation Accuracy: 38.08%\n",
            "Epoch [10/10], Training Loss: 1.687, Validation Accuracy: 38.49%\n",
            "Epoch [1/10], Training Loss: 1.682, Validation Accuracy: 39.03%\n",
            "Epoch [2/10], Training Loss: 1.661, Validation Accuracy: 39.37%\n",
            "Epoch [3/10], Training Loss: 1.646, Validation Accuracy: 39.38%\n",
            "Epoch [4/10], Training Loss: 1.625, Validation Accuracy: 40.37%\n",
            "Epoch [5/10], Training Loss: 1.617, Validation Accuracy: 40.67%\n",
            "Epoch [6/10], Training Loss: 1.597, Validation Accuracy: 41.16%\n",
            "Epoch [7/10], Training Loss: 1.588, Validation Accuracy: 41.11%\n",
            "Epoch [8/10], Training Loss: 1.585, Validation Accuracy: 40.93%\n",
            "Epoch [9/10], Training Loss: 1.571, Validation Accuracy: 41.76%\n",
            "Epoch [10/10], Training Loss: 1.557, Validation Accuracy: 41.77%\n",
            "Epoch [1/10], Training Loss: 1.597, Validation Accuracy: 42.02%\n",
            "Epoch [2/10], Training Loss: 1.588, Validation Accuracy: 41.78%\n",
            "Epoch [3/10], Training Loss: 1.577, Validation Accuracy: 42.35%\n",
            "Epoch [4/10], Training Loss: 1.573, Validation Accuracy: 43.13%\n",
            "Epoch [5/10], Training Loss: 1.564, Validation Accuracy: 43.12%\n",
            "Epoch [6/10], Training Loss: 1.554, Validation Accuracy: 43.18%\n",
            "Epoch [7/10], Training Loss: 1.545, Validation Accuracy: 43.44%\n",
            "Epoch [8/10], Training Loss: 1.537, Validation Accuracy: 43.69%\n",
            "Epoch [9/10], Training Loss: 1.532, Validation Accuracy: 44.26%\n",
            "Epoch [10/10], Training Loss: 1.524, Validation Accuracy: 44.10%\n",
            "Epoch [1/10], Training Loss: 1.525, Validation Accuracy: 44.11%\n",
            "Epoch [2/10], Training Loss: 1.514, Validation Accuracy: 42.29%\n",
            "Epoch [3/10], Training Loss: 1.506, Validation Accuracy: 44.26%\n",
            "Epoch [4/10], Training Loss: 1.487, Validation Accuracy: 44.33%\n",
            "Epoch [5/10], Training Loss: 1.483, Validation Accuracy: 44.84%\n",
            "Epoch [6/10], Training Loss: 1.475, Validation Accuracy: 45.13%\n",
            "Epoch [7/10], Training Loss: 1.465, Validation Accuracy: 44.90%\n",
            "Epoch [8/10], Training Loss: 1.459, Validation Accuracy: 45.68%\n",
            "Epoch [9/10], Training Loss: 1.452, Validation Accuracy: 45.88%\n",
            "Epoch [10/10], Training Loss: 1.439, Validation Accuracy: 45.73%\n",
            "Epoch [1/10], Training Loss: 1.477, Validation Accuracy: 46.20%\n",
            "Epoch [2/10], Training Loss: 1.471, Validation Accuracy: 46.46%\n",
            "Epoch [3/10], Training Loss: 1.455, Validation Accuracy: 46.50%\n",
            "Epoch [4/10], Training Loss: 1.446, Validation Accuracy: 46.76%\n",
            "Epoch [5/10], Training Loss: 1.438, Validation Accuracy: 46.11%\n",
            "Epoch [6/10], Training Loss: 1.425, Validation Accuracy: 46.86%\n",
            "Epoch [7/10], Training Loss: 1.424, Validation Accuracy: 47.13%\n",
            "Epoch [8/10], Training Loss: 1.421, Validation Accuracy: 47.11%\n",
            "Epoch [9/10], Training Loss: 1.419, Validation Accuracy: 47.05%\n",
            "Epoch [10/10], Training Loss: 1.404, Validation Accuracy: 47.50%\n",
            "Epoch [1/10], Training Loss: 1.447, Validation Accuracy: 47.50%\n",
            "Epoch [2/10], Training Loss: 1.425, Validation Accuracy: 47.48%\n",
            "Epoch [3/10], Training Loss: 1.421, Validation Accuracy: 47.81%\n",
            "Epoch [4/10], Training Loss: 1.409, Validation Accuracy: 47.52%\n",
            "Epoch [5/10], Training Loss: 1.400, Validation Accuracy: 47.59%\n",
            "Epoch [6/10], Training Loss: 1.389, Validation Accuracy: 48.18%\n",
            "Epoch [7/10], Training Loss: 1.383, Validation Accuracy: 48.29%\n",
            "Epoch [8/10], Training Loss: 1.381, Validation Accuracy: 47.62%\n",
            "Epoch [9/10], Training Loss: 1.377, Validation Accuracy: 48.56%\n",
            "Epoch [10/10], Training Loss: 1.367, Validation Accuracy: 47.94%\n",
            "Epoch [1/10], Training Loss: 1.399, Validation Accuracy: 48.19%\n",
            "Epoch [2/10], Training Loss: 1.376, Validation Accuracy: 47.94%\n",
            "Epoch [3/10], Training Loss: 1.369, Validation Accuracy: 49.02%\n",
            "Epoch [4/10], Training Loss: 1.358, Validation Accuracy: 48.88%\n",
            "Epoch [5/10], Training Loss: 1.354, Validation Accuracy: 49.48%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 49.86%\n",
            "Epoch [7/10], Training Loss: 1.336, Validation Accuracy: 49.15%\n",
            "Epoch [8/10], Training Loss: 1.334, Validation Accuracy: 49.50%\n",
            "Epoch [9/10], Training Loss: 1.326, Validation Accuracy: 49.27%\n",
            "Epoch [10/10], Training Loss: 1.318, Validation Accuracy: 49.44%\n",
            "Epoch [1/10], Training Loss: 1.405, Validation Accuracy: 49.59%\n",
            "Epoch [2/10], Training Loss: 1.383, Validation Accuracy: 49.66%\n",
            "Epoch [3/10], Training Loss: 1.370, Validation Accuracy: 50.40%\n",
            "Epoch [4/10], Training Loss: 1.368, Validation Accuracy: 49.39%\n",
            "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 50.09%\n",
            "Epoch [6/10], Training Loss: 1.352, Validation Accuracy: 49.99%\n",
            "Epoch [7/10], Training Loss: 1.344, Validation Accuracy: 50.28%\n",
            "Epoch [8/10], Training Loss: 1.328, Validation Accuracy: 50.21%\n",
            "Epoch [9/10], Training Loss: 1.331, Validation Accuracy: 50.15%\n",
            "Epoch [10/10], Training Loss: 1.318, Validation Accuracy: 50.05%\n",
            "Epoch [1/10], Training Loss: 1.338, Validation Accuracy: 50.65%\n",
            "Epoch [2/10], Training Loss: 1.317, Validation Accuracy: 50.54%\n",
            "Epoch [3/10], Training Loss: 1.313, Validation Accuracy: 50.53%\n",
            "Epoch [4/10], Training Loss: 1.302, Validation Accuracy: 50.71%\n",
            "Epoch [5/10], Training Loss: 1.299, Validation Accuracy: 50.62%\n",
            "Epoch [6/10], Training Loss: 1.286, Validation Accuracy: 51.03%\n",
            "Epoch [7/10], Training Loss: 1.284, Validation Accuracy: 50.89%\n",
            "Epoch [8/10], Training Loss: 1.272, Validation Accuracy: 51.69%\n",
            "Epoch [9/10], Training Loss: 1.262, Validation Accuracy: 51.44%\n",
            "Epoch [10/10], Training Loss: 1.251, Validation Accuracy: 51.51%\n",
            "Epoch [1/10], Training Loss: 1.320, Validation Accuracy: 51.59%\n",
            "Epoch [2/10], Training Loss: 1.303, Validation Accuracy: 51.49%\n",
            "Epoch [3/10], Training Loss: 1.295, Validation Accuracy: 52.23%\n",
            "Epoch [4/10], Training Loss: 1.283, Validation Accuracy: 51.41%\n",
            "Epoch [5/10], Training Loss: 1.275, Validation Accuracy: 52.24%\n",
            "Epoch [6/10], Training Loss: 1.263, Validation Accuracy: 52.33%\n",
            "Epoch [7/10], Training Loss: 1.266, Validation Accuracy: 51.45%\n",
            "Epoch [8/10], Training Loss: 1.250, Validation Accuracy: 52.09%\n",
            "Epoch [9/10], Training Loss: 1.237, Validation Accuracy: 52.54%\n",
            "Epoch [10/10], Training Loss: 1.232, Validation Accuracy: 51.86%\n",
            "Epoch [1/10], Training Loss: 1.300, Validation Accuracy: 51.87%\n",
            "Epoch [2/10], Training Loss: 1.272, Validation Accuracy: 52.32%\n",
            "Epoch [3/10], Training Loss: 1.259, Validation Accuracy: 52.83%\n",
            "Epoch [4/10], Training Loss: 1.250, Validation Accuracy: 52.47%\n",
            "Epoch [5/10], Training Loss: 1.241, Validation Accuracy: 51.48%\n",
            "Epoch [6/10], Training Loss: 1.231, Validation Accuracy: 52.38%\n",
            "Epoch [7/10], Training Loss: 1.221, Validation Accuracy: 52.75%\n",
            "Epoch [8/10], Training Loss: 1.206, Validation Accuracy: 51.87%\n",
            "Epoch [9/10], Training Loss: 1.212, Validation Accuracy: 52.34%\n",
            "Epoch [10/10], Training Loss: 1.200, Validation Accuracy: 52.88%\n",
            "Epoch [1/10], Training Loss: 1.252, Validation Accuracy: 52.73%\n",
            "Epoch [2/10], Training Loss: 1.229, Validation Accuracy: 53.12%\n",
            "Epoch [3/10], Training Loss: 1.216, Validation Accuracy: 53.07%\n",
            "Epoch [4/10], Training Loss: 1.200, Validation Accuracy: 53.71%\n",
            "Epoch [5/10], Training Loss: 1.197, Validation Accuracy: 52.78%\n",
            "Epoch [6/10], Training Loss: 1.183, Validation Accuracy: 53.42%\n",
            "Epoch [7/10], Training Loss: 1.179, Validation Accuracy: 53.57%\n",
            "Epoch [8/10], Training Loss: 1.174, Validation Accuracy: 53.95%\n",
            "Epoch [9/10], Training Loss: 1.162, Validation Accuracy: 53.70%\n",
            "Epoch [10/10], Training Loss: 1.154, Validation Accuracy: 53.61%\n",
            "Epoch [1/10], Training Loss: 1.278, Validation Accuracy: 53.73%\n",
            "Epoch [2/10], Training Loss: 1.256, Validation Accuracy: 54.50%\n",
            "Epoch [3/10], Training Loss: 1.230, Validation Accuracy: 54.03%\n",
            "Epoch [4/10], Training Loss: 1.218, Validation Accuracy: 54.20%\n",
            "Epoch [5/10], Training Loss: 1.222, Validation Accuracy: 54.23%\n",
            "Epoch [6/10], Training Loss: 1.200, Validation Accuracy: 54.64%\n",
            "Epoch [7/10], Training Loss: 1.186, Validation Accuracy: 54.42%\n",
            "Epoch [8/10], Training Loss: 1.178, Validation Accuracy: 53.31%\n",
            "Epoch [9/10], Training Loss: 1.172, Validation Accuracy: 54.63%\n",
            "Epoch [10/10], Training Loss: 1.166, Validation Accuracy: 54.74%\n",
            "Epoch [1/10], Training Loss: 1.225, Validation Accuracy: 53.76%\n",
            "Epoch [2/10], Training Loss: 1.200, Validation Accuracy: 55.28%\n",
            "Epoch [3/10], Training Loss: 1.183, Validation Accuracy: 54.54%\n",
            "Epoch [4/10], Training Loss: 1.174, Validation Accuracy: 54.83%\n",
            "Epoch [5/10], Training Loss: 1.159, Validation Accuracy: 54.99%\n",
            "Epoch [6/10], Training Loss: 1.155, Validation Accuracy: 55.18%\n",
            "Epoch [7/10], Training Loss: 1.139, Validation Accuracy: 55.61%\n",
            "Epoch [8/10], Training Loss: 1.128, Validation Accuracy: 55.15%\n",
            "Epoch [9/10], Training Loss: 1.122, Validation Accuracy: 54.30%\n",
            "Epoch [10/10], Training Loss: 1.112, Validation Accuracy: 55.48%\n",
            "Epoch [1/10], Training Loss: 1.203, Validation Accuracy: 55.58%\n",
            "Epoch [2/10], Training Loss: 1.173, Validation Accuracy: 56.10%\n",
            "Epoch [3/10], Training Loss: 1.163, Validation Accuracy: 55.50%\n",
            "Epoch [4/10], Training Loss: 1.153, Validation Accuracy: 56.31%\n",
            "Epoch [5/10], Training Loss: 1.136, Validation Accuracy: 55.44%\n",
            "Epoch [6/10], Training Loss: 1.137, Validation Accuracy: 55.07%\n",
            "Epoch [7/10], Training Loss: 1.117, Validation Accuracy: 56.34%\n",
            "Epoch [8/10], Training Loss: 1.110, Validation Accuracy: 55.39%\n",
            "Epoch [9/10], Training Loss: 1.096, Validation Accuracy: 56.09%\n",
            "Epoch [10/10], Training Loss: 1.087, Validation Accuracy: 55.29%\n",
            "Epoch [1/10], Training Loss: 1.182, Validation Accuracy: 55.00%\n",
            "Epoch [2/10], Training Loss: 1.155, Validation Accuracy: 56.26%\n",
            "Epoch [3/10], Training Loss: 1.139, Validation Accuracy: 56.42%\n",
            "Epoch [4/10], Training Loss: 1.126, Validation Accuracy: 56.15%\n",
            "Epoch [5/10], Training Loss: 1.108, Validation Accuracy: 55.52%\n",
            "Epoch [6/10], Training Loss: 1.106, Validation Accuracy: 55.67%\n",
            "Epoch [7/10], Training Loss: 1.089, Validation Accuracy: 54.80%\n",
            "Epoch [8/10], Training Loss: 1.080, Validation Accuracy: 54.84%\n",
            "Epoch [9/10], Training Loss: 1.066, Validation Accuracy: 54.66%\n",
            "Epoch [10/10], Training Loss: 1.064, Validation Accuracy: 56.72%\n",
            "Epoch [1/10], Training Loss: 1.144, Validation Accuracy: 56.95%\n",
            "Epoch [2/10], Training Loss: 1.122, Validation Accuracy: 57.17%\n",
            "Epoch [3/10], Training Loss: 1.106, Validation Accuracy: 56.63%\n",
            "Epoch [4/10], Training Loss: 1.089, Validation Accuracy: 56.85%\n",
            "Epoch [5/10], Training Loss: 1.080, Validation Accuracy: 56.24%\n",
            "Epoch [6/10], Training Loss: 1.060, Validation Accuracy: 56.12%\n",
            "Epoch [7/10], Training Loss: 1.054, Validation Accuracy: 57.27%\n",
            "Epoch [8/10], Training Loss: 1.034, Validation Accuracy: 56.75%\n",
            "Epoch [9/10], Training Loss: 1.024, Validation Accuracy: 56.52%\n",
            "Epoch [10/10], Training Loss: 1.036, Validation Accuracy: 56.90%\n",
            "Epoch [1/10], Training Loss: 1.168, Validation Accuracy: 57.06%\n",
            "Epoch [2/10], Training Loss: 1.143, Validation Accuracy: 57.07%\n",
            "Epoch [3/10], Training Loss: 1.118, Validation Accuracy: 57.59%\n",
            "Epoch [4/10], Training Loss: 1.103, Validation Accuracy: 57.57%\n",
            "Epoch [5/10], Training Loss: 1.088, Validation Accuracy: 56.72%\n",
            "Epoch [6/10], Training Loss: 1.072, Validation Accuracy: 57.47%\n",
            "Epoch [7/10], Training Loss: 1.066, Validation Accuracy: 57.02%\n",
            "Epoch [8/10], Training Loss: 1.047, Validation Accuracy: 57.52%\n",
            "Epoch [9/10], Training Loss: 1.037, Validation Accuracy: 57.90%\n",
            "Epoch [10/10], Training Loss: 1.027, Validation Accuracy: 57.46%\n",
            "Epoch [1/10], Training Loss: 1.124, Validation Accuracy: 57.41%\n",
            "Epoch [2/10], Training Loss: 1.096, Validation Accuracy: 57.42%\n",
            "Epoch [3/10], Training Loss: 1.072, Validation Accuracy: 58.19%\n",
            "Epoch [4/10], Training Loss: 1.055, Validation Accuracy: 56.29%\n",
            "Epoch [5/10], Training Loss: 1.051, Validation Accuracy: 58.17%\n",
            "Epoch [6/10], Training Loss: 1.031, Validation Accuracy: 57.19%\n",
            "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 57.39%\n",
            "Epoch [8/10], Training Loss: 1.008, Validation Accuracy: 57.03%\n",
            "Epoch [9/10], Training Loss: 1.002, Validation Accuracy: 57.67%\n",
            "Epoch [10/10], Training Loss: 0.987, Validation Accuracy: 58.26%\n",
            "Epoch [1/10], Training Loss: 1.110, Validation Accuracy: 57.79%\n",
            "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.11%\n",
            "Epoch [3/10], Training Loss: 1.060, Validation Accuracy: 58.07%\n",
            "Epoch [4/10], Training Loss: 1.034, Validation Accuracy: 58.46%\n",
            "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 58.15%\n",
            "Epoch [6/10], Training Loss: 1.012, Validation Accuracy: 57.70%\n",
            "Epoch [7/10], Training Loss: 1.004, Validation Accuracy: 58.11%\n",
            "Epoch [8/10], Training Loss: 0.988, Validation Accuracy: 57.76%\n",
            "Epoch [9/10], Training Loss: 0.983, Validation Accuracy: 58.03%\n",
            "Epoch [10/10], Training Loss: 0.967, Validation Accuracy: 57.85%\n",
            "Epoch [1/10], Training Loss: 1.095, Validation Accuracy: 58.27%\n",
            "Epoch [2/10], Training Loss: 1.057, Validation Accuracy: 58.35%\n",
            "Epoch [3/10], Training Loss: 1.032, Validation Accuracy: 57.87%\n",
            "Epoch [4/10], Training Loss: 1.013, Validation Accuracy: 57.87%\n",
            "Epoch [5/10], Training Loss: 0.997, Validation Accuracy: 58.41%\n",
            "Epoch [6/10], Training Loss: 0.978, Validation Accuracy: 58.45%\n",
            "Epoch [7/10], Training Loss: 0.968, Validation Accuracy: 58.42%\n",
            "Epoch [8/10], Training Loss: 0.964, Validation Accuracy: 58.46%\n",
            "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 58.04%\n",
            "Epoch [10/10], Training Loss: 0.941, Validation Accuracy: 58.00%\n",
            "Epoch [1/10], Training Loss: 1.065, Validation Accuracy: 57.78%\n",
            "Epoch [2/10], Training Loss: 1.043, Validation Accuracy: 58.79%\n",
            "Epoch [3/10], Training Loss: 1.006, Validation Accuracy: 58.67%\n",
            "Epoch [4/10], Training Loss: 0.984, Validation Accuracy: 59.16%\n",
            "Epoch [5/10], Training Loss: 0.963, Validation Accuracy: 58.55%\n",
            "Epoch [6/10], Training Loss: 0.950, Validation Accuracy: 57.99%\n",
            "Epoch [7/10], Training Loss: 0.949, Validation Accuracy: 58.47%\n",
            "Epoch [8/10], Training Loss: 0.924, Validation Accuracy: 58.10%\n",
            "Epoch [9/10], Training Loss: 0.917, Validation Accuracy: 58.77%\n",
            "Epoch [10/10], Training Loss: 0.901, Validation Accuracy: 58.82%\n",
            "Epoch [1/10], Training Loss: 1.086, Validation Accuracy: 58.75%\n",
            "Epoch [2/10], Training Loss: 1.047, Validation Accuracy: 58.32%\n",
            "Epoch [3/10], Training Loss: 1.017, Validation Accuracy: 58.31%\n",
            "Epoch [4/10], Training Loss: 0.995, Validation Accuracy: 58.76%\n",
            "Epoch [5/10], Training Loss: 0.981, Validation Accuracy: 59.19%\n",
            "Epoch [6/10], Training Loss: 0.959, Validation Accuracy: 58.57%\n",
            "Epoch [7/10], Training Loss: 0.946, Validation Accuracy: 58.99%\n",
            "Epoch [8/10], Training Loss: 0.932, Validation Accuracy: 59.21%\n",
            "Epoch [9/10], Training Loss: 0.921, Validation Accuracy: 58.59%\n",
            "Epoch [10/10], Training Loss: 0.909, Validation Accuracy: 59.06%\n",
            "Epoch [1/10], Training Loss: 1.051, Validation Accuracy: 58.93%\n",
            "Epoch [2/10], Training Loss: 1.013, Validation Accuracy: 59.86%\n",
            "Epoch [3/10], Training Loss: 0.976, Validation Accuracy: 58.91%\n",
            "Epoch [4/10], Training Loss: 0.954, Validation Accuracy: 59.88%\n",
            "Epoch [5/10], Training Loss: 0.939, Validation Accuracy: 59.50%\n",
            "Epoch [6/10], Training Loss: 0.933, Validation Accuracy: 59.38%\n",
            "Epoch [7/10], Training Loss: 0.914, Validation Accuracy: 59.67%\n",
            "Epoch [8/10], Training Loss: 0.905, Validation Accuracy: 58.09%\n",
            "Epoch [9/10], Training Loss: 0.882, Validation Accuracy: 59.53%\n",
            "Epoch [10/10], Training Loss: 0.870, Validation Accuracy: 59.45%\n",
            "Epoch [1/10], Training Loss: 1.044, Validation Accuracy: 58.71%\n",
            "Epoch [2/10], Training Loss: 0.995, Validation Accuracy: 59.59%\n",
            "Epoch [3/10], Training Loss: 0.968, Validation Accuracy: 58.63%\n",
            "Epoch [4/10], Training Loss: 0.938, Validation Accuracy: 59.25%\n",
            "Epoch [5/10], Training Loss: 0.925, Validation Accuracy: 59.67%\n",
            "Epoch [6/10], Training Loss: 0.918, Validation Accuracy: 59.81%\n",
            "Epoch [7/10], Training Loss: 0.899, Validation Accuracy: 59.14%\n",
            "Epoch [8/10], Training Loss: 0.886, Validation Accuracy: 59.31%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 58.74%\n",
            "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 59.61%\n",
            "Epoch [1/10], Training Loss: 1.018, Validation Accuracy: 59.64%\n",
            "Epoch [2/10], Training Loss: 0.977, Validation Accuracy: 59.69%\n",
            "Epoch [3/10], Training Loss: 0.937, Validation Accuracy: 59.82%\n",
            "Epoch [4/10], Training Loss: 0.921, Validation Accuracy: 59.50%\n",
            "Epoch [5/10], Training Loss: 0.911, Validation Accuracy: 59.27%\n",
            "Epoch [6/10], Training Loss: 0.881, Validation Accuracy: 59.25%\n",
            "Epoch [7/10], Training Loss: 0.868, Validation Accuracy: 59.57%\n",
            "Epoch [8/10], Training Loss: 0.866, Validation Accuracy: 59.07%\n",
            "Epoch [9/10], Training Loss: 0.847, Validation Accuracy: 58.90%\n",
            "Epoch [10/10], Training Loss: 0.838, Validation Accuracy: 59.02%\n",
            "Epoch [1/10], Training Loss: 0.988, Validation Accuracy: 60.52%\n",
            "Epoch [2/10], Training Loss: 0.943, Validation Accuracy: 59.89%\n",
            "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 60.33%\n",
            "Epoch [4/10], Training Loss: 0.890, Validation Accuracy: 60.01%\n",
            "Epoch [5/10], Training Loss: 0.867, Validation Accuracy: 60.43%\n",
            "Epoch [6/10], Training Loss: 0.845, Validation Accuracy: 60.20%\n",
            "Epoch [7/10], Training Loss: 0.838, Validation Accuracy: 60.60%\n",
            "Epoch [8/10], Training Loss: 0.818, Validation Accuracy: 59.84%\n",
            "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 59.14%\n",
            "Epoch [10/10], Training Loss: 0.798, Validation Accuracy: 60.24%\n",
            "Epoch [1/10], Training Loss: 1.021, Validation Accuracy: 60.26%\n",
            "Epoch [2/10], Training Loss: 0.964, Validation Accuracy: 60.51%\n",
            "Epoch [3/10], Training Loss: 0.940, Validation Accuracy: 60.25%\n",
            "Epoch [4/10], Training Loss: 0.900, Validation Accuracy: 60.08%\n",
            "Epoch [5/10], Training Loss: 0.894, Validation Accuracy: 59.83%\n",
            "Epoch [6/10], Training Loss: 0.868, Validation Accuracy: 59.44%\n",
            "Epoch [7/10], Training Loss: 0.852, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.835, Validation Accuracy: 59.61%\n",
            "Epoch [9/10], Training Loss: 0.814, Validation Accuracy: 60.34%\n",
            "Epoch [10/10], Training Loss: 0.799, Validation Accuracy: 60.15%\n",
            "Epoch [1/10], Training Loss: 0.988, Validation Accuracy: 60.23%\n",
            "Epoch [2/10], Training Loss: 0.925, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 0.895, Validation Accuracy: 59.18%\n",
            "Epoch [4/10], Training Loss: 0.868, Validation Accuracy: 60.02%\n",
            "Epoch [5/10], Training Loss: 0.856, Validation Accuracy: 59.75%\n",
            "Epoch [6/10], Training Loss: 0.834, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.815, Validation Accuracy: 59.98%\n",
            "Epoch [8/10], Training Loss: 0.798, Validation Accuracy: 60.41%\n",
            "Epoch [9/10], Training Loss: 0.777, Validation Accuracy: 59.77%\n",
            "Epoch [10/10], Training Loss: 0.774, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.991, Validation Accuracy: 60.17%\n",
            "Epoch [2/10], Training Loss: 0.920, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.878, Validation Accuracy: 60.63%\n",
            "Epoch [4/10], Training Loss: 0.861, Validation Accuracy: 60.59%\n",
            "Epoch [5/10], Training Loss: 0.833, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.816, Validation Accuracy: 59.83%\n",
            "Epoch [7/10], Training Loss: 0.798, Validation Accuracy: 60.54%\n",
            "Epoch [8/10], Training Loss: 0.788, Validation Accuracy: 59.66%\n",
            "Epoch [9/10], Training Loss: 0.782, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.762, Validation Accuracy: 59.92%\n",
            "Epoch [1/10], Training Loss: 0.959, Validation Accuracy: 60.21%\n",
            "Epoch [2/10], Training Loss: 0.911, Validation Accuracy: 60.55%\n",
            "Epoch [3/10], Training Loss: 0.866, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.844, Validation Accuracy: 60.12%\n",
            "Epoch [5/10], Training Loss: 0.821, Validation Accuracy: 60.51%\n",
            "Epoch [6/10], Training Loss: 0.797, Validation Accuracy: 60.42%\n",
            "Epoch [7/10], Training Loss: 0.777, Validation Accuracy: 59.74%\n",
            "Epoch [8/10], Training Loss: 0.764, Validation Accuracy: 60.34%\n",
            "Epoch [9/10], Training Loss: 0.748, Validation Accuracy: 60.49%\n",
            "Epoch [10/10], Training Loss: 0.732, Validation Accuracy: 60.02%\n",
            "Epoch [1/10], Training Loss: 0.934, Validation Accuracy: 60.27%\n",
            "Epoch [2/10], Training Loss: 0.872, Validation Accuracy: 60.99%\n",
            "Epoch [3/10], Training Loss: 0.824, Validation Accuracy: 60.86%\n",
            "Epoch [4/10], Training Loss: 0.805, Validation Accuracy: 61.03%\n",
            "Epoch [5/10], Training Loss: 0.778, Validation Accuracy: 60.89%\n",
            "Epoch [6/10], Training Loss: 0.762, Validation Accuracy: 60.73%\n",
            "Epoch [7/10], Training Loss: 0.738, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.722, Validation Accuracy: 60.44%\n",
            "Epoch [9/10], Training Loss: 0.708, Validation Accuracy: 60.36%\n",
            "Epoch [10/10], Training Loss: 0.697, Validation Accuracy: 60.06%\n",
            "Epoch [1/10], Training Loss: 0.953, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 60.19%\n",
            "Epoch [3/10], Training Loss: 0.860, Validation Accuracy: 61.03%\n",
            "Epoch [4/10], Training Loss: 0.824, Validation Accuracy: 60.88%\n",
            "Epoch [5/10], Training Loss: 0.802, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.780, Validation Accuracy: 60.42%\n",
            "Epoch [7/10], Training Loss: 0.760, Validation Accuracy: 60.33%\n",
            "Epoch [8/10], Training Loss: 0.750, Validation Accuracy: 60.37%\n",
            "Epoch [9/10], Training Loss: 0.722, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.929, Validation Accuracy: 60.88%\n",
            "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 60.62%\n",
            "Epoch [3/10], Training Loss: 0.824, Validation Accuracy: 60.95%\n",
            "Epoch [4/10], Training Loss: 0.800, Validation Accuracy: 61.36%\n",
            "Epoch [5/10], Training Loss: 0.773, Validation Accuracy: 60.09%\n",
            "Epoch [6/10], Training Loss: 0.748, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.719, Validation Accuracy: 60.52%\n",
            "Epoch [8/10], Training Loss: 0.710, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.694, Validation Accuracy: 60.42%\n",
            "Epoch [10/10], Training Loss: 0.677, Validation Accuracy: 60.46%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 60.20%\n",
            "Epoch [2/10], Training Loss: 0.856, Validation Accuracy: 60.81%\n",
            "Epoch [3/10], Training Loss: 0.815, Validation Accuracy: 61.40%\n",
            "Epoch [4/10], Training Loss: 0.786, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.760, Validation Accuracy: 61.02%\n",
            "Epoch [6/10], Training Loss: 0.731, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.718, Validation Accuracy: 60.16%\n",
            "Epoch [8/10], Training Loss: 0.697, Validation Accuracy: 61.07%\n",
            "Epoch [9/10], Training Loss: 0.683, Validation Accuracy: 61.33%\n",
            "Epoch [10/10], Training Loss: 0.668, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.917, Validation Accuracy: 60.24%\n",
            "Epoch [2/10], Training Loss: 0.833, Validation Accuracy: 60.79%\n",
            "Epoch [3/10], Training Loss: 0.789, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 60.28%\n",
            "Epoch [5/10], Training Loss: 0.738, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.707, Validation Accuracy: 60.71%\n",
            "Epoch [7/10], Training Loss: 0.691, Validation Accuracy: 60.82%\n",
            "Epoch [8/10], Training Loss: 0.683, Validation Accuracy: 60.19%\n",
            "Epoch [9/10], Training Loss: 0.655, Validation Accuracy: 61.06%\n",
            "Epoch [10/10], Training Loss: 0.638, Validation Accuracy: 60.89%\n",
            "Epoch [1/10], Training Loss: 0.876, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.802, Validation Accuracy: 60.79%\n",
            "Epoch [3/10], Training Loss: 0.756, Validation Accuracy: 61.26%\n",
            "Epoch [4/10], Training Loss: 0.725, Validation Accuracy: 60.70%\n",
            "Epoch [5/10], Training Loss: 0.708, Validation Accuracy: 61.33%\n",
            "Epoch [6/10], Training Loss: 0.672, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.659, Validation Accuracy: 61.00%\n",
            "Epoch [8/10], Training Loss: 0.627, Validation Accuracy: 61.30%\n",
            "Epoch [9/10], Training Loss: 0.612, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.597, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 0.915, Validation Accuracy: 60.33%\n",
            "Epoch [2/10], Training Loss: 0.831, Validation Accuracy: 61.12%\n",
            "Epoch [3/10], Training Loss: 0.781, Validation Accuracy: 60.33%\n",
            "Epoch [4/10], Training Loss: 0.746, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.716, Validation Accuracy: 61.00%\n",
            "Epoch [6/10], Training Loss: 0.698, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.682, Validation Accuracy: 60.39%\n",
            "Epoch [8/10], Training Loss: 0.659, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.636, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.617, Validation Accuracy: 60.44%\n",
            "Epoch [1/10], Training Loss: 0.878, Validation Accuracy: 61.20%\n",
            "Epoch [2/10], Training Loss: 0.794, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.756, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.713, Validation Accuracy: 61.05%\n",
            "Epoch [5/10], Training Loss: 0.695, Validation Accuracy: 60.72%\n",
            "Epoch [6/10], Training Loss: 0.668, Validation Accuracy: 60.35%\n",
            "Epoch [7/10], Training Loss: 0.644, Validation Accuracy: 61.55%\n",
            "Epoch [8/10], Training Loss: 0.625, Validation Accuracy: 61.09%\n",
            "Epoch [9/10], Training Loss: 0.604, Validation Accuracy: 60.20%\n",
            "Epoch [10/10], Training Loss: 0.586, Validation Accuracy: 60.07%\n",
            "Epoch [1/10], Training Loss: 0.874, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.792, Validation Accuracy: 61.41%\n",
            "Epoch [3/10], Training Loss: 0.737, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 60.89%\n",
            "Epoch [5/10], Training Loss: 0.690, Validation Accuracy: 61.38%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.633, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.607, Validation Accuracy: 60.68%\n",
            "Epoch [9/10], Training Loss: 0.590, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.571, Validation Accuracy: 61.16%\n",
            "Epoch [1/10], Training Loss: 0.868, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.772, Validation Accuracy: 60.73%\n",
            "Epoch [3/10], Training Loss: 0.717, Validation Accuracy: 60.18%\n",
            "Epoch [4/10], Training Loss: 0.691, Validation Accuracy: 60.89%\n",
            "Epoch [5/10], Training Loss: 0.661, Validation Accuracy: 60.64%\n",
            "Epoch [6/10], Training Loss: 0.636, Validation Accuracy: 60.40%\n",
            "Epoch [7/10], Training Loss: 0.613, Validation Accuracy: 60.66%\n",
            "Epoch [8/10], Training Loss: 0.591, Validation Accuracy: 60.57%\n",
            "Epoch [9/10], Training Loss: 0.569, Validation Accuracy: 59.87%\n",
            "Epoch [10/10], Training Loss: 0.552, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 0.833, Validation Accuracy: 60.82%\n",
            "Epoch [2/10], Training Loss: 0.739, Validation Accuracy: 61.52%\n",
            "Epoch [3/10], Training Loss: 0.682, Validation Accuracy: 60.81%\n",
            "Epoch [4/10], Training Loss: 0.649, Validation Accuracy: 60.56%\n",
            "Epoch [5/10], Training Loss: 0.613, Validation Accuracy: 60.88%\n",
            "Epoch [6/10], Training Loss: 0.595, Validation Accuracy: 61.37%\n",
            "Epoch [7/10], Training Loss: 0.574, Validation Accuracy: 61.03%\n",
            "Epoch [8/10], Training Loss: 0.547, Validation Accuracy: 61.00%\n",
            "Epoch [9/10], Training Loss: 0.525, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.513, Validation Accuracy: 61.32%\n",
            "Epoch [1/10], Training Loss: 0.878, Validation Accuracy: 60.10%\n",
            "Epoch [2/10], Training Loss: 0.764, Validation Accuracy: 60.95%\n",
            "Epoch [3/10], Training Loss: 0.711, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.670, Validation Accuracy: 61.38%\n",
            "Epoch [5/10], Training Loss: 0.640, Validation Accuracy: 61.05%\n",
            "Epoch [6/10], Training Loss: 0.614, Validation Accuracy: 60.79%\n",
            "Epoch [7/10], Training Loss: 0.585, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.549, Validation Accuracy: 61.06%\n",
            "Epoch [10/10], Training Loss: 0.531, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.849, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.743, Validation Accuracy: 60.95%\n",
            "Epoch [3/10], Training Loss: 0.695, Validation Accuracy: 60.73%\n",
            "Epoch [4/10], Training Loss: 0.638, Validation Accuracy: 61.51%\n",
            "Epoch [5/10], Training Loss: 0.614, Validation Accuracy: 61.14%\n",
            "Epoch [6/10], Training Loss: 0.588, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.565, Validation Accuracy: 61.16%\n",
            "Epoch [8/10], Training Loss: 0.541, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.522, Validation Accuracy: 60.96%\n",
            "Epoch [10/10], Training Loss: 0.498, Validation Accuracy: 60.51%\n",
            "Epoch [1/10], Training Loss: 0.852, Validation Accuracy: 60.83%\n",
            "Epoch [2/10], Training Loss: 0.732, Validation Accuracy: 60.94%\n",
            "Epoch [3/10], Training Loss: 0.675, Validation Accuracy: 61.15%\n",
            "Epoch [4/10], Training Loss: 0.644, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.596, Validation Accuracy: 61.52%\n",
            "Epoch [6/10], Training Loss: 0.577, Validation Accuracy: 61.44%\n",
            "Epoch [7/10], Training Loss: 0.554, Validation Accuracy: 61.11%\n",
            "Epoch [8/10], Training Loss: 0.521, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.505, Validation Accuracy: 60.84%\n",
            "Epoch [10/10], Training Loss: 0.494, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.715, Validation Accuracy: 61.17%\n",
            "Epoch [3/10], Training Loss: 0.657, Validation Accuracy: 60.80%\n",
            "Epoch [4/10], Training Loss: 0.614, Validation Accuracy: 60.42%\n",
            "Epoch [5/10], Training Loss: 0.583, Validation Accuracy: 60.42%\n",
            "Epoch [6/10], Training Loss: 0.564, Validation Accuracy: 60.22%\n",
            "Epoch [7/10], Training Loss: 0.537, Validation Accuracy: 60.24%\n",
            "Epoch [8/10], Training Loss: 0.504, Validation Accuracy: 60.49%\n",
            "Epoch [9/10], Training Loss: 0.491, Validation Accuracy: 60.09%\n",
            "Epoch [10/10], Training Loss: 0.473, Validation Accuracy: 60.09%\n",
            "Epoch [1/10], Training Loss: 0.791, Validation Accuracy: 60.78%\n",
            "Epoch [2/10], Training Loss: 0.678, Validation Accuracy: 61.01%\n",
            "Epoch [3/10], Training Loss: 0.622, Validation Accuracy: 60.91%\n",
            "Epoch [4/10], Training Loss: 0.572, Validation Accuracy: 61.25%\n",
            "Epoch [5/10], Training Loss: 0.544, Validation Accuracy: 61.44%\n",
            "Epoch [6/10], Training Loss: 0.509, Validation Accuracy: 60.64%\n",
            "Epoch [7/10], Training Loss: 0.488, Validation Accuracy: 61.09%\n",
            "Epoch [8/10], Training Loss: 0.459, Validation Accuracy: 61.34%\n",
            "Epoch [9/10], Training Loss: 0.441, Validation Accuracy: 60.40%\n",
            "Epoch [10/10], Training Loss: 0.428, Validation Accuracy: 61.14%\n",
            "Confusion Matrix:\n",
            "[[724  36  53  21  13   7  14  15  79  38]\n",
            " [ 30 777   8  14   8   7  17   1  40  98]\n",
            " [ 82  12 533  63  98  83  63  33  16  17]\n",
            " [ 43  21  87 414  70 184  73  58  13  37]\n",
            " [ 44   7 120  63 554  49  82  62  13   6]\n",
            " [ 17   5  74 205  67 508  38  60   9  17]\n",
            " [  8  14  66  70  62  27 717  16  10  10]\n",
            " [ 24  10  47  53  89  84  19 627   7  40]\n",
            " [101  57  34  21  13  18   7   6 713  30]\n",
            " [ 54 183  19  26   6  15  14  24  52 607]]\n",
            "Test Accuracy: 61.74%\n",
            "True Positives (TP): [724 777 533 414 554 508 717 627 713 607]\n",
            "False Positives (FP): [403 345 508 536 426 474 327 275 239 293]\n",
            "True Negatives (TN): [8597 8655 8492 8464 8574 8526 8673 8725 8761 8707]\n",
            "False Negatives (FN): [276 223 467 586 446 492 283 373 287 393]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.64241349 0.69251337 0.51200768 0.43578947 0.56530612 0.51731161\n",
            " 0.68678161 0.69512195 0.74894958 0.67444444]\n",
            "Recall: [0.724 0.777 0.533 0.414 0.554 0.508 0.717 0.627 0.713 0.607]\n",
            "F1 Score: [0.68077104 0.73232799 0.52229299 0.42461538 0.55959596 0.51261352\n",
            " 0.70156556 0.65930599 0.73053279 0.63894737]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Tuple\n",
        "\n",
        "# Define Beta-VAE model\n",
        "class BetaVAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(BetaVAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2 * z_dim)  # Outputs mu and logvar\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define Beta-VAE loss function\n",
        "def beta_vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    \"\"\"Loss function for Beta-VAE.\"\"\"\n",
        "    # Reconstruction loss (MSE)\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    # KL Divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "# Training function for Beta-VAE\n",
        "def beta_vae_train(vae: BetaVAE, trainloader: DataLoader, epochs: int, beta: float) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        total_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = beta_vae_loss(recon_x, inputs, mu, logvar, beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(trainloader):.3f}\")\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "train_set = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Create training and test loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Initialize and train Beta-VAE\n",
        "if __name__ == \"__main__\":\n",
        "    x_dim = 3 * 32 * 32\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    beta = 4.0  # Hyperparameter for Beta-VAE (higher values promote disentanglement)\n",
        "\n",
        "    vae = BetaVAE(x_dim, h_dim, z_dim)\n",
        "\n",
        "    print(\"Training Beta-VAE...\")\n",
        "    beta_vae_train(vae, trainloader, epochs=50, beta=beta)\n",
        "\n",
        "    print(\"Beta-VAE training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7rV4yBLdD39",
        "outputId": "6875a19f-ca38-4cb9-8ddb-9c67c6ef8552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 79.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training Beta-VAE...\n",
            "Epoch [1/50], Loss: 98565.544\n",
            "Epoch [2/50], Loss: 83792.941\n",
            "Epoch [3/50], Loss: 79094.141\n",
            "Epoch [4/50], Loss: 77644.770\n",
            "Epoch [5/50], Loss: 76686.872\n",
            "Epoch [6/50], Loss: 76278.620\n",
            "Epoch [7/50], Loss: 76051.354\n",
            "Epoch [8/50], Loss: 75850.353\n",
            "Epoch [9/50], Loss: 75685.102\n",
            "Epoch [10/50], Loss: 75571.210\n",
            "Epoch [11/50], Loss: 75496.717\n",
            "Epoch [12/50], Loss: 75423.431\n",
            "Epoch [13/50], Loss: 75342.111\n",
            "Epoch [14/50], Loss: 75289.826\n",
            "Epoch [15/50], Loss: 75249.690\n",
            "Epoch [16/50], Loss: 75214.751\n",
            "Epoch [17/50], Loss: 75173.706\n",
            "Epoch [18/50], Loss: 75121.234\n",
            "Epoch [19/50], Loss: 75108.132\n",
            "Epoch [20/50], Loss: 75085.241\n",
            "Epoch [21/50], Loss: 74873.221\n",
            "Epoch [22/50], Loss: 74839.520\n",
            "Epoch [23/50], Loss: 74825.750\n",
            "Epoch [24/50], Loss: 74803.555\n",
            "Epoch [25/50], Loss: 74784.587\n",
            "Epoch [26/50], Loss: 74795.221\n",
            "Epoch [27/50], Loss: 74747.432\n",
            "Epoch [28/50], Loss: 74751.732\n",
            "Epoch [29/50], Loss: 74732.321\n",
            "Epoch [30/50], Loss: 74720.584\n",
            "Epoch [31/50], Loss: 74685.406\n",
            "Epoch [32/50], Loss: 74683.688\n",
            "Epoch [33/50], Loss: 74679.645\n",
            "Epoch [34/50], Loss: 74660.381\n",
            "Epoch [35/50], Loss: 74642.480\n",
            "Epoch [36/50], Loss: 74632.413\n",
            "Epoch [37/50], Loss: 74625.445\n",
            "Epoch [38/50], Loss: 74606.919\n",
            "Epoch [39/50], Loss: 74590.366\n",
            "Epoch [40/50], Loss: 74573.733\n",
            "Epoch [41/50], Loss: 74482.720\n",
            "Epoch [42/50], Loss: 74457.858\n",
            "Epoch [43/50], Loss: 74442.418\n",
            "Epoch [44/50], Loss: 74433.078\n",
            "Epoch [45/50], Loss: 74423.334\n",
            "Epoch [46/50], Loss: 74397.902\n",
            "Epoch [47/50], Loss: 74400.972\n",
            "Epoch [48/50], Loss: 74386.560\n",
            "Epoch [49/50], Loss: 74393.541\n",
            "Epoch [50/50], Loss: 74386.555\n",
            "Beta-VAE training completed.\n"
          ]
        }
      ]
    }
  ]
}