{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Kl-FedDis-Research-/blob/main/normal_uniform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzklOfPpyr99"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNXA5i6ve3Wz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from typing import Dict\n",
        "from scipy.stats import truncnorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp2zIBwYfDJX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVN9tba-fG27"
      },
      "outputs": [],
      "source": [
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJLVsmDtfKj4"
      },
      "outputs": [],
      "source": [
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlz4UTFXfQNp",
        "outputId": "aff9fb5d-d245-4f5e-caf8-a4d915342c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 63066760.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raWvvQKufUjz"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arC0NiXAfYXH"
      },
      "outputs": [],
      "source": [
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4g2A2W6fcNd"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "453JmsekfhkR"
      },
      "outputs": [],
      "source": [
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJq5CiZQflxQ"
      },
      "outputs": [],
      "source": [
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gOSb3rRfpxk"
      },
      "outputs": [],
      "source": [
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ry_R_Myftfz"
      },
      "outputs": [],
      "source": [
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXS1j6PEfxMR"
      },
      "outputs": [],
      "source": [
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xtNAWdRf237"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_uniform: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both normal and uniform distributions\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    # Generate augmented data from normal distribution\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "    # Generate augmented data using Uniform distribution\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_normal + augmented_data_uniform) / 2\n",
        "\n",
        "    return augmented_data_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2S5RwKHf7Gd"
      },
      "outputs": [],
      "source": [
        "# Define the federated training logic\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs9eycMOf-_5"
      },
      "outputs": [],
      "source": [
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ev0rP0gDCT"
      },
      "outputs": [],
      "source": [
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0a2npibgGZW",
        "outputId": "4b588c06-7ee1-417c-8ad2-5ac464d4d387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.18%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.18%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.18%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 10.18%\n",
            "Epoch [5/10], Training Loss: 2.300, Validation Accuracy: 10.18%\n",
            "Epoch [6/10], Training Loss: 2.299, Validation Accuracy: 10.18%\n",
            "Epoch [7/10], Training Loss: 2.298, Validation Accuracy: 10.18%\n",
            "Epoch [8/10], Training Loss: 2.296, Validation Accuracy: 10.18%\n",
            "Epoch [9/10], Training Loss: 2.294, Validation Accuracy: 10.24%\n",
            "Epoch [10/10], Training Loss: 2.292, Validation Accuracy: 10.93%\n",
            "Epoch [1/10], Training Loss: 2.289, Validation Accuracy: 12.14%\n",
            "Epoch [2/10], Training Loss: 2.284, Validation Accuracy: 13.77%\n",
            "Epoch [3/10], Training Loss: 2.277, Validation Accuracy: 15.59%\n",
            "Epoch [4/10], Training Loss: 2.265, Validation Accuracy: 16.43%\n",
            "Epoch [5/10], Training Loss: 2.249, Validation Accuracy: 17.58%\n",
            "Epoch [6/10], Training Loss: 2.224, Validation Accuracy: 19.83%\n",
            "Epoch [7/10], Training Loss: 2.193, Validation Accuracy: 20.68%\n",
            "Epoch [8/10], Training Loss: 2.162, Validation Accuracy: 22.52%\n",
            "Epoch [9/10], Training Loss: 2.138, Validation Accuracy: 23.54%\n",
            "Epoch [10/10], Training Loss: 2.119, Validation Accuracy: 24.50%\n",
            "Epoch [1/10], Training Loss: 2.115, Validation Accuracy: 25.41%\n",
            "Epoch [2/10], Training Loss: 2.101, Validation Accuracy: 25.73%\n",
            "Epoch [3/10], Training Loss: 2.088, Validation Accuracy: 26.39%\n",
            "Epoch [4/10], Training Loss: 2.074, Validation Accuracy: 25.62%\n",
            "Epoch [5/10], Training Loss: 2.060, Validation Accuracy: 27.31%\n",
            "Epoch [6/10], Training Loss: 2.048, Validation Accuracy: 26.24%\n",
            "Epoch [7/10], Training Loss: 2.031, Validation Accuracy: 28.02%\n",
            "Epoch [8/10], Training Loss: 2.016, Validation Accuracy: 27.88%\n",
            "Epoch [9/10], Training Loss: 2.002, Validation Accuracy: 28.78%\n",
            "Epoch [10/10], Training Loss: 1.984, Validation Accuracy: 28.89%\n",
            "Epoch [1/10], Training Loss: 1.979, Validation Accuracy: 29.55%\n",
            "Epoch [2/10], Training Loss: 1.955, Validation Accuracy: 30.18%\n",
            "Epoch [3/10], Training Loss: 1.933, Validation Accuracy: 31.87%\n",
            "Epoch [4/10], Training Loss: 1.913, Validation Accuracy: 32.44%\n",
            "Epoch [5/10], Training Loss: 1.890, Validation Accuracy: 31.98%\n",
            "Epoch [6/10], Training Loss: 1.874, Validation Accuracy: 32.58%\n",
            "Epoch [7/10], Training Loss: 1.860, Validation Accuracy: 33.23%\n",
            "Epoch [8/10], Training Loss: 1.840, Validation Accuracy: 34.43%\n",
            "Epoch [9/10], Training Loss: 1.827, Validation Accuracy: 34.62%\n",
            "Epoch [10/10], Training Loss: 1.811, Validation Accuracy: 35.06%\n",
            "Epoch [1/10], Training Loss: 1.817, Validation Accuracy: 35.60%\n",
            "Epoch [2/10], Training Loss: 1.795, Validation Accuracy: 35.35%\n",
            "Epoch [3/10], Training Loss: 1.779, Validation Accuracy: 35.55%\n",
            "Epoch [4/10], Training Loss: 1.763, Validation Accuracy: 36.67%\n",
            "Epoch [5/10], Training Loss: 1.746, Validation Accuracy: 37.55%\n",
            "Epoch [6/10], Training Loss: 1.729, Validation Accuracy: 38.05%\n",
            "Epoch [7/10], Training Loss: 1.716, Validation Accuracy: 38.21%\n",
            "Epoch [8/10], Training Loss: 1.694, Validation Accuracy: 38.53%\n",
            "Epoch [9/10], Training Loss: 1.684, Validation Accuracy: 39.31%\n",
            "Epoch [10/10], Training Loss: 1.665, Validation Accuracy: 39.56%\n",
            "Epoch [1/10], Training Loss: 1.699, Validation Accuracy: 40.07%\n",
            "Epoch [2/10], Training Loss: 1.681, Validation Accuracy: 39.87%\n",
            "Epoch [3/10], Training Loss: 1.667, Validation Accuracy: 40.33%\n",
            "Epoch [4/10], Training Loss: 1.660, Validation Accuracy: 39.92%\n",
            "Epoch [5/10], Training Loss: 1.650, Validation Accuracy: 40.55%\n",
            "Epoch [6/10], Training Loss: 1.634, Validation Accuracy: 39.96%\n",
            "Epoch [7/10], Training Loss: 1.628, Validation Accuracy: 40.72%\n",
            "Epoch [8/10], Training Loss: 1.621, Validation Accuracy: 41.67%\n",
            "Epoch [9/10], Training Loss: 1.604, Validation Accuracy: 41.70%\n",
            "Epoch [10/10], Training Loss: 1.598, Validation Accuracy: 41.70%\n",
            "Epoch [1/10], Training Loss: 1.583, Validation Accuracy: 42.20%\n",
            "Epoch [2/10], Training Loss: 1.562, Validation Accuracy: 42.79%\n",
            "Epoch [3/10], Training Loss: 1.556, Validation Accuracy: 43.08%\n",
            "Epoch [4/10], Training Loss: 1.538, Validation Accuracy: 42.65%\n",
            "Epoch [5/10], Training Loss: 1.522, Validation Accuracy: 43.22%\n",
            "Epoch [6/10], Training Loss: 1.520, Validation Accuracy: 43.32%\n",
            "Epoch [7/10], Training Loss: 1.515, Validation Accuracy: 43.72%\n",
            "Epoch [8/10], Training Loss: 1.501, Validation Accuracy: 43.65%\n",
            "Epoch [9/10], Training Loss: 1.485, Validation Accuracy: 43.84%\n",
            "Epoch [10/10], Training Loss: 1.483, Validation Accuracy: 44.84%\n",
            "Epoch [1/10], Training Loss: 1.525, Validation Accuracy: 44.43%\n",
            "Epoch [2/10], Training Loss: 1.516, Validation Accuracy: 44.80%\n",
            "Epoch [3/10], Training Loss: 1.500, Validation Accuracy: 45.49%\n",
            "Epoch [4/10], Training Loss: 1.493, Validation Accuracy: 44.81%\n",
            "Epoch [5/10], Training Loss: 1.480, Validation Accuracy: 45.06%\n",
            "Epoch [6/10], Training Loss: 1.467, Validation Accuracy: 45.15%\n",
            "Epoch [7/10], Training Loss: 1.461, Validation Accuracy: 45.21%\n",
            "Epoch [8/10], Training Loss: 1.450, Validation Accuracy: 44.09%\n",
            "Epoch [9/10], Training Loss: 1.443, Validation Accuracy: 45.95%\n",
            "Epoch [10/10], Training Loss: 1.440, Validation Accuracy: 45.40%\n",
            "Epoch [1/10], Training Loss: 1.484, Validation Accuracy: 46.02%\n",
            "Epoch [2/10], Training Loss: 1.472, Validation Accuracy: 47.72%\n",
            "Epoch [3/10], Training Loss: 1.454, Validation Accuracy: 47.32%\n",
            "Epoch [4/10], Training Loss: 1.443, Validation Accuracy: 47.26%\n",
            "Epoch [5/10], Training Loss: 1.428, Validation Accuracy: 47.67%\n",
            "Epoch [6/10], Training Loss: 1.421, Validation Accuracy: 46.60%\n",
            "Epoch [7/10], Training Loss: 1.412, Validation Accuracy: 46.71%\n",
            "Epoch [8/10], Training Loss: 1.399, Validation Accuracy: 47.44%\n",
            "Epoch [9/10], Training Loss: 1.392, Validation Accuracy: 47.58%\n",
            "Epoch [10/10], Training Loss: 1.381, Validation Accuracy: 47.50%\n",
            "Epoch [1/10], Training Loss: 1.435, Validation Accuracy: 48.66%\n",
            "Epoch [2/10], Training Loss: 1.417, Validation Accuracy: 48.22%\n",
            "Epoch [3/10], Training Loss: 1.410, Validation Accuracy: 48.46%\n",
            "Epoch [4/10], Training Loss: 1.401, Validation Accuracy: 48.43%\n",
            "Epoch [5/10], Training Loss: 1.378, Validation Accuracy: 49.37%\n",
            "Epoch [6/10], Training Loss: 1.366, Validation Accuracy: 49.12%\n",
            "Epoch [7/10], Training Loss: 1.358, Validation Accuracy: 48.07%\n",
            "Epoch [8/10], Training Loss: 1.345, Validation Accuracy: 49.23%\n",
            "Epoch [9/10], Training Loss: 1.339, Validation Accuracy: 48.54%\n",
            "Epoch [10/10], Training Loss: 1.329, Validation Accuracy: 49.31%\n",
            "Epoch [1/10], Training Loss: 1.418, Validation Accuracy: 48.98%\n",
            "Epoch [2/10], Training Loss: 1.396, Validation Accuracy: 50.03%\n",
            "Epoch [3/10], Training Loss: 1.375, Validation Accuracy: 49.42%\n",
            "Epoch [4/10], Training Loss: 1.361, Validation Accuracy: 49.56%\n",
            "Epoch [5/10], Training Loss: 1.354, Validation Accuracy: 48.77%\n",
            "Epoch [6/10], Training Loss: 1.340, Validation Accuracy: 49.99%\n",
            "Epoch [7/10], Training Loss: 1.331, Validation Accuracy: 49.49%\n",
            "Epoch [8/10], Training Loss: 1.327, Validation Accuracy: 50.63%\n",
            "Epoch [9/10], Training Loss: 1.315, Validation Accuracy: 50.84%\n",
            "Epoch [10/10], Training Loss: 1.306, Validation Accuracy: 50.49%\n",
            "Epoch [1/10], Training Loss: 1.343, Validation Accuracy: 50.42%\n",
            "Epoch [2/10], Training Loss: 1.322, Validation Accuracy: 50.67%\n",
            "Epoch [3/10], Training Loss: 1.309, Validation Accuracy: 51.58%\n",
            "Epoch [4/10], Training Loss: 1.299, Validation Accuracy: 51.45%\n",
            "Epoch [5/10], Training Loss: 1.287, Validation Accuracy: 51.40%\n",
            "Epoch [6/10], Training Loss: 1.273, Validation Accuracy: 51.43%\n",
            "Epoch [7/10], Training Loss: 1.265, Validation Accuracy: 51.30%\n",
            "Epoch [8/10], Training Loss: 1.253, Validation Accuracy: 50.85%\n",
            "Epoch [9/10], Training Loss: 1.246, Validation Accuracy: 51.37%\n",
            "Epoch [10/10], Training Loss: 1.239, Validation Accuracy: 51.96%\n",
            "Epoch [1/10], Training Loss: 1.333, Validation Accuracy: 51.16%\n",
            "Epoch [2/10], Training Loss: 1.293, Validation Accuracy: 52.07%\n",
            "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 51.82%\n",
            "Epoch [4/10], Training Loss: 1.262, Validation Accuracy: 52.20%\n",
            "Epoch [5/10], Training Loss: 1.248, Validation Accuracy: 51.92%\n",
            "Epoch [6/10], Training Loss: 1.241, Validation Accuracy: 52.10%\n",
            "Epoch [7/10], Training Loss: 1.225, Validation Accuracy: 52.69%\n",
            "Epoch [8/10], Training Loss: 1.215, Validation Accuracy: 51.56%\n",
            "Epoch [9/10], Training Loss: 1.210, Validation Accuracy: 52.60%\n",
            "Epoch [10/10], Training Loss: 1.201, Validation Accuracy: 51.42%\n",
            "Epoch [1/10], Training Loss: 1.299, Validation Accuracy: 52.39%\n",
            "Epoch [2/10], Training Loss: 1.268, Validation Accuracy: 53.00%\n",
            "Epoch [3/10], Training Loss: 1.246, Validation Accuracy: 51.94%\n",
            "Epoch [4/10], Training Loss: 1.246, Validation Accuracy: 51.44%\n",
            "Epoch [5/10], Training Loss: 1.221, Validation Accuracy: 53.44%\n",
            "Epoch [6/10], Training Loss: 1.215, Validation Accuracy: 53.16%\n",
            "Epoch [7/10], Training Loss: 1.193, Validation Accuracy: 53.61%\n",
            "Epoch [8/10], Training Loss: 1.179, Validation Accuracy: 53.66%\n",
            "Epoch [9/10], Training Loss: 1.177, Validation Accuracy: 52.38%\n",
            "Epoch [10/10], Training Loss: 1.163, Validation Accuracy: 53.78%\n",
            "Epoch [1/10], Training Loss: 1.261, Validation Accuracy: 53.45%\n",
            "Epoch [2/10], Training Loss: 1.231, Validation Accuracy: 53.58%\n",
            "Epoch [3/10], Training Loss: 1.214, Validation Accuracy: 54.17%\n",
            "Epoch [4/10], Training Loss: 1.198, Validation Accuracy: 54.26%\n",
            "Epoch [5/10], Training Loss: 1.181, Validation Accuracy: 54.54%\n",
            "Epoch [6/10], Training Loss: 1.166, Validation Accuracy: 54.94%\n",
            "Epoch [7/10], Training Loss: 1.156, Validation Accuracy: 55.13%\n",
            "Epoch [8/10], Training Loss: 1.145, Validation Accuracy: 54.73%\n",
            "Epoch [9/10], Training Loss: 1.147, Validation Accuracy: 54.80%\n",
            "Epoch [10/10], Training Loss: 1.124, Validation Accuracy: 55.04%\n",
            "Epoch [1/10], Training Loss: 1.266, Validation Accuracy: 54.82%\n",
            "Epoch [2/10], Training Loss: 1.238, Validation Accuracy: 54.36%\n",
            "Epoch [3/10], Training Loss: 1.218, Validation Accuracy: 54.18%\n",
            "Epoch [4/10], Training Loss: 1.199, Validation Accuracy: 54.62%\n",
            "Epoch [5/10], Training Loss: 1.185, Validation Accuracy: 54.46%\n",
            "Epoch [6/10], Training Loss: 1.177, Validation Accuracy: 53.67%\n",
            "Epoch [7/10], Training Loss: 1.154, Validation Accuracy: 54.82%\n",
            "Epoch [8/10], Training Loss: 1.146, Validation Accuracy: 55.76%\n",
            "Epoch [9/10], Training Loss: 1.130, Validation Accuracy: 55.01%\n",
            "Epoch [10/10], Training Loss: 1.120, Validation Accuracy: 55.38%\n",
            "Epoch [1/10], Training Loss: 1.211, Validation Accuracy: 54.68%\n",
            "Epoch [2/10], Training Loss: 1.181, Validation Accuracy: 55.88%\n",
            "Epoch [3/10], Training Loss: 1.154, Validation Accuracy: 55.89%\n",
            "Epoch [4/10], Training Loss: 1.143, Validation Accuracy: 56.35%\n",
            "Epoch [5/10], Training Loss: 1.130, Validation Accuracy: 56.08%\n",
            "Epoch [6/10], Training Loss: 1.119, Validation Accuracy: 55.30%\n",
            "Epoch [7/10], Training Loss: 1.100, Validation Accuracy: 56.68%\n",
            "Epoch [8/10], Training Loss: 1.090, Validation Accuracy: 55.98%\n",
            "Epoch [9/10], Training Loss: 1.075, Validation Accuracy: 55.80%\n",
            "Epoch [10/10], Training Loss: 1.072, Validation Accuracy: 55.30%\n",
            "Epoch [1/10], Training Loss: 1.169, Validation Accuracy: 56.00%\n",
            "Epoch [2/10], Training Loss: 1.140, Validation Accuracy: 56.44%\n",
            "Epoch [3/10], Training Loss: 1.122, Validation Accuracy: 55.52%\n",
            "Epoch [4/10], Training Loss: 1.106, Validation Accuracy: 56.68%\n",
            "Epoch [5/10], Training Loss: 1.089, Validation Accuracy: 56.72%\n",
            "Epoch [6/10], Training Loss: 1.076, Validation Accuracy: 56.74%\n",
            "Epoch [7/10], Training Loss: 1.056, Validation Accuracy: 56.36%\n",
            "Epoch [8/10], Training Loss: 1.049, Validation Accuracy: 56.61%\n",
            "Epoch [9/10], Training Loss: 1.041, Validation Accuracy: 56.23%\n",
            "Epoch [10/10], Training Loss: 1.040, Validation Accuracy: 56.16%\n",
            "Epoch [1/10], Training Loss: 1.184, Validation Accuracy: 56.76%\n",
            "Epoch [2/10], Training Loss: 1.143, Validation Accuracy: 56.39%\n",
            "Epoch [3/10], Training Loss: 1.119, Validation Accuracy: 57.06%\n",
            "Epoch [4/10], Training Loss: 1.096, Validation Accuracy: 56.99%\n",
            "Epoch [5/10], Training Loss: 1.081, Validation Accuracy: 55.65%\n",
            "Epoch [6/10], Training Loss: 1.073, Validation Accuracy: 57.20%\n",
            "Epoch [7/10], Training Loss: 1.059, Validation Accuracy: 56.83%\n",
            "Epoch [8/10], Training Loss: 1.049, Validation Accuracy: 56.34%\n",
            "Epoch [9/10], Training Loss: 1.024, Validation Accuracy: 57.34%\n",
            "Epoch [10/10], Training Loss: 1.027, Validation Accuracy: 56.36%\n",
            "Epoch [1/10], Training Loss: 1.144, Validation Accuracy: 57.52%\n",
            "Epoch [2/10], Training Loss: 1.109, Validation Accuracy: 57.85%\n",
            "Epoch [3/10], Training Loss: 1.080, Validation Accuracy: 57.19%\n",
            "Epoch [4/10], Training Loss: 1.066, Validation Accuracy: 57.73%\n",
            "Epoch [5/10], Training Loss: 1.041, Validation Accuracy: 58.11%\n",
            "Epoch [6/10], Training Loss: 1.021, Validation Accuracy: 58.12%\n",
            "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 56.06%\n",
            "Epoch [8/10], Training Loss: 1.002, Validation Accuracy: 56.95%\n",
            "Epoch [9/10], Training Loss: 0.991, Validation Accuracy: 57.39%\n",
            "Epoch [10/10], Training Loss: 0.981, Validation Accuracy: 56.70%\n",
            "Epoch [1/10], Training Loss: 1.161, Validation Accuracy: 57.83%\n",
            "Epoch [2/10], Training Loss: 1.117, Validation Accuracy: 58.32%\n",
            "Epoch [3/10], Training Loss: 1.107, Validation Accuracy: 56.51%\n",
            "Epoch [4/10], Training Loss: 1.083, Validation Accuracy: 57.64%\n",
            "Epoch [5/10], Training Loss: 1.057, Validation Accuracy: 58.62%\n",
            "Epoch [6/10], Training Loss: 1.031, Validation Accuracy: 57.91%\n",
            "Epoch [7/10], Training Loss: 1.026, Validation Accuracy: 57.09%\n",
            "Epoch [8/10], Training Loss: 1.017, Validation Accuracy: 57.86%\n",
            "Epoch [9/10], Training Loss: 0.997, Validation Accuracy: 58.08%\n",
            "Epoch [10/10], Training Loss: 0.984, Validation Accuracy: 57.44%\n",
            "Epoch [1/10], Training Loss: 1.113, Validation Accuracy: 58.49%\n",
            "Epoch [2/10], Training Loss: 1.072, Validation Accuracy: 58.97%\n",
            "Epoch [3/10], Training Loss: 1.039, Validation Accuracy: 58.30%\n",
            "Epoch [4/10], Training Loss: 1.032, Validation Accuracy: 57.63%\n",
            "Epoch [5/10], Training Loss: 1.002, Validation Accuracy: 58.23%\n",
            "Epoch [6/10], Training Loss: 0.993, Validation Accuracy: 57.33%\n",
            "Epoch [7/10], Training Loss: 0.969, Validation Accuracy: 59.08%\n",
            "Epoch [8/10], Training Loss: 0.950, Validation Accuracy: 58.80%\n",
            "Epoch [9/10], Training Loss: 0.946, Validation Accuracy: 58.32%\n",
            "Epoch [10/10], Training Loss: 0.935, Validation Accuracy: 59.34%\n",
            "Epoch [1/10], Training Loss: 1.073, Validation Accuracy: 57.89%\n",
            "Epoch [2/10], Training Loss: 1.030, Validation Accuracy: 58.47%\n",
            "Epoch [3/10], Training Loss: 1.001, Validation Accuracy: 58.12%\n",
            "Epoch [4/10], Training Loss: 0.981, Validation Accuracy: 59.13%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 58.94%\n",
            "Epoch [6/10], Training Loss: 0.946, Validation Accuracy: 58.85%\n",
            "Epoch [7/10], Training Loss: 0.941, Validation Accuracy: 58.55%\n",
            "Epoch [8/10], Training Loss: 0.921, Validation Accuracy: 59.16%\n",
            "Epoch [9/10], Training Loss: 0.904, Validation Accuracy: 58.96%\n",
            "Epoch [10/10], Training Loss: 0.892, Validation Accuracy: 58.83%\n",
            "Epoch [1/10], Training Loss: 1.091, Validation Accuracy: 59.39%\n",
            "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 58.19%\n",
            "Epoch [3/10], Training Loss: 1.017, Validation Accuracy: 59.05%\n",
            "Epoch [4/10], Training Loss: 0.995, Validation Accuracy: 59.02%\n",
            "Epoch [5/10], Training Loss: 0.984, Validation Accuracy: 58.67%\n",
            "Epoch [6/10], Training Loss: 0.969, Validation Accuracy: 59.19%\n",
            "Epoch [7/10], Training Loss: 0.945, Validation Accuracy: 58.84%\n",
            "Epoch [8/10], Training Loss: 0.935, Validation Accuracy: 58.98%\n",
            "Epoch [9/10], Training Loss: 0.914, Validation Accuracy: 58.50%\n",
            "Epoch [10/10], Training Loss: 0.899, Validation Accuracy: 58.04%\n",
            "Epoch [1/10], Training Loss: 1.070, Validation Accuracy: 59.25%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 59.10%\n",
            "Epoch [3/10], Training Loss: 0.983, Validation Accuracy: 58.96%\n",
            "Epoch [4/10], Training Loss: 0.949, Validation Accuracy: 58.77%\n",
            "Epoch [5/10], Training Loss: 0.938, Validation Accuracy: 59.31%\n",
            "Epoch [6/10], Training Loss: 0.914, Validation Accuracy: 59.11%\n",
            "Epoch [7/10], Training Loss: 0.905, Validation Accuracy: 59.68%\n",
            "Epoch [8/10], Training Loss: 0.885, Validation Accuracy: 59.71%\n",
            "Epoch [9/10], Training Loss: 0.867, Validation Accuracy: 59.31%\n",
            "Epoch [10/10], Training Loss: 0.858, Validation Accuracy: 59.58%\n",
            "Test Accuracy: 59.24%\n"
          ]
        }
      ],
      "source": [
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=5)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na6fxgaHcSv8",
        "outputId": "e6075a7d-0637-410d-e0d4-64feb4737544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44975392.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 9.89%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 9.89%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 9.89%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 9.89%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 9.89%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 9.89%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 9.89%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 9.89%\n",
            "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 9.89%\n",
            "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 9.89%\n",
            "Epoch [1/10], Training Loss: 2.297, Validation Accuracy: 9.89%\n",
            "Epoch [2/10], Training Loss: 2.296, Validation Accuracy: 9.90%\n",
            "Epoch [3/10], Training Loss: 2.294, Validation Accuracy: 10.05%\n",
            "Epoch [4/10], Training Loss: 2.291, Validation Accuracy: 11.80%\n",
            "Epoch [5/10], Training Loss: 2.288, Validation Accuracy: 14.01%\n",
            "Epoch [6/10], Training Loss: 2.283, Validation Accuracy: 14.36%\n",
            "Epoch [7/10], Training Loss: 2.276, Validation Accuracy: 14.27%\n",
            "Epoch [8/10], Training Loss: 2.266, Validation Accuracy: 15.41%\n",
            "Epoch [9/10], Training Loss: 2.252, Validation Accuracy: 16.01%\n",
            "Epoch [10/10], Training Loss: 2.231, Validation Accuracy: 17.05%\n",
            "Epoch [1/10], Training Loss: 2.213, Validation Accuracy: 19.88%\n",
            "Epoch [2/10], Training Loss: 2.179, Validation Accuracy: 21.49%\n",
            "Epoch [3/10], Training Loss: 2.142, Validation Accuracy: 23.10%\n",
            "Epoch [4/10], Training Loss: 2.115, Validation Accuracy: 24.23%\n",
            "Epoch [5/10], Training Loss: 2.090, Validation Accuracy: 25.19%\n",
            "Epoch [6/10], Training Loss: 2.063, Validation Accuracy: 26.34%\n",
            "Epoch [7/10], Training Loss: 2.039, Validation Accuracy: 26.71%\n",
            "Epoch [8/10], Training Loss: 2.013, Validation Accuracy: 27.22%\n",
            "Epoch [9/10], Training Loss: 1.991, Validation Accuracy: 27.68%\n",
            "Epoch [10/10], Training Loss: 1.972, Validation Accuracy: 27.52%\n",
            "Epoch [1/10], Training Loss: 1.961, Validation Accuracy: 28.60%\n",
            "Epoch [2/10], Training Loss: 1.944, Validation Accuracy: 29.16%\n",
            "Epoch [3/10], Training Loss: 1.930, Validation Accuracy: 29.75%\n",
            "Epoch [4/10], Training Loss: 1.917, Validation Accuracy: 30.42%\n",
            "Epoch [5/10], Training Loss: 1.902, Validation Accuracy: 31.17%\n",
            "Epoch [6/10], Training Loss: 1.886, Validation Accuracy: 31.34%\n",
            "Epoch [7/10], Training Loss: 1.871, Validation Accuracy: 31.20%\n",
            "Epoch [8/10], Training Loss: 1.857, Validation Accuracy: 31.76%\n",
            "Epoch [9/10], Training Loss: 1.837, Validation Accuracy: 32.39%\n",
            "Epoch [10/10], Training Loss: 1.820, Validation Accuracy: 33.22%\n",
            "Epoch [1/10], Training Loss: 1.830, Validation Accuracy: 33.81%\n",
            "Epoch [2/10], Training Loss: 1.816, Validation Accuracy: 34.08%\n",
            "Epoch [3/10], Training Loss: 1.802, Validation Accuracy: 34.79%\n",
            "Epoch [4/10], Training Loss: 1.787, Validation Accuracy: 34.94%\n",
            "Epoch [5/10], Training Loss: 1.775, Validation Accuracy: 35.08%\n",
            "Epoch [6/10], Training Loss: 1.759, Validation Accuracy: 35.60%\n",
            "Epoch [7/10], Training Loss: 1.748, Validation Accuracy: 35.07%\n",
            "Epoch [8/10], Training Loss: 1.738, Validation Accuracy: 36.25%\n",
            "Epoch [9/10], Training Loss: 1.722, Validation Accuracy: 36.16%\n",
            "Epoch [10/10], Training Loss: 1.708, Validation Accuracy: 37.01%\n",
            "Epoch [1/10], Training Loss: 1.713, Validation Accuracy: 37.73%\n",
            "Epoch [2/10], Training Loss: 1.697, Validation Accuracy: 37.92%\n",
            "Epoch [3/10], Training Loss: 1.679, Validation Accuracy: 38.34%\n",
            "Epoch [4/10], Training Loss: 1.670, Validation Accuracy: 39.26%\n",
            "Epoch [5/10], Training Loss: 1.648, Validation Accuracy: 39.74%\n",
            "Epoch [6/10], Training Loss: 1.633, Validation Accuracy: 40.38%\n",
            "Epoch [7/10], Training Loss: 1.621, Validation Accuracy: 40.66%\n",
            "Epoch [8/10], Training Loss: 1.613, Validation Accuracy: 40.50%\n",
            "Epoch [9/10], Training Loss: 1.606, Validation Accuracy: 41.16%\n",
            "Epoch [10/10], Training Loss: 1.589, Validation Accuracy: 41.16%\n",
            "Epoch [1/10], Training Loss: 1.594, Validation Accuracy: 41.27%\n",
            "Epoch [2/10], Training Loss: 1.582, Validation Accuracy: 41.93%\n",
            "Epoch [3/10], Training Loss: 1.569, Validation Accuracy: 42.55%\n",
            "Epoch [4/10], Training Loss: 1.566, Validation Accuracy: 41.94%\n",
            "Epoch [5/10], Training Loss: 1.550, Validation Accuracy: 42.29%\n",
            "Epoch [6/10], Training Loss: 1.542, Validation Accuracy: 43.00%\n",
            "Epoch [7/10], Training Loss: 1.536, Validation Accuracy: 42.71%\n",
            "Epoch [8/10], Training Loss: 1.527, Validation Accuracy: 43.48%\n",
            "Epoch [9/10], Training Loss: 1.525, Validation Accuracy: 43.33%\n",
            "Epoch [10/10], Training Loss: 1.519, Validation Accuracy: 43.78%\n",
            "Epoch [1/10], Training Loss: 1.537, Validation Accuracy: 43.60%\n",
            "Epoch [2/10], Training Loss: 1.523, Validation Accuracy: 44.25%\n",
            "Epoch [3/10], Training Loss: 1.513, Validation Accuracy: 44.79%\n",
            "Epoch [4/10], Training Loss: 1.506, Validation Accuracy: 44.12%\n",
            "Epoch [5/10], Training Loss: 1.502, Validation Accuracy: 44.43%\n",
            "Epoch [6/10], Training Loss: 1.498, Validation Accuracy: 44.76%\n",
            "Epoch [7/10], Training Loss: 1.488, Validation Accuracy: 45.19%\n",
            "Epoch [8/10], Training Loss: 1.483, Validation Accuracy: 45.29%\n",
            "Epoch [9/10], Training Loss: 1.474, Validation Accuracy: 45.16%\n",
            "Epoch [10/10], Training Loss: 1.464, Validation Accuracy: 45.61%\n",
            "Epoch [1/10], Training Loss: 1.470, Validation Accuracy: 45.77%\n",
            "Epoch [2/10], Training Loss: 1.459, Validation Accuracy: 45.85%\n",
            "Epoch [3/10], Training Loss: 1.454, Validation Accuracy: 46.24%\n",
            "Epoch [4/10], Training Loss: 1.441, Validation Accuracy: 46.14%\n",
            "Epoch [5/10], Training Loss: 1.429, Validation Accuracy: 46.65%\n",
            "Epoch [6/10], Training Loss: 1.438, Validation Accuracy: 46.16%\n",
            "Epoch [7/10], Training Loss: 1.423, Validation Accuracy: 46.26%\n",
            "Epoch [8/10], Training Loss: 1.416, Validation Accuracy: 46.58%\n",
            "Epoch [9/10], Training Loss: 1.409, Validation Accuracy: 47.02%\n",
            "Epoch [10/10], Training Loss: 1.402, Validation Accuracy: 47.42%\n",
            "Epoch [1/10], Training Loss: 1.458, Validation Accuracy: 47.18%\n",
            "Epoch [2/10], Training Loss: 1.449, Validation Accuracy: 47.64%\n",
            "Epoch [3/10], Training Loss: 1.441, Validation Accuracy: 47.32%\n",
            "Epoch [4/10], Training Loss: 1.434, Validation Accuracy: 47.77%\n",
            "Epoch [5/10], Training Loss: 1.421, Validation Accuracy: 47.66%\n",
            "Epoch [6/10], Training Loss: 1.415, Validation Accuracy: 47.22%\n",
            "Epoch [7/10], Training Loss: 1.413, Validation Accuracy: 48.34%\n",
            "Epoch [8/10], Training Loss: 1.401, Validation Accuracy: 48.53%\n",
            "Epoch [9/10], Training Loss: 1.389, Validation Accuracy: 48.21%\n",
            "Epoch [10/10], Training Loss: 1.385, Validation Accuracy: 46.46%\n",
            "Epoch [1/10], Training Loss: 1.431, Validation Accuracy: 48.53%\n",
            "Epoch [2/10], Training Loss: 1.413, Validation Accuracy: 48.52%\n",
            "Epoch [3/10], Training Loss: 1.401, Validation Accuracy: 48.53%\n",
            "Epoch [4/10], Training Loss: 1.395, Validation Accuracy: 49.06%\n",
            "Epoch [5/10], Training Loss: 1.383, Validation Accuracy: 48.32%\n",
            "Epoch [6/10], Training Loss: 1.374, Validation Accuracy: 49.13%\n",
            "Epoch [7/10], Training Loss: 1.371, Validation Accuracy: 49.09%\n",
            "Epoch [8/10], Training Loss: 1.360, Validation Accuracy: 48.39%\n",
            "Epoch [9/10], Training Loss: 1.356, Validation Accuracy: 49.79%\n",
            "Epoch [10/10], Training Loss: 1.344, Validation Accuracy: 49.42%\n",
            "Epoch [1/10], Training Loss: 1.383, Validation Accuracy: 49.62%\n",
            "Epoch [2/10], Training Loss: 1.365, Validation Accuracy: 50.18%\n",
            "Epoch [3/10], Training Loss: 1.350, Validation Accuracy: 49.71%\n",
            "Epoch [4/10], Training Loss: 1.346, Validation Accuracy: 50.38%\n",
            "Epoch [5/10], Training Loss: 1.339, Validation Accuracy: 50.64%\n",
            "Epoch [6/10], Training Loss: 1.332, Validation Accuracy: 50.49%\n",
            "Epoch [7/10], Training Loss: 1.320, Validation Accuracy: 50.81%\n",
            "Epoch [8/10], Training Loss: 1.319, Validation Accuracy: 50.69%\n",
            "Epoch [9/10], Training Loss: 1.306, Validation Accuracy: 50.79%\n",
            "Epoch [10/10], Training Loss: 1.305, Validation Accuracy: 50.40%\n",
            "Epoch [1/10], Training Loss: 1.347, Validation Accuracy: 51.25%\n",
            "Epoch [2/10], Training Loss: 1.339, Validation Accuracy: 51.56%\n",
            "Epoch [3/10], Training Loss: 1.319, Validation Accuracy: 51.83%\n",
            "Epoch [4/10], Training Loss: 1.307, Validation Accuracy: 51.59%\n",
            "Epoch [5/10], Training Loss: 1.309, Validation Accuracy: 51.28%\n",
            "Epoch [6/10], Training Loss: 1.293, Validation Accuracy: 51.29%\n",
            "Epoch [7/10], Training Loss: 1.291, Validation Accuracy: 51.41%\n",
            "Epoch [8/10], Training Loss: 1.277, Validation Accuracy: 52.19%\n",
            "Epoch [9/10], Training Loss: 1.266, Validation Accuracy: 51.60%\n",
            "Epoch [10/10], Training Loss: 1.257, Validation Accuracy: 51.63%\n",
            "Epoch [1/10], Training Loss: 1.304, Validation Accuracy: 52.33%\n",
            "Epoch [2/10], Training Loss: 1.282, Validation Accuracy: 52.37%\n",
            "Epoch [3/10], Training Loss: 1.263, Validation Accuracy: 52.24%\n",
            "Epoch [4/10], Training Loss: 1.257, Validation Accuracy: 52.40%\n",
            "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 52.10%\n",
            "Epoch [6/10], Training Loss: 1.239, Validation Accuracy: 53.48%\n",
            "Epoch [7/10], Training Loss: 1.235, Validation Accuracy: 53.22%\n",
            "Epoch [8/10], Training Loss: 1.220, Validation Accuracy: 52.68%\n",
            "Epoch [9/10], Training Loss: 1.212, Validation Accuracy: 52.91%\n",
            "Epoch [10/10], Training Loss: 1.215, Validation Accuracy: 51.90%\n",
            "Epoch [1/10], Training Loss: 1.296, Validation Accuracy: 52.50%\n",
            "Epoch [2/10], Training Loss: 1.270, Validation Accuracy: 52.99%\n",
            "Epoch [3/10], Training Loss: 1.252, Validation Accuracy: 53.83%\n",
            "Epoch [4/10], Training Loss: 1.240, Validation Accuracy: 53.30%\n",
            "Epoch [5/10], Training Loss: 1.233, Validation Accuracy: 53.85%\n",
            "Epoch [6/10], Training Loss: 1.225, Validation Accuracy: 53.61%\n",
            "Epoch [7/10], Training Loss: 1.209, Validation Accuracy: 53.76%\n",
            "Epoch [8/10], Training Loss: 1.207, Validation Accuracy: 52.71%\n",
            "Epoch [9/10], Training Loss: 1.200, Validation Accuracy: 53.21%\n",
            "Epoch [10/10], Training Loss: 1.188, Validation Accuracy: 54.21%\n",
            "Epoch [1/10], Training Loss: 1.267, Validation Accuracy: 54.21%\n",
            "Epoch [2/10], Training Loss: 1.249, Validation Accuracy: 53.70%\n",
            "Epoch [3/10], Training Loss: 1.229, Validation Accuracy: 54.41%\n",
            "Epoch [4/10], Training Loss: 1.213, Validation Accuracy: 54.99%\n",
            "Epoch [5/10], Training Loss: 1.199, Validation Accuracy: 54.79%\n",
            "Epoch [6/10], Training Loss: 1.199, Validation Accuracy: 54.27%\n",
            "Epoch [7/10], Training Loss: 1.184, Validation Accuracy: 54.76%\n",
            "Epoch [8/10], Training Loss: 1.174, Validation Accuracy: 54.35%\n",
            "Epoch [9/10], Training Loss: 1.166, Validation Accuracy: 54.77%\n",
            "Epoch [10/10], Training Loss: 1.157, Validation Accuracy: 54.92%\n",
            "Epoch [1/10], Training Loss: 1.236, Validation Accuracy: 55.68%\n",
            "Epoch [2/10], Training Loss: 1.213, Validation Accuracy: 55.70%\n",
            "Epoch [3/10], Training Loss: 1.202, Validation Accuracy: 54.97%\n",
            "Epoch [4/10], Training Loss: 1.191, Validation Accuracy: 55.97%\n",
            "Epoch [5/10], Training Loss: 1.176, Validation Accuracy: 55.82%\n",
            "Epoch [6/10], Training Loss: 1.165, Validation Accuracy: 55.66%\n",
            "Epoch [7/10], Training Loss: 1.160, Validation Accuracy: 55.47%\n",
            "Epoch [8/10], Training Loss: 1.149, Validation Accuracy: 56.19%\n",
            "Epoch [9/10], Training Loss: 1.138, Validation Accuracy: 56.17%\n",
            "Epoch [10/10], Training Loss: 1.130, Validation Accuracy: 55.90%\n",
            "Epoch [1/10], Training Loss: 1.218, Validation Accuracy: 55.22%\n",
            "Epoch [2/10], Training Loss: 1.190, Validation Accuracy: 56.28%\n",
            "Epoch [3/10], Training Loss: 1.182, Validation Accuracy: 55.57%\n",
            "Epoch [4/10], Training Loss: 1.163, Validation Accuracy: 56.08%\n",
            "Epoch [5/10], Training Loss: 1.154, Validation Accuracy: 55.89%\n",
            "Epoch [6/10], Training Loss: 1.136, Validation Accuracy: 55.74%\n",
            "Epoch [7/10], Training Loss: 1.128, Validation Accuracy: 55.81%\n",
            "Epoch [8/10], Training Loss: 1.121, Validation Accuracy: 56.81%\n",
            "Epoch [9/10], Training Loss: 1.106, Validation Accuracy: 56.53%\n",
            "Epoch [10/10], Training Loss: 1.101, Validation Accuracy: 55.60%\n",
            "Epoch [1/10], Training Loss: 1.169, Validation Accuracy: 54.27%\n",
            "Epoch [2/10], Training Loss: 1.147, Validation Accuracy: 57.01%\n",
            "Epoch [3/10], Training Loss: 1.126, Validation Accuracy: 56.94%\n",
            "Epoch [4/10], Training Loss: 1.110, Validation Accuracy: 56.63%\n",
            "Epoch [5/10], Training Loss: 1.097, Validation Accuracy: 56.18%\n",
            "Epoch [6/10], Training Loss: 1.093, Validation Accuracy: 56.34%\n",
            "Epoch [7/10], Training Loss: 1.087, Validation Accuracy: 56.97%\n",
            "Epoch [8/10], Training Loss: 1.059, Validation Accuracy: 56.58%\n",
            "Epoch [9/10], Training Loss: 1.057, Validation Accuracy: 57.59%\n",
            "Epoch [10/10], Training Loss: 1.037, Validation Accuracy: 57.36%\n",
            "Epoch [1/10], Training Loss: 1.177, Validation Accuracy: 57.54%\n",
            "Epoch [2/10], Training Loss: 1.143, Validation Accuracy: 57.49%\n",
            "Epoch [3/10], Training Loss: 1.124, Validation Accuracy: 57.50%\n",
            "Epoch [4/10], Training Loss: 1.103, Validation Accuracy: 57.57%\n",
            "Epoch [5/10], Training Loss: 1.093, Validation Accuracy: 57.98%\n",
            "Epoch [6/10], Training Loss: 1.084, Validation Accuracy: 57.73%\n",
            "Epoch [7/10], Training Loss: 1.067, Validation Accuracy: 57.86%\n",
            "Epoch [8/10], Training Loss: 1.065, Validation Accuracy: 56.37%\n",
            "Epoch [9/10], Training Loss: 1.047, Validation Accuracy: 57.81%\n",
            "Epoch [10/10], Training Loss: 1.045, Validation Accuracy: 57.96%\n",
            "Epoch [1/10], Training Loss: 1.155, Validation Accuracy: 57.84%\n",
            "Epoch [2/10], Training Loss: 1.123, Validation Accuracy: 57.90%\n",
            "Epoch [3/10], Training Loss: 1.101, Validation Accuracy: 58.31%\n",
            "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 58.11%\n",
            "Epoch [5/10], Training Loss: 1.071, Validation Accuracy: 57.74%\n",
            "Epoch [6/10], Training Loss: 1.061, Validation Accuracy: 57.87%\n",
            "Epoch [7/10], Training Loss: 1.052, Validation Accuracy: 57.58%\n",
            "Epoch [8/10], Training Loss: 1.035, Validation Accuracy: 57.87%\n",
            "Epoch [9/10], Training Loss: 1.021, Validation Accuracy: 57.69%\n",
            "Epoch [10/10], Training Loss: 1.018, Validation Accuracy: 58.39%\n",
            "Epoch [1/10], Training Loss: 1.143, Validation Accuracy: 57.43%\n",
            "Epoch [2/10], Training Loss: 1.112, Validation Accuracy: 57.52%\n",
            "Epoch [3/10], Training Loss: 1.092, Validation Accuracy: 58.54%\n",
            "Epoch [4/10], Training Loss: 1.073, Validation Accuracy: 58.99%\n",
            "Epoch [5/10], Training Loss: 1.053, Validation Accuracy: 58.65%\n",
            "Epoch [6/10], Training Loss: 1.035, Validation Accuracy: 58.87%\n",
            "Epoch [7/10], Training Loss: 1.033, Validation Accuracy: 58.70%\n",
            "Epoch [8/10], Training Loss: 1.023, Validation Accuracy: 57.97%\n",
            "Epoch [9/10], Training Loss: 1.005, Validation Accuracy: 58.61%\n",
            "Epoch [10/10], Training Loss: 0.991, Validation Accuracy: 58.78%\n",
            "Epoch [1/10], Training Loss: 1.124, Validation Accuracy: 58.41%\n",
            "Epoch [2/10], Training Loss: 1.092, Validation Accuracy: 58.99%\n",
            "Epoch [3/10], Training Loss: 1.069, Validation Accuracy: 58.86%\n",
            "Epoch [4/10], Training Loss: 1.054, Validation Accuracy: 58.51%\n",
            "Epoch [5/10], Training Loss: 1.035, Validation Accuracy: 59.07%\n",
            "Epoch [6/10], Training Loss: 1.027, Validation Accuracy: 58.84%\n",
            "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 58.63%\n",
            "Epoch [8/10], Training Loss: 0.995, Validation Accuracy: 58.58%\n",
            "Epoch [9/10], Training Loss: 0.987, Validation Accuracy: 58.82%\n",
            "Epoch [10/10], Training Loss: 0.976, Validation Accuracy: 59.16%\n",
            "Epoch [1/10], Training Loss: 1.078, Validation Accuracy: 58.80%\n",
            "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.16%\n",
            "Epoch [3/10], Training Loss: 1.014, Validation Accuracy: 59.32%\n",
            "Epoch [4/10], Training Loss: 0.995, Validation Accuracy: 58.74%\n",
            "Epoch [5/10], Training Loss: 0.984, Validation Accuracy: 58.73%\n",
            "Epoch [6/10], Training Loss: 0.969, Validation Accuracy: 58.45%\n",
            "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 58.41%\n",
            "Epoch [8/10], Training Loss: 0.950, Validation Accuracy: 58.57%\n",
            "Epoch [9/10], Training Loss: 0.936, Validation Accuracy: 58.25%\n",
            "Epoch [10/10], Training Loss: 0.923, Validation Accuracy: 59.41%\n",
            "Epoch [1/10], Training Loss: 1.084, Validation Accuracy: 59.75%\n",
            "Epoch [2/10], Training Loss: 1.048, Validation Accuracy: 59.60%\n",
            "Epoch [3/10], Training Loss: 1.024, Validation Accuracy: 58.71%\n",
            "Epoch [4/10], Training Loss: 0.999, Validation Accuracy: 58.89%\n",
            "Epoch [5/10], Training Loss: 0.986, Validation Accuracy: 59.49%\n",
            "Epoch [6/10], Training Loss: 0.972, Validation Accuracy: 59.39%\n",
            "Epoch [7/10], Training Loss: 0.956, Validation Accuracy: 59.59%\n",
            "Epoch [8/10], Training Loss: 0.939, Validation Accuracy: 58.78%\n",
            "Epoch [9/10], Training Loss: 0.930, Validation Accuracy: 60.04%\n",
            "Epoch [10/10], Training Loss: 0.925, Validation Accuracy: 59.48%\n",
            "Epoch [1/10], Training Loss: 1.071, Validation Accuracy: 60.47%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 60.00%\n",
            "Epoch [3/10], Training Loss: 1.005, Validation Accuracy: 60.19%\n",
            "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 59.80%\n",
            "Epoch [5/10], Training Loss: 0.962, Validation Accuracy: 59.99%\n",
            "Epoch [6/10], Training Loss: 0.952, Validation Accuracy: 59.92%\n",
            "Epoch [7/10], Training Loss: 0.932, Validation Accuracy: 59.81%\n",
            "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 60.10%\n",
            "Epoch [9/10], Training Loss: 0.910, Validation Accuracy: 60.04%\n",
            "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 59.38%\n",
            "Epoch [1/10], Training Loss: 1.059, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 1.016, Validation Accuracy: 59.64%\n",
            "Epoch [3/10], Training Loss: 0.993, Validation Accuracy: 59.80%\n",
            "Epoch [4/10], Training Loss: 0.974, Validation Accuracy: 59.81%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.12%\n",
            "Epoch [6/10], Training Loss: 0.933, Validation Accuracy: 60.03%\n",
            "Epoch [7/10], Training Loss: 0.926, Validation Accuracy: 58.60%\n",
            "Epoch [8/10], Training Loss: 0.912, Validation Accuracy: 60.66%\n",
            "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 60.23%\n",
            "Epoch [10/10], Training Loss: 0.882, Validation Accuracy: 60.01%\n",
            "Epoch [1/10], Training Loss: 1.054, Validation Accuracy: 60.57%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 60.59%\n",
            "Epoch [3/10], Training Loss: 0.980, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.959, Validation Accuracy: 60.45%\n",
            "Epoch [5/10], Training Loss: 0.941, Validation Accuracy: 59.54%\n",
            "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 60.58%\n",
            "Epoch [7/10], Training Loss: 0.904, Validation Accuracy: 60.20%\n",
            "Epoch [8/10], Training Loss: 0.907, Validation Accuracy: 60.44%\n",
            "Epoch [9/10], Training Loss: 0.887, Validation Accuracy: 59.70%\n",
            "Epoch [10/10], Training Loss: 0.881, Validation Accuracy: 59.99%\n",
            "Epoch [1/10], Training Loss: 1.015, Validation Accuracy: 60.68%\n",
            "Epoch [2/10], Training Loss: 0.954, Validation Accuracy: 61.33%\n",
            "Epoch [3/10], Training Loss: 0.931, Validation Accuracy: 61.07%\n",
            "Epoch [4/10], Training Loss: 0.908, Validation Accuracy: 60.68%\n",
            "Epoch [5/10], Training Loss: 0.901, Validation Accuracy: 60.42%\n",
            "Epoch [6/10], Training Loss: 0.876, Validation Accuracy: 60.09%\n",
            "Epoch [7/10], Training Loss: 0.862, Validation Accuracy: 60.44%\n",
            "Epoch [8/10], Training Loss: 0.846, Validation Accuracy: 60.24%\n",
            "Epoch [9/10], Training Loss: 0.836, Validation Accuracy: 59.63%\n",
            "Epoch [10/10], Training Loss: 0.814, Validation Accuracy: 60.46%\n",
            "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 60.50%\n",
            "Epoch [2/10], Training Loss: 0.971, Validation Accuracy: 60.71%\n",
            "Epoch [3/10], Training Loss: 0.931, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.915, Validation Accuracy: 59.92%\n",
            "Epoch [5/10], Training Loss: 0.895, Validation Accuracy: 60.71%\n",
            "Epoch [6/10], Training Loss: 0.883, Validation Accuracy: 60.16%\n",
            "Epoch [7/10], Training Loss: 0.864, Validation Accuracy: 60.86%\n",
            "Epoch [8/10], Training Loss: 0.852, Validation Accuracy: 60.39%\n",
            "Epoch [9/10], Training Loss: 0.828, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.825, Validation Accuracy: 60.05%\n",
            "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.947, Validation Accuracy: 60.87%\n",
            "Epoch [3/10], Training Loss: 0.911, Validation Accuracy: 61.27%\n",
            "Epoch [4/10], Training Loss: 0.894, Validation Accuracy: 60.63%\n",
            "Epoch [5/10], Training Loss: 0.866, Validation Accuracy: 61.32%\n",
            "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.832, Validation Accuracy: 60.39%\n",
            "Epoch [8/10], Training Loss: 0.822, Validation Accuracy: 61.19%\n",
            "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 60.94%\n",
            "Epoch [10/10], Training Loss: 0.788, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 1.002, Validation Accuracy: 60.92%\n",
            "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 61.37%\n",
            "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 61.11%\n",
            "Epoch [4/10], Training Loss: 0.884, Validation Accuracy: 61.27%\n",
            "Epoch [5/10], Training Loss: 0.862, Validation Accuracy: 61.47%\n",
            "Epoch [6/10], Training Loss: 0.844, Validation Accuracy: 61.10%\n",
            "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.19%\n",
            "Epoch [8/10], Training Loss: 0.803, Validation Accuracy: 61.06%\n",
            "Epoch [9/10], Training Loss: 0.798, Validation Accuracy: 60.75%\n",
            "Epoch [10/10], Training Loss: 0.775, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.993, Validation Accuracy: 61.34%\n",
            "Epoch [2/10], Training Loss: 0.941, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.904, Validation Accuracy: 61.32%\n",
            "Epoch [4/10], Training Loss: 0.877, Validation Accuracy: 60.36%\n",
            "Epoch [5/10], Training Loss: 0.854, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.830, Validation Accuracy: 60.32%\n",
            "Epoch [7/10], Training Loss: 0.805, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.789, Validation Accuracy: 61.06%\n",
            "Epoch [9/10], Training Loss: 0.781, Validation Accuracy: 60.72%\n",
            "Epoch [10/10], Training Loss: 0.775, Validation Accuracy: 60.84%\n",
            "Epoch [1/10], Training Loss: 0.953, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.24%\n",
            "Epoch [3/10], Training Loss: 0.848, Validation Accuracy: 60.67%\n",
            "Epoch [4/10], Training Loss: 0.829, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 61.20%\n",
            "Epoch [6/10], Training Loss: 0.784, Validation Accuracy: 60.65%\n",
            "Epoch [7/10], Training Loss: 0.767, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.745, Validation Accuracy: 60.59%\n",
            "Epoch [9/10], Training Loss: 0.731, Validation Accuracy: 60.77%\n",
            "Epoch [10/10], Training Loss: 0.720, Validation Accuracy: 60.91%\n",
            "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 61.14%\n",
            "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 60.96%\n",
            "Epoch [3/10], Training Loss: 0.857, Validation Accuracy: 59.82%\n",
            "Epoch [4/10], Training Loss: 0.828, Validation Accuracy: 60.69%\n",
            "Epoch [5/10], Training Loss: 0.798, Validation Accuracy: 61.06%\n",
            "Epoch [6/10], Training Loss: 0.782, Validation Accuracy: 60.88%\n",
            "Epoch [7/10], Training Loss: 0.765, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.744, Validation Accuracy: 60.98%\n",
            "Epoch [9/10], Training Loss: 0.720, Validation Accuracy: 61.06%\n",
            "Epoch [10/10], Training Loss: 0.713, Validation Accuracy: 60.95%\n",
            "Epoch [1/10], Training Loss: 0.955, Validation Accuracy: 61.04%\n",
            "Epoch [2/10], Training Loss: 0.875, Validation Accuracy: 61.61%\n",
            "Epoch [3/10], Training Loss: 0.837, Validation Accuracy: 62.18%\n",
            "Epoch [4/10], Training Loss: 0.806, Validation Accuracy: 61.62%\n",
            "Epoch [5/10], Training Loss: 0.786, Validation Accuracy: 60.82%\n",
            "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.740, Validation Accuracy: 61.27%\n",
            "Epoch [8/10], Training Loss: 0.726, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.710, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.686, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.945, Validation Accuracy: 60.91%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.66%\n",
            "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 62.10%\n",
            "Epoch [4/10], Training Loss: 0.795, Validation Accuracy: 60.03%\n",
            "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 61.86%\n",
            "Epoch [6/10], Training Loss: 0.760, Validation Accuracy: 61.55%\n",
            "Epoch [7/10], Training Loss: 0.726, Validation Accuracy: 61.93%\n",
            "Epoch [8/10], Training Loss: 0.712, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 61.50%\n",
            "Epoch [10/10], Training Loss: 0.681, Validation Accuracy: 61.02%\n",
            "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 60.93%\n",
            "Epoch [2/10], Training Loss: 0.869, Validation Accuracy: 61.45%\n",
            "Epoch [3/10], Training Loss: 0.821, Validation Accuracy: 61.74%\n",
            "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.762, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.736, Validation Accuracy: 61.70%\n",
            "Epoch [7/10], Training Loss: 0.721, Validation Accuracy: 61.10%\n",
            "Epoch [8/10], Training Loss: 0.706, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.678, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.662, Validation Accuracy: 61.28%\n",
            "Epoch [1/10], Training Loss: 0.917, Validation Accuracy: 61.15%\n",
            "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 61.20%\n",
            "Epoch [3/10], Training Loss: 0.787, Validation Accuracy: 60.89%\n",
            "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 61.70%\n",
            "Epoch [5/10], Training Loss: 0.726, Validation Accuracy: 61.28%\n",
            "Epoch [6/10], Training Loss: 0.692, Validation Accuracy: 61.60%\n",
            "Epoch [7/10], Training Loss: 0.674, Validation Accuracy: 61.25%\n",
            "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 61.01%\n",
            "Epoch [9/10], Training Loss: 0.649, Validation Accuracy: 61.49%\n",
            "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 61.07%\n",
            "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 61.71%\n",
            "Epoch [2/10], Training Loss: 0.828, Validation Accuracy: 61.67%\n",
            "Epoch [3/10], Training Loss: 0.772, Validation Accuracy: 60.88%\n",
            "Epoch [4/10], Training Loss: 0.749, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.719, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 60.87%\n",
            "Epoch [7/10], Training Loss: 0.669, Validation Accuracy: 61.56%\n",
            "Epoch [8/10], Training Loss: 0.645, Validation Accuracy: 61.32%\n",
            "Epoch [9/10], Training Loss: 0.626, Validation Accuracy: 60.62%\n",
            "Epoch [10/10], Training Loss: 0.610, Validation Accuracy: 61.23%\n",
            "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 61.46%\n",
            "Epoch [2/10], Training Loss: 0.801, Validation Accuracy: 61.48%\n",
            "Epoch [3/10], Training Loss: 0.760, Validation Accuracy: 61.71%\n",
            "Epoch [4/10], Training Loss: 0.728, Validation Accuracy: 61.61%\n",
            "Epoch [5/10], Training Loss: 0.696, Validation Accuracy: 61.89%\n",
            "Epoch [6/10], Training Loss: 0.664, Validation Accuracy: 61.72%\n",
            "Epoch [7/10], Training Loss: 0.642, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 61.40%\n",
            "Epoch [9/10], Training Loss: 0.602, Validation Accuracy: 61.59%\n",
            "Epoch [10/10], Training Loss: 0.592, Validation Accuracy: 60.90%\n",
            "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 62.03%\n",
            "Epoch [2/10], Training Loss: 0.813, Validation Accuracy: 61.40%\n",
            "Epoch [3/10], Training Loss: 0.759, Validation Accuracy: 61.87%\n",
            "Epoch [4/10], Training Loss: 0.724, Validation Accuracy: 61.33%\n",
            "Epoch [5/10], Training Loss: 0.689, Validation Accuracy: 61.73%\n",
            "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 61.53%\n",
            "Epoch [7/10], Training Loss: 0.639, Validation Accuracy: 61.85%\n",
            "Epoch [8/10], Training Loss: 0.622, Validation Accuracy: 61.31%\n",
            "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.579, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 61.86%\n",
            "Epoch [2/10], Training Loss: 0.800, Validation Accuracy: 61.14%\n",
            "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 61.24%\n",
            "Epoch [4/10], Training Loss: 0.712, Validation Accuracy: 61.37%\n",
            "Epoch [5/10], Training Loss: 0.680, Validation Accuracy: 61.77%\n",
            "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.629, Validation Accuracy: 61.60%\n",
            "Epoch [8/10], Training Loss: 0.610, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.583, Validation Accuracy: 60.78%\n",
            "Epoch [10/10], Training Loss: 0.570, Validation Accuracy: 61.44%\n",
            "Epoch [1/10], Training Loss: 0.866, Validation Accuracy: 60.89%\n",
            "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 61.59%\n",
            "Epoch [3/10], Training Loss: 0.708, Validation Accuracy: 61.15%\n",
            "Epoch [4/10], Training Loss: 0.671, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.647, Validation Accuracy: 61.38%\n",
            "Epoch [6/10], Training Loss: 0.618, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.592, Validation Accuracy: 61.12%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 61.18%\n",
            "Epoch [9/10], Training Loss: 0.550, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 61.33%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 61.15%\n",
            "Epoch [2/10], Training Loss: 0.752, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 60.15%\n",
            "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 61.62%\n",
            "Epoch [5/10], Training Loss: 0.637, Validation Accuracy: 61.13%\n",
            "Epoch [6/10], Training Loss: 0.604, Validation Accuracy: 61.75%\n",
            "Epoch [7/10], Training Loss: 0.569, Validation Accuracy: 61.07%\n",
            "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.527, Validation Accuracy: 61.45%\n",
            "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 61.20%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 60.95%\n",
            "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 60.90%\n",
            "Epoch [3/10], Training Loss: 0.692, Validation Accuracy: 61.92%\n",
            "Epoch [4/10], Training Loss: 0.641, Validation Accuracy: 61.84%\n",
            "Epoch [5/10], Training Loss: 0.607, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.552, Validation Accuracy: 61.90%\n",
            "Epoch [8/10], Training Loss: 0.532, Validation Accuracy: 61.44%\n",
            "Epoch [9/10], Training Loss: 0.505, Validation Accuracy: 61.18%\n",
            "Epoch [10/10], Training Loss: 0.489, Validation Accuracy: 61.78%\n",
            "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 60.89%\n",
            "Epoch [2/10], Training Loss: 0.743, Validation Accuracy: 61.65%\n",
            "Epoch [3/10], Training Loss: 0.681, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.646, Validation Accuracy: 61.27%\n",
            "Epoch [5/10], Training Loss: 0.613, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.578, Validation Accuracy: 61.52%\n",
            "Epoch [7/10], Training Loss: 0.548, Validation Accuracy: 60.91%\n",
            "Epoch [8/10], Training Loss: 0.526, Validation Accuracy: 61.24%\n",
            "Epoch [9/10], Training Loss: 0.503, Validation Accuracy: 61.15%\n",
            "Epoch [10/10], Training Loss: 0.491, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.81%\n",
            "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 61.49%\n",
            "Epoch [3/10], Training Loss: 0.675, Validation Accuracy: 61.07%\n",
            "Epoch [4/10], Training Loss: 0.625, Validation Accuracy: 61.57%\n",
            "Epoch [5/10], Training Loss: 0.603, Validation Accuracy: 60.67%\n",
            "Epoch [6/10], Training Loss: 0.568, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.541, Validation Accuracy: 61.32%\n",
            "Epoch [8/10], Training Loss: 0.514, Validation Accuracy: 60.98%\n",
            "Epoch [9/10], Training Loss: 0.497, Validation Accuracy: 60.43%\n",
            "Epoch [10/10], Training Loss: 0.477, Validation Accuracy: 60.64%\n",
            "Epoch [1/10], Training Loss: 0.841, Validation Accuracy: 60.88%\n",
            "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 61.34%\n",
            "Epoch [3/10], Training Loss: 0.647, Validation Accuracy: 60.69%\n",
            "Epoch [4/10], Training Loss: 0.609, Validation Accuracy: 61.52%\n",
            "Epoch [5/10], Training Loss: 0.561, Validation Accuracy: 60.97%\n",
            "Epoch [6/10], Training Loss: 0.540, Validation Accuracy: 61.43%\n",
            "Epoch [7/10], Training Loss: 0.509, Validation Accuracy: 61.33%\n",
            "Epoch [8/10], Training Loss: 0.486, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.464, Validation Accuracy: 60.26%\n",
            "Epoch [10/10], Training Loss: 0.439, Validation Accuracy: 60.43%\n",
            "Epoch [1/10], Training Loss: 0.819, Validation Accuracy: 60.46%\n",
            "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 60.77%\n",
            "Epoch [3/10], Training Loss: 0.619, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.574, Validation Accuracy: 61.28%\n",
            "Epoch [5/10], Training Loss: 0.542, Validation Accuracy: 61.05%\n",
            "Epoch [6/10], Training Loss: 0.509, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.482, Validation Accuracy: 60.70%\n",
            "Epoch [8/10], Training Loss: 0.474, Validation Accuracy: 60.91%\n",
            "Epoch [9/10], Training Loss: 0.450, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.418, Validation Accuracy: 61.30%\n",
            "Confusion Matrix:\n",
            "[[679  41  35  23  37   7  13  24  90  51]\n",
            " [ 36 766   2  18   5   8  11   5  43 106]\n",
            " [ 78  15 422  94 133  80  76  61  22  19]\n",
            " [ 24  25  51 421  72 198  83  74  21  31]\n",
            " [ 29  10  60  64 555  56  78 116  15  17]\n",
            " [ 24  12  47 194  58 518  40  80  13  14]\n",
            " [  7  16  45  71  54  46 710  21  10  20]\n",
            " [ 13   9  23  52  60  86  14 703   5  35]\n",
            " [111  62  12  27  10  11  12  11 702  42]\n",
            " [ 54 150  12  24   8  14  10  31  39 658]]\n",
            "Test Accuracy: 61.34%\n",
            "True Positives (TP): [679 766 422 421 555 518 710 703 702 658]\n",
            "False Positives (FP): [376 340 287 567 437 506 337 423 258 335]\n",
            "True Negatives (TN): [8624 8660 8713 8433 8563 8494 8663 8577 8742 8665]\n",
            "False Negatives (FN): [321 234 578 579 445 482 290 297 298 342]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.6436019  0.6925859  0.59520451 0.42611336 0.55947581 0.50585938\n",
            " 0.67812798 0.62433393 0.73125    0.66263847]\n",
            "Recall: [0.679 0.766 0.422 0.421 0.555 0.518 0.71  0.703 0.702 0.658]\n",
            "F1 Score: [0.66082725 0.72744539 0.49385606 0.42354125 0.55722892 0.51185771\n",
            " 0.69369809 0.66133584 0.71632653 0.66031109]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_normal: Dict, distribution_info_uniform: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both normal and uniform distributions\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    # Generate augmented data from normal distribution\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "    # Generate augmented data using Uniform distribution\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_normal + augmented_data_uniform) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"normal\"], other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 9.89%\n",
        "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 9.89%\n",
        "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 9.89%\n",
        "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 9.89%\n",
        "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 9.89%\n",
        "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 9.89%\n",
        "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 9.89%\n",
        "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 9.89%\n",
        "Epoch [9/10], Training Loss: 2.298, Validation Accuracy: 9.89%\n",
        "Epoch [10/10], Training Loss: 2.297, Validation Accuracy: 9.89%\n",
        "Epoch [1/10], Training Loss: 2.297, Validation Accuracy: 9.89%\n",
        "Epoch [2/10], Training Loss: 2.296, Validation Accuracy: 9.90%\n",
        "Epoch [3/10], Training Loss: 2.294, Validation Accuracy: 10.05%\n",
        "Epoch [4/10], Training Loss: 2.291, Validation Accuracy: 11.80%\n",
        "Epoch [5/10], Training Loss: 2.288, Validation Accuracy: 14.01%\n",
        "Epoch [6/10], Training Loss: 2.283, Validation Accuracy: 14.36%\n",
        "Epoch [7/10], Training Loss: 2.276, Validation Accuracy: 14.27%\n",
        "Epoch [8/10], Training Loss: 2.266, Validation Accuracy: 15.41%\n",
        "Epoch [9/10], Training Loss: 2.252, Validation Accuracy: 16.01%\n",
        "Epoch [10/10], Training Loss: 2.231, Validation Accuracy: 17.05%\n",
        "Epoch [1/10], Training Loss: 2.213, Validation Accuracy: 19.88%\n",
        "Epoch [2/10], Training Loss: 2.179, Validation Accuracy: 21.49%\n",
        "Epoch [3/10], Training Loss: 2.142, Validation Accuracy: 23.10%\n",
        "Epoch [4/10], Training Loss: 2.115, Validation Accuracy: 24.23%\n",
        "Epoch [5/10], Training Loss: 2.090, Validation Accuracy: 25.19%\n",
        "Epoch [6/10], Training Loss: 2.063, Validation Accuracy: 26.34%\n",
        "Epoch [7/10], Training Loss: 2.039, Validation Accuracy: 26.71%\n",
        "Epoch [8/10], Training Loss: 2.013, Validation Accuracy: 27.22%\n",
        "Epoch [9/10], Training Loss: 1.991, Validation Accuracy: 27.68%\n",
        "Epoch [10/10], Training Loss: 1.972, Validation Accuracy: 27.52%\n",
        "Epoch [1/10], Training Loss: 1.961, Validation Accuracy: 28.60%\n",
        "Epoch [2/10], Training Loss: 1.944, Validation Accuracy: 29.16%\n",
        "Epoch [3/10], Training Loss: 1.930, Validation Accuracy: 29.75%\n",
        "Epoch [4/10], Training Loss: 1.917, Validation Accuracy: 30.42%\n",
        "Epoch [5/10], Training Loss: 1.902, Validation Accuracy: 31.17%\n",
        "Epoch [6/10], Training Loss: 1.886, Validation Accuracy: 31.34%\n",
        "Epoch [7/10], Training Loss: 1.871, Validation Accuracy: 31.20%\n",
        "Epoch [8/10], Training Loss: 1.857, Validation Accuracy: 31.76%\n",
        "Epoch [9/10], Training Loss: 1.837, Validation Accuracy: 32.39%\n",
        "Epoch [10/10], Training Loss: 1.820, Validation Accuracy: 33.22%\n",
        "Epoch [1/10], Training Loss: 1.830, Validation Accuracy: 33.81%\n",
        "Epoch [2/10], Training Loss: 1.816, Validation Accuracy: 34.08%\n",
        "Epoch [3/10], Training Loss: 1.802, Validation Accuracy: 34.79%\n",
        "Epoch [4/10], Training Loss: 1.787, Validation Accuracy: 34.94%\n",
        "Epoch [5/10], Training Loss: 1.775, Validation Accuracy: 35.08%\n",
        "Epoch [6/10], Training Loss: 1.759, Validation Accuracy: 35.60%\n",
        "Epoch [7/10], Training Loss: 1.748, Validation Accuracy: 35.07%\n",
        "Epoch [8/10], Training Loss: 1.738, Validation Accuracy: 36.25%\n",
        "Epoch [9/10], Training Loss: 1.722, Validation Accuracy: 36.16%\n",
        "Epoch [10/10], Training Loss: 1.708, Validation Accuracy: 37.01%\n",
        "Epoch [1/10], Training Loss: 1.713, Validation Accuracy: 37.73%\n",
        "Epoch [2/10], Training Loss: 1.697, Validation Accuracy: 37.92%\n",
        "Epoch [3/10], Training Loss: 1.679, Validation Accuracy: 38.34%\n",
        "Epoch [4/10], Training Loss: 1.670, Validation Accuracy: 39.26%\n",
        "Epoch [5/10], Training Loss: 1.648, Validation Accuracy: 39.74%\n",
        "Epoch [6/10], Training Loss: 1.633, Validation Accuracy: 40.38%\n",
        "Epoch [7/10], Training Loss: 1.621, Validation Accuracy: 40.66%\n",
        "Epoch [8/10], Training Loss: 1.613, Validation Accuracy: 40.50%\n",
        "Epoch [9/10], Training Loss: 1.606, Validation Accuracy: 41.16%\n",
        "Epoch [10/10], Training Loss: 1.589, Validation Accuracy: 41.16%\n",
        "Epoch [1/10], Training Loss: 1.594, Validation Accuracy: 41.27%\n",
        "Epoch [2/10], Training Loss: 1.582, Validation Accuracy: 41.93%\n",
        "Epoch [3/10], Training Loss: 1.569, Validation Accuracy: 42.55%\n",
        "Epoch [4/10], Training Loss: 1.566, Validation Accuracy: 41.94%\n",
        "Epoch [5/10], Training Loss: 1.550, Validation Accuracy: 42.29%\n",
        "Epoch [6/10], Training Loss: 1.542, Validation Accuracy: 43.00%\n",
        "Epoch [7/10], Training Loss: 1.536, Validation Accuracy: 42.71%\n",
        "Epoch [8/10], Training Loss: 1.527, Validation Accuracy: 43.48%\n",
        "Epoch [9/10], Training Loss: 1.525, Validation Accuracy: 43.33%\n",
        "Epoch [10/10], Training Loss: 1.519, Validation Accuracy: 43.78%\n",
        "Epoch [1/10], Training Loss: 1.537, Validation Accuracy: 43.60%\n",
        "Epoch [2/10], Training Loss: 1.523, Validation Accuracy: 44.25%\n",
        "Epoch [3/10], Training Loss: 1.513, Validation Accuracy: 44.79%\n",
        "Epoch [4/10], Training Loss: 1.506, Validation Accuracy: 44.12%\n",
        "Epoch [5/10], Training Loss: 1.502, Validation Accuracy: 44.43%\n",
        "Epoch [6/10], Training Loss: 1.498, Validation Accuracy: 44.76%\n",
        "Epoch [7/10], Training Loss: 1.488, Validation Accuracy: 45.19%\n",
        "Epoch [8/10], Training Loss: 1.483, Validation Accuracy: 45.29%\n",
        "Epoch [9/10], Training Loss: 1.474, Validation Accuracy: 45.16%\n",
        "Epoch [10/10], Training Loss: 1.464, Validation Accuracy: 45.61%\n",
        "Epoch [1/10], Training Loss: 1.470, Validation Accuracy: 45.77%\n",
        "Epoch [2/10], Training Loss: 1.459, Validation Accuracy: 45.85%\n",
        "Epoch [3/10], Training Loss: 1.454, Validation Accuracy: 46.24%\n",
        "Epoch [4/10], Training Loss: 1.441, Validation Accuracy: 46.14%\n",
        "Epoch [5/10], Training Loss: 1.429, Validation Accuracy: 46.65%\n",
        "Epoch [6/10], Training Loss: 1.438, Validation Accuracy: 46.16%\n",
        "Epoch [7/10], Training Loss: 1.423, Validation Accuracy: 46.26%\n",
        "Epoch [8/10], Training Loss: 1.416, Validation Accuracy: 46.58%\n",
        "Epoch [9/10], Training Loss: 1.409, Validation Accuracy: 47.02%\n",
        "Epoch [10/10], Training Loss: 1.402, Validation Accuracy: 47.42%\n",
        "Epoch [1/10], Training Loss: 1.458, Validation Accuracy: 47.18%\n",
        "Epoch [2/10], Training Loss: 1.449, Validation Accuracy: 47.64%\n",
        "Epoch [3/10], Training Loss: 1.441, Validation Accuracy: 47.32%\n",
        "Epoch [4/10], Training Loss: 1.434, Validation Accuracy: 47.77%\n",
        "Epoch [5/10], Training Loss: 1.421, Validation Accuracy: 47.66%\n",
        "Epoch [6/10], Training Loss: 1.415, Validation Accuracy: 47.22%\n",
        "Epoch [7/10], Training Loss: 1.413, Validation Accuracy: 48.34%\n",
        "Epoch [8/10], Training Loss: 1.401, Validation Accuracy: 48.53%\n",
        "Epoch [9/10], Training Loss: 1.389, Validation Accuracy: 48.21%\n",
        "Epoch [10/10], Training Loss: 1.385, Validation Accuracy: 46.46%\n",
        "Epoch [1/10], Training Loss: 1.431, Validation Accuracy: 48.53%\n",
        "Epoch [2/10], Training Loss: 1.413, Validation Accuracy: 48.52%\n",
        "Epoch [3/10], Training Loss: 1.401, Validation Accuracy: 48.53%\n",
        "Epoch [4/10], Training Loss: 1.395, Validation Accuracy: 49.06%\n",
        "Epoch [5/10], Training Loss: 1.383, Validation Accuracy: 48.32%\n",
        "Epoch [6/10], Training Loss: 1.374, Validation Accuracy: 49.13%\n",
        "Epoch [7/10], Training Loss: 1.371, Validation Accuracy: 49.09%\n",
        "Epoch [8/10], Training Loss: 1.360, Validation Accuracy: 48.39%\n",
        "Epoch [9/10], Training Loss: 1.356, Validation Accuracy: 49.79%\n",
        "Epoch [10/10], Training Loss: 1.344, Validation Accuracy: 49.42%\n",
        "Epoch [1/10], Training Loss: 1.383, Validation Accuracy: 49.62%\n",
        "Epoch [2/10], Training Loss: 1.365, Validation Accuracy: 50.18%\n",
        "Epoch [3/10], Training Loss: 1.350, Validation Accuracy: 49.71%\n",
        "Epoch [4/10], Training Loss: 1.346, Validation Accuracy: 50.38%\n",
        "Epoch [5/10], Training Loss: 1.339, Validation Accuracy: 50.64%\n",
        "Epoch [6/10], Training Loss: 1.332, Validation Accuracy: 50.49%\n",
        "Epoch [7/10], Training Loss: 1.320, Validation Accuracy: 50.81%\n",
        "Epoch [8/10], Training Loss: 1.319, Validation Accuracy: 50.69%\n",
        "Epoch [9/10], Training Loss: 1.306, Validation Accuracy: 50.79%\n",
        "Epoch [10/10], Training Loss: 1.305, Validation Accuracy: 50.40%\n",
        "Epoch [1/10], Training Loss: 1.347, Validation Accuracy: 51.25%\n",
        "Epoch [2/10], Training Loss: 1.339, Validation Accuracy: 51.56%\n",
        "Epoch [3/10], Training Loss: 1.319, Validation Accuracy: 51.83%\n",
        "Epoch [4/10], Training Loss: 1.307, Validation Accuracy: 51.59%\n",
        "Epoch [5/10], Training Loss: 1.309, Validation Accuracy: 51.28%\n",
        "Epoch [6/10], Training Loss: 1.293, Validation Accuracy: 51.29%\n",
        "Epoch [7/10], Training Loss: 1.291, Validation Accuracy: 51.41%\n",
        "Epoch [8/10], Training Loss: 1.277, Validation Accuracy: 52.19%\n",
        "Epoch [9/10], Training Loss: 1.266, Validation Accuracy: 51.60%\n",
        "Epoch [10/10], Training Loss: 1.257, Validation Accuracy: 51.63%\n",
        "Epoch [1/10], Training Loss: 1.304, Validation Accuracy: 52.33%\n",
        "Epoch [2/10], Training Loss: 1.282, Validation Accuracy: 52.37%\n",
        "Epoch [3/10], Training Loss: 1.263, Validation Accuracy: 52.24%\n",
        "Epoch [4/10], Training Loss: 1.257, Validation Accuracy: 52.40%\n",
        "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 52.10%\n",
        "Epoch [6/10], Training Loss: 1.239, Validation Accuracy: 53.48%\n",
        "Epoch [7/10], Training Loss: 1.235, Validation Accuracy: 53.22%\n",
        "Epoch [8/10], Training Loss: 1.220, Validation Accuracy: 52.68%\n",
        "Epoch [9/10], Training Loss: 1.212, Validation Accuracy: 52.91%\n",
        "Epoch [10/10], Training Loss: 1.215, Validation Accuracy: 51.90%\n",
        "Epoch [1/10], Training Loss: 1.296, Validation Accuracy: 52.50%\n",
        "Epoch [2/10], Training Loss: 1.270, Validation Accuracy: 52.99%\n",
        "Epoch [3/10], Training Loss: 1.252, Validation Accuracy: 53.83%\n",
        "Epoch [4/10], Training Loss: 1.240, Validation Accuracy: 53.30%\n",
        "Epoch [5/10], Training Loss: 1.233, Validation Accuracy: 53.85%\n",
        "Epoch [6/10], Training Loss: 1.225, Validation Accuracy: 53.61%\n",
        "Epoch [7/10], Training Loss: 1.209, Validation Accuracy: 53.76%\n",
        "Epoch [8/10], Training Loss: 1.207, Validation Accuracy: 52.71%\n",
        "Epoch [9/10], Training Loss: 1.200, Validation Accuracy: 53.21%\n",
        "Epoch [10/10], Training Loss: 1.188, Validation Accuracy: 54.21%\n",
        "Epoch [1/10], Training Loss: 1.267, Validation Accuracy: 54.21%\n",
        "Epoch [2/10], Training Loss: 1.249, Validation Accuracy: 53.70%\n",
        "Epoch [3/10], Training Loss: 1.229, Validation Accuracy: 54.41%\n",
        "Epoch [4/10], Training Loss: 1.213, Validation Accuracy: 54.99%\n",
        "Epoch [5/10], Training Loss: 1.199, Validation Accuracy: 54.79%\n",
        "Epoch [6/10], Training Loss: 1.199, Validation Accuracy: 54.27%\n",
        "Epoch [7/10], Training Loss: 1.184, Validation Accuracy: 54.76%\n",
        "Epoch [8/10], Training Loss: 1.174, Validation Accuracy: 54.35%\n",
        "Epoch [9/10], Training Loss: 1.166, Validation Accuracy: 54.77%\n",
        "Epoch [10/10], Training Loss: 1.157, Validation Accuracy: 54.92%\n",
        "Epoch [1/10], Training Loss: 1.236, Validation Accuracy: 55.68%\n",
        "Epoch [2/10], Training Loss: 1.213, Validation Accuracy: 55.70%\n",
        "Epoch [3/10], Training Loss: 1.202, Validation Accuracy: 54.97%\n",
        "Epoch [4/10], Training Loss: 1.191, Validation Accuracy: 55.97%\n",
        "Epoch [5/10], Training Loss: 1.176, Validation Accuracy: 55.82%\n",
        "Epoch [6/10], Training Loss: 1.165, Validation Accuracy: 55.66%\n",
        "Epoch [7/10], Training Loss: 1.160, Validation Accuracy: 55.47%\n",
        "Epoch [8/10], Training Loss: 1.149, Validation Accuracy: 56.19%\n",
        "Epoch [9/10], Training Loss: 1.138, Validation Accuracy: 56.17%\n",
        "Epoch [10/10], Training Loss: 1.130, Validation Accuracy: 55.90%\n",
        "Epoch [1/10], Training Loss: 1.218, Validation Accuracy: 55.22%\n",
        "Epoch [2/10], Training Loss: 1.190, Validation Accuracy: 56.28%\n",
        "Epoch [3/10], Training Loss: 1.182, Validation Accuracy: 55.57%\n",
        "Epoch [4/10], Training Loss: 1.163, Validation Accuracy: 56.08%\n",
        "Epoch [5/10], Training Loss: 1.154, Validation Accuracy: 55.89%\n",
        "Epoch [6/10], Training Loss: 1.136, Validation Accuracy: 55.74%\n",
        "Epoch [7/10], Training Loss: 1.128, Validation Accuracy: 55.81%\n",
        "Epoch [8/10], Training Loss: 1.121, Validation Accuracy: 56.81%\n",
        "Epoch [9/10], Training Loss: 1.106, Validation Accuracy: 56.53%\n",
        "Epoch [10/10], Training Loss: 1.101, Validation Accuracy: 55.60%\n",
        "Epoch [1/10], Training Loss: 1.169, Validation Accuracy: 54.27%\n",
        "Epoch [2/10], Training Loss: 1.147, Validation Accuracy: 57.01%\n",
        "Epoch [3/10], Training Loss: 1.126, Validation Accuracy: 56.94%\n",
        "Epoch [4/10], Training Loss: 1.110, Validation Accuracy: 56.63%\n",
        "Epoch [5/10], Training Loss: 1.097, Validation Accuracy: 56.18%\n",
        "Epoch [6/10], Training Loss: 1.093, Validation Accuracy: 56.34%\n",
        "Epoch [7/10], Training Loss: 1.087, Validation Accuracy: 56.97%\n",
        "Epoch [8/10], Training Loss: 1.059, Validation Accuracy: 56.58%\n",
        "Epoch [9/10], Training Loss: 1.057, Validation Accuracy: 57.59%\n",
        "Epoch [10/10], Training Loss: 1.037, Validation Accuracy: 57.36%\n",
        "Epoch [1/10], Training Loss: 1.177, Validation Accuracy: 57.54%\n",
        "Epoch [2/10], Training Loss: 1.143, Validation Accuracy: 57.49%\n",
        "Epoch [3/10], Training Loss: 1.124, Validation Accuracy: 57.50%\n",
        "Epoch [4/10], Training Loss: 1.103, Validation Accuracy: 57.57%\n",
        "Epoch [5/10], Training Loss: 1.093, Validation Accuracy: 57.98%\n",
        "Epoch [6/10], Training Loss: 1.084, Validation Accuracy: 57.73%\n",
        "Epoch [7/10], Training Loss: 1.067, Validation Accuracy: 57.86%\n",
        "Epoch [8/10], Training Loss: 1.065, Validation Accuracy: 56.37%\n",
        "Epoch [9/10], Training Loss: 1.047, Validation Accuracy: 57.81%\n",
        "Epoch [10/10], Training Loss: 1.045, Validation Accuracy: 57.96%\n",
        "Epoch [1/10], Training Loss: 1.155, Validation Accuracy: 57.84%\n",
        "Epoch [2/10], Training Loss: 1.123, Validation Accuracy: 57.90%\n",
        "Epoch [3/10], Training Loss: 1.101, Validation Accuracy: 58.31%\n",
        "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 58.11%\n",
        "Epoch [5/10], Training Loss: 1.071, Validation Accuracy: 57.74%\n",
        "Epoch [6/10], Training Loss: 1.061, Validation Accuracy: 57.87%\n",
        "Epoch [7/10], Training Loss: 1.052, Validation Accuracy: 57.58%\n",
        "Epoch [8/10], Training Loss: 1.035, Validation Accuracy: 57.87%\n",
        "Epoch [9/10], Training Loss: 1.021, Validation Accuracy: 57.69%\n",
        "Epoch [10/10], Training Loss: 1.018, Validation Accuracy: 58.39%\n",
        "Epoch [1/10], Training Loss: 1.143, Validation Accuracy: 57.43%\n",
        "Epoch [2/10], Training Loss: 1.112, Validation Accuracy: 57.52%\n",
        "Epoch [3/10], Training Loss: 1.092, Validation Accuracy: 58.54%\n",
        "Epoch [4/10], Training Loss: 1.073, Validation Accuracy: 58.99%\n",
        "Epoch [5/10], Training Loss: 1.053, Validation Accuracy: 58.65%\n",
        "Epoch [6/10], Training Loss: 1.035, Validation Accuracy: 58.87%\n",
        "Epoch [7/10], Training Loss: 1.033, Validation Accuracy: 58.70%\n",
        "Epoch [8/10], Training Loss: 1.023, Validation Accuracy: 57.97%\n",
        "Epoch [9/10], Training Loss: 1.005, Validation Accuracy: 58.61%\n",
        "Epoch [10/10], Training Loss: 0.991, Validation Accuracy: 58.78%\n",
        "Epoch [1/10], Training Loss: 1.124, Validation Accuracy: 58.41%\n",
        "Epoch [2/10], Training Loss: 1.092, Validation Accuracy: 58.99%\n",
        "Epoch [3/10], Training Loss: 1.069, Validation Accuracy: 58.86%\n",
        "Epoch [4/10], Training Loss: 1.054, Validation Accuracy: 58.51%\n",
        "Epoch [5/10], Training Loss: 1.035, Validation Accuracy: 59.07%\n",
        "Epoch [6/10], Training Loss: 1.027, Validation Accuracy: 58.84%\n",
        "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 58.63%\n",
        "Epoch [8/10], Training Loss: 0.995, Validation Accuracy: 58.58%\n",
        "Epoch [9/10], Training Loss: 0.987, Validation Accuracy: 58.82%\n",
        "Epoch [10/10], Training Loss: 0.976, Validation Accuracy: 59.16%\n",
        "Epoch [1/10], Training Loss: 1.078, Validation Accuracy: 58.80%\n",
        "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.16%\n",
        "Epoch [3/10], Training Loss: 1.014, Validation Accuracy: 59.32%\n",
        "Epoch [4/10], Training Loss: 0.995, Validation Accuracy: 58.74%\n",
        "Epoch [5/10], Training Loss: 0.984, Validation Accuracy: 58.73%\n",
        "Epoch [6/10], Training Loss: 0.969, Validation Accuracy: 58.45%\n",
        "Epoch [7/10], Training Loss: 0.958, Validation Accuracy: 58.41%\n",
        "Epoch [8/10], Training Loss: 0.950, Validation Accuracy: 58.57%\n",
        "Epoch [9/10], Training Loss: 0.936, Validation Accuracy: 58.25%\n",
        "Epoch [10/10], Training Loss: 0.923, Validation Accuracy: 59.41%\n",
        "Epoch [1/10], Training Loss: 1.084, Validation Accuracy: 59.75%\n",
        "Epoch [2/10], Training Loss: 1.048, Validation Accuracy: 59.60%\n",
        "Epoch [3/10], Training Loss: 1.024, Validation Accuracy: 58.71%\n",
        "Epoch [4/10], Training Loss: 0.999, Validation Accuracy: 58.89%\n",
        "Epoch [5/10], Training Loss: 0.986, Validation Accuracy: 59.49%\n",
        "Epoch [6/10], Training Loss: 0.972, Validation Accuracy: 59.39%\n",
        "Epoch [7/10], Training Loss: 0.956, Validation Accuracy: 59.59%\n",
        "Epoch [8/10], Training Loss: 0.939, Validation Accuracy: 58.78%\n",
        "Epoch [9/10], Training Loss: 0.930, Validation Accuracy: 60.04%\n",
        "Epoch [10/10], Training Loss: 0.925, Validation Accuracy: 59.48%\n",
        "Epoch [1/10], Training Loss: 1.071, Validation Accuracy: 60.47%\n",
        "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 60.00%\n",
        "Epoch [3/10], Training Loss: 1.005, Validation Accuracy: 60.19%\n",
        "Epoch [4/10], Training Loss: 0.978, Validation Accuracy: 59.80%\n",
        "Epoch [5/10], Training Loss: 0.962, Validation Accuracy: 59.99%\n",
        "Epoch [6/10], Training Loss: 0.952, Validation Accuracy: 59.92%\n",
        "Epoch [7/10], Training Loss: 0.932, Validation Accuracy: 59.81%\n",
        "Epoch [8/10], Training Loss: 0.920, Validation Accuracy: 60.10%\n",
        "Epoch [9/10], Training Loss: 0.910, Validation Accuracy: 60.04%\n",
        "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 59.38%\n",
        "Epoch [1/10], Training Loss: 1.059, Validation Accuracy: 60.48%\n",
        "Epoch [2/10], Training Loss: 1.016, Validation Accuracy: 59.64%\n",
        "Epoch [3/10], Training Loss: 0.993, Validation Accuracy: 59.80%\n",
        "Epoch [4/10], Training Loss: 0.974, Validation Accuracy: 59.81%\n",
        "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.12%\n",
        "Epoch [6/10], Training Loss: 0.933, Validation Accuracy: 60.03%\n",
        "Epoch [7/10], Training Loss: 0.926, Validation Accuracy: 58.60%\n",
        "Epoch [8/10], Training Loss: 0.912, Validation Accuracy: 60.66%\n",
        "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 60.23%\n",
        "Epoch [10/10], Training Loss: 0.882, Validation Accuracy: 60.01%\n",
        "Epoch [1/10], Training Loss: 1.054, Validation Accuracy: 60.57%\n",
        "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 60.59%\n",
        "Epoch [3/10], Training Loss: 0.980, Validation Accuracy: 60.49%\n",
        "Epoch [4/10], Training Loss: 0.959, Validation Accuracy: 60.45%\n",
        "Epoch [5/10], Training Loss: 0.941, Validation Accuracy: 59.54%\n",
        "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 60.58%\n",
        "Epoch [7/10], Training Loss: 0.904, Validation Accuracy: 60.20%\n",
        "Epoch [8/10], Training Loss: 0.907, Validation Accuracy: 60.44%\n",
        "Epoch [9/10], Training Loss: 0.887, Validation Accuracy: 59.70%\n",
        "Epoch [10/10], Training Loss: 0.881, Validation Accuracy: 59.99%\n",
        "Epoch [1/10], Training Loss: 1.015, Validation Accuracy: 60.68%\n",
        "Epoch [2/10], Training Loss: 0.954, Validation Accuracy: 61.33%\n",
        "Epoch [3/10], Training Loss: 0.931, Validation Accuracy: 61.07%\n",
        "Epoch [4/10], Training Loss: 0.908, Validation Accuracy: 60.68%\n",
        "Epoch [5/10], Training Loss: 0.901, Validation Accuracy: 60.42%\n",
        "Epoch [6/10], Training Loss: 0.876, Validation Accuracy: 60.09%\n",
        "Epoch [7/10], Training Loss: 0.862, Validation Accuracy: 60.44%\n",
        "Epoch [8/10], Training Loss: 0.846, Validation Accuracy: 60.24%\n",
        "Epoch [9/10], Training Loss: 0.836, Validation Accuracy: 59.63%\n",
        "Epoch [10/10], Training Loss: 0.814, Validation Accuracy: 60.46%\n",
        "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 60.50%\n",
        "Epoch [2/10], Training Loss: 0.971, Validation Accuracy: 60.71%\n",
        "Epoch [3/10], Training Loss: 0.931, Validation Accuracy: 60.49%\n",
        "Epoch [4/10], Training Loss: 0.915, Validation Accuracy: 59.92%\n",
        "Epoch [5/10], Training Loss: 0.895, Validation Accuracy: 60.71%\n",
        "Epoch [6/10], Training Loss: 0.883, Validation Accuracy: 60.16%\n",
        "Epoch [7/10], Training Loss: 0.864, Validation Accuracy: 60.86%\n",
        "Epoch [8/10], Training Loss: 0.852, Validation Accuracy: 60.39%\n",
        "Epoch [9/10], Training Loss: 0.828, Validation Accuracy: 60.67%\n",
        "Epoch [10/10], Training Loss: 0.825, Validation Accuracy: 60.05%\n",
        "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.73%\n",
        "Epoch [2/10], Training Loss: 0.947, Validation Accuracy: 60.87%\n",
        "Epoch [3/10], Training Loss: 0.911, Validation Accuracy: 61.27%\n",
        "Epoch [4/10], Training Loss: 0.894, Validation Accuracy: 60.63%\n",
        "Epoch [5/10], Training Loss: 0.866, Validation Accuracy: 61.32%\n",
        "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 61.15%\n",
        "Epoch [7/10], Training Loss: 0.832, Validation Accuracy: 60.39%\n",
        "Epoch [8/10], Training Loss: 0.822, Validation Accuracy: 61.19%\n",
        "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 60.94%\n",
        "Epoch [10/10], Training Loss: 0.788, Validation Accuracy: 61.11%\n",
        "Epoch [1/10], Training Loss: 1.002, Validation Accuracy: 60.92%\n",
        "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 61.37%\n",
        "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 61.11%\n",
        "Epoch [4/10], Training Loss: 0.884, Validation Accuracy: 61.27%\n",
        "Epoch [5/10], Training Loss: 0.862, Validation Accuracy: 61.47%\n",
        "Epoch [6/10], Training Loss: 0.844, Validation Accuracy: 61.10%\n",
        "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.19%\n",
        "Epoch [8/10], Training Loss: 0.803, Validation Accuracy: 61.06%\n",
        "Epoch [9/10], Training Loss: 0.798, Validation Accuracy: 60.75%\n",
        "Epoch [10/10], Training Loss: 0.775, Validation Accuracy: 60.55%\n",
        "Epoch [1/10], Training Loss: 0.993, Validation Accuracy: 61.34%\n",
        "Epoch [2/10], Training Loss: 0.941, Validation Accuracy: 60.97%\n",
        "Epoch [3/10], Training Loss: 0.904, Validation Accuracy: 61.32%\n",
        "Epoch [4/10], Training Loss: 0.877, Validation Accuracy: 60.36%\n",
        "Epoch [5/10], Training Loss: 0.854, Validation Accuracy: 61.23%\n",
        "Epoch [6/10], Training Loss: 0.830, Validation Accuracy: 60.32%\n",
        "Epoch [7/10], Training Loss: 0.805, Validation Accuracy: 61.30%\n",
        "Epoch [8/10], Training Loss: 0.789, Validation Accuracy: 61.06%\n",
        "Epoch [9/10], Training Loss: 0.781, Validation Accuracy: 60.72%\n",
        "Epoch [10/10], Training Loss: 0.775, Validation Accuracy: 60.84%\n",
        "Epoch [1/10], Training Loss: 0.953, Validation Accuracy: 61.68%\n",
        "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.24%\n",
        "Epoch [3/10], Training Loss: 0.848, Validation Accuracy: 60.67%\n",
        "Epoch [4/10], Training Loss: 0.829, Validation Accuracy: 61.34%\n",
        "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 61.20%\n",
        "Epoch [6/10], Training Loss: 0.784, Validation Accuracy: 60.65%\n",
        "Epoch [7/10], Training Loss: 0.767, Validation Accuracy: 60.73%\n",
        "Epoch [8/10], Training Loss: 0.745, Validation Accuracy: 60.59%\n",
        "Epoch [9/10], Training Loss: 0.731, Validation Accuracy: 60.77%\n",
        "Epoch [10/10], Training Loss: 0.720, Validation Accuracy: 60.91%\n",
        "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 61.14%\n",
        "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 60.96%\n",
        "Epoch [3/10], Training Loss: 0.857, Validation Accuracy: 59.82%\n",
        "Epoch [4/10], Training Loss: 0.828, Validation Accuracy: 60.69%\n",
        "Epoch [5/10], Training Loss: 0.798, Validation Accuracy: 61.06%\n",
        "Epoch [6/10], Training Loss: 0.782, Validation Accuracy: 60.88%\n",
        "Epoch [7/10], Training Loss: 0.765, Validation Accuracy: 60.95%\n",
        "Epoch [8/10], Training Loss: 0.744, Validation Accuracy: 60.98%\n",
        "Epoch [9/10], Training Loss: 0.720, Validation Accuracy: 61.06%\n",
        "Epoch [10/10], Training Loss: 0.713, Validation Accuracy: 60.95%\n",
        "Epoch [1/10], Training Loss: 0.955, Validation Accuracy: 61.04%\n",
        "Epoch [2/10], Training Loss: 0.875, Validation Accuracy: 61.61%\n",
        "Epoch [3/10], Training Loss: 0.837, Validation Accuracy: 62.18%\n",
        "Epoch [4/10], Training Loss: 0.806, Validation Accuracy: 61.62%\n",
        "Epoch [5/10], Training Loss: 0.786, Validation Accuracy: 60.82%\n",
        "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 61.63%\n",
        "Epoch [7/10], Training Loss: 0.740, Validation Accuracy: 61.27%\n",
        "Epoch [8/10], Training Loss: 0.726, Validation Accuracy: 60.90%\n",
        "Epoch [9/10], Training Loss: 0.710, Validation Accuracy: 60.89%\n",
        "Epoch [10/10], Training Loss: 0.686, Validation Accuracy: 61.21%\n",
        "Epoch [1/10], Training Loss: 0.945, Validation Accuracy: 60.91%\n",
        "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.66%\n",
        "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 62.10%\n",
        "Epoch [4/10], Training Loss: 0.795, Validation Accuracy: 60.03%\n",
        "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 61.86%\n",
        "Epoch [6/10], Training Loss: 0.760, Validation Accuracy: 61.55%\n",
        "Epoch [7/10], Training Loss: 0.726, Validation Accuracy: 61.93%\n",
        "Epoch [8/10], Training Loss: 0.712, Validation Accuracy: 60.96%\n",
        "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 61.50%\n",
        "Epoch [10/10], Training Loss: 0.681, Validation Accuracy: 61.02%\n",
        "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 60.93%\n",
        "Epoch [2/10], Training Loss: 0.869, Validation Accuracy: 61.45%\n",
        "Epoch [3/10], Training Loss: 0.821, Validation Accuracy: 61.74%\n",
        "Epoch [4/10], Training Loss: 0.788, Validation Accuracy: 61.18%\n",
        "Epoch [5/10], Training Loss: 0.762, Validation Accuracy: 61.11%\n",
        "Epoch [6/10], Training Loss: 0.736, Validation Accuracy: 61.70%\n",
        "Epoch [7/10], Training Loss: 0.721, Validation Accuracy: 61.10%\n",
        "Epoch [8/10], Training Loss: 0.706, Validation Accuracy: 60.79%\n",
        "Epoch [9/10], Training Loss: 0.678, Validation Accuracy: 61.04%\n",
        "Epoch [10/10], Training Loss: 0.662, Validation Accuracy: 61.28%\n",
        "Epoch [1/10], Training Loss: 0.917, Validation Accuracy: 61.15%\n",
        "Epoch [2/10], Training Loss: 0.835, Validation Accuracy: 61.20%\n",
        "Epoch [3/10], Training Loss: 0.787, Validation Accuracy: 60.89%\n",
        "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 61.70%\n",
        "Epoch [5/10], Training Loss: 0.726, Validation Accuracy: 61.28%\n",
        "Epoch [6/10], Training Loss: 0.692, Validation Accuracy: 61.60%\n",
        "Epoch [7/10], Training Loss: 0.674, Validation Accuracy: 61.25%\n",
        "Epoch [8/10], Training Loss: 0.658, Validation Accuracy: 61.01%\n",
        "Epoch [9/10], Training Loss: 0.649, Validation Accuracy: 61.49%\n",
        "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 61.07%\n",
        "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 61.71%\n",
        "Epoch [2/10], Training Loss: 0.828, Validation Accuracy: 61.67%\n",
        "Epoch [3/10], Training Loss: 0.772, Validation Accuracy: 60.88%\n",
        "Epoch [4/10], Training Loss: 0.749, Validation Accuracy: 61.34%\n",
        "Epoch [5/10], Training Loss: 0.719, Validation Accuracy: 61.42%\n",
        "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 60.87%\n",
        "Epoch [7/10], Training Loss: 0.669, Validation Accuracy: 61.56%\n",
        "Epoch [8/10], Training Loss: 0.645, Validation Accuracy: 61.32%\n",
        "Epoch [9/10], Training Loss: 0.626, Validation Accuracy: 60.62%\n",
        "Epoch [10/10], Training Loss: 0.610, Validation Accuracy: 61.23%\n",
        "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 61.46%\n",
        "Epoch [2/10], Training Loss: 0.801, Validation Accuracy: 61.48%\n",
        "Epoch [3/10], Training Loss: 0.760, Validation Accuracy: 61.71%\n",
        "Epoch [4/10], Training Loss: 0.728, Validation Accuracy: 61.61%\n",
        "Epoch [5/10], Training Loss: 0.696, Validation Accuracy: 61.89%\n",
        "Epoch [6/10], Training Loss: 0.664, Validation Accuracy: 61.72%\n",
        "Epoch [7/10], Training Loss: 0.642, Validation Accuracy: 61.24%\n",
        "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 61.40%\n",
        "Epoch [9/10], Training Loss: 0.602, Validation Accuracy: 61.59%\n",
        "Epoch [10/10], Training Loss: 0.592, Validation Accuracy: 60.90%\n",
        "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 62.03%\n",
        "Epoch [2/10], Training Loss: 0.813, Validation Accuracy: 61.40%\n",
        "Epoch [3/10], Training Loss: 0.759, Validation Accuracy: 61.87%\n",
        "Epoch [4/10], Training Loss: 0.724, Validation Accuracy: 61.33%\n",
        "Epoch [5/10], Training Loss: 0.689, Validation Accuracy: 61.73%\n",
        "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 61.53%\n",
        "Epoch [7/10], Training Loss: 0.639, Validation Accuracy: 61.85%\n",
        "Epoch [8/10], Training Loss: 0.622, Validation Accuracy: 61.31%\n",
        "Epoch [9/10], Training Loss: 0.598, Validation Accuracy: 61.02%\n",
        "Epoch [10/10], Training Loss: 0.579, Validation Accuracy: 60.55%\n",
        "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 61.86%\n",
        "Epoch [2/10], Training Loss: 0.800, Validation Accuracy: 61.14%\n",
        "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 61.24%\n",
        "Epoch [4/10], Training Loss: 0.712, Validation Accuracy: 61.37%\n",
        "Epoch [5/10], Training Loss: 0.680, Validation Accuracy: 61.77%\n",
        "Epoch [6/10], Training Loss: 0.662, Validation Accuracy: 61.05%\n",
        "Epoch [7/10], Training Loss: 0.629, Validation Accuracy: 61.60%\n",
        "Epoch [8/10], Training Loss: 0.610, Validation Accuracy: 60.62%\n",
        "Epoch [9/10], Training Loss: 0.583, Validation Accuracy: 60.78%\n",
        "Epoch [10/10], Training Loss: 0.570, Validation Accuracy: 61.44%\n",
        "Epoch [1/10], Training Loss: 0.866, Validation Accuracy: 60.89%\n",
        "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 61.59%\n",
        "Epoch [3/10], Training Loss: 0.708, Validation Accuracy: 61.15%\n",
        "Epoch [4/10], Training Loss: 0.671, Validation Accuracy: 61.54%\n",
        "Epoch [5/10], Training Loss: 0.647, Validation Accuracy: 61.38%\n",
        "Epoch [6/10], Training Loss: 0.618, Validation Accuracy: 61.50%\n",
        "Epoch [7/10], Training Loss: 0.592, Validation Accuracy: 61.12%\n",
        "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 61.18%\n",
        "Epoch [9/10], Training Loss: 0.550, Validation Accuracy: 60.64%\n",
        "Epoch [10/10], Training Loss: 0.534, Validation Accuracy: 61.33%\n",
        "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 61.15%\n",
        "Epoch [2/10], Training Loss: 0.752, Validation Accuracy: 60.97%\n",
        "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 60.15%\n",
        "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 61.62%\n",
        "Epoch [5/10], Training Loss: 0.637, Validation Accuracy: 61.13%\n",
        "Epoch [6/10], Training Loss: 0.604, Validation Accuracy: 61.75%\n",
        "Epoch [7/10], Training Loss: 0.569, Validation Accuracy: 61.07%\n",
        "Epoch [8/10], Training Loss: 0.554, Validation Accuracy: 61.46%\n",
        "Epoch [9/10], Training Loss: 0.527, Validation Accuracy: 61.45%\n",
        "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 61.20%\n",
        "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 60.95%\n",
        "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 60.90%\n",
        "Epoch [3/10], Training Loss: 0.692, Validation Accuracy: 61.92%\n",
        "Epoch [4/10], Training Loss: 0.641, Validation Accuracy: 61.84%\n",
        "Epoch [5/10], Training Loss: 0.607, Validation Accuracy: 61.36%\n",
        "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 61.63%\n",
        "Epoch [7/10], Training Loss: 0.552, Validation Accuracy: 61.90%\n",
        "Epoch [8/10], Training Loss: 0.532, Validation Accuracy: 61.44%\n",
        "Epoch [9/10], Training Loss: 0.505, Validation Accuracy: 61.18%\n",
        "Epoch [10/10], Training Loss: 0.489, Validation Accuracy: 61.78%\n",
        "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 60.89%\n",
        "Epoch [2/10], Training Loss: 0.743, Validation Accuracy: 61.65%\n",
        "Epoch [3/10], Training Loss: 0.681, Validation Accuracy: 61.73%\n",
        "Epoch [4/10], Training Loss: 0.646, Validation Accuracy: 61.27%\n",
        "Epoch [5/10], Training Loss: 0.613, Validation Accuracy: 61.22%\n",
        "Epoch [6/10], Training Loss: 0.578, Validation Accuracy: 61.52%\n",
        "Epoch [7/10], Training Loss: 0.548, Validation Accuracy: 60.91%\n",
        "Epoch [8/10], Training Loss: 0.526, Validation Accuracy: 61.24%\n",
        "Epoch [9/10], Training Loss: 0.503, Validation Accuracy: 61.15%\n",
        "Epoch [10/10], Training Loss: 0.491, Validation Accuracy: 61.04%\n",
        "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.81%\n",
        "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 61.49%\n",
        "Epoch [3/10], Training Loss: 0.675, Validation Accuracy: 61.07%\n",
        "Epoch [4/10], Training Loss: 0.625, Validation Accuracy: 61.57%\n",
        "Epoch [5/10], Training Loss: 0.603, Validation Accuracy: 60.67%\n",
        "Epoch [6/10], Training Loss: 0.568, Validation Accuracy: 61.15%\n",
        "Epoch [7/10], Training Loss: 0.541, Validation Accuracy: 61.32%\n",
        "Epoch [8/10], Training Loss: 0.514, Validation Accuracy: 60.98%\n",
        "Epoch [9/10], Training Loss: 0.497, Validation Accuracy: 60.43%\n",
        "Epoch [10/10], Training Loss: 0.477, Validation Accuracy: 60.64%\n",
        "Epoch [1/10], Training Loss: 0.841, Validation Accuracy: 60.88%\n",
        "Epoch [2/10], Training Loss: 0.713, Validation Accuracy: 61.34%\n",
        "Epoch [3/10], Training Loss: 0.647, Validation Accuracy: 60.69%\n",
        "Epoch [4/10], Training Loss: 0.609, Validation Accuracy: 61.52%\n",
        "Epoch [5/10], Training Loss: 0.561, Validation Accuracy: 60.97%\n",
        "Epoch [6/10], Training Loss: 0.540, Validation Accuracy: 61.43%\n",
        "Epoch [7/10], Training Loss: 0.509, Validation Accuracy: 61.33%\n",
        "Epoch [8/10], Training Loss: 0.486, Validation Accuracy: 61.12%\n",
        "Epoch [9/10], Training Loss: 0.464, Validation Accuracy: 60.26%\n",
        "Epoch [10/10], Training Loss: 0.439, Validation Accuracy: 60.43%\n",
        "Epoch [1/10], Training Loss: 0.819, Validation Accuracy: 60.46%\n",
        "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 60.77%\n",
        "Epoch [3/10], Training Loss: 0.619, Validation Accuracy: 61.06%\n",
        "Epoch [4/10], Training Loss: 0.574, Validation Accuracy: 61.28%\n",
        "Epoch [5/10], Training Loss: 0.542, Validation Accuracy: 61.05%\n",
        "Epoch [6/10], Training Loss: 0.509, Validation Accuracy: 60.91%\n",
        "Epoch [7/10], Training Loss: 0.482, Validation Accuracy: 60.70%\n",
        "Epoch [8/10], Training Loss: 0.474, Validation Accuracy: 60.91%\n",
        "Epoch [9/10], Training Loss: 0.450, Validation Accuracy: 61.02%\n",
        "Epoch [10/10], Training Loss: 0.418, Validation Accuracy: 61.30%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCspAaNR94yo",
        "outputId": "d8d8cb38-e35f-417a-ddee-a16429bffd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.9, 10.05, 11.8, 14.01, 14.36, 14.27, 15.41, 16.01, 17.05, 19.88, 21.49, 23.1, 24.23, 25.19, 26.34, 26.71, 27.22, 27.68, 27.52, 28.6, 29.16, 29.75, 30.42, 31.17, 31.34, 31.2, 31.76, 32.39, 33.22, 33.81, 34.08, 34.79, 34.94, 35.08, 35.6, 35.07, 36.25, 36.16, 37.01, 37.73, 37.92, 38.34, 39.26, 39.74, 40.38, 40.66, 40.5, 41.16, 41.16, 41.27, 41.93, 42.55, 41.94, 42.29, 43.0, 42.71, 43.48, 43.33, 43.78, 43.6, 44.25, 44.79, 44.12, 44.43, 44.76, 45.19, 45.29, 45.16, 45.61, 45.77, 45.85, 46.24, 46.14, 46.65, 46.16, 46.26, 46.58, 47.02, 47.42, 47.18, 47.64, 47.32, 47.77, 47.66, 47.22, 48.34, 48.53, 48.21, 46.46, 48.53, 48.52, 48.53, 49.06, 48.32, 49.13, 49.09, 48.39, 49.79, 49.42, 49.62, 50.18, 49.71, 50.38, 50.64, 50.49, 50.81, 50.69, 50.79, 50.4, 51.25, 51.56, 51.83, 51.59, 51.28, 51.29, 51.41, 52.19, 51.6, 51.63, 52.33, 52.37, 52.24, 52.4, 52.1, 53.48, 53.22, 52.68, 52.91, 51.9, 52.5, 52.99, 53.83, 53.3, 53.85, 53.61, 53.76, 52.71, 53.21, 54.21, 54.21, 53.7, 54.41, 54.99, 54.79, 54.27, 54.76, 54.35, 54.77, 54.92, 55.68, 55.7, 54.97, 55.97, 55.82, 55.66, 55.47, 56.19, 56.17, 55.9, 55.22, 56.28, 55.57, 56.08, 55.89, 55.74, 55.81, 56.81, 56.53, 55.6, 54.27, 57.01, 56.94, 56.63, 56.18, 56.34, 56.97, 56.58, 57.59, 57.36, 57.54, 57.49, 57.5, 57.57, 57.98, 57.73, 57.86, 56.37, 57.81, 57.96, 57.84, 57.9, 58.31, 58.11, 57.74, 57.87, 57.58, 57.87, 57.69, 58.39, 57.43, 57.52, 58.54, 58.99, 58.65, 58.87, 58.7, 57.97, 58.61, 58.78, 58.41, 58.99, 58.86, 58.51, 59.07, 58.84, 58.63, 58.58, 58.82, 59.16, 58.8, 59.16, 59.32, 58.74, 58.73, 58.45, 58.41, 58.57, 58.25, 59.41, 59.75, 59.6, 58.71, 58.89, 59.49, 59.39, 59.59, 58.78, 60.04, 59.48, 60.47, 60.0, 60.19, 59.8, 59.99, 59.92, 59.81, 60.1, 60.04, 59.38, 60.48, 59.64, 59.8, 59.81, 60.12, 60.03, 58.6, 60.66, 60.23, 60.01, 60.57, 60.59, 60.49, 60.45, 59.54, 60.58, 60.2, 60.44, 59.7, 59.99, 60.68, 61.33, 61.07, 60.68, 60.42, 60.09, 60.44, 60.24, 59.63, 60.46, 60.5, 60.71, 60.49, 59.92, 60.71, 60.16, 60.86, 60.39, 60.67, 60.05, 60.73, 60.87, 61.27, 60.63, 61.32, 61.15, 60.39, 61.19, 60.94, 61.11, 60.92, 61.37, 61.11, 61.27, 61.47, 61.1, 61.19, 61.06, 60.75, 60.55, 61.34, 60.97, 61.32, 60.36, 61.23, 60.32, 61.3, 61.06, 60.72, 60.84, 61.68, 61.24, 60.67, 61.34, 61.2, 60.65, 60.73, 60.59, 60.77, 60.91, 61.14, 60.96, 59.82, 60.69, 61.06, 60.88, 60.95, 60.98, 61.06, 60.95, 61.04, 61.61, 62.18, 61.62, 60.82, 61.63, 61.27, 60.9, 60.89, 61.21, 60.91, 61.66, 62.1, 60.03, 61.86, 61.55, 61.93, 60.96, 61.5, 61.02, 60.93, 61.45, 61.74, 61.18, 61.11, 61.7, 61.1, 60.79, 61.04, 61.28, 61.15, 61.2, 60.89, 61.7, 61.28, 61.6, 61.25, 61.01, 61.49, 61.07, 61.71, 61.67, 60.88, 61.34, 61.42, 60.87, 61.56, 61.32, 60.62, 61.23, 61.46, 61.48, 61.71, 61.61, 61.89, 61.72, 61.24, 61.4, 61.59, 60.9, 62.03, 61.4, 61.87, 61.33, 61.73, 61.53, 61.85, 61.31, 61.02, 60.55, 61.86, 61.14, 61.24, 61.37, 61.77, 61.05, 61.6, 60.62, 60.78, 61.44, 60.89, 61.59, 61.15, 61.54, 61.38, 61.5, 61.12, 61.18, 60.64, 61.33, 61.15, 60.97, 60.15, 61.62, 61.13, 61.75, 61.07, 61.46, 61.45, 61.2, 60.95, 60.9, 61.92, 61.84, 61.36, 61.63, 61.9, 61.44, 61.18, 61.78, 60.89, 61.65, 61.73, 61.27, 61.22, 61.52, 60.91, 61.24, 61.15, 61.04, 60.81, 61.49, 61.07, 61.57, 60.67, 61.15, 61.32, 60.98, 60.43, 60.64, 60.88, 61.34, 60.69, 61.52, 60.97, 61.43, 61.33, 61.12, 60.26, 60.43, 60.46, 60.77, 61.06, 61.28, 61.05, 60.91, 60.7, 60.91, 61.02, 61.3]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict, distribution_info_normal: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_uniform + augmented_data_normal ) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"], other_distribution_info[\"normal\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_JjXb3YgC7t",
        "outputId": "cb4d3aae-5850-4226-c462-d619df850bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 93.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6038 6060 6069 5999 5989 6051 5982 6005 5876 5931]\n",
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 10.34%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.30%\n",
            "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 10.49%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 11.05%\n",
            "Epoch [5/10], Training Loss: 2.296, Validation Accuracy: 11.75%\n",
            "Epoch [6/10], Training Loss: 2.293, Validation Accuracy: 12.24%\n",
            "Epoch [7/10], Training Loss: 2.289, Validation Accuracy: 12.45%\n",
            "Epoch [8/10], Training Loss: 2.283, Validation Accuracy: 15.12%\n",
            "Epoch [9/10], Training Loss: 2.276, Validation Accuracy: 17.40%\n",
            "Epoch [10/10], Training Loss: 2.266, Validation Accuracy: 18.49%\n",
            "Epoch [1/10], Training Loss: 2.256, Validation Accuracy: 18.26%\n",
            "Epoch [2/10], Training Loss: 2.242, Validation Accuracy: 18.88%\n",
            "Epoch [3/10], Training Loss: 2.223, Validation Accuracy: 20.73%\n",
            "Epoch [4/10], Training Loss: 2.197, Validation Accuracy: 22.43%\n",
            "Epoch [5/10], Training Loss: 2.168, Validation Accuracy: 23.29%\n",
            "Epoch [6/10], Training Loss: 2.138, Validation Accuracy: 26.29%\n",
            "Epoch [7/10], Training Loss: 2.118, Validation Accuracy: 27.01%\n",
            "Epoch [8/10], Training Loss: 2.102, Validation Accuracy: 27.20%\n",
            "Epoch [9/10], Training Loss: 2.087, Validation Accuracy: 27.19%\n",
            "Epoch [10/10], Training Loss: 2.071, Validation Accuracy: 27.50%\n",
            "Epoch [1/10], Training Loss: 2.038, Validation Accuracy: 26.39%\n",
            "Epoch [2/10], Training Loss: 2.020, Validation Accuracy: 28.42%\n",
            "Epoch [3/10], Training Loss: 2.003, Validation Accuracy: 28.60%\n",
            "Epoch [4/10], Training Loss: 1.990, Validation Accuracy: 28.81%\n",
            "Epoch [5/10], Training Loss: 1.970, Validation Accuracy: 29.77%\n",
            "Epoch [6/10], Training Loss: 1.955, Validation Accuracy: 29.89%\n",
            "Epoch [7/10], Training Loss: 1.942, Validation Accuracy: 30.61%\n",
            "Epoch [8/10], Training Loss: 1.920, Validation Accuracy: 31.12%\n",
            "Epoch [9/10], Training Loss: 1.900, Validation Accuracy: 31.63%\n",
            "Epoch [10/10], Training Loss: 1.884, Validation Accuracy: 31.96%\n",
            "Epoch [1/10], Training Loss: 1.909, Validation Accuracy: 33.55%\n",
            "Epoch [2/10], Training Loss: 1.889, Validation Accuracy: 32.80%\n",
            "Epoch [3/10], Training Loss: 1.872, Validation Accuracy: 33.40%\n",
            "Epoch [4/10], Training Loss: 1.857, Validation Accuracy: 34.79%\n",
            "Epoch [5/10], Training Loss: 1.842, Validation Accuracy: 34.94%\n",
            "Epoch [6/10], Training Loss: 1.823, Validation Accuracy: 35.74%\n",
            "Epoch [7/10], Training Loss: 1.808, Validation Accuracy: 35.06%\n",
            "Epoch [8/10], Training Loss: 1.796, Validation Accuracy: 36.08%\n",
            "Epoch [9/10], Training Loss: 1.780, Validation Accuracy: 36.70%\n",
            "Epoch [10/10], Training Loss: 1.767, Validation Accuracy: 35.39%\n",
            "Epoch [1/10], Training Loss: 1.789, Validation Accuracy: 37.45%\n",
            "Epoch [2/10], Training Loss: 1.765, Validation Accuracy: 37.89%\n",
            "Epoch [3/10], Training Loss: 1.747, Validation Accuracy: 38.94%\n",
            "Epoch [4/10], Training Loss: 1.733, Validation Accuracy: 39.32%\n",
            "Epoch [5/10], Training Loss: 1.718, Validation Accuracy: 39.46%\n",
            "Epoch [6/10], Training Loss: 1.698, Validation Accuracy: 39.68%\n",
            "Epoch [7/10], Training Loss: 1.684, Validation Accuracy: 39.67%\n",
            "Epoch [8/10], Training Loss: 1.673, Validation Accuracy: 40.03%\n",
            "Epoch [9/10], Training Loss: 1.655, Validation Accuracy: 40.11%\n",
            "Epoch [10/10], Training Loss: 1.644, Validation Accuracy: 41.04%\n",
            "Epoch [1/10], Training Loss: 1.640, Validation Accuracy: 41.27%\n",
            "Epoch [2/10], Training Loss: 1.631, Validation Accuracy: 41.38%\n",
            "Epoch [3/10], Training Loss: 1.617, Validation Accuracy: 42.16%\n",
            "Epoch [4/10], Training Loss: 1.594, Validation Accuracy: 42.49%\n",
            "Epoch [5/10], Training Loss: 1.590, Validation Accuracy: 42.07%\n",
            "Epoch [6/10], Training Loss: 1.577, Validation Accuracy: 43.36%\n",
            "Epoch [7/10], Training Loss: 1.562, Validation Accuracy: 43.20%\n",
            "Epoch [8/10], Training Loss: 1.549, Validation Accuracy: 43.27%\n",
            "Epoch [9/10], Training Loss: 1.543, Validation Accuracy: 44.33%\n",
            "Epoch [10/10], Training Loss: 1.538, Validation Accuracy: 43.43%\n",
            "Epoch [1/10], Training Loss: 1.561, Validation Accuracy: 44.09%\n",
            "Epoch [2/10], Training Loss: 1.548, Validation Accuracy: 44.37%\n",
            "Epoch [3/10], Training Loss: 1.533, Validation Accuracy: 44.40%\n",
            "Epoch [4/10], Training Loss: 1.525, Validation Accuracy: 43.98%\n",
            "Epoch [5/10], Training Loss: 1.521, Validation Accuracy: 45.08%\n",
            "Epoch [6/10], Training Loss: 1.505, Validation Accuracy: 45.37%\n",
            "Epoch [7/10], Training Loss: 1.502, Validation Accuracy: 45.18%\n",
            "Epoch [8/10], Training Loss: 1.485, Validation Accuracy: 45.15%\n",
            "Epoch [9/10], Training Loss: 1.478, Validation Accuracy: 45.85%\n",
            "Epoch [10/10], Training Loss: 1.473, Validation Accuracy: 46.03%\n",
            "Epoch [1/10], Training Loss: 1.469, Validation Accuracy: 46.43%\n",
            "Epoch [2/10], Training Loss: 1.451, Validation Accuracy: 45.51%\n",
            "Epoch [3/10], Training Loss: 1.452, Validation Accuracy: 46.53%\n",
            "Epoch [4/10], Training Loss: 1.426, Validation Accuracy: 46.31%\n",
            "Epoch [5/10], Training Loss: 1.427, Validation Accuracy: 46.87%\n",
            "Epoch [6/10], Training Loss: 1.409, Validation Accuracy: 46.87%\n",
            "Epoch [7/10], Training Loss: 1.416, Validation Accuracy: 47.37%\n",
            "Epoch [8/10], Training Loss: 1.395, Validation Accuracy: 46.94%\n",
            "Epoch [9/10], Training Loss: 1.385, Validation Accuracy: 47.77%\n",
            "Epoch [10/10], Training Loss: 1.383, Validation Accuracy: 47.28%\n",
            "Epoch [1/10], Training Loss: 1.445, Validation Accuracy: 48.27%\n",
            "Epoch [2/10], Training Loss: 1.438, Validation Accuracy: 48.57%\n",
            "Epoch [3/10], Training Loss: 1.414, Validation Accuracy: 48.05%\n",
            "Epoch [4/10], Training Loss: 1.401, Validation Accuracy: 48.28%\n",
            "Epoch [5/10], Training Loss: 1.391, Validation Accuracy: 48.74%\n",
            "Epoch [6/10], Training Loss: 1.382, Validation Accuracy: 48.92%\n",
            "Epoch [7/10], Training Loss: 1.369, Validation Accuracy: 47.21%\n",
            "Epoch [8/10], Training Loss: 1.359, Validation Accuracy: 49.35%\n",
            "Epoch [9/10], Training Loss: 1.351, Validation Accuracy: 48.97%\n",
            "Epoch [10/10], Training Loss: 1.350, Validation Accuracy: 49.21%\n",
            "Epoch [1/10], Training Loss: 1.409, Validation Accuracy: 49.23%\n",
            "Epoch [2/10], Training Loss: 1.390, Validation Accuracy: 48.86%\n",
            "Epoch [3/10], Training Loss: 1.382, Validation Accuracy: 49.86%\n",
            "Epoch [4/10], Training Loss: 1.370, Validation Accuracy: 49.93%\n",
            "Epoch [5/10], Training Loss: 1.357, Validation Accuracy: 49.84%\n",
            "Epoch [6/10], Training Loss: 1.347, Validation Accuracy: 49.44%\n",
            "Epoch [7/10], Training Loss: 1.349, Validation Accuracy: 50.97%\n",
            "Epoch [8/10], Training Loss: 1.342, Validation Accuracy: 50.44%\n",
            "Epoch [9/10], Training Loss: 1.322, Validation Accuracy: 51.30%\n",
            "Epoch [10/10], Training Loss: 1.314, Validation Accuracy: 50.78%\n",
            "Epoch [1/10], Training Loss: 1.366, Validation Accuracy: 51.51%\n",
            "Epoch [2/10], Training Loss: 1.354, Validation Accuracy: 51.64%\n",
            "Epoch [3/10], Training Loss: 1.331, Validation Accuracy: 51.77%\n",
            "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 50.46%\n",
            "Epoch [5/10], Training Loss: 1.311, Validation Accuracy: 51.98%\n",
            "Epoch [6/10], Training Loss: 1.309, Validation Accuracy: 51.86%\n",
            "Epoch [7/10], Training Loss: 1.291, Validation Accuracy: 51.88%\n",
            "Epoch [8/10], Training Loss: 1.281, Validation Accuracy: 51.45%\n",
            "Epoch [9/10], Training Loss: 1.274, Validation Accuracy: 52.23%\n",
            "Epoch [10/10], Training Loss: 1.267, Validation Accuracy: 52.37%\n",
            "Epoch [1/10], Training Loss: 1.338, Validation Accuracy: 52.06%\n",
            "Epoch [2/10], Training Loss: 1.312, Validation Accuracy: 51.97%\n",
            "Epoch [3/10], Training Loss: 1.312, Validation Accuracy: 53.12%\n",
            "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 52.70%\n",
            "Epoch [5/10], Training Loss: 1.282, Validation Accuracy: 52.39%\n",
            "Epoch [6/10], Training Loss: 1.263, Validation Accuracy: 52.49%\n",
            "Epoch [7/10], Training Loss: 1.261, Validation Accuracy: 52.72%\n",
            "Epoch [8/10], Training Loss: 1.254, Validation Accuracy: 52.09%\n",
            "Epoch [9/10], Training Loss: 1.240, Validation Accuracy: 52.49%\n",
            "Epoch [10/10], Training Loss: 1.238, Validation Accuracy: 52.20%\n",
            "Epoch [1/10], Training Loss: 1.275, Validation Accuracy: 53.35%\n",
            "Epoch [2/10], Training Loss: 1.264, Validation Accuracy: 53.00%\n",
            "Epoch [3/10], Training Loss: 1.241, Validation Accuracy: 52.74%\n",
            "Epoch [4/10], Training Loss: 1.228, Validation Accuracy: 53.15%\n",
            "Epoch [5/10], Training Loss: 1.217, Validation Accuracy: 53.42%\n",
            "Epoch [6/10], Training Loss: 1.212, Validation Accuracy: 53.79%\n",
            "Epoch [7/10], Training Loss: 1.187, Validation Accuracy: 53.46%\n",
            "Epoch [8/10], Training Loss: 1.182, Validation Accuracy: 53.54%\n",
            "Epoch [9/10], Training Loss: 1.167, Validation Accuracy: 53.58%\n",
            "Epoch [10/10], Training Loss: 1.170, Validation Accuracy: 52.89%\n",
            "Epoch [1/10], Training Loss: 1.272, Validation Accuracy: 54.33%\n",
            "Epoch [2/10], Training Loss: 1.251, Validation Accuracy: 53.09%\n",
            "Epoch [3/10], Training Loss: 1.230, Validation Accuracy: 54.03%\n",
            "Epoch [4/10], Training Loss: 1.214, Validation Accuracy: 54.15%\n",
            "Epoch [5/10], Training Loss: 1.204, Validation Accuracy: 53.98%\n",
            "Epoch [6/10], Training Loss: 1.203, Validation Accuracy: 54.18%\n",
            "Epoch [7/10], Training Loss: 1.188, Validation Accuracy: 54.87%\n",
            "Epoch [8/10], Training Loss: 1.179, Validation Accuracy: 54.66%\n",
            "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 54.34%\n",
            "Epoch [10/10], Training Loss: 1.159, Validation Accuracy: 54.49%\n",
            "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 55.55%\n",
            "Epoch [2/10], Training Loss: 1.232, Validation Accuracy: 55.03%\n",
            "Epoch [3/10], Training Loss: 1.217, Validation Accuracy: 55.06%\n",
            "Epoch [4/10], Training Loss: 1.200, Validation Accuracy: 55.69%\n",
            "Epoch [5/10], Training Loss: 1.194, Validation Accuracy: 54.86%\n",
            "Epoch [6/10], Training Loss: 1.183, Validation Accuracy: 55.93%\n",
            "Epoch [7/10], Training Loss: 1.172, Validation Accuracy: 55.37%\n",
            "Epoch [8/10], Training Loss: 1.154, Validation Accuracy: 55.47%\n",
            "Epoch [9/10], Training Loss: 1.150, Validation Accuracy: 55.77%\n",
            "Epoch [10/10], Training Loss: 1.141, Validation Accuracy: 55.49%\n",
            "Epoch [1/10], Training Loss: 1.222, Validation Accuracy: 55.70%\n",
            "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 55.88%\n",
            "Epoch [3/10], Training Loss: 1.171, Validation Accuracy: 56.18%\n",
            "Epoch [4/10], Training Loss: 1.171, Validation Accuracy: 55.40%\n",
            "Epoch [5/10], Training Loss: 1.156, Validation Accuracy: 56.57%\n",
            "Epoch [6/10], Training Loss: 1.139, Validation Accuracy: 55.85%\n",
            "Epoch [7/10], Training Loss: 1.127, Validation Accuracy: 54.97%\n",
            "Epoch [8/10], Training Loss: 1.124, Validation Accuracy: 56.36%\n",
            "Epoch [9/10], Training Loss: 1.102, Validation Accuracy: 56.35%\n",
            "Epoch [10/10], Training Loss: 1.092, Validation Accuracy: 55.38%\n",
            "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 56.42%\n",
            "Epoch [2/10], Training Loss: 1.172, Validation Accuracy: 56.23%\n",
            "Epoch [3/10], Training Loss: 1.164, Validation Accuracy: 56.12%\n",
            "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 56.25%\n",
            "Epoch [5/10], Training Loss: 1.125, Validation Accuracy: 56.35%\n",
            "Epoch [6/10], Training Loss: 1.114, Validation Accuracy: 56.48%\n",
            "Epoch [7/10], Training Loss: 1.104, Validation Accuracy: 56.86%\n",
            "Epoch [8/10], Training Loss: 1.091, Validation Accuracy: 57.16%\n",
            "Epoch [9/10], Training Loss: 1.086, Validation Accuracy: 56.74%\n",
            "Epoch [10/10], Training Loss: 1.071, Validation Accuracy: 56.52%\n",
            "Epoch [1/10], Training Loss: 1.151, Validation Accuracy: 57.10%\n",
            "Epoch [2/10], Training Loss: 1.122, Validation Accuracy: 57.27%\n",
            "Epoch [3/10], Training Loss: 1.096, Validation Accuracy: 57.50%\n",
            "Epoch [4/10], Training Loss: 1.087, Validation Accuracy: 56.01%\n",
            "Epoch [5/10], Training Loss: 1.071, Validation Accuracy: 57.93%\n",
            "Epoch [6/10], Training Loss: 1.052, Validation Accuracy: 57.72%\n",
            "Epoch [7/10], Training Loss: 1.033, Validation Accuracy: 57.47%\n",
            "Epoch [8/10], Training Loss: 1.028, Validation Accuracy: 56.71%\n",
            "Epoch [9/10], Training Loss: 1.018, Validation Accuracy: 57.45%\n",
            "Epoch [10/10], Training Loss: 1.018, Validation Accuracy: 57.50%\n",
            "Epoch [1/10], Training Loss: 1.165, Validation Accuracy: 57.08%\n",
            "Epoch [2/10], Training Loss: 1.128, Validation Accuracy: 57.65%\n",
            "Epoch [3/10], Training Loss: 1.106, Validation Accuracy: 57.93%\n",
            "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 57.91%\n",
            "Epoch [5/10], Training Loss: 1.066, Validation Accuracy: 57.61%\n",
            "Epoch [6/10], Training Loss: 1.059, Validation Accuracy: 57.55%\n",
            "Epoch [7/10], Training Loss: 1.038, Validation Accuracy: 57.34%\n",
            "Epoch [8/10], Training Loss: 1.045, Validation Accuracy: 57.53%\n",
            "Epoch [9/10], Training Loss: 1.014, Validation Accuracy: 58.23%\n",
            "Epoch [10/10], Training Loss: 1.009, Validation Accuracy: 57.96%\n",
            "Epoch [1/10], Training Loss: 1.151, Validation Accuracy: 58.06%\n",
            "Epoch [2/10], Training Loss: 1.117, Validation Accuracy: 58.65%\n",
            "Epoch [3/10], Training Loss: 1.100, Validation Accuracy: 58.38%\n",
            "Epoch [4/10], Training Loss: 1.069, Validation Accuracy: 58.56%\n",
            "Epoch [5/10], Training Loss: 1.060, Validation Accuracy: 57.84%\n",
            "Epoch [6/10], Training Loss: 1.043, Validation Accuracy: 57.87%\n",
            "Epoch [7/10], Training Loss: 1.036, Validation Accuracy: 57.78%\n",
            "Epoch [8/10], Training Loss: 1.023, Validation Accuracy: 57.54%\n",
            "Epoch [9/10], Training Loss: 1.013, Validation Accuracy: 58.31%\n",
            "Epoch [10/10], Training Loss: 0.992, Validation Accuracy: 58.22%\n",
            "Epoch [1/10], Training Loss: 1.129, Validation Accuracy: 57.28%\n",
            "Epoch [2/10], Training Loss: 1.092, Validation Accuracy: 59.14%\n",
            "Epoch [3/10], Training Loss: 1.068, Validation Accuracy: 58.27%\n",
            "Epoch [4/10], Training Loss: 1.046, Validation Accuracy: 58.78%\n",
            "Epoch [5/10], Training Loss: 1.032, Validation Accuracy: 58.61%\n",
            "Epoch [6/10], Training Loss: 1.012, Validation Accuracy: 58.23%\n",
            "Epoch [7/10], Training Loss: 1.006, Validation Accuracy: 58.64%\n",
            "Epoch [8/10], Training Loss: 0.989, Validation Accuracy: 58.34%\n",
            "Epoch [9/10], Training Loss: 0.984, Validation Accuracy: 59.02%\n",
            "Epoch [10/10], Training Loss: 0.973, Validation Accuracy: 58.37%\n",
            "Epoch [1/10], Training Loss: 1.103, Validation Accuracy: 58.27%\n",
            "Epoch [2/10], Training Loss: 1.080, Validation Accuracy: 58.32%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 58.60%\n",
            "Epoch [4/10], Training Loss: 1.030, Validation Accuracy: 58.55%\n",
            "Epoch [5/10], Training Loss: 1.010, Validation Accuracy: 58.77%\n",
            "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 58.65%\n",
            "Epoch [7/10], Training Loss: 0.986, Validation Accuracy: 58.77%\n",
            "Epoch [8/10], Training Loss: 0.972, Validation Accuracy: 58.93%\n",
            "Epoch [9/10], Training Loss: 0.952, Validation Accuracy: 58.32%\n",
            "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 58.54%\n",
            "Epoch [1/10], Training Loss: 1.070, Validation Accuracy: 59.56%\n",
            "Epoch [2/10], Training Loss: 1.026, Validation Accuracy: 59.29%\n",
            "Epoch [3/10], Training Loss: 1.013, Validation Accuracy: 58.67%\n",
            "Epoch [4/10], Training Loss: 0.983, Validation Accuracy: 59.30%\n",
            "Epoch [5/10], Training Loss: 0.967, Validation Accuracy: 59.23%\n",
            "Epoch [6/10], Training Loss: 0.938, Validation Accuracy: 59.09%\n",
            "Epoch [7/10], Training Loss: 0.933, Validation Accuracy: 59.37%\n",
            "Epoch [8/10], Training Loss: 0.916, Validation Accuracy: 59.20%\n",
            "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 59.13%\n",
            "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 59.19%\n",
            "Epoch [1/10], Training Loss: 1.085, Validation Accuracy: 59.52%\n",
            "Epoch [2/10], Training Loss: 1.028, Validation Accuracy: 58.62%\n",
            "Epoch [3/10], Training Loss: 1.005, Validation Accuracy: 58.92%\n",
            "Epoch [4/10], Training Loss: 0.987, Validation Accuracy: 59.80%\n",
            "Epoch [5/10], Training Loss: 0.964, Validation Accuracy: 58.54%\n",
            "Epoch [6/10], Training Loss: 0.957, Validation Accuracy: 59.97%\n",
            "Epoch [7/10], Training Loss: 0.937, Validation Accuracy: 59.30%\n",
            "Epoch [8/10], Training Loss: 0.929, Validation Accuracy: 59.29%\n",
            "Epoch [9/10], Training Loss: 0.909, Validation Accuracy: 58.65%\n",
            "Epoch [10/10], Training Loss: 0.893, Validation Accuracy: 58.57%\n",
            "Epoch [1/10], Training Loss: 1.076, Validation Accuracy: 58.19%\n",
            "Epoch [2/10], Training Loss: 1.021, Validation Accuracy: 59.74%\n",
            "Epoch [3/10], Training Loss: 1.001, Validation Accuracy: 59.76%\n",
            "Epoch [4/10], Training Loss: 0.977, Validation Accuracy: 60.15%\n",
            "Epoch [5/10], Training Loss: 0.968, Validation Accuracy: 59.78%\n",
            "Epoch [6/10], Training Loss: 0.944, Validation Accuracy: 58.73%\n",
            "Epoch [7/10], Training Loss: 0.928, Validation Accuracy: 60.43%\n",
            "Epoch [8/10], Training Loss: 0.912, Validation Accuracy: 59.84%\n",
            "Epoch [9/10], Training Loss: 0.898, Validation Accuracy: 59.46%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 59.61%\n",
            "Epoch [1/10], Training Loss: 1.048, Validation Accuracy: 59.96%\n",
            "Epoch [2/10], Training Loss: 1.002, Validation Accuracy: 60.33%\n",
            "Epoch [3/10], Training Loss: 0.972, Validation Accuracy: 59.91%\n",
            "Epoch [4/10], Training Loss: 0.952, Validation Accuracy: 60.15%\n",
            "Epoch [5/10], Training Loss: 0.938, Validation Accuracy: 59.99%\n",
            "Epoch [6/10], Training Loss: 0.913, Validation Accuracy: 59.48%\n",
            "Epoch [7/10], Training Loss: 0.898, Validation Accuracy: 59.83%\n",
            "Epoch [8/10], Training Loss: 0.888, Validation Accuracy: 60.02%\n",
            "Epoch [9/10], Training Loss: 0.869, Validation Accuracy: 59.95%\n",
            "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 59.76%\n",
            "Epoch [1/10], Training Loss: 1.032, Validation Accuracy: 59.61%\n",
            "Epoch [2/10], Training Loss: 0.979, Validation Accuracy: 59.72%\n",
            "Epoch [3/10], Training Loss: 0.964, Validation Accuracy: 59.41%\n",
            "Epoch [4/10], Training Loss: 0.930, Validation Accuracy: 60.16%\n",
            "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 60.08%\n",
            "Epoch [6/10], Training Loss: 0.893, Validation Accuracy: 60.08%\n",
            "Epoch [7/10], Training Loss: 0.878, Validation Accuracy: 59.35%\n",
            "Epoch [8/10], Training Loss: 0.868, Validation Accuracy: 59.67%\n",
            "Epoch [9/10], Training Loss: 0.853, Validation Accuracy: 59.74%\n",
            "Epoch [10/10], Training Loss: 0.839, Validation Accuracy: 60.45%\n",
            "Epoch [1/10], Training Loss: 1.004, Validation Accuracy: 60.10%\n",
            "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 60.73%\n",
            "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 60.89%\n",
            "Epoch [4/10], Training Loss: 0.900, Validation Accuracy: 60.71%\n",
            "Epoch [5/10], Training Loss: 0.861, Validation Accuracy: 60.62%\n",
            "Epoch [6/10], Training Loss: 0.853, Validation Accuracy: 61.08%\n",
            "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.20%\n",
            "Epoch [8/10], Training Loss: 0.814, Validation Accuracy: 60.47%\n",
            "Epoch [9/10], Training Loss: 0.797, Validation Accuracy: 60.93%\n",
            "Epoch [10/10], Training Loss: 0.791, Validation Accuracy: 60.74%\n",
            "Epoch [1/10], Training Loss: 1.014, Validation Accuracy: 60.54%\n",
            "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 60.95%\n",
            "Epoch [3/10], Training Loss: 0.928, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.885, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.868, Validation Accuracy: 60.51%\n",
            "Epoch [6/10], Training Loss: 0.850, Validation Accuracy: 60.26%\n",
            "Epoch [7/10], Training Loss: 0.845, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.818, Validation Accuracy: 60.30%\n",
            "Epoch [9/10], Training Loss: 0.806, Validation Accuracy: 59.54%\n",
            "Epoch [10/10], Training Loss: 0.794, Validation Accuracy: 60.17%\n",
            "Epoch [1/10], Training Loss: 1.008, Validation Accuracy: 59.57%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 60.94%\n",
            "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.884, Validation Accuracy: 61.13%\n",
            "Epoch [5/10], Training Loss: 0.871, Validation Accuracy: 61.35%\n",
            "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 61.07%\n",
            "Epoch [7/10], Training Loss: 0.828, Validation Accuracy: 61.42%\n",
            "Epoch [8/10], Training Loss: 0.811, Validation Accuracy: 61.73%\n",
            "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 61.10%\n",
            "Epoch [10/10], Training Loss: 0.777, Validation Accuracy: 60.39%\n",
            "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.46%\n",
            "Epoch [2/10], Training Loss: 0.927, Validation Accuracy: 60.90%\n",
            "Epoch [3/10], Training Loss: 0.893, Validation Accuracy: 60.70%\n",
            "Epoch [4/10], Training Loss: 0.870, Validation Accuracy: 60.59%\n",
            "Epoch [5/10], Training Loss: 0.847, Validation Accuracy: 60.78%\n",
            "Epoch [6/10], Training Loss: 0.828, Validation Accuracy: 60.03%\n",
            "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 60.35%\n",
            "Epoch [8/10], Training Loss: 0.793, Validation Accuracy: 60.34%\n",
            "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.756, Validation Accuracy: 60.13%\n",
            "Epoch [1/10], Training Loss: 0.979, Validation Accuracy: 60.64%\n",
            "Epoch [2/10], Training Loss: 0.917, Validation Accuracy: 60.00%\n",
            "Epoch [3/10], Training Loss: 0.876, Validation Accuracy: 61.40%\n",
            "Epoch [4/10], Training Loss: 0.849, Validation Accuracy: 60.23%\n",
            "Epoch [5/10], Training Loss: 0.836, Validation Accuracy: 61.03%\n",
            "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 60.53%\n",
            "Epoch [7/10], Training Loss: 0.782, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.770, Validation Accuracy: 60.76%\n",
            "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 60.95%\n",
            "Epoch [10/10], Training Loss: 0.727, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 61.20%\n",
            "Epoch [2/10], Training Loss: 0.874, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 61.18%\n",
            "Epoch [4/10], Training Loss: 0.801, Validation Accuracy: 61.49%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 61.84%\n",
            "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 61.31%\n",
            "Epoch [7/10], Training Loss: 0.730, Validation Accuracy: 61.41%\n",
            "Epoch [8/10], Training Loss: 0.730, Validation Accuracy: 61.11%\n",
            "Epoch [9/10], Training Loss: 0.704, Validation Accuracy: 61.39%\n",
            "Epoch [10/10], Training Loss: 0.690, Validation Accuracy: 61.23%\n",
            "Epoch [1/10], Training Loss: 0.968, Validation Accuracy: 61.31%\n",
            "Epoch [2/10], Training Loss: 0.894, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.852, Validation Accuracy: 60.85%\n",
            "Epoch [4/10], Training Loss: 0.820, Validation Accuracy: 61.04%\n",
            "Epoch [5/10], Training Loss: 0.796, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.766, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.749, Validation Accuracy: 61.09%\n",
            "Epoch [8/10], Training Loss: 0.732, Validation Accuracy: 60.16%\n",
            "Epoch [9/10], Training Loss: 0.710, Validation Accuracy: 61.23%\n",
            "Epoch [10/10], Training Loss: 0.693, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 59.94%\n",
            "Epoch [2/10], Training Loss: 0.889, Validation Accuracy: 61.78%\n",
            "Epoch [3/10], Training Loss: 0.843, Validation Accuracy: 60.82%\n",
            "Epoch [4/10], Training Loss: 0.826, Validation Accuracy: 61.40%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 62.43%\n",
            "Epoch [6/10], Training Loss: 0.760, Validation Accuracy: 62.30%\n",
            "Epoch [7/10], Training Loss: 0.742, Validation Accuracy: 61.08%\n",
            "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.708, Validation Accuracy: 60.98%\n",
            "Epoch [10/10], Training Loss: 0.689, Validation Accuracy: 61.03%\n",
            "Epoch [1/10], Training Loss: 0.932, Validation Accuracy: 61.41%\n",
            "Epoch [2/10], Training Loss: 0.856, Validation Accuracy: 60.87%\n",
            "Epoch [3/10], Training Loss: 0.816, Validation Accuracy: 61.03%\n",
            "Epoch [4/10], Training Loss: 0.784, Validation Accuracy: 61.44%\n",
            "Epoch [5/10], Training Loss: 0.766, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.732, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.708, Validation Accuracy: 60.37%\n",
            "Epoch [8/10], Training Loss: 0.687, Validation Accuracy: 60.74%\n",
            "Epoch [9/10], Training Loss: 0.674, Validation Accuracy: 60.86%\n",
            "Epoch [10/10], Training Loss: 0.652, Validation Accuracy: 61.28%\n",
            "Epoch [1/10], Training Loss: 0.936, Validation Accuracy: 60.56%\n",
            "Epoch [2/10], Training Loss: 0.850, Validation Accuracy: 61.32%\n",
            "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 61.47%\n",
            "Epoch [4/10], Training Loss: 0.772, Validation Accuracy: 61.45%\n",
            "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 61.52%\n",
            "Epoch [6/10], Training Loss: 0.710, Validation Accuracy: 61.68%\n",
            "Epoch [7/10], Training Loss: 0.686, Validation Accuracy: 59.85%\n",
            "Epoch [8/10], Training Loss: 0.683, Validation Accuracy: 61.43%\n",
            "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.637, Validation Accuracy: 61.42%\n",
            "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 60.67%\n",
            "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 61.69%\n",
            "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 61.99%\n",
            "Epoch [5/10], Training Loss: 0.693, Validation Accuracy: 61.38%\n",
            "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 61.40%\n",
            "Epoch [7/10], Training Loss: 0.644, Validation Accuracy: 61.46%\n",
            "Epoch [8/10], Training Loss: 0.630, Validation Accuracy: 61.20%\n",
            "Epoch [9/10], Training Loss: 0.610, Validation Accuracy: 61.29%\n",
            "Epoch [10/10], Training Loss: 0.588, Validation Accuracy: 61.55%\n",
            "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 60.70%\n",
            "Epoch [2/10], Training Loss: 0.827, Validation Accuracy: 60.99%\n",
            "Epoch [3/10], Training Loss: 0.778, Validation Accuracy: 61.21%\n",
            "Epoch [4/10], Training Loss: 0.733, Validation Accuracy: 61.79%\n",
            "Epoch [5/10], Training Loss: 0.697, Validation Accuracy: 61.07%\n",
            "Epoch [6/10], Training Loss: 0.685, Validation Accuracy: 61.23%\n",
            "Epoch [7/10], Training Loss: 0.655, Validation Accuracy: 60.96%\n",
            "Epoch [8/10], Training Loss: 0.636, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.618, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.593, Validation Accuracy: 60.59%\n",
            "Epoch [1/10], Training Loss: 0.921, Validation Accuracy: 61.38%\n",
            "Epoch [2/10], Training Loss: 0.833, Validation Accuracy: 61.53%\n",
            "Epoch [3/10], Training Loss: 0.769, Validation Accuracy: 62.04%\n",
            "Epoch [4/10], Training Loss: 0.738, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.707, Validation Accuracy: 60.26%\n",
            "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.656, Validation Accuracy: 61.72%\n",
            "Epoch [8/10], Training Loss: 0.629, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 61.19%\n",
            "Epoch [10/10], Training Loss: 0.597, Validation Accuracy: 61.34%\n",
            "Epoch [1/10], Training Loss: 0.895, Validation Accuracy: 61.63%\n",
            "Epoch [2/10], Training Loss: 0.794, Validation Accuracy: 61.48%\n",
            "Epoch [3/10], Training Loss: 0.738, Validation Accuracy: 60.74%\n",
            "Epoch [4/10], Training Loss: 0.703, Validation Accuracy: 61.27%\n",
            "Epoch [5/10], Training Loss: 0.668, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.645, Validation Accuracy: 60.45%\n",
            "Epoch [7/10], Training Loss: 0.622, Validation Accuracy: 60.83%\n",
            "Epoch [8/10], Training Loss: 0.598, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 60.90%\n",
            "Epoch [10/10], Training Loss: 0.568, Validation Accuracy: 60.99%\n",
            "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 61.18%\n",
            "Epoch [2/10], Training Loss: 0.784, Validation Accuracy: 61.46%\n",
            "Epoch [3/10], Training Loss: 0.726, Validation Accuracy: 61.18%\n",
            "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 61.52%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.13%\n",
            "Epoch [7/10], Training Loss: 0.599, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.583, Validation Accuracy: 61.31%\n",
            "Epoch [9/10], Training Loss: 0.559, Validation Accuracy: 61.37%\n",
            "Epoch [10/10], Training Loss: 0.543, Validation Accuracy: 60.80%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 61.31%\n",
            "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.644, Validation Accuracy: 61.11%\n",
            "Epoch [5/10], Training Loss: 0.616, Validation Accuracy: 61.44%\n",
            "Epoch [6/10], Training Loss: 0.587, Validation Accuracy: 61.97%\n",
            "Epoch [7/10], Training Loss: 0.554, Validation Accuracy: 61.81%\n",
            "Epoch [8/10], Training Loss: 0.540, Validation Accuracy: 61.25%\n",
            "Epoch [9/10], Training Loss: 0.520, Validation Accuracy: 61.53%\n",
            "Epoch [10/10], Training Loss: 0.512, Validation Accuracy: 61.52%\n",
            "Epoch [1/10], Training Loss: 0.883, Validation Accuracy: 60.91%\n",
            "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 60.37%\n",
            "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 60.91%\n",
            "Epoch [4/10], Training Loss: 0.667, Validation Accuracy: 61.35%\n",
            "Epoch [5/10], Training Loss: 0.626, Validation Accuracy: 60.49%\n",
            "Epoch [6/10], Training Loss: 0.607, Validation Accuracy: 61.29%\n",
            "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.34%\n",
            "Epoch [8/10], Training Loss: 0.552, Validation Accuracy: 60.05%\n",
            "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 60.84%\n",
            "Epoch [10/10], Training Loss: 0.522, Validation Accuracy: 60.98%\n",
            "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 59.76%\n",
            "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 60.85%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 61.15%\n",
            "Epoch [4/10], Training Loss: 0.656, Validation Accuracy: 61.13%\n",
            "Epoch [5/10], Training Loss: 0.625, Validation Accuracy: 61.00%\n",
            "Epoch [6/10], Training Loss: 0.599, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.580, Validation Accuracy: 61.16%\n",
            "Epoch [8/10], Training Loss: 0.560, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.525, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.503, Validation Accuracy: 61.17%\n",
            "Epoch [1/10], Training Loss: 0.853, Validation Accuracy: 60.83%\n",
            "Epoch [2/10], Training Loss: 0.737, Validation Accuracy: 61.56%\n",
            "Epoch [3/10], Training Loss: 0.672, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.630, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.588, Validation Accuracy: 60.57%\n",
            "Epoch [6/10], Training Loss: 0.565, Validation Accuracy: 61.26%\n",
            "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 60.21%\n",
            "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 60.14%\n",
            "Epoch [9/10], Training Loss: 0.493, Validation Accuracy: 59.85%\n",
            "Epoch [10/10], Training Loss: 0.466, Validation Accuracy: 59.98%\n",
            "Epoch [1/10], Training Loss: 0.853, Validation Accuracy: 60.70%\n",
            "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 60.36%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.07%\n",
            "Epoch [4/10], Training Loss: 0.608, Validation Accuracy: 61.37%\n",
            "Epoch [5/10], Training Loss: 0.567, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 61.21%\n",
            "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 61.08%\n",
            "Epoch [8/10], Training Loss: 0.488, Validation Accuracy: 61.47%\n",
            "Epoch [9/10], Training Loss: 0.469, Validation Accuracy: 60.38%\n",
            "Epoch [10/10], Training Loss: 0.460, Validation Accuracy: 60.49%\n",
            "Epoch [1/10], Training Loss: 0.814, Validation Accuracy: 60.29%\n",
            "Epoch [2/10], Training Loss: 0.687, Validation Accuracy: 61.36%\n",
            "Epoch [3/10], Training Loss: 0.610, Validation Accuracy: 60.48%\n",
            "Epoch [4/10], Training Loss: 0.590, Validation Accuracy: 62.01%\n",
            "Epoch [5/10], Training Loss: 0.541, Validation Accuracy: 61.52%\n",
            "Epoch [6/10], Training Loss: 0.506, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.472, Validation Accuracy: 61.31%\n",
            "Epoch [8/10], Training Loss: 0.462, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.433, Validation Accuracy: 60.84%\n",
            "Epoch [10/10], Training Loss: 0.412, Validation Accuracy: 60.82%\n",
            "Epoch [1/10], Training Loss: 0.844, Validation Accuracy: 59.78%\n",
            "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.642, Validation Accuracy: 61.14%\n",
            "Epoch [4/10], Training Loss: 0.588, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.552, Validation Accuracy: 61.14%\n",
            "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.497, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.484, Validation Accuracy: 60.88%\n",
            "Epoch [9/10], Training Loss: 0.445, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.434, Validation Accuracy: 60.17%\n",
            "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 60.90%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 61.00%\n",
            "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.578, Validation Accuracy: 60.20%\n",
            "Epoch [5/10], Training Loss: 0.537, Validation Accuracy: 61.16%\n",
            "Epoch [6/10], Training Loss: 0.505, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.488, Validation Accuracy: 61.03%\n",
            "Epoch [8/10], Training Loss: 0.460, Validation Accuracy: 60.58%\n",
            "Epoch [9/10], Training Loss: 0.447, Validation Accuracy: 60.49%\n",
            "Epoch [10/10], Training Loss: 0.416, Validation Accuracy: 60.63%\n",
            "Confusion Matrix:\n",
            "[[739  26  46  14  24  13  10   9  72  47]\n",
            " [ 57 682  16  14   5   9  11  14  39 153]\n",
            " [ 93  10 510  72  98  76  61  47  21  12]\n",
            " [ 32  16  98 401  79 180  78  51  30  35]\n",
            " [ 44   7 134  61 519  38  93  80  17   7]\n",
            " [ 25  11  88 169  74 498  35  73  16  11]\n",
            " [ 15  11  84  76  58  44 671  18   6  17]\n",
            " [ 22  10  53  47  90  89  14 621   5  49]\n",
            " [129  48  19  14  20  13   9   6 691  51]\n",
            " [ 67 111   8  15  10  17  11  21  38 702]]\n",
            "Test Accuracy: 60.34%\n",
            "True Positives (TP): [739 682 510 401 519 498 671 621 691 702]\n",
            "False Positives (FP): [484 250 546 482 458 479 322 319 244 382]\n",
            "True Negatives (TN): [8516 8750 8454 8518 8542 8521 8678 8681 8756 8618]\n",
            "False Negatives (FN): [261 318 490 599 481 502 329 379 309 298]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.60425184 0.73175966 0.48295455 0.45413364 0.53121801 0.50972364\n",
            " 0.67573011 0.6606383  0.73903743 0.64760148]\n",
            "Recall: [0.739 0.682 0.51  0.401 0.519 0.498 0.671 0.621 0.691 0.702]\n",
            "F1 Score: [0.6648673  0.70600414 0.49610895 0.42591609 0.52503794 0.50379363\n",
            " 0.67335675 0.64020619 0.71421189 0.67370441]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SulcAvnOovOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 10.34%\n",
        "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.30%\n",
        "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 10.49%\n",
        "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 11.05%\n",
        "Epoch [5/10], Training Loss: 2.296, Validation Accuracy: 11.75%\n",
        "Epoch [6/10], Training Loss: 2.293, Validation Accuracy: 12.24%\n",
        "Epoch [7/10], Training Loss: 2.289, Validation Accuracy: 12.45%\n",
        "Epoch [8/10], Training Loss: 2.283, Validation Accuracy: 15.12%\n",
        "Epoch [9/10], Training Loss: 2.276, Validation Accuracy: 17.40%\n",
        "Epoch [10/10], Training Loss: 2.266, Validation Accuracy: 18.49%\n",
        "Epoch [1/10], Training Loss: 2.256, Validation Accuracy: 18.26%\n",
        "Epoch [2/10], Training Loss: 2.242, Validation Accuracy: 18.88%\n",
        "Epoch [3/10], Training Loss: 2.223, Validation Accuracy: 20.73%\n",
        "Epoch [4/10], Training Loss: 2.197, Validation Accuracy: 22.43%\n",
        "Epoch [5/10], Training Loss: 2.168, Validation Accuracy: 23.29%\n",
        "Epoch [6/10], Training Loss: 2.138, Validation Accuracy: 26.29%\n",
        "Epoch [7/10], Training Loss: 2.118, Validation Accuracy: 27.01%\n",
        "Epoch [8/10], Training Loss: 2.102, Validation Accuracy: 27.20%\n",
        "Epoch [9/10], Training Loss: 2.087, Validation Accuracy: 27.19%\n",
        "Epoch [10/10], Training Loss: 2.071, Validation Accuracy: 27.50%\n",
        "Epoch [1/10], Training Loss: 2.038, Validation Accuracy: 26.39%\n",
        "Epoch [2/10], Training Loss: 2.020, Validation Accuracy: 28.42%\n",
        "Epoch [3/10], Training Loss: 2.003, Validation Accuracy: 28.60%\n",
        "Epoch [4/10], Training Loss: 1.990, Validation Accuracy: 28.81%\n",
        "Epoch [5/10], Training Loss: 1.970, Validation Accuracy: 29.77%\n",
        "Epoch [6/10], Training Loss: 1.955, Validation Accuracy: 29.89%\n",
        "Epoch [7/10], Training Loss: 1.942, Validation Accuracy: 30.61%\n",
        "Epoch [8/10], Training Loss: 1.920, Validation Accuracy: 31.12%\n",
        "Epoch [9/10], Training Loss: 1.900, Validation Accuracy: 31.63%\n",
        "Epoch [10/10], Training Loss: 1.884, Validation Accuracy: 31.96%\n",
        "Epoch [1/10], Training Loss: 1.909, Validation Accuracy: 33.55%\n",
        "Epoch [2/10], Training Loss: 1.889, Validation Accuracy: 32.80%\n",
        "Epoch [3/10], Training Loss: 1.872, Validation Accuracy: 33.40%\n",
        "Epoch [4/10], Training Loss: 1.857, Validation Accuracy: 34.79%\n",
        "Epoch [5/10], Training Loss: 1.842, Validation Accuracy: 34.94%\n",
        "Epoch [6/10], Training Loss: 1.823, Validation Accuracy: 35.74%\n",
        "Epoch [7/10], Training Loss: 1.808, Validation Accuracy: 35.06%\n",
        "Epoch [8/10], Training Loss: 1.796, Validation Accuracy: 36.08%\n",
        "Epoch [9/10], Training Loss: 1.780, Validation Accuracy: 36.70%\n",
        "Epoch [10/10], Training Loss: 1.767, Validation Accuracy: 35.39%\n",
        "Epoch [1/10], Training Loss: 1.789, Validation Accuracy: 37.45%\n",
        "Epoch [2/10], Training Loss: 1.765, Validation Accuracy: 37.89%\n",
        "Epoch [3/10], Training Loss: 1.747, Validation Accuracy: 38.94%\n",
        "Epoch [4/10], Training Loss: 1.733, Validation Accuracy: 39.32%\n",
        "Epoch [5/10], Training Loss: 1.718, Validation Accuracy: 39.46%\n",
        "Epoch [6/10], Training Loss: 1.698, Validation Accuracy: 39.68%\n",
        "Epoch [7/10], Training Loss: 1.684, Validation Accuracy: 39.67%\n",
        "Epoch [8/10], Training Loss: 1.673, Validation Accuracy: 40.03%\n",
        "Epoch [9/10], Training Loss: 1.655, Validation Accuracy: 40.11%\n",
        "Epoch [10/10], Training Loss: 1.644, Validation Accuracy: 41.04%\n",
        "Epoch [1/10], Training Loss: 1.640, Validation Accuracy: 41.27%\n",
        "Epoch [2/10], Training Loss: 1.631, Validation Accuracy: 41.38%\n",
        "Epoch [3/10], Training Loss: 1.617, Validation Accuracy: 42.16%\n",
        "Epoch [4/10], Training Loss: 1.594, Validation Accuracy: 42.49%\n",
        "Epoch [5/10], Training Loss: 1.590, Validation Accuracy: 42.07%\n",
        "Epoch [6/10], Training Loss: 1.577, Validation Accuracy: 43.36%\n",
        "Epoch [7/10], Training Loss: 1.562, Validation Accuracy: 43.20%\n",
        "Epoch [8/10], Training Loss: 1.549, Validation Accuracy: 43.27%\n",
        "Epoch [9/10], Training Loss: 1.543, Validation Accuracy: 44.33%\n",
        "Epoch [10/10], Training Loss: 1.538, Validation Accuracy: 43.43%\n",
        "Epoch [1/10], Training Loss: 1.561, Validation Accuracy: 44.09%\n",
        "Epoch [2/10], Training Loss: 1.548, Validation Accuracy: 44.37%\n",
        "Epoch [3/10], Training Loss: 1.533, Validation Accuracy: 44.40%\n",
        "Epoch [4/10], Training Loss: 1.525, Validation Accuracy: 43.98%\n",
        "Epoch [5/10], Training Loss: 1.521, Validation Accuracy: 45.08%\n",
        "Epoch [6/10], Training Loss: 1.505, Validation Accuracy: 45.37%\n",
        "Epoch [7/10], Training Loss: 1.502, Validation Accuracy: 45.18%\n",
        "Epoch [8/10], Training Loss: 1.485, Validation Accuracy: 45.15%\n",
        "Epoch [9/10], Training Loss: 1.478, Validation Accuracy: 45.85%\n",
        "Epoch [10/10], Training Loss: 1.473, Validation Accuracy: 46.03%\n",
        "Epoch [1/10], Training Loss: 1.469, Validation Accuracy: 46.43%\n",
        "Epoch [2/10], Training Loss: 1.451, Validation Accuracy: 45.51%\n",
        "Epoch [3/10], Training Loss: 1.452, Validation Accuracy: 46.53%\n",
        "Epoch [4/10], Training Loss: 1.426, Validation Accuracy: 46.31%\n",
        "Epoch [5/10], Training Loss: 1.427, Validation Accuracy: 46.87%\n",
        "Epoch [6/10], Training Loss: 1.409, Validation Accuracy: 46.87%\n",
        "Epoch [7/10], Training Loss: 1.416, Validation Accuracy: 47.37%\n",
        "Epoch [8/10], Training Loss: 1.395, Validation Accuracy: 46.94%\n",
        "Epoch [9/10], Training Loss: 1.385, Validation Accuracy: 47.77%\n",
        "Epoch [10/10], Training Loss: 1.383, Validation Accuracy: 47.28%\n",
        "Epoch [1/10], Training Loss: 1.445, Validation Accuracy: 48.27%\n",
        "Epoch [2/10], Training Loss: 1.438, Validation Accuracy: 48.57%\n",
        "Epoch [3/10], Training Loss: 1.414, Validation Accuracy: 48.05%\n",
        "Epoch [4/10], Training Loss: 1.401, Validation Accuracy: 48.28%\n",
        "Epoch [5/10], Training Loss: 1.391, Validation Accuracy: 48.74%\n",
        "Epoch [6/10], Training Loss: 1.382, Validation Accuracy: 48.92%\n",
        "Epoch [7/10], Training Loss: 1.369, Validation Accuracy: 47.21%\n",
        "Epoch [8/10], Training Loss: 1.359, Validation Accuracy: 49.35%\n",
        "Epoch [9/10], Training Loss: 1.351, Validation Accuracy: 48.97%\n",
        "Epoch [10/10], Training Loss: 1.350, Validation Accuracy: 49.21%\n",
        "Epoch [1/10], Training Loss: 1.409, Validation Accuracy: 49.23%\n",
        "Epoch [2/10], Training Loss: 1.390, Validation Accuracy: 48.86%\n",
        "Epoch [3/10], Training Loss: 1.382, Validation Accuracy: 49.86%\n",
        "Epoch [4/10], Training Loss: 1.370, Validation Accuracy: 49.93%\n",
        "Epoch [5/10], Training Loss: 1.357, Validation Accuracy: 49.84%\n",
        "Epoch [6/10], Training Loss: 1.347, Validation Accuracy: 49.44%\n",
        "Epoch [7/10], Training Loss: 1.349, Validation Accuracy: 50.97%\n",
        "Epoch [8/10], Training Loss: 1.342, Validation Accuracy: 50.44%\n",
        "Epoch [9/10], Training Loss: 1.322, Validation Accuracy: 51.30%\n",
        "Epoch [10/10], Training Loss: 1.314, Validation Accuracy: 50.78%\n",
        "Epoch [1/10], Training Loss: 1.366, Validation Accuracy: 51.51%\n",
        "Epoch [2/10], Training Loss: 1.354, Validation Accuracy: 51.64%\n",
        "Epoch [3/10], Training Loss: 1.331, Validation Accuracy: 51.77%\n",
        "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 50.46%\n",
        "Epoch [5/10], Training Loss: 1.311, Validation Accuracy: 51.98%\n",
        "Epoch [6/10], Training Loss: 1.309, Validation Accuracy: 51.86%\n",
        "Epoch [7/10], Training Loss: 1.291, Validation Accuracy: 51.88%\n",
        "Epoch [8/10], Training Loss: 1.281, Validation Accuracy: 51.45%\n",
        "Epoch [9/10], Training Loss: 1.274, Validation Accuracy: 52.23%\n",
        "Epoch [10/10], Training Loss: 1.267, Validation Accuracy: 52.37%\n",
        "Epoch [1/10], Training Loss: 1.338, Validation Accuracy: 52.06%\n",
        "Epoch [2/10], Training Loss: 1.312, Validation Accuracy: 51.97%\n",
        "Epoch [3/10], Training Loss: 1.312, Validation Accuracy: 53.12%\n",
        "Epoch [4/10], Training Loss: 1.289, Validation Accuracy: 52.70%\n",
        "Epoch [5/10], Training Loss: 1.282, Validation Accuracy: 52.39%\n",
        "Epoch [6/10], Training Loss: 1.263, Validation Accuracy: 52.49%\n",
        "Epoch [7/10], Training Loss: 1.261, Validation Accuracy: 52.72%\n",
        "Epoch [8/10], Training Loss: 1.254, Validation Accuracy: 52.09%\n",
        "Epoch [9/10], Training Loss: 1.240, Validation Accuracy: 52.49%\n",
        "Epoch [10/10], Training Loss: 1.238, Validation Accuracy: 52.20%\n",
        "Epoch [1/10], Training Loss: 1.275, Validation Accuracy: 53.35%\n",
        "Epoch [2/10], Training Loss: 1.264, Validation Accuracy: 53.00%\n",
        "Epoch [3/10], Training Loss: 1.241, Validation Accuracy: 52.74%\n",
        "Epoch [4/10], Training Loss: 1.228, Validation Accuracy: 53.15%\n",
        "Epoch [5/10], Training Loss: 1.217, Validation Accuracy: 53.42%\n",
        "Epoch [6/10], Training Loss: 1.212, Validation Accuracy: 53.79%\n",
        "Epoch [7/10], Training Loss: 1.187, Validation Accuracy: 53.46%\n",
        "Epoch [8/10], Training Loss: 1.182, Validation Accuracy: 53.54%\n",
        "Epoch [9/10], Training Loss: 1.167, Validation Accuracy: 53.58%\n",
        "Epoch [10/10], Training Loss: 1.170, Validation Accuracy: 52.89%\n",
        "Epoch [1/10], Training Loss: 1.272, Validation Accuracy: 54.33%\n",
        "Epoch [2/10], Training Loss: 1.251, Validation Accuracy: 53.09%\n",
        "Epoch [3/10], Training Loss: 1.230, Validation Accuracy: 54.03%\n",
        "Epoch [4/10], Training Loss: 1.214, Validation Accuracy: 54.15%\n",
        "Epoch [5/10], Training Loss: 1.204, Validation Accuracy: 53.98%\n",
        "Epoch [6/10], Training Loss: 1.203, Validation Accuracy: 54.18%\n",
        "Epoch [7/10], Training Loss: 1.188, Validation Accuracy: 54.87%\n",
        "Epoch [8/10], Training Loss: 1.179, Validation Accuracy: 54.66%\n",
        "Epoch [9/10], Training Loss: 1.171, Validation Accuracy: 54.34%\n",
        "Epoch [10/10], Training Loss: 1.159, Validation Accuracy: 54.49%\n",
        "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 55.55%\n",
        "Epoch [2/10], Training Loss: 1.232, Validation Accuracy: 55.03%\n",
        "Epoch [3/10], Training Loss: 1.217, Validation Accuracy: 55.06%\n",
        "Epoch [4/10], Training Loss: 1.200, Validation Accuracy: 55.69%\n",
        "Epoch [5/10], Training Loss: 1.194, Validation Accuracy: 54.86%\n",
        "Epoch [6/10], Training Loss: 1.183, Validation Accuracy: 55.93%\n",
        "Epoch [7/10], Training Loss: 1.172, Validation Accuracy: 55.37%\n",
        "Epoch [8/10], Training Loss: 1.154, Validation Accuracy: 55.47%\n",
        "Epoch [9/10], Training Loss: 1.150, Validation Accuracy: 55.77%\n",
        "Epoch [10/10], Training Loss: 1.141, Validation Accuracy: 55.49%\n",
        "Epoch [1/10], Training Loss: 1.222, Validation Accuracy: 55.70%\n",
        "Epoch [2/10], Training Loss: 1.192, Validation Accuracy: 55.88%\n",
        "Epoch [3/10], Training Loss: 1.171, Validation Accuracy: 56.18%\n",
        "Epoch [4/10], Training Loss: 1.171, Validation Accuracy: 55.40%\n",
        "Epoch [5/10], Training Loss: 1.156, Validation Accuracy: 56.57%\n",
        "Epoch [6/10], Training Loss: 1.139, Validation Accuracy: 55.85%\n",
        "Epoch [7/10], Training Loss: 1.127, Validation Accuracy: 54.97%\n",
        "Epoch [8/10], Training Loss: 1.124, Validation Accuracy: 56.36%\n",
        "Epoch [9/10], Training Loss: 1.102, Validation Accuracy: 56.35%\n",
        "Epoch [10/10], Training Loss: 1.092, Validation Accuracy: 55.38%\n",
        "Epoch [1/10], Training Loss: 1.206, Validation Accuracy: 56.42%\n",
        "Epoch [2/10], Training Loss: 1.172, Validation Accuracy: 56.23%\n",
        "Epoch [3/10], Training Loss: 1.164, Validation Accuracy: 56.12%\n",
        "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 56.25%\n",
        "Epoch [5/10], Training Loss: 1.125, Validation Accuracy: 56.35%\n",
        "Epoch [6/10], Training Loss: 1.114, Validation Accuracy: 56.48%\n",
        "Epoch [7/10], Training Loss: 1.104, Validation Accuracy: 56.86%\n",
        "Epoch [8/10], Training Loss: 1.091, Validation Accuracy: 57.16%\n",
        "Epoch [9/10], Training Loss: 1.086, Validation Accuracy: 56.74%\n",
        "Epoch [10/10], Training Loss: 1.071, Validation Accuracy: 56.52%\n",
        "Epoch [1/10], Training Loss: 1.151, Validation Accuracy: 57.10%\n",
        "Epoch [2/10], Training Loss: 1.122, Validation Accuracy: 57.27%\n",
        "Epoch [3/10], Training Loss: 1.096, Validation Accuracy: 57.50%\n",
        "Epoch [4/10], Training Loss: 1.087, Validation Accuracy: 56.01%\n",
        "Epoch [5/10], Training Loss: 1.071, Validation Accuracy: 57.93%\n",
        "Epoch [6/10], Training Loss: 1.052, Validation Accuracy: 57.72%\n",
        "Epoch [7/10], Training Loss: 1.033, Validation Accuracy: 57.47%\n",
        "Epoch [8/10], Training Loss: 1.028, Validation Accuracy: 56.71%\n",
        "Epoch [9/10], Training Loss: 1.018, Validation Accuracy: 57.45%\n",
        "Epoch [10/10], Training Loss: 1.018, Validation Accuracy: 57.50%\n",
        "Epoch [1/10], Training Loss: 1.165, Validation Accuracy: 57.08%\n",
        "Epoch [2/10], Training Loss: 1.128, Validation Accuracy: 57.65%\n",
        "Epoch [3/10], Training Loss: 1.106, Validation Accuracy: 57.93%\n",
        "Epoch [4/10], Training Loss: 1.085, Validation Accuracy: 57.91%\n",
        "Epoch [5/10], Training Loss: 1.066, Validation Accuracy: 57.61%\n",
        "Epoch [6/10], Training Loss: 1.059, Validation Accuracy: 57.55%\n",
        "Epoch [7/10], Training Loss: 1.038, Validation Accuracy: 57.34%\n",
        "Epoch [8/10], Training Loss: 1.045, Validation Accuracy: 57.53%\n",
        "Epoch [9/10], Training Loss: 1.014, Validation Accuracy: 58.23%\n",
        "Epoch [10/10], Training Loss: 1.009, Validation Accuracy: 57.96%\n",
        "Epoch [1/10], Training Loss: 1.151, Validation Accuracy: 58.06%\n",
        "Epoch [2/10], Training Loss: 1.117, Validation Accuracy: 58.65%\n",
        "Epoch [3/10], Training Loss: 1.100, Validation Accuracy: 58.38%\n",
        "Epoch [4/10], Training Loss: 1.069, Validation Accuracy: 58.56%\n",
        "Epoch [5/10], Training Loss: 1.060, Validation Accuracy: 57.84%\n",
        "Epoch [6/10], Training Loss: 1.043, Validation Accuracy: 57.87%\n",
        "Epoch [7/10], Training Loss: 1.036, Validation Accuracy: 57.78%\n",
        "Epoch [8/10], Training Loss: 1.023, Validation Accuracy: 57.54%\n",
        "Epoch [9/10], Training Loss: 1.013, Validation Accuracy: 58.31%\n",
        "Epoch [10/10], Training Loss: 0.992, Validation Accuracy: 58.22%\n",
        "Epoch [1/10], Training Loss: 1.129, Validation Accuracy: 57.28%\n",
        "Epoch [2/10], Training Loss: 1.092, Validation Accuracy: 59.14%\n",
        "Epoch [3/10], Training Loss: 1.068, Validation Accuracy: 58.27%\n",
        "Epoch [4/10], Training Loss: 1.046, Validation Accuracy: 58.78%\n",
        "Epoch [5/10], Training Loss: 1.032, Validation Accuracy: 58.61%\n",
        "Epoch [6/10], Training Loss: 1.012, Validation Accuracy: 58.23%\n",
        "Epoch [7/10], Training Loss: 1.006, Validation Accuracy: 58.64%\n",
        "Epoch [8/10], Training Loss: 0.989, Validation Accuracy: 58.34%\n",
        "Epoch [9/10], Training Loss: 0.984, Validation Accuracy: 59.02%\n",
        "Epoch [10/10], Training Loss: 0.973, Validation Accuracy: 58.37%\n",
        "Epoch [1/10], Training Loss: 1.103, Validation Accuracy: 58.27%\n",
        "Epoch [2/10], Training Loss: 1.080, Validation Accuracy: 58.32%\n",
        "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 58.60%\n",
        "Epoch [4/10], Training Loss: 1.030, Validation Accuracy: 58.55%\n",
        "Epoch [5/10], Training Loss: 1.010, Validation Accuracy: 58.77%\n",
        "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 58.65%\n",
        "Epoch [7/10], Training Loss: 0.986, Validation Accuracy: 58.77%\n",
        "Epoch [8/10], Training Loss: 0.972, Validation Accuracy: 58.93%\n",
        "Epoch [9/10], Training Loss: 0.952, Validation Accuracy: 58.32%\n",
        "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 58.54%\n",
        "Epoch [1/10], Training Loss: 1.070, Validation Accuracy: 59.56%\n",
        "Epoch [2/10], Training Loss: 1.026, Validation Accuracy: 59.29%\n",
        "Epoch [3/10], Training Loss: 1.013, Validation Accuracy: 58.67%\n",
        "Epoch [4/10], Training Loss: 0.983, Validation Accuracy: 59.30%\n",
        "Epoch [5/10], Training Loss: 0.967, Validation Accuracy: 59.23%\n",
        "Epoch [6/10], Training Loss: 0.938, Validation Accuracy: 59.09%\n",
        "Epoch [7/10], Training Loss: 0.933, Validation Accuracy: 59.37%\n",
        "Epoch [8/10], Training Loss: 0.916, Validation Accuracy: 59.20%\n",
        "Epoch [9/10], Training Loss: 0.894, Validation Accuracy: 59.13%\n",
        "Epoch [10/10], Training Loss: 0.896, Validation Accuracy: 59.19%\n",
        "Epoch [1/10], Training Loss: 1.085, Validation Accuracy: 59.52%\n",
        "Epoch [2/10], Training Loss: 1.028, Validation Accuracy: 58.62%\n",
        "Epoch [3/10], Training Loss: 1.005, Validation Accuracy: 58.92%\n",
        "Epoch [4/10], Training Loss: 0.987, Validation Accuracy: 59.80%\n",
        "Epoch [5/10], Training Loss: 0.964, Validation Accuracy: 58.54%\n",
        "Epoch [6/10], Training Loss: 0.957, Validation Accuracy: 59.97%\n",
        "Epoch [7/10], Training Loss: 0.937, Validation Accuracy: 59.30%\n",
        "Epoch [8/10], Training Loss: 0.929, Validation Accuracy: 59.29%\n",
        "Epoch [9/10], Training Loss: 0.909, Validation Accuracy: 58.65%\n",
        "Epoch [10/10], Training Loss: 0.893, Validation Accuracy: 58.57%\n",
        "Epoch [1/10], Training Loss: 1.076, Validation Accuracy: 58.19%\n",
        "Epoch [2/10], Training Loss: 1.021, Validation Accuracy: 59.74%\n",
        "Epoch [3/10], Training Loss: 1.001, Validation Accuracy: 59.76%\n",
        "Epoch [4/10], Training Loss: 0.977, Validation Accuracy: 60.15%\n",
        "Epoch [5/10], Training Loss: 0.968, Validation Accuracy: 59.78%\n",
        "Epoch [6/10], Training Loss: 0.944, Validation Accuracy: 58.73%\n",
        "Epoch [7/10], Training Loss: 0.928, Validation Accuracy: 60.43%\n",
        "Epoch [8/10], Training Loss: 0.912, Validation Accuracy: 59.84%\n",
        "Epoch [9/10], Training Loss: 0.898, Validation Accuracy: 59.46%\n",
        "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 59.61%\n",
        "Epoch [1/10], Training Loss: 1.048, Validation Accuracy: 59.96%\n",
        "Epoch [2/10], Training Loss: 1.002, Validation Accuracy: 60.33%\n",
        "Epoch [3/10], Training Loss: 0.972, Validation Accuracy: 59.91%\n",
        "Epoch [4/10], Training Loss: 0.952, Validation Accuracy: 60.15%\n",
        "Epoch [5/10], Training Loss: 0.938, Validation Accuracy: 59.99%\n",
        "Epoch [6/10], Training Loss: 0.913, Validation Accuracy: 59.48%\n",
        "Epoch [7/10], Training Loss: 0.898, Validation Accuracy: 59.83%\n",
        "Epoch [8/10], Training Loss: 0.888, Validation Accuracy: 60.02%\n",
        "Epoch [9/10], Training Loss: 0.869, Validation Accuracy: 59.95%\n",
        "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 59.76%\n",
        "Epoch [1/10], Training Loss: 1.032, Validation Accuracy: 59.61%\n",
        "Epoch [2/10], Training Loss: 0.979, Validation Accuracy: 59.72%\n",
        "Epoch [3/10], Training Loss: 0.964, Validation Accuracy: 59.41%\n",
        "Epoch [4/10], Training Loss: 0.930, Validation Accuracy: 60.16%\n",
        "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 60.08%\n",
        "Epoch [6/10], Training Loss: 0.893, Validation Accuracy: 60.08%\n",
        "Epoch [7/10], Training Loss: 0.878, Validation Accuracy: 59.35%\n",
        "Epoch [8/10], Training Loss: 0.868, Validation Accuracy: 59.67%\n",
        "Epoch [9/10], Training Loss: 0.853, Validation Accuracy: 59.74%\n",
        "Epoch [10/10], Training Loss: 0.839, Validation Accuracy: 60.45%\n",
        "Epoch [1/10], Training Loss: 1.004, Validation Accuracy: 60.10%\n",
        "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 60.73%\n",
        "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 60.89%\n",
        "Epoch [4/10], Training Loss: 0.900, Validation Accuracy: 60.71%\n",
        "Epoch [5/10], Training Loss: 0.861, Validation Accuracy: 60.62%\n",
        "Epoch [6/10], Training Loss: 0.853, Validation Accuracy: 61.08%\n",
        "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.20%\n",
        "Epoch [8/10], Training Loss: 0.814, Validation Accuracy: 60.47%\n",
        "Epoch [9/10], Training Loss: 0.797, Validation Accuracy: 60.93%\n",
        "Epoch [10/10], Training Loss: 0.791, Validation Accuracy: 60.74%\n",
        "Epoch [1/10], Training Loss: 1.014, Validation Accuracy: 60.54%\n",
        "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 60.95%\n",
        "Epoch [3/10], Training Loss: 0.928, Validation Accuracy: 60.64%\n",
        "Epoch [4/10], Training Loss: 0.885, Validation Accuracy: 60.95%\n",
        "Epoch [5/10], Training Loss: 0.868, Validation Accuracy: 60.51%\n",
        "Epoch [6/10], Training Loss: 0.850, Validation Accuracy: 60.26%\n",
        "Epoch [7/10], Training Loss: 0.845, Validation Accuracy: 60.55%\n",
        "Epoch [8/10], Training Loss: 0.818, Validation Accuracy: 60.30%\n",
        "Epoch [9/10], Training Loss: 0.806, Validation Accuracy: 59.54%\n",
        "Epoch [10/10], Training Loss: 0.794, Validation Accuracy: 60.17%\n",
        "Epoch [1/10], Training Loss: 1.008, Validation Accuracy: 59.57%\n",
        "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 60.94%\n",
        "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 60.97%\n",
        "Epoch [4/10], Training Loss: 0.884, Validation Accuracy: 61.13%\n",
        "Epoch [5/10], Training Loss: 0.871, Validation Accuracy: 61.35%\n",
        "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 61.07%\n",
        "Epoch [7/10], Training Loss: 0.828, Validation Accuracy: 61.42%\n",
        "Epoch [8/10], Training Loss: 0.811, Validation Accuracy: 61.73%\n",
        "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 61.10%\n",
        "Epoch [10/10], Training Loss: 0.777, Validation Accuracy: 60.39%\n",
        "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.46%\n",
        "Epoch [2/10], Training Loss: 0.927, Validation Accuracy: 60.90%\n",
        "Epoch [3/10], Training Loss: 0.893, Validation Accuracy: 60.70%\n",
        "Epoch [4/10], Training Loss: 0.870, Validation Accuracy: 60.59%\n",
        "Epoch [5/10], Training Loss: 0.847, Validation Accuracy: 60.78%\n",
        "Epoch [6/10], Training Loss: 0.828, Validation Accuracy: 60.03%\n",
        "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 60.35%\n",
        "Epoch [8/10], Training Loss: 0.793, Validation Accuracy: 60.34%\n",
        "Epoch [9/10], Training Loss: 0.769, Validation Accuracy: 60.53%\n",
        "Epoch [10/10], Training Loss: 0.756, Validation Accuracy: 60.13%\n",
        "Epoch [1/10], Training Loss: 0.979, Validation Accuracy: 60.64%\n",
        "Epoch [2/10], Training Loss: 0.917, Validation Accuracy: 60.00%\n",
        "Epoch [3/10], Training Loss: 0.876, Validation Accuracy: 61.40%\n",
        "Epoch [4/10], Training Loss: 0.849, Validation Accuracy: 60.23%\n",
        "Epoch [5/10], Training Loss: 0.836, Validation Accuracy: 61.03%\n",
        "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 60.53%\n",
        "Epoch [7/10], Training Loss: 0.782, Validation Accuracy: 61.30%\n",
        "Epoch [8/10], Training Loss: 0.770, Validation Accuracy: 60.76%\n",
        "Epoch [9/10], Training Loss: 0.749, Validation Accuracy: 60.95%\n",
        "Epoch [10/10], Training Loss: 0.727, Validation Accuracy: 61.11%\n",
        "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 61.20%\n",
        "Epoch [2/10], Training Loss: 0.874, Validation Accuracy: 60.76%\n",
        "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 61.18%\n",
        "Epoch [4/10], Training Loss: 0.801, Validation Accuracy: 61.49%\n",
        "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 61.84%\n",
        "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 61.31%\n",
        "Epoch [7/10], Training Loss: 0.730, Validation Accuracy: 61.41%\n",
        "Epoch [8/10], Training Loss: 0.730, Validation Accuracy: 61.11%\n",
        "Epoch [9/10], Training Loss: 0.704, Validation Accuracy: 61.39%\n",
        "Epoch [10/10], Training Loss: 0.690, Validation Accuracy: 61.23%\n",
        "Epoch [1/10], Training Loss: 0.968, Validation Accuracy: 61.31%\n",
        "Epoch [2/10], Training Loss: 0.894, Validation Accuracy: 61.39%\n",
        "Epoch [3/10], Training Loss: 0.852, Validation Accuracy: 60.85%\n",
        "Epoch [4/10], Training Loss: 0.820, Validation Accuracy: 61.04%\n",
        "Epoch [5/10], Training Loss: 0.796, Validation Accuracy: 61.41%\n",
        "Epoch [6/10], Training Loss: 0.766, Validation Accuracy: 61.15%\n",
        "Epoch [7/10], Training Loss: 0.749, Validation Accuracy: 61.09%\n",
        "Epoch [8/10], Training Loss: 0.732, Validation Accuracy: 60.16%\n",
        "Epoch [9/10], Training Loss: 0.710, Validation Accuracy: 61.23%\n",
        "Epoch [10/10], Training Loss: 0.693, Validation Accuracy: 60.86%\n",
        "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 59.94%\n",
        "Epoch [2/10], Training Loss: 0.889, Validation Accuracy: 61.78%\n",
        "Epoch [3/10], Training Loss: 0.843, Validation Accuracy: 60.82%\n",
        "Epoch [4/10], Training Loss: 0.826, Validation Accuracy: 61.40%\n",
        "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 62.43%\n",
        "Epoch [6/10], Training Loss: 0.760, Validation Accuracy: 62.30%\n",
        "Epoch [7/10], Training Loss: 0.742, Validation Accuracy: 61.08%\n",
        "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 61.46%\n",
        "Epoch [9/10], Training Loss: 0.708, Validation Accuracy: 60.98%\n",
        "Epoch [10/10], Training Loss: 0.689, Validation Accuracy: 61.03%\n",
        "Epoch [1/10], Training Loss: 0.932, Validation Accuracy: 61.41%\n",
        "Epoch [2/10], Training Loss: 0.856, Validation Accuracy: 60.87%\n",
        "Epoch [3/10], Training Loss: 0.816, Validation Accuracy: 61.03%\n",
        "Epoch [4/10], Training Loss: 0.784, Validation Accuracy: 61.44%\n",
        "Epoch [5/10], Training Loss: 0.766, Validation Accuracy: 61.36%\n",
        "Epoch [6/10], Training Loss: 0.732, Validation Accuracy: 61.28%\n",
        "Epoch [7/10], Training Loss: 0.708, Validation Accuracy: 60.37%\n",
        "Epoch [8/10], Training Loss: 0.687, Validation Accuracy: 60.74%\n",
        "Epoch [9/10], Training Loss: 0.674, Validation Accuracy: 60.86%\n",
        "Epoch [10/10], Training Loss: 0.652, Validation Accuracy: 61.28%\n",
        "Epoch [1/10], Training Loss: 0.936, Validation Accuracy: 60.56%\n",
        "Epoch [2/10], Training Loss: 0.850, Validation Accuracy: 61.32%\n",
        "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 61.47%\n",
        "Epoch [4/10], Training Loss: 0.772, Validation Accuracy: 61.45%\n",
        "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 61.52%\n",
        "Epoch [6/10], Training Loss: 0.710, Validation Accuracy: 61.68%\n",
        "Epoch [7/10], Training Loss: 0.686, Validation Accuracy: 59.85%\n",
        "Epoch [8/10], Training Loss: 0.683, Validation Accuracy: 61.43%\n",
        "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 61.04%\n",
        "Epoch [10/10], Training Loss: 0.637, Validation Accuracy: 61.42%\n",
        "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 60.67%\n",
        "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 61.44%\n",
        "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 61.69%\n",
        "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 61.99%\n",
        "Epoch [5/10], Training Loss: 0.693, Validation Accuracy: 61.38%\n",
        "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 61.40%\n",
        "Epoch [7/10], Training Loss: 0.644, Validation Accuracy: 61.46%\n",
        "Epoch [8/10], Training Loss: 0.630, Validation Accuracy: 61.20%\n",
        "Epoch [9/10], Training Loss: 0.610, Validation Accuracy: 61.29%\n",
        "Epoch [10/10], Training Loss: 0.588, Validation Accuracy: 61.55%\n",
        "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 60.70%\n",
        "Epoch [2/10], Training Loss: 0.827, Validation Accuracy: 60.99%\n",
        "Epoch [3/10], Training Loss: 0.778, Validation Accuracy: 61.21%\n",
        "Epoch [4/10], Training Loss: 0.733, Validation Accuracy: 61.79%\n",
        "Epoch [5/10], Training Loss: 0.697, Validation Accuracy: 61.07%\n",
        "Epoch [6/10], Training Loss: 0.685, Validation Accuracy: 61.23%\n",
        "Epoch [7/10], Training Loss: 0.655, Validation Accuracy: 60.96%\n",
        "Epoch [8/10], Training Loss: 0.636, Validation Accuracy: 60.93%\n",
        "Epoch [9/10], Training Loss: 0.618, Validation Accuracy: 60.89%\n",
        "Epoch [10/10], Training Loss: 0.593, Validation Accuracy: 60.59%\n",
        "Epoch [1/10], Training Loss: 0.921, Validation Accuracy: 61.38%\n",
        "Epoch [2/10], Training Loss: 0.833, Validation Accuracy: 61.53%\n",
        "Epoch [3/10], Training Loss: 0.769, Validation Accuracy: 62.04%\n",
        "Epoch [4/10], Training Loss: 0.738, Validation Accuracy: 61.20%\n",
        "Epoch [5/10], Training Loss: 0.707, Validation Accuracy: 60.26%\n",
        "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 61.20%\n",
        "Epoch [7/10], Training Loss: 0.656, Validation Accuracy: 61.72%\n",
        "Epoch [8/10], Training Loss: 0.629, Validation Accuracy: 60.75%\n",
        "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 61.19%\n",
        "Epoch [10/10], Training Loss: 0.597, Validation Accuracy: 61.34%\n",
        "Epoch [1/10], Training Loss: 0.895, Validation Accuracy: 61.63%\n",
        "Epoch [2/10], Training Loss: 0.794, Validation Accuracy: 61.48%\n",
        "Epoch [3/10], Training Loss: 0.738, Validation Accuracy: 60.74%\n",
        "Epoch [4/10], Training Loss: 0.703, Validation Accuracy: 61.27%\n",
        "Epoch [5/10], Training Loss: 0.668, Validation Accuracy: 61.29%\n",
        "Epoch [6/10], Training Loss: 0.645, Validation Accuracy: 60.45%\n",
        "Epoch [7/10], Training Loss: 0.622, Validation Accuracy: 60.83%\n",
        "Epoch [8/10], Training Loss: 0.598, Validation Accuracy: 61.05%\n",
        "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 60.90%\n",
        "Epoch [10/10], Training Loss: 0.568, Validation Accuracy: 60.99%\n",
        "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 61.18%\n",
        "Epoch [2/10], Training Loss: 0.784, Validation Accuracy: 61.46%\n",
        "Epoch [3/10], Training Loss: 0.726, Validation Accuracy: 61.18%\n",
        "Epoch [4/10], Training Loss: 0.685, Validation Accuracy: 61.54%\n",
        "Epoch [5/10], Training Loss: 0.648, Validation Accuracy: 61.52%\n",
        "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.13%\n",
        "Epoch [7/10], Training Loss: 0.599, Validation Accuracy: 61.35%\n",
        "Epoch [8/10], Training Loss: 0.583, Validation Accuracy: 61.31%\n",
        "Epoch [9/10], Training Loss: 0.559, Validation Accuracy: 61.37%\n",
        "Epoch [10/10], Training Loss: 0.543, Validation Accuracy: 60.80%\n",
        "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 60.62%\n",
        "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 61.31%\n",
        "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 61.25%\n",
        "Epoch [4/10], Training Loss: 0.644, Validation Accuracy: 61.11%\n",
        "Epoch [5/10], Training Loss: 0.616, Validation Accuracy: 61.44%\n",
        "Epoch [6/10], Training Loss: 0.587, Validation Accuracy: 61.97%\n",
        "Epoch [7/10], Training Loss: 0.554, Validation Accuracy: 61.81%\n",
        "Epoch [8/10], Training Loss: 0.540, Validation Accuracy: 61.25%\n",
        "Epoch [9/10], Training Loss: 0.520, Validation Accuracy: 61.53%\n",
        "Epoch [10/10], Training Loss: 0.512, Validation Accuracy: 61.52%\n",
        "Epoch [1/10], Training Loss: 0.883, Validation Accuracy: 60.91%\n",
        "Epoch [2/10], Training Loss: 0.765, Validation Accuracy: 60.37%\n",
        "Epoch [3/10], Training Loss: 0.721, Validation Accuracy: 60.91%\n",
        "Epoch [4/10], Training Loss: 0.667, Validation Accuracy: 61.35%\n",
        "Epoch [5/10], Training Loss: 0.626, Validation Accuracy: 60.49%\n",
        "Epoch [6/10], Training Loss: 0.607, Validation Accuracy: 61.29%\n",
        "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.34%\n",
        "Epoch [8/10], Training Loss: 0.552, Validation Accuracy: 60.05%\n",
        "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 60.84%\n",
        "Epoch [10/10], Training Loss: 0.522, Validation Accuracy: 60.98%\n",
        "Epoch [1/10], Training Loss: 0.903, Validation Accuracy: 59.76%\n",
        "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 60.85%\n",
        "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 61.15%\n",
        "Epoch [4/10], Training Loss: 0.656, Validation Accuracy: 61.13%\n",
        "Epoch [5/10], Training Loss: 0.625, Validation Accuracy: 61.00%\n",
        "Epoch [6/10], Training Loss: 0.599, Validation Accuracy: 61.28%\n",
        "Epoch [7/10], Training Loss: 0.580, Validation Accuracy: 61.16%\n",
        "Epoch [8/10], Training Loss: 0.560, Validation Accuracy: 61.03%\n",
        "Epoch [9/10], Training Loss: 0.525, Validation Accuracy: 60.70%\n",
        "Epoch [10/10], Training Loss: 0.503, Validation Accuracy: 61.17%\n",
        "Epoch [1/10], Training Loss: 0.853, Validation Accuracy: 60.83%\n",
        "Epoch [2/10], Training Loss: 0.737, Validation Accuracy: 61.56%\n",
        "Epoch [3/10], Training Loss: 0.672, Validation Accuracy: 61.25%\n",
        "Epoch [4/10], Training Loss: 0.630, Validation Accuracy: 60.95%\n",
        "Epoch [5/10], Training Loss: 0.588, Validation Accuracy: 60.57%\n",
        "Epoch [6/10], Training Loss: 0.565, Validation Accuracy: 61.26%\n",
        "Epoch [7/10], Training Loss: 0.540, Validation Accuracy: 60.21%\n",
        "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 60.14%\n",
        "Epoch [9/10], Training Loss: 0.493, Validation Accuracy: 59.85%\n",
        "Epoch [10/10], Training Loss: 0.466, Validation Accuracy: 59.98%\n",
        "Epoch [1/10], Training Loss: 0.853, Validation Accuracy: 60.70%\n",
        "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 60.36%\n",
        "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.07%\n",
        "Epoch [4/10], Training Loss: 0.608, Validation Accuracy: 61.37%\n",
        "Epoch [5/10], Training Loss: 0.567, Validation Accuracy: 61.24%\n",
        "Epoch [6/10], Training Loss: 0.537, Validation Accuracy: 61.21%\n",
        "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 61.08%\n",
        "Epoch [8/10], Training Loss: 0.488, Validation Accuracy: 61.47%\n",
        "Epoch [9/10], Training Loss: 0.469, Validation Accuracy: 60.38%\n",
        "Epoch [10/10], Training Loss: 0.460, Validation Accuracy: 60.49%\n",
        "Epoch [1/10], Training Loss: 0.814, Validation Accuracy: 60.29%\n",
        "Epoch [2/10], Training Loss: 0.687, Validation Accuracy: 61.36%\n",
        "Epoch [3/10], Training Loss: 0.610, Validation Accuracy: 60.48%\n",
        "Epoch [4/10], Training Loss: 0.590, Validation Accuracy: 62.01%\n",
        "Epoch [5/10], Training Loss: 0.541, Validation Accuracy: 61.52%\n",
        "Epoch [6/10], Training Loss: 0.506, Validation Accuracy: 61.05%\n",
        "Epoch [7/10], Training Loss: 0.472, Validation Accuracy: 61.31%\n",
        "Epoch [8/10], Training Loss: 0.462, Validation Accuracy: 61.05%\n",
        "Epoch [9/10], Training Loss: 0.433, Validation Accuracy: 60.84%\n",
        "Epoch [10/10], Training Loss: 0.412, Validation Accuracy: 60.82%\n",
        "Epoch [1/10], Training Loss: 0.844, Validation Accuracy: 59.78%\n",
        "Epoch [2/10], Training Loss: 0.712, Validation Accuracy: 60.97%\n",
        "Epoch [3/10], Training Loss: 0.642, Validation Accuracy: 61.14%\n",
        "Epoch [4/10], Training Loss: 0.588, Validation Accuracy: 60.75%\n",
        "Epoch [5/10], Training Loss: 0.552, Validation Accuracy: 61.14%\n",
        "Epoch [6/10], Training Loss: 0.530, Validation Accuracy: 61.50%\n",
        "Epoch [7/10], Training Loss: 0.497, Validation Accuracy: 60.76%\n",
        "Epoch [8/10], Training Loss: 0.484, Validation Accuracy: 60.88%\n",
        "Epoch [9/10], Training Loss: 0.445, Validation Accuracy: 60.46%\n",
        "Epoch [10/10], Training Loss: 0.434, Validation Accuracy: 60.17%\n",
        "Epoch [1/10], Training Loss: 0.855, Validation Accuracy: 60.90%\n",
        "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 61.00%\n",
        "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 61.05%\n",
        "Epoch [4/10], Training Loss: 0.578, Validation Accuracy: 60.20%\n",
        "Epoch [5/10], Training Loss: 0.537, Validation Accuracy: 61.16%\n",
        "Epoch [6/10], Training Loss: 0.505, Validation Accuracy: 61.05%\n",
        "Epoch [7/10], Training Loss: 0.488, Validation Accuracy: 61.03%\n",
        "Epoch [8/10], Training Loss: 0.460, Validation Accuracy: 60.58%\n",
        "Epoch [9/10], Training Loss: 0.447, Validation Accuracy: 60.49%\n",
        "Epoch [10/10], Training Loss: 0.416, Validation Accuracy: 60.63%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9a83fb-f46d-48b9-a89a-62ff5ab36269",
        "id": "wPdKra2qnYGr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [10.34, 10.3, 10.49, 11.05, 11.75, 12.24, 12.45, 15.12, 17.4, 18.49, 18.26, 18.88, 20.73, 22.43, 23.29, 26.29, 27.01, 27.2, 27.19, 27.5, 26.39, 28.42, 28.6, 28.81, 29.77, 29.89, 30.61, 31.12, 31.63, 31.96, 33.55, 32.8, 33.4, 34.79, 34.94, 35.74, 35.06, 36.08, 36.7, 35.39, 37.45, 37.89, 38.94, 39.32, 39.46, 39.68, 39.67, 40.03, 40.11, 41.04, 41.27, 41.38, 42.16, 42.49, 42.07, 43.36, 43.2, 43.27, 44.33, 43.43, 44.09, 44.37, 44.4, 43.98, 45.08, 45.37, 45.18, 45.15, 45.85, 46.03, 46.43, 45.51, 46.53, 46.31, 46.87, 46.87, 47.37, 46.94, 47.77, 47.28, 48.27, 48.57, 48.05, 48.28, 48.74, 48.92, 47.21, 49.35, 48.97, 49.21, 49.23, 48.86, 49.86, 49.93, 49.84, 49.44, 50.97, 50.44, 51.3, 50.78, 51.51, 51.64, 51.77, 50.46, 51.98, 51.86, 51.88, 51.45, 52.23, 52.37, 52.06, 51.97, 53.12, 52.7, 52.39, 52.49, 52.72, 52.09, 52.49, 52.2, 53.35, 53.0, 52.74, 53.15, 53.42, 53.79, 53.46, 53.54, 53.58, 52.89, 54.33, 53.09, 54.03, 54.15, 53.98, 54.18, 54.87, 54.66, 54.34, 54.49, 55.55, 55.03, 55.06, 55.69, 54.86, 55.93, 55.37, 55.47, 55.77, 55.49, 55.7, 55.88, 56.18, 55.4, 56.57, 55.85, 54.97, 56.36, 56.35, 55.38, 56.42, 56.23, 56.12, 56.25, 56.35, 56.48, 56.86, 57.16, 56.74, 56.52, 57.1, 57.27, 57.5, 56.01, 57.93, 57.72, 57.47, 56.71, 57.45, 57.5, 57.08, 57.65, 57.93, 57.91, 57.61, 57.55, 57.34, 57.53, 58.23, 57.96, 58.06, 58.65, 58.38, 58.56, 57.84, 57.87, 57.78, 57.54, 58.31, 58.22, 57.28, 59.14, 58.27, 58.78, 58.61, 58.23, 58.64, 58.34, 59.02, 58.37, 58.27, 58.32, 58.6, 58.55, 58.77, 58.65, 58.77, 58.93, 58.32, 58.54, 59.56, 59.29, 58.67, 59.3, 59.23, 59.09, 59.37, 59.2, 59.13, 59.19, 59.52, 58.62, 58.92, 59.8, 58.54, 59.97, 59.3, 59.29, 58.65, 58.57, 58.19, 59.74, 59.76, 60.15, 59.78, 58.73, 60.43, 59.84, 59.46, 59.61, 59.96, 60.33, 59.91, 60.15, 59.99, 59.48, 59.83, 60.02, 59.95, 59.76, 59.61, 59.72, 59.41, 60.16, 60.08, 60.08, 59.35, 59.67, 59.74, 60.45, 60.1, 60.73, 60.89, 60.71, 60.62, 61.08, 60.2, 60.47, 60.93, 60.74, 60.54, 60.95, 60.64, 60.95, 60.51, 60.26, 60.55, 60.3, 59.54, 60.17, 59.57, 60.94, 60.97, 61.13, 61.35, 61.07, 61.42, 61.73, 61.1, 60.39, 60.46, 60.9, 60.7, 60.59, 60.78, 60.03, 60.35, 60.34, 60.53, 60.13, 60.64, 60.0, 61.4, 60.23, 61.03, 60.53, 61.3, 60.76, 60.95, 61.11, 61.2, 60.76, 61.18, 61.49, 61.84, 61.31, 61.41, 61.11, 61.39, 61.23, 61.31, 61.39, 60.85, 61.04, 61.41, 61.15, 61.09, 60.16, 61.23, 60.86, 59.94, 61.78, 60.82, 61.4, 62.43, 62.3, 61.08, 61.46, 60.98, 61.03, 61.41, 60.87, 61.03, 61.44, 61.36, 61.28, 60.37, 60.74, 60.86, 61.28, 60.56, 61.32, 61.47, 61.45, 61.52, 61.68, 59.85, 61.43, 61.04, 61.42, 60.67, 61.44, 61.69, 61.99, 61.38, 61.4, 61.46, 61.2, 61.29, 61.55, 60.7, 60.99, 61.21, 61.79, 61.07, 61.23, 60.96, 60.93, 60.89, 60.59, 61.38, 61.53, 62.04, 61.2, 60.26, 61.2, 61.72, 60.75, 61.19, 61.34, 61.63, 61.48, 60.74, 61.27, 61.29, 60.45, 60.83, 61.05, 60.9, 60.99, 61.18, 61.46, 61.18, 61.54, 61.52, 61.13, 61.35, 61.31, 61.37, 60.8, 60.62, 61.31, 61.25, 61.11, 61.44, 61.97, 61.81, 61.25, 61.53, 61.52, 60.91, 60.37, 60.91, 61.35, 60.49, 61.29, 61.34, 60.05, 60.84, 60.98, 59.76, 60.85, 61.15, 61.13, 61.0, 61.28, 61.16, 61.03, 60.7, 61.17, 60.83, 61.56, 61.25, 60.95, 60.57, 61.26, 60.21, 60.14, 59.85, 59.98, 60.7, 60.36, 61.07, 61.37, 61.24, 61.21, 61.08, 61.47, 60.38, 60.49, 60.29, 61.36, 60.48, 62.01, 61.52, 61.05, 61.31, 61.05, 60.84, 60.82, 59.78, 60.97, 61.14, 60.75, 61.14, 61.5, 60.76, 60.88, 60.46, 60.17, 60.9, 61.0, 61.05, 60.2, 61.16, 61.05, 61.03, 60.58, 60.49, 60.63]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbCE0qEq2kTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"normal\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict, distribution_info_normal: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "    mean_normal = distribution_info_normal[\"mean\"]\n",
        "    std_normal = distribution_info_normal[\"std\"]\n",
        "\n",
        "    augmented_data_normal = torch.randn(64, vae.z_dim) * std_normal + mean_normal\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_uniform + augmented_data_normal ) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"], other_distribution_info[\"normal\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"normal\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98830847-d2c0-47d2-a5df-8c55a63b3c4b",
        "id": "6WfExeQf2k79"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:07<00:00, 21.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [5915 5870 6025 6095 6076 6021 5977 5943 6058 6020]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.99%\n",
            "Epoch [2/10], Training Loss: 2.302, Validation Accuracy: 9.95%\n",
            "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 10.47%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 11.61%\n",
            "Epoch [5/10], Training Loss: 2.297, Validation Accuracy: 13.41%\n",
            "Epoch [6/10], Training Loss: 2.294, Validation Accuracy: 14.80%\n",
            "Epoch [7/10], Training Loss: 2.291, Validation Accuracy: 15.18%\n",
            "Epoch [8/10], Training Loss: 2.286, Validation Accuracy: 15.46%\n",
            "Epoch [9/10], Training Loss: 2.280, Validation Accuracy: 15.18%\n",
            "Epoch [10/10], Training Loss: 2.269, Validation Accuracy: 16.26%\n",
            "Epoch [1/10], Training Loss: 2.259, Validation Accuracy: 16.59%\n",
            "Epoch [2/10], Training Loss: 2.238, Validation Accuracy: 19.19%\n",
            "Epoch [3/10], Training Loss: 2.208, Validation Accuracy: 19.83%\n",
            "Epoch [4/10], Training Loss: 2.176, Validation Accuracy: 22.35%\n",
            "Epoch [5/10], Training Loss: 2.152, Validation Accuracy: 23.46%\n",
            "Epoch [6/10], Training Loss: 2.133, Validation Accuracy: 23.85%\n",
            "Epoch [7/10], Training Loss: 2.118, Validation Accuracy: 25.58%\n",
            "Epoch [8/10], Training Loss: 2.104, Validation Accuracy: 25.46%\n",
            "Epoch [9/10], Training Loss: 2.091, Validation Accuracy: 25.80%\n",
            "Epoch [10/10], Training Loss: 2.078, Validation Accuracy: 26.65%\n",
            "Epoch [1/10], Training Loss: 2.073, Validation Accuracy: 26.87%\n",
            "Epoch [2/10], Training Loss: 2.061, Validation Accuracy: 27.09%\n",
            "Epoch [3/10], Training Loss: 2.044, Validation Accuracy: 27.88%\n",
            "Epoch [4/10], Training Loss: 2.032, Validation Accuracy: 27.58%\n",
            "Epoch [5/10], Training Loss: 2.020, Validation Accuracy: 28.20%\n",
            "Epoch [6/10], Training Loss: 2.002, Validation Accuracy: 28.92%\n",
            "Epoch [7/10], Training Loss: 1.989, Validation Accuracy: 30.70%\n",
            "Epoch [8/10], Training Loss: 1.973, Validation Accuracy: 30.59%\n",
            "Epoch [9/10], Training Loss: 1.955, Validation Accuracy: 30.97%\n",
            "Epoch [10/10], Training Loss: 1.932, Validation Accuracy: 31.97%\n",
            "Epoch [1/10], Training Loss: 1.916, Validation Accuracy: 32.05%\n",
            "Epoch [2/10], Training Loss: 1.891, Validation Accuracy: 32.89%\n",
            "Epoch [3/10], Training Loss: 1.874, Validation Accuracy: 32.85%\n",
            "Epoch [4/10], Training Loss: 1.859, Validation Accuracy: 33.79%\n",
            "Epoch [5/10], Training Loss: 1.843, Validation Accuracy: 34.95%\n",
            "Epoch [6/10], Training Loss: 1.823, Validation Accuracy: 35.38%\n",
            "Epoch [7/10], Training Loss: 1.810, Validation Accuracy: 35.76%\n",
            "Epoch [8/10], Training Loss: 1.788, Validation Accuracy: 35.38%\n",
            "Epoch [9/10], Training Loss: 1.788, Validation Accuracy: 36.52%\n",
            "Epoch [10/10], Training Loss: 1.767, Validation Accuracy: 37.27%\n",
            "Epoch [1/10], Training Loss: 1.765, Validation Accuracy: 37.45%\n",
            "Epoch [2/10], Training Loss: 1.747, Validation Accuracy: 37.42%\n",
            "Epoch [3/10], Training Loss: 1.725, Validation Accuracy: 37.80%\n",
            "Epoch [4/10], Training Loss: 1.706, Validation Accuracy: 38.37%\n",
            "Epoch [5/10], Training Loss: 1.690, Validation Accuracy: 39.34%\n",
            "Epoch [6/10], Training Loss: 1.670, Validation Accuracy: 39.25%\n",
            "Epoch [7/10], Training Loss: 1.656, Validation Accuracy: 39.18%\n",
            "Epoch [8/10], Training Loss: 1.647, Validation Accuracy: 41.19%\n",
            "Epoch [9/10], Training Loss: 1.634, Validation Accuracy: 39.95%\n",
            "Epoch [10/10], Training Loss: 1.615, Validation Accuracy: 40.68%\n",
            "Epoch [1/10], Training Loss: 1.660, Validation Accuracy: 41.41%\n",
            "Epoch [2/10], Training Loss: 1.636, Validation Accuracy: 41.21%\n",
            "Epoch [3/10], Training Loss: 1.624, Validation Accuracy: 42.35%\n",
            "Epoch [4/10], Training Loss: 1.612, Validation Accuracy: 41.62%\n",
            "Epoch [5/10], Training Loss: 1.599, Validation Accuracy: 42.22%\n",
            "Epoch [6/10], Training Loss: 1.588, Validation Accuracy: 43.22%\n",
            "Epoch [7/10], Training Loss: 1.583, Validation Accuracy: 42.93%\n",
            "Epoch [8/10], Training Loss: 1.562, Validation Accuracy: 43.17%\n",
            "Epoch [9/10], Training Loss: 1.557, Validation Accuracy: 43.20%\n",
            "Epoch [10/10], Training Loss: 1.547, Validation Accuracy: 43.70%\n",
            "Epoch [1/10], Training Loss: 1.575, Validation Accuracy: 44.11%\n",
            "Epoch [2/10], Training Loss: 1.560, Validation Accuracy: 44.15%\n",
            "Epoch [3/10], Training Loss: 1.544, Validation Accuracy: 44.48%\n",
            "Epoch [4/10], Training Loss: 1.533, Validation Accuracy: 44.58%\n",
            "Epoch [5/10], Training Loss: 1.524, Validation Accuracy: 45.01%\n",
            "Epoch [6/10], Training Loss: 1.516, Validation Accuracy: 45.33%\n",
            "Epoch [7/10], Training Loss: 1.510, Validation Accuracy: 44.56%\n",
            "Epoch [8/10], Training Loss: 1.491, Validation Accuracy: 43.90%\n",
            "Epoch [9/10], Training Loss: 1.492, Validation Accuracy: 45.25%\n",
            "Epoch [10/10], Training Loss: 1.477, Validation Accuracy: 46.10%\n",
            "Epoch [1/10], Training Loss: 1.514, Validation Accuracy: 46.40%\n",
            "Epoch [2/10], Training Loss: 1.500, Validation Accuracy: 46.05%\n",
            "Epoch [3/10], Training Loss: 1.484, Validation Accuracy: 45.58%\n",
            "Epoch [4/10], Training Loss: 1.474, Validation Accuracy: 46.81%\n",
            "Epoch [5/10], Training Loss: 1.462, Validation Accuracy: 46.80%\n",
            "Epoch [6/10], Training Loss: 1.454, Validation Accuracy: 46.29%\n",
            "Epoch [7/10], Training Loss: 1.446, Validation Accuracy: 47.26%\n",
            "Epoch [8/10], Training Loss: 1.436, Validation Accuracy: 47.60%\n",
            "Epoch [9/10], Training Loss: 1.425, Validation Accuracy: 46.39%\n",
            "Epoch [10/10], Training Loss: 1.421, Validation Accuracy: 47.58%\n",
            "Epoch [1/10], Training Loss: 1.457, Validation Accuracy: 48.43%\n",
            "Epoch [2/10], Training Loss: 1.448, Validation Accuracy: 48.00%\n",
            "Epoch [3/10], Training Loss: 1.432, Validation Accuracy: 48.65%\n",
            "Epoch [4/10], Training Loss: 1.419, Validation Accuracy: 48.49%\n",
            "Epoch [5/10], Training Loss: 1.413, Validation Accuracy: 48.15%\n",
            "Epoch [6/10], Training Loss: 1.400, Validation Accuracy: 48.83%\n",
            "Epoch [7/10], Training Loss: 1.385, Validation Accuracy: 48.61%\n",
            "Epoch [8/10], Training Loss: 1.385, Validation Accuracy: 49.08%\n",
            "Epoch [9/10], Training Loss: 1.380, Validation Accuracy: 48.16%\n",
            "Epoch [10/10], Training Loss: 1.368, Validation Accuracy: 49.22%\n",
            "Epoch [1/10], Training Loss: 1.410, Validation Accuracy: 48.54%\n",
            "Epoch [2/10], Training Loss: 1.390, Validation Accuracy: 50.05%\n",
            "Epoch [3/10], Training Loss: 1.368, Validation Accuracy: 48.86%\n",
            "Epoch [4/10], Training Loss: 1.370, Validation Accuracy: 50.61%\n",
            "Epoch [5/10], Training Loss: 1.350, Validation Accuracy: 49.51%\n",
            "Epoch [6/10], Training Loss: 1.342, Validation Accuracy: 50.67%\n",
            "Epoch [7/10], Training Loss: 1.334, Validation Accuracy: 50.74%\n",
            "Epoch [8/10], Training Loss: 1.326, Validation Accuracy: 50.39%\n",
            "Epoch [9/10], Training Loss: 1.316, Validation Accuracy: 51.04%\n",
            "Epoch [10/10], Training Loss: 1.311, Validation Accuracy: 51.62%\n",
            "Epoch [1/10], Training Loss: 1.386, Validation Accuracy: 51.50%\n",
            "Epoch [2/10], Training Loss: 1.358, Validation Accuracy: 51.75%\n",
            "Epoch [3/10], Training Loss: 1.347, Validation Accuracy: 50.21%\n",
            "Epoch [4/10], Training Loss: 1.332, Validation Accuracy: 51.09%\n",
            "Epoch [5/10], Training Loss: 1.327, Validation Accuracy: 51.60%\n",
            "Epoch [6/10], Training Loss: 1.323, Validation Accuracy: 51.66%\n",
            "Epoch [7/10], Training Loss: 1.308, Validation Accuracy: 51.84%\n",
            "Epoch [8/10], Training Loss: 1.303, Validation Accuracy: 51.97%\n",
            "Epoch [9/10], Training Loss: 1.296, Validation Accuracy: 51.51%\n",
            "Epoch [10/10], Training Loss: 1.290, Validation Accuracy: 52.54%\n",
            "Epoch [1/10], Training Loss: 1.346, Validation Accuracy: 52.57%\n",
            "Epoch [2/10], Training Loss: 1.325, Validation Accuracy: 52.44%\n",
            "Epoch [3/10], Training Loss: 1.317, Validation Accuracy: 52.62%\n",
            "Epoch [4/10], Training Loss: 1.292, Validation Accuracy: 52.67%\n",
            "Epoch [5/10], Training Loss: 1.286, Validation Accuracy: 53.33%\n",
            "Epoch [6/10], Training Loss: 1.275, Validation Accuracy: 52.15%\n",
            "Epoch [7/10], Training Loss: 1.271, Validation Accuracy: 53.06%\n",
            "Epoch [8/10], Training Loss: 1.259, Validation Accuracy: 53.51%\n",
            "Epoch [9/10], Training Loss: 1.251, Validation Accuracy: 53.07%\n",
            "Epoch [10/10], Training Loss: 1.245, Validation Accuracy: 53.29%\n",
            "Epoch [1/10], Training Loss: 1.311, Validation Accuracy: 53.32%\n",
            "Epoch [2/10], Training Loss: 1.288, Validation Accuracy: 54.49%\n",
            "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 53.56%\n",
            "Epoch [4/10], Training Loss: 1.257, Validation Accuracy: 54.36%\n",
            "Epoch [5/10], Training Loss: 1.250, Validation Accuracy: 54.50%\n",
            "Epoch [6/10], Training Loss: 1.232, Validation Accuracy: 53.89%\n",
            "Epoch [7/10], Training Loss: 1.232, Validation Accuracy: 54.53%\n",
            "Epoch [8/10], Training Loss: 1.216, Validation Accuracy: 54.95%\n",
            "Epoch [9/10], Training Loss: 1.206, Validation Accuracy: 55.00%\n",
            "Epoch [10/10], Training Loss: 1.195, Validation Accuracy: 53.59%\n",
            "Epoch [1/10], Training Loss: 1.287, Validation Accuracy: 54.35%\n",
            "Epoch [2/10], Training Loss: 1.263, Validation Accuracy: 55.18%\n",
            "Epoch [3/10], Training Loss: 1.257, Validation Accuracy: 54.64%\n",
            "Epoch [4/10], Training Loss: 1.240, Validation Accuracy: 54.77%\n",
            "Epoch [5/10], Training Loss: 1.231, Validation Accuracy: 55.18%\n",
            "Epoch [6/10], Training Loss: 1.213, Validation Accuracy: 55.16%\n",
            "Epoch [7/10], Training Loss: 1.214, Validation Accuracy: 55.45%\n",
            "Epoch [8/10], Training Loss: 1.203, Validation Accuracy: 54.89%\n",
            "Epoch [9/10], Training Loss: 1.184, Validation Accuracy: 55.70%\n",
            "Epoch [10/10], Training Loss: 1.173, Validation Accuracy: 55.72%\n",
            "Epoch [1/10], Training Loss: 1.241, Validation Accuracy: 55.25%\n",
            "Epoch [2/10], Training Loss: 1.224, Validation Accuracy: 56.14%\n",
            "Epoch [3/10], Training Loss: 1.202, Validation Accuracy: 54.95%\n",
            "Epoch [4/10], Training Loss: 1.190, Validation Accuracy: 55.56%\n",
            "Epoch [5/10], Training Loss: 1.184, Validation Accuracy: 55.49%\n",
            "Epoch [6/10], Training Loss: 1.171, Validation Accuracy: 55.53%\n",
            "Epoch [7/10], Training Loss: 1.156, Validation Accuracy: 56.00%\n",
            "Epoch [8/10], Training Loss: 1.157, Validation Accuracy: 55.84%\n",
            "Epoch [9/10], Training Loss: 1.135, Validation Accuracy: 56.29%\n",
            "Epoch [10/10], Training Loss: 1.128, Validation Accuracy: 55.13%\n",
            "Epoch [1/10], Training Loss: 1.236, Validation Accuracy: 56.14%\n",
            "Epoch [2/10], Training Loss: 1.207, Validation Accuracy: 56.34%\n",
            "Epoch [3/10], Training Loss: 1.185, Validation Accuracy: 56.60%\n",
            "Epoch [4/10], Training Loss: 1.164, Validation Accuracy: 56.56%\n",
            "Epoch [5/10], Training Loss: 1.160, Validation Accuracy: 56.44%\n",
            "Epoch [6/10], Training Loss: 1.147, Validation Accuracy: 56.42%\n",
            "Epoch [7/10], Training Loss: 1.135, Validation Accuracy: 56.99%\n",
            "Epoch [8/10], Training Loss: 1.127, Validation Accuracy: 56.57%\n",
            "Epoch [9/10], Training Loss: 1.116, Validation Accuracy: 56.37%\n",
            "Epoch [10/10], Training Loss: 1.099, Validation Accuracy: 57.22%\n",
            "Epoch [1/10], Training Loss: 1.202, Validation Accuracy: 57.06%\n",
            "Epoch [2/10], Training Loss: 1.178, Validation Accuracy: 56.72%\n",
            "Epoch [3/10], Training Loss: 1.162, Validation Accuracy: 57.43%\n",
            "Epoch [4/10], Training Loss: 1.138, Validation Accuracy: 57.76%\n",
            "Epoch [5/10], Training Loss: 1.126, Validation Accuracy: 57.65%\n",
            "Epoch [6/10], Training Loss: 1.110, Validation Accuracy: 57.17%\n",
            "Epoch [7/10], Training Loss: 1.103, Validation Accuracy: 57.63%\n",
            "Epoch [8/10], Training Loss: 1.083, Validation Accuracy: 57.44%\n",
            "Epoch [9/10], Training Loss: 1.084, Validation Accuracy: 57.46%\n",
            "Epoch [10/10], Training Loss: 1.068, Validation Accuracy: 57.01%\n",
            "Epoch [1/10], Training Loss: 1.177, Validation Accuracy: 58.15%\n",
            "Epoch [2/10], Training Loss: 1.151, Validation Accuracy: 57.66%\n",
            "Epoch [3/10], Training Loss: 1.126, Validation Accuracy: 57.71%\n",
            "Epoch [4/10], Training Loss: 1.111, Validation Accuracy: 57.61%\n",
            "Epoch [5/10], Training Loss: 1.093, Validation Accuracy: 57.79%\n",
            "Epoch [6/10], Training Loss: 1.085, Validation Accuracy: 57.85%\n",
            "Epoch [7/10], Training Loss: 1.074, Validation Accuracy: 58.06%\n",
            "Epoch [8/10], Training Loss: 1.061, Validation Accuracy: 57.84%\n",
            "Epoch [9/10], Training Loss: 1.047, Validation Accuracy: 58.13%\n",
            "Epoch [10/10], Training Loss: 1.042, Validation Accuracy: 57.41%\n",
            "Epoch [1/10], Training Loss: 1.166, Validation Accuracy: 58.02%\n",
            "Epoch [2/10], Training Loss: 1.140, Validation Accuracy: 57.68%\n",
            "Epoch [3/10], Training Loss: 1.111, Validation Accuracy: 58.21%\n",
            "Epoch [4/10], Training Loss: 1.100, Validation Accuracy: 57.39%\n",
            "Epoch [5/10], Training Loss: 1.089, Validation Accuracy: 58.36%\n",
            "Epoch [6/10], Training Loss: 1.065, Validation Accuracy: 58.27%\n",
            "Epoch [7/10], Training Loss: 1.059, Validation Accuracy: 58.00%\n",
            "Epoch [8/10], Training Loss: 1.049, Validation Accuracy: 58.34%\n",
            "Epoch [9/10], Training Loss: 1.028, Validation Accuracy: 58.05%\n",
            "Epoch [10/10], Training Loss: 1.020, Validation Accuracy: 58.13%\n",
            "Epoch [1/10], Training Loss: 1.135, Validation Accuracy: 58.92%\n",
            "Epoch [2/10], Training Loss: 1.109, Validation Accuracy: 58.20%\n",
            "Epoch [3/10], Training Loss: 1.092, Validation Accuracy: 58.84%\n",
            "Epoch [4/10], Training Loss: 1.061, Validation Accuracy: 58.66%\n",
            "Epoch [5/10], Training Loss: 1.049, Validation Accuracy: 59.02%\n",
            "Epoch [6/10], Training Loss: 1.026, Validation Accuracy: 58.74%\n",
            "Epoch [7/10], Training Loss: 1.021, Validation Accuracy: 59.05%\n",
            "Epoch [8/10], Training Loss: 1.007, Validation Accuracy: 59.35%\n",
            "Epoch [9/10], Training Loss: 0.997, Validation Accuracy: 59.75%\n",
            "Epoch [10/10], Training Loss: 0.989, Validation Accuracy: 58.47%\n",
            "Epoch [1/10], Training Loss: 1.134, Validation Accuracy: 58.79%\n",
            "Epoch [2/10], Training Loss: 1.089, Validation Accuracy: 58.53%\n",
            "Epoch [3/10], Training Loss: 1.066, Validation Accuracy: 59.45%\n",
            "Epoch [4/10], Training Loss: 1.041, Validation Accuracy: 58.77%\n",
            "Epoch [5/10], Training Loss: 1.028, Validation Accuracy: 59.11%\n",
            "Epoch [6/10], Training Loss: 1.013, Validation Accuracy: 58.87%\n",
            "Epoch [7/10], Training Loss: 0.992, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.979, Validation Accuracy: 59.37%\n",
            "Epoch [9/10], Training Loss: 0.971, Validation Accuracy: 59.42%\n",
            "Epoch [10/10], Training Loss: 0.965, Validation Accuracy: 58.77%\n",
            "Epoch [1/10], Training Loss: 1.111, Validation Accuracy: 59.05%\n",
            "Epoch [2/10], Training Loss: 1.066, Validation Accuracy: 58.74%\n",
            "Epoch [3/10], Training Loss: 1.043, Validation Accuracy: 60.01%\n",
            "Epoch [4/10], Training Loss: 1.018, Validation Accuracy: 60.45%\n",
            "Epoch [5/10], Training Loss: 1.007, Validation Accuracy: 59.85%\n",
            "Epoch [6/10], Training Loss: 0.986, Validation Accuracy: 59.58%\n",
            "Epoch [7/10], Training Loss: 0.977, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.961, Validation Accuracy: 59.31%\n",
            "Epoch [9/10], Training Loss: 0.949, Validation Accuracy: 59.39%\n",
            "Epoch [10/10], Training Loss: 0.936, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.29%\n",
            "Epoch [2/10], Training Loss: 1.039, Validation Accuracy: 59.87%\n",
            "Epoch [3/10], Training Loss: 1.014, Validation Accuracy: 60.32%\n",
            "Epoch [4/10], Training Loss: 0.997, Validation Accuracy: 60.38%\n",
            "Epoch [5/10], Training Loss: 0.978, Validation Accuracy: 59.88%\n",
            "Epoch [6/10], Training Loss: 0.961, Validation Accuracy: 59.74%\n",
            "Epoch [7/10], Training Loss: 0.942, Validation Accuracy: 60.11%\n",
            "Epoch [8/10], Training Loss: 0.933, Validation Accuracy: 59.88%\n",
            "Epoch [9/10], Training Loss: 0.921, Validation Accuracy: 59.79%\n",
            "Epoch [10/10], Training Loss: 0.905, Validation Accuracy: 59.29%\n",
            "Epoch [1/10], Training Loss: 1.080, Validation Accuracy: 59.29%\n",
            "Epoch [2/10], Training Loss: 1.039, Validation Accuracy: 60.12%\n",
            "Epoch [3/10], Training Loss: 1.001, Validation Accuracy: 60.09%\n",
            "Epoch [4/10], Training Loss: 0.987, Validation Accuracy: 60.46%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.08%\n",
            "Epoch [6/10], Training Loss: 0.953, Validation Accuracy: 59.95%\n",
            "Epoch [7/10], Training Loss: 0.934, Validation Accuracy: 59.83%\n",
            "Epoch [8/10], Training Loss: 0.918, Validation Accuracy: 59.87%\n",
            "Epoch [9/10], Training Loss: 0.906, Validation Accuracy: 59.74%\n",
            "Epoch [10/10], Training Loss: 0.887, Validation Accuracy: 60.04%\n",
            "Epoch [1/10], Training Loss: 1.062, Validation Accuracy: 60.40%\n",
            "Epoch [2/10], Training Loss: 1.009, Validation Accuracy: 59.80%\n",
            "Epoch [3/10], Training Loss: 0.976, Validation Accuracy: 60.19%\n",
            "Epoch [4/10], Training Loss: 0.963, Validation Accuracy: 60.60%\n",
            "Epoch [5/10], Training Loss: 0.944, Validation Accuracy: 60.75%\n",
            "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 61.10%\n",
            "Epoch [7/10], Training Loss: 0.904, Validation Accuracy: 60.64%\n",
            "Epoch [8/10], Training Loss: 0.888, Validation Accuracy: 60.50%\n",
            "Epoch [9/10], Training Loss: 0.878, Validation Accuracy: 60.30%\n",
            "Epoch [10/10], Training Loss: 0.861, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 1.048, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 1.007, Validation Accuracy: 61.22%\n",
            "Epoch [3/10], Training Loss: 0.957, Validation Accuracy: 60.99%\n",
            "Epoch [4/10], Training Loss: 0.932, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.920, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.902, Validation Accuracy: 60.53%\n",
            "Epoch [7/10], Training Loss: 0.874, Validation Accuracy: 60.29%\n",
            "Epoch [8/10], Training Loss: 0.858, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.844, Validation Accuracy: 60.38%\n",
            "Epoch [10/10], Training Loss: 0.834, Validation Accuracy: 60.47%\n",
            "Epoch [1/10], Training Loss: 1.036, Validation Accuracy: 61.41%\n",
            "Epoch [2/10], Training Loss: 0.974, Validation Accuracy: 60.51%\n",
            "Epoch [3/10], Training Loss: 0.947, Validation Accuracy: 60.86%\n",
            "Epoch [4/10], Training Loss: 0.923, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.891, Validation Accuracy: 61.34%\n",
            "Epoch [6/10], Training Loss: 0.881, Validation Accuracy: 61.15%\n",
            "Epoch [7/10], Training Loss: 0.853, Validation Accuracy: 61.14%\n",
            "Epoch [8/10], Training Loss: 0.847, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.827, Validation Accuracy: 61.26%\n",
            "Epoch [10/10], Training Loss: 0.808, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 1.022, Validation Accuracy: 61.41%\n",
            "Epoch [2/10], Training Loss: 0.956, Validation Accuracy: 61.49%\n",
            "Epoch [3/10], Training Loss: 0.922, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.901, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.885, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.860, Validation Accuracy: 61.11%\n",
            "Epoch [7/10], Training Loss: 0.842, Validation Accuracy: 61.53%\n",
            "Epoch [8/10], Training Loss: 0.828, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.810, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.796, Validation Accuracy: 60.46%\n",
            "Epoch [1/10], Training Loss: 1.006, Validation Accuracy: 61.60%\n",
            "Epoch [2/10], Training Loss: 0.949, Validation Accuracy: 61.61%\n",
            "Epoch [3/10], Training Loss: 0.913, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.892, Validation Accuracy: 61.91%\n",
            "Epoch [5/10], Training Loss: 0.862, Validation Accuracy: 61.79%\n",
            "Epoch [6/10], Training Loss: 0.838, Validation Accuracy: 61.73%\n",
            "Epoch [7/10], Training Loss: 0.827, Validation Accuracy: 60.75%\n",
            "Epoch [8/10], Training Loss: 0.804, Validation Accuracy: 61.53%\n",
            "Epoch [9/10], Training Loss: 0.792, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.779, Validation Accuracy: 61.05%\n",
            "Epoch [1/10], Training Loss: 0.992, Validation Accuracy: 61.76%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 61.56%\n",
            "Epoch [3/10], Training Loss: 0.892, Validation Accuracy: 61.67%\n",
            "Epoch [4/10], Training Loss: 0.869, Validation Accuracy: 61.45%\n",
            "Epoch [5/10], Training Loss: 0.839, Validation Accuracy: 61.88%\n",
            "Epoch [6/10], Training Loss: 0.820, Validation Accuracy: 61.96%\n",
            "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 62.31%\n",
            "Epoch [8/10], Training Loss: 0.788, Validation Accuracy: 60.77%\n",
            "Epoch [9/10], Training Loss: 0.770, Validation Accuracy: 61.66%\n",
            "Epoch [10/10], Training Loss: 0.758, Validation Accuracy: 61.76%\n",
            "Epoch [1/10], Training Loss: 0.978, Validation Accuracy: 60.92%\n",
            "Epoch [2/10], Training Loss: 0.910, Validation Accuracy: 61.53%\n",
            "Epoch [3/10], Training Loss: 0.874, Validation Accuracy: 61.73%\n",
            "Epoch [4/10], Training Loss: 0.856, Validation Accuracy: 60.66%\n",
            "Epoch [5/10], Training Loss: 0.819, Validation Accuracy: 61.72%\n",
            "Epoch [6/10], Training Loss: 0.790, Validation Accuracy: 61.38%\n",
            "Epoch [7/10], Training Loss: 0.775, Validation Accuracy: 60.55%\n",
            "Epoch [8/10], Training Loss: 0.755, Validation Accuracy: 61.51%\n",
            "Epoch [9/10], Training Loss: 0.740, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.732, Validation Accuracy: 61.43%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 61.38%\n",
            "Epoch [2/10], Training Loss: 0.912, Validation Accuracy: 60.26%\n",
            "Epoch [3/10], Training Loss: 0.865, Validation Accuracy: 61.69%\n",
            "Epoch [4/10], Training Loss: 0.822, Validation Accuracy: 61.27%\n",
            "Epoch [5/10], Training Loss: 0.796, Validation Accuracy: 62.32%\n",
            "Epoch [6/10], Training Loss: 0.771, Validation Accuracy: 61.17%\n",
            "Epoch [7/10], Training Loss: 0.760, Validation Accuracy: 62.25%\n",
            "Epoch [8/10], Training Loss: 0.738, Validation Accuracy: 62.03%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.713, Validation Accuracy: 60.91%\n",
            "Epoch [1/10], Training Loss: 0.967, Validation Accuracy: 62.02%\n",
            "Epoch [2/10], Training Loss: 0.893, Validation Accuracy: 61.77%\n",
            "Epoch [3/10], Training Loss: 0.852, Validation Accuracy: 61.20%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 62.05%\n",
            "Epoch [5/10], Training Loss: 0.795, Validation Accuracy: 61.90%\n",
            "Epoch [6/10], Training Loss: 0.763, Validation Accuracy: 61.55%\n",
            "Epoch [7/10], Training Loss: 0.747, Validation Accuracy: 61.59%\n",
            "Epoch [8/10], Training Loss: 0.722, Validation Accuracy: 61.68%\n",
            "Epoch [9/10], Training Loss: 0.713, Validation Accuracy: 61.67%\n",
            "Epoch [10/10], Training Loss: 0.693, Validation Accuracy: 60.94%\n",
            "Epoch [1/10], Training Loss: 0.947, Validation Accuracy: 60.92%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 62.29%\n",
            "Epoch [3/10], Training Loss: 0.829, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.801, Validation Accuracy: 60.25%\n",
            "Epoch [5/10], Training Loss: 0.770, Validation Accuracy: 61.49%\n",
            "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.736, Validation Accuracy: 62.37%\n",
            "Epoch [8/10], Training Loss: 0.715, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.690, Validation Accuracy: 61.81%\n",
            "Epoch [10/10], Training Loss: 0.677, Validation Accuracy: 61.93%\n",
            "Epoch [1/10], Training Loss: 0.945, Validation Accuracy: 61.93%\n",
            "Epoch [2/10], Training Loss: 0.863, Validation Accuracy: 62.05%\n",
            "Epoch [3/10], Training Loss: 0.824, Validation Accuracy: 62.45%\n",
            "Epoch [4/10], Training Loss: 0.783, Validation Accuracy: 62.46%\n",
            "Epoch [5/10], Training Loss: 0.754, Validation Accuracy: 62.28%\n",
            "Epoch [6/10], Training Loss: 0.735, Validation Accuracy: 62.30%\n",
            "Epoch [7/10], Training Loss: 0.715, Validation Accuracy: 62.01%\n",
            "Epoch [8/10], Training Loss: 0.700, Validation Accuracy: 61.48%\n",
            "Epoch [9/10], Training Loss: 0.689, Validation Accuracy: 61.87%\n",
            "Epoch [10/10], Training Loss: 0.663, Validation Accuracy: 61.71%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 61.75%\n",
            "Epoch [2/10], Training Loss: 0.837, Validation Accuracy: 61.83%\n",
            "Epoch [3/10], Training Loss: 0.794, Validation Accuracy: 61.53%\n",
            "Epoch [4/10], Training Loss: 0.753, Validation Accuracy: 61.74%\n",
            "Epoch [5/10], Training Loss: 0.732, Validation Accuracy: 61.75%\n",
            "Epoch [6/10], Training Loss: 0.708, Validation Accuracy: 60.73%\n",
            "Epoch [7/10], Training Loss: 0.685, Validation Accuracy: 61.32%\n",
            "Epoch [8/10], Training Loss: 0.668, Validation Accuracy: 61.39%\n",
            "Epoch [9/10], Training Loss: 0.655, Validation Accuracy: 61.20%\n",
            "Epoch [10/10], Training Loss: 0.628, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.922, Validation Accuracy: 61.84%\n",
            "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 60.83%\n",
            "Epoch [3/10], Training Loss: 0.785, Validation Accuracy: 61.69%\n",
            "Epoch [4/10], Training Loss: 0.748, Validation Accuracy: 61.80%\n",
            "Epoch [5/10], Training Loss: 0.709, Validation Accuracy: 61.35%\n",
            "Epoch [6/10], Training Loss: 0.690, Validation Accuracy: 61.74%\n",
            "Epoch [7/10], Training Loss: 0.663, Validation Accuracy: 61.95%\n",
            "Epoch [8/10], Training Loss: 0.647, Validation Accuracy: 61.56%\n",
            "Epoch [9/10], Training Loss: 0.626, Validation Accuracy: 61.17%\n",
            "Epoch [10/10], Training Loss: 0.616, Validation Accuracy: 61.33%\n",
            "Epoch [1/10], Training Loss: 0.912, Validation Accuracy: 61.56%\n",
            "Epoch [2/10], Training Loss: 0.831, Validation Accuracy: 61.49%\n",
            "Epoch [3/10], Training Loss: 0.779, Validation Accuracy: 62.04%\n",
            "Epoch [4/10], Training Loss: 0.733, Validation Accuracy: 61.99%\n",
            "Epoch [5/10], Training Loss: 0.700, Validation Accuracy: 61.33%\n",
            "Epoch [6/10], Training Loss: 0.690, Validation Accuracy: 62.15%\n",
            "Epoch [7/10], Training Loss: 0.654, Validation Accuracy: 61.84%\n",
            "Epoch [8/10], Training Loss: 0.630, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.616, Validation Accuracy: 61.99%\n",
            "Epoch [10/10], Training Loss: 0.601, Validation Accuracy: 61.92%\n",
            "Epoch [1/10], Training Loss: 0.895, Validation Accuracy: 61.54%\n",
            "Epoch [2/10], Training Loss: 0.802, Validation Accuracy: 60.58%\n",
            "Epoch [3/10], Training Loss: 0.760, Validation Accuracy: 61.48%\n",
            "Epoch [4/10], Training Loss: 0.711, Validation Accuracy: 61.80%\n",
            "Epoch [5/10], Training Loss: 0.690, Validation Accuracy: 62.41%\n",
            "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 61.45%\n",
            "Epoch [7/10], Training Loss: 0.637, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.616, Validation Accuracy: 61.92%\n",
            "Epoch [9/10], Training Loss: 0.600, Validation Accuracy: 62.16%\n",
            "Epoch [10/10], Training Loss: 0.580, Validation Accuracy: 60.80%\n",
            "Epoch [1/10], Training Loss: 0.912, Validation Accuracy: 61.65%\n",
            "Epoch [2/10], Training Loss: 0.808, Validation Accuracy: 61.43%\n",
            "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 61.91%\n",
            "Epoch [4/10], Training Loss: 0.725, Validation Accuracy: 62.09%\n",
            "Epoch [5/10], Training Loss: 0.679, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.656, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.632, Validation Accuracy: 61.57%\n",
            "Epoch [8/10], Training Loss: 0.610, Validation Accuracy: 61.85%\n",
            "Epoch [9/10], Training Loss: 0.582, Validation Accuracy: 61.87%\n",
            "Epoch [10/10], Training Loss: 0.572, Validation Accuracy: 61.63%\n",
            "Epoch [1/10], Training Loss: 0.877, Validation Accuracy: 61.36%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 61.41%\n",
            "Epoch [3/10], Training Loss: 0.724, Validation Accuracy: 61.86%\n",
            "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 61.61%\n",
            "Epoch [5/10], Training Loss: 0.638, Validation Accuracy: 61.64%\n",
            "Epoch [6/10], Training Loss: 0.615, Validation Accuracy: 60.97%\n",
            "Epoch [7/10], Training Loss: 0.588, Validation Accuracy: 60.87%\n",
            "Epoch [8/10], Training Loss: 0.566, Validation Accuracy: 60.95%\n",
            "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.521, Validation Accuracy: 60.76%\n",
            "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 60.65%\n",
            "Epoch [2/10], Training Loss: 0.757, Validation Accuracy: 61.85%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 61.84%\n",
            "Epoch [4/10], Training Loss: 0.663, Validation Accuracy: 61.83%\n",
            "Epoch [5/10], Training Loss: 0.631, Validation Accuracy: 61.09%\n",
            "Epoch [6/10], Training Loss: 0.608, Validation Accuracy: 61.46%\n",
            "Epoch [7/10], Training Loss: 0.580, Validation Accuracy: 61.54%\n",
            "Epoch [8/10], Training Loss: 0.560, Validation Accuracy: 61.50%\n",
            "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 61.46%\n",
            "Epoch [10/10], Training Loss: 0.516, Validation Accuracy: 61.26%\n",
            "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 61.04%\n",
            "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 61.17%\n",
            "Epoch [3/10], Training Loss: 0.710, Validation Accuracy: 61.85%\n",
            "Epoch [4/10], Training Loss: 0.668, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.634, Validation Accuracy: 61.62%\n",
            "Epoch [6/10], Training Loss: 0.597, Validation Accuracy: 61.93%\n",
            "Epoch [7/10], Training Loss: 0.570, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.555, Validation Accuracy: 61.66%\n",
            "Epoch [9/10], Training Loss: 0.523, Validation Accuracy: 61.65%\n",
            "Epoch [10/10], Training Loss: 0.525, Validation Accuracy: 61.15%\n",
            "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 61.67%\n",
            "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 62.03%\n",
            "Epoch [3/10], Training Loss: 0.677, Validation Accuracy: 61.47%\n",
            "Epoch [4/10], Training Loss: 0.646, Validation Accuracy: 61.26%\n",
            "Epoch [5/10], Training Loss: 0.606, Validation Accuracy: 61.81%\n",
            "Epoch [6/10], Training Loss: 0.577, Validation Accuracy: 60.73%\n",
            "Epoch [7/10], Training Loss: 0.552, Validation Accuracy: 61.61%\n",
            "Epoch [8/10], Training Loss: 0.526, Validation Accuracy: 61.48%\n",
            "Epoch [9/10], Training Loss: 0.501, Validation Accuracy: 61.57%\n",
            "Epoch [10/10], Training Loss: 0.487, Validation Accuracy: 61.30%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 61.04%\n",
            "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 61.55%\n",
            "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 61.77%\n",
            "Epoch [4/10], Training Loss: 0.633, Validation Accuracy: 61.94%\n",
            "Epoch [5/10], Training Loss: 0.597, Validation Accuracy: 61.39%\n",
            "Epoch [6/10], Training Loss: 0.585, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.549, Validation Accuracy: 60.99%\n",
            "Epoch [8/10], Training Loss: 0.520, Validation Accuracy: 60.92%\n",
            "Epoch [9/10], Training Loss: 0.501, Validation Accuracy: 61.18%\n",
            "Epoch [10/10], Training Loss: 0.485, Validation Accuracy: 61.25%\n",
            "Epoch [1/10], Training Loss: 0.854, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 0.733, Validation Accuracy: 61.18%\n",
            "Epoch [3/10], Training Loss: 0.651, Validation Accuracy: 60.86%\n",
            "Epoch [4/10], Training Loss: 0.609, Validation Accuracy: 61.53%\n",
            "Epoch [5/10], Training Loss: 0.563, Validation Accuracy: 61.49%\n",
            "Epoch [6/10], Training Loss: 0.534, Validation Accuracy: 60.67%\n",
            "Epoch [7/10], Training Loss: 0.517, Validation Accuracy: 60.93%\n",
            "Epoch [8/10], Training Loss: 0.489, Validation Accuracy: 60.83%\n",
            "Epoch [9/10], Training Loss: 0.461, Validation Accuracy: 60.97%\n",
            "Epoch [10/10], Training Loss: 0.449, Validation Accuracy: 60.50%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 60.86%\n",
            "Epoch [2/10], Training Loss: 0.709, Validation Accuracy: 60.69%\n",
            "Epoch [3/10], Training Loss: 0.643, Validation Accuracy: 60.88%\n",
            "Epoch [4/10], Training Loss: 0.585, Validation Accuracy: 61.19%\n",
            "Epoch [5/10], Training Loss: 0.550, Validation Accuracy: 61.33%\n",
            "Epoch [6/10], Training Loss: 0.531, Validation Accuracy: 60.90%\n",
            "Epoch [7/10], Training Loss: 0.491, Validation Accuracy: 61.05%\n",
            "Epoch [8/10], Training Loss: 0.473, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.453, Validation Accuracy: 60.89%\n",
            "Epoch [10/10], Training Loss: 0.427, Validation Accuracy: 60.56%\n",
            "Epoch [1/10], Training Loss: 0.858, Validation Accuracy: 60.60%\n",
            "Epoch [2/10], Training Loss: 0.699, Validation Accuracy: 61.70%\n",
            "Epoch [3/10], Training Loss: 0.636, Validation Accuracy: 62.21%\n",
            "Epoch [4/10], Training Loss: 0.595, Validation Accuracy: 61.38%\n",
            "Epoch [5/10], Training Loss: 0.553, Validation Accuracy: 61.61%\n",
            "Epoch [6/10], Training Loss: 0.518, Validation Accuracy: 61.59%\n",
            "Epoch [7/10], Training Loss: 0.496, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.468, Validation Accuracy: 61.39%\n",
            "Epoch [9/10], Training Loss: 0.454, Validation Accuracy: 60.56%\n",
            "Epoch [10/10], Training Loss: 0.433, Validation Accuracy: 61.61%\n",
            "Epoch [1/10], Training Loss: 0.819, Validation Accuracy: 61.11%\n",
            "Epoch [2/10], Training Loss: 0.688, Validation Accuracy: 62.05%\n",
            "Epoch [3/10], Training Loss: 0.617, Validation Accuracy: 61.19%\n",
            "Epoch [4/10], Training Loss: 0.567, Validation Accuracy: 60.81%\n",
            "Epoch [5/10], Training Loss: 0.531, Validation Accuracy: 62.16%\n",
            "Epoch [6/10], Training Loss: 0.489, Validation Accuracy: 62.09%\n",
            "Epoch [7/10], Training Loss: 0.466, Validation Accuracy: 61.63%\n",
            "Epoch [8/10], Training Loss: 0.441, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.415, Validation Accuracy: 61.10%\n",
            "Epoch [10/10], Training Loss: 0.403, Validation Accuracy: 61.32%\n",
            "Epoch [1/10], Training Loss: 0.831, Validation Accuracy: 60.52%\n",
            "Epoch [2/10], Training Loss: 0.680, Validation Accuracy: 61.78%\n",
            "Epoch [3/10], Training Loss: 0.617, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.561, Validation Accuracy: 61.16%\n",
            "Epoch [5/10], Training Loss: 0.515, Validation Accuracy: 61.50%\n",
            "Epoch [6/10], Training Loss: 0.488, Validation Accuracy: 61.09%\n",
            "Epoch [7/10], Training Loss: 0.463, Validation Accuracy: 61.33%\n",
            "Epoch [8/10], Training Loss: 0.436, Validation Accuracy: 61.13%\n",
            "Epoch [9/10], Training Loss: 0.412, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.401, Validation Accuracy: 61.21%\n",
            "Confusion Matrix:\n",
            "[[728  30  34  19  25  10  25   7  76  46]\n",
            " [ 54 702   6  12  12  14  21   4  44 131]\n",
            " [116   9 365 103 129  94 105  39  26  14]\n",
            " [ 46  18  56 431 102 167 112  25  17  26]\n",
            " [ 48   3  63  67 563  52 111  61  22  10]\n",
            " [ 22   6  38 243  95 463  67  37  15  14]\n",
            " [ 20  10  29  56  68  25 759  10  14   9]\n",
            " [ 37   9  25  65 124  85  26 597   9  23]\n",
            " [122  55   6   9  13  13  17   3 731  31]\n",
            " [ 81 156  11  20  13  18  27  22  49 603]]\n",
            "Test Accuracy: 59.42%\n",
            "True Positives (TP): [728 702 365 431 563 463 759 597 731 603]\n",
            "False Positives (FP): [546 296 268 594 581 478 511 208 272 304]\n",
            "True Negatives (TN): [8454 8704 8732 8406 8419 8522 8489 8792 8728 8696]\n",
            "False Negatives (FN): [272 298 635 569 437 537 241 403 269 397]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.57142857 0.70340681 0.57661927 0.4204878  0.49213287 0.49202976\n",
            " 0.5976378  0.74161491 0.72881356 0.66482911]\n",
            "Recall: [0.728 0.702 0.365 0.431 0.563 0.463 0.759 0.597 0.731 0.603]\n",
            "F1 Score: [0.64028144 0.7027027  0.44703001 0.42567901 0.52518657 0.47707367\n",
            " 0.66872247 0.66149584 0.72990514 0.63240692]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}