{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Kl-FedDis-Research-/blob/main/uniform_truncated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTWrQnIKlvaZ",
        "outputId": "302a3098-13e5-4d6b-d071-3e74b65f33d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 10.49%\n",
            "Epoch [2/10], Training Loss: 2.305, Validation Accuracy: 10.52%\n",
            "Epoch [3/10], Training Loss: 2.304, Validation Accuracy: 10.44%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 10.51%\n",
            "Epoch [5/10], Training Loss: 2.303, Validation Accuracy: 10.61%\n",
            "Epoch [6/10], Training Loss: 2.302, Validation Accuracy: 10.67%\n",
            "Epoch [7/10], Training Loss: 2.302, Validation Accuracy: 10.97%\n",
            "Epoch [8/10], Training Loss: 2.301, Validation Accuracy: 11.34%\n",
            "Epoch [9/10], Training Loss: 2.301, Validation Accuracy: 11.72%\n",
            "Epoch [10/10], Training Loss: 2.300, Validation Accuracy: 12.03%\n",
            "Epoch [1/10], Training Loss: 2.299, Validation Accuracy: 11.57%\n",
            "Epoch [2/10], Training Loss: 2.298, Validation Accuracy: 12.37%\n",
            "Epoch [3/10], Training Loss: 2.296, Validation Accuracy: 12.71%\n",
            "Epoch [4/10], Training Loss: 2.294, Validation Accuracy: 13.19%\n",
            "Epoch [5/10], Training Loss: 2.292, Validation Accuracy: 12.54%\n",
            "Epoch [6/10], Training Loss: 2.288, Validation Accuracy: 12.56%\n",
            "Epoch [7/10], Training Loss: 2.283, Validation Accuracy: 12.75%\n",
            "Epoch [8/10], Training Loss: 2.276, Validation Accuracy: 14.95%\n",
            "Epoch [9/10], Training Loss: 2.267, Validation Accuracy: 16.23%\n",
            "Epoch [10/10], Training Loss: 2.255, Validation Accuracy: 16.99%\n",
            "Epoch [1/10], Training Loss: 2.247, Validation Accuracy: 17.97%\n",
            "Epoch [2/10], Training Loss: 2.225, Validation Accuracy: 19.50%\n",
            "Epoch [3/10], Training Loss: 2.196, Validation Accuracy: 20.81%\n",
            "Epoch [4/10], Training Loss: 2.165, Validation Accuracy: 22.00%\n",
            "Epoch [5/10], Training Loss: 2.140, Validation Accuracy: 23.25%\n",
            "Epoch [6/10], Training Loss: 2.121, Validation Accuracy: 24.05%\n",
            "Epoch [7/10], Training Loss: 2.103, Validation Accuracy: 24.85%\n",
            "Epoch [8/10], Training Loss: 2.085, Validation Accuracy: 26.23%\n",
            "Epoch [9/10], Training Loss: 2.069, Validation Accuracy: 26.35%\n",
            "Epoch [10/10], Training Loss: 2.053, Validation Accuracy: 27.20%\n",
            "Epoch [1/10], Training Loss: 2.037, Validation Accuracy: 27.86%\n",
            "Epoch [2/10], Training Loss: 2.012, Validation Accuracy: 28.99%\n",
            "Epoch [3/10], Training Loss: 1.990, Validation Accuracy: 29.53%\n",
            "Epoch [4/10], Training Loss: 1.966, Validation Accuracy: 30.15%\n",
            "Epoch [5/10], Training Loss: 1.945, Validation Accuracy: 30.40%\n",
            "Epoch [6/10], Training Loss: 1.926, Validation Accuracy: 30.96%\n",
            "Epoch [7/10], Training Loss: 1.899, Validation Accuracy: 31.20%\n",
            "Epoch [8/10], Training Loss: 1.880, Validation Accuracy: 31.91%\n",
            "Epoch [9/10], Training Loss: 1.860, Validation Accuracy: 33.12%\n",
            "Epoch [10/10], Training Loss: 1.836, Validation Accuracy: 32.13%\n",
            "Epoch [1/10], Training Loss: 1.822, Validation Accuracy: 33.98%\n",
            "Epoch [2/10], Training Loss: 1.799, Validation Accuracy: 34.74%\n",
            "Epoch [3/10], Training Loss: 1.779, Validation Accuracy: 35.68%\n",
            "Epoch [4/10], Training Loss: 1.756, Validation Accuracy: 35.30%\n",
            "Epoch [5/10], Training Loss: 1.749, Validation Accuracy: 36.68%\n",
            "Epoch [6/10], Training Loss: 1.722, Validation Accuracy: 37.59%\n",
            "Epoch [7/10], Training Loss: 1.706, Validation Accuracy: 36.94%\n",
            "Epoch [8/10], Training Loss: 1.686, Validation Accuracy: 37.78%\n",
            "Epoch [9/10], Training Loss: 1.676, Validation Accuracy: 37.58%\n",
            "Epoch [10/10], Training Loss: 1.659, Validation Accuracy: 38.83%\n",
            "Epoch [1/10], Training Loss: 1.682, Validation Accuracy: 38.68%\n",
            "Epoch [2/10], Training Loss: 1.678, Validation Accuracy: 38.65%\n",
            "Epoch [3/10], Training Loss: 1.656, Validation Accuracy: 39.73%\n",
            "Epoch [4/10], Training Loss: 1.639, Validation Accuracy: 39.57%\n",
            "Epoch [5/10], Training Loss: 1.628, Validation Accuracy: 40.25%\n",
            "Epoch [6/10], Training Loss: 1.616, Validation Accuracy: 39.94%\n",
            "Epoch [7/10], Training Loss: 1.600, Validation Accuracy: 40.80%\n",
            "Epoch [8/10], Training Loss: 1.596, Validation Accuracy: 40.33%\n",
            "Epoch [9/10], Training Loss: 1.579, Validation Accuracy: 40.61%\n",
            "Epoch [10/10], Training Loss: 1.569, Validation Accuracy: 41.58%\n",
            "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 42.38%\n",
            "Epoch [2/10], Training Loss: 1.571, Validation Accuracy: 42.63%\n",
            "Epoch [3/10], Training Loss: 1.555, Validation Accuracy: 42.52%\n",
            "Epoch [4/10], Training Loss: 1.544, Validation Accuracy: 43.12%\n",
            "Epoch [5/10], Training Loss: 1.532, Validation Accuracy: 42.77%\n",
            "Epoch [6/10], Training Loss: 1.530, Validation Accuracy: 43.20%\n",
            "Epoch [7/10], Training Loss: 1.522, Validation Accuracy: 43.12%\n",
            "Epoch [8/10], Training Loss: 1.506, Validation Accuracy: 44.00%\n",
            "Epoch [9/10], Training Loss: 1.498, Validation Accuracy: 44.28%\n",
            "Epoch [10/10], Training Loss: 1.484, Validation Accuracy: 44.14%\n",
            "Epoch [1/10], Training Loss: 1.535, Validation Accuracy: 44.21%\n",
            "Epoch [2/10], Training Loss: 1.519, Validation Accuracy: 45.17%\n",
            "Epoch [3/10], Training Loss: 1.514, Validation Accuracy: 44.41%\n",
            "Epoch [4/10], Training Loss: 1.502, Validation Accuracy: 45.62%\n",
            "Epoch [5/10], Training Loss: 1.486, Validation Accuracy: 44.69%\n",
            "Epoch [6/10], Training Loss: 1.485, Validation Accuracy: 45.73%\n",
            "Epoch [7/10], Training Loss: 1.479, Validation Accuracy: 45.51%\n",
            "Epoch [8/10], Training Loss: 1.470, Validation Accuracy: 46.17%\n",
            "Epoch [9/10], Training Loss: 1.463, Validation Accuracy: 46.50%\n",
            "Epoch [10/10], Training Loss: 1.452, Validation Accuracy: 47.02%\n",
            "Epoch [1/10], Training Loss: 1.468, Validation Accuracy: 46.63%\n",
            "Epoch [2/10], Training Loss: 1.451, Validation Accuracy: 47.07%\n",
            "Epoch [3/10], Training Loss: 1.449, Validation Accuracy: 45.33%\n",
            "Epoch [4/10], Training Loss: 1.442, Validation Accuracy: 46.67%\n",
            "Epoch [5/10], Training Loss: 1.436, Validation Accuracy: 47.09%\n",
            "Epoch [6/10], Training Loss: 1.421, Validation Accuracy: 46.71%\n",
            "Epoch [7/10], Training Loss: 1.409, Validation Accuracy: 46.51%\n",
            "Epoch [8/10], Training Loss: 1.413, Validation Accuracy: 47.63%\n",
            "Epoch [9/10], Training Loss: 1.404, Validation Accuracy: 47.40%\n",
            "Epoch [10/10], Training Loss: 1.401, Validation Accuracy: 48.20%\n",
            "Epoch [1/10], Training Loss: 1.411, Validation Accuracy: 48.03%\n",
            "Epoch [2/10], Training Loss: 1.397, Validation Accuracy: 48.47%\n",
            "Epoch [3/10], Training Loss: 1.381, Validation Accuracy: 47.91%\n",
            "Epoch [4/10], Training Loss: 1.371, Validation Accuracy: 48.12%\n",
            "Epoch [5/10], Training Loss: 1.370, Validation Accuracy: 47.96%\n",
            "Epoch [6/10], Training Loss: 1.359, Validation Accuracy: 47.87%\n",
            "Epoch [7/10], Training Loss: 1.354, Validation Accuracy: 47.74%\n",
            "Epoch [8/10], Training Loss: 1.345, Validation Accuracy: 47.72%\n",
            "Epoch [9/10], Training Loss: 1.339, Validation Accuracy: 46.46%\n",
            "Epoch [10/10], Training Loss: 1.337, Validation Accuracy: 48.78%\n",
            "Epoch [1/10], Training Loss: 1.399, Validation Accuracy: 48.96%\n",
            "Epoch [2/10], Training Loss: 1.381, Validation Accuracy: 48.91%\n",
            "Epoch [3/10], Training Loss: 1.370, Validation Accuracy: 48.90%\n",
            "Epoch [4/10], Training Loss: 1.366, Validation Accuracy: 48.57%\n",
            "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 49.47%\n",
            "Epoch [6/10], Training Loss: 1.344, Validation Accuracy: 49.99%\n",
            "Epoch [7/10], Training Loss: 1.329, Validation Accuracy: 50.10%\n",
            "Epoch [8/10], Training Loss: 1.325, Validation Accuracy: 49.88%\n",
            "Epoch [9/10], Training Loss: 1.314, Validation Accuracy: 49.33%\n",
            "Epoch [10/10], Training Loss: 1.315, Validation Accuracy: 49.96%\n",
            "Epoch [1/10], Training Loss: 1.365, Validation Accuracy: 50.60%\n",
            "Epoch [2/10], Training Loss: 1.346, Validation Accuracy: 51.01%\n",
            "Epoch [3/10], Training Loss: 1.332, Validation Accuracy: 51.06%\n",
            "Epoch [4/10], Training Loss: 1.316, Validation Accuracy: 50.49%\n",
            "Epoch [5/10], Training Loss: 1.318, Validation Accuracy: 50.70%\n",
            "Epoch [6/10], Training Loss: 1.312, Validation Accuracy: 51.46%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 50.65%\n",
            "Epoch [8/10], Training Loss: 1.296, Validation Accuracy: 50.12%\n",
            "Epoch [9/10], Training Loss: 1.277, Validation Accuracy: 51.25%\n",
            "Epoch [10/10], Training Loss: 1.286, Validation Accuracy: 50.83%\n",
            "Epoch [1/10], Training Loss: 1.343, Validation Accuracy: 50.40%\n",
            "Epoch [2/10], Training Loss: 1.325, Validation Accuracy: 51.50%\n",
            "Epoch [3/10], Training Loss: 1.310, Validation Accuracy: 49.52%\n",
            "Epoch [4/10], Training Loss: 1.297, Validation Accuracy: 48.86%\n",
            "Epoch [5/10], Training Loss: 1.291, Validation Accuracy: 51.82%\n",
            "Epoch [6/10], Training Loss: 1.274, Validation Accuracy: 51.79%\n",
            "Epoch [7/10], Training Loss: 1.273, Validation Accuracy: 52.08%\n",
            "Epoch [8/10], Training Loss: 1.259, Validation Accuracy: 51.80%\n",
            "Epoch [9/10], Training Loss: 1.256, Validation Accuracy: 51.76%\n",
            "Epoch [10/10], Training Loss: 1.252, Validation Accuracy: 52.18%\n",
            "Epoch [1/10], Training Loss: 1.320, Validation Accuracy: 52.28%\n",
            "Epoch [2/10], Training Loss: 1.285, Validation Accuracy: 52.01%\n",
            "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 51.91%\n",
            "Epoch [4/10], Training Loss: 1.267, Validation Accuracy: 49.96%\n",
            "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 52.47%\n",
            "Epoch [6/10], Training Loss: 1.251, Validation Accuracy: 52.60%\n",
            "Epoch [7/10], Training Loss: 1.226, Validation Accuracy: 52.56%\n",
            "Epoch [8/10], Training Loss: 1.224, Validation Accuracy: 52.46%\n",
            "Epoch [9/10], Training Loss: 1.214, Validation Accuracy: 52.67%\n",
            "Epoch [10/10], Training Loss: 1.208, Validation Accuracy: 52.96%\n",
            "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 52.77%\n",
            "Epoch [2/10], Training Loss: 1.245, Validation Accuracy: 52.32%\n",
            "Epoch [3/10], Training Loss: 1.236, Validation Accuracy: 52.67%\n",
            "Epoch [4/10], Training Loss: 1.218, Validation Accuracy: 51.33%\n",
            "Epoch [5/10], Training Loss: 1.203, Validation Accuracy: 51.99%\n",
            "Epoch [6/10], Training Loss: 1.196, Validation Accuracy: 52.84%\n",
            "Epoch [7/10], Training Loss: 1.188, Validation Accuracy: 53.13%\n",
            "Epoch [8/10], Training Loss: 1.172, Validation Accuracy: 53.78%\n",
            "Epoch [9/10], Training Loss: 1.170, Validation Accuracy: 53.08%\n",
            "Epoch [10/10], Training Loss: 1.162, Validation Accuracy: 53.75%\n",
            "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 54.47%\n",
            "Epoch [2/10], Training Loss: 1.228, Validation Accuracy: 54.18%\n",
            "Epoch [3/10], Training Loss: 1.217, Validation Accuracy: 54.16%\n",
            "Epoch [4/10], Training Loss: 1.208, Validation Accuracy: 53.96%\n",
            "Epoch [5/10], Training Loss: 1.200, Validation Accuracy: 54.41%\n",
            "Epoch [6/10], Training Loss: 1.188, Validation Accuracy: 54.32%\n",
            "Epoch [7/10], Training Loss: 1.164, Validation Accuracy: 54.21%\n",
            "Epoch [8/10], Training Loss: 1.161, Validation Accuracy: 53.22%\n",
            "Epoch [9/10], Training Loss: 1.154, Validation Accuracy: 54.33%\n",
            "Epoch [10/10], Training Loss: 1.146, Validation Accuracy: 54.57%\n",
            "Epoch [1/10], Training Loss: 1.242, Validation Accuracy: 54.89%\n",
            "Epoch [2/10], Training Loss: 1.207, Validation Accuracy: 54.26%\n",
            "Epoch [3/10], Training Loss: 1.196, Validation Accuracy: 54.76%\n",
            "Epoch [4/10], Training Loss: 1.176, Validation Accuracy: 53.44%\n",
            "Epoch [5/10], Training Loss: 1.174, Validation Accuracy: 55.41%\n",
            "Epoch [6/10], Training Loss: 1.165, Validation Accuracy: 55.47%\n",
            "Epoch [7/10], Training Loss: 1.149, Validation Accuracy: 54.07%\n",
            "Epoch [8/10], Training Loss: 1.137, Validation Accuracy: 55.06%\n",
            "Epoch [9/10], Training Loss: 1.132, Validation Accuracy: 55.37%\n",
            "Epoch [10/10], Training Loss: 1.114, Validation Accuracy: 54.68%\n",
            "Epoch [1/10], Training Loss: 1.216, Validation Accuracy: 55.13%\n",
            "Epoch [2/10], Training Loss: 1.194, Validation Accuracy: 55.78%\n",
            "Epoch [3/10], Training Loss: 1.175, Validation Accuracy: 54.68%\n",
            "Epoch [4/10], Training Loss: 1.165, Validation Accuracy: 56.29%\n",
            "Epoch [5/10], Training Loss: 1.138, Validation Accuracy: 56.06%\n",
            "Epoch [6/10], Training Loss: 1.126, Validation Accuracy: 56.28%\n",
            "Epoch [7/10], Training Loss: 1.115, Validation Accuracy: 55.22%\n",
            "Epoch [8/10], Training Loss: 1.120, Validation Accuracy: 54.97%\n",
            "Epoch [9/10], Training Loss: 1.106, Validation Accuracy: 55.87%\n",
            "Epoch [10/10], Training Loss: 1.104, Validation Accuracy: 55.51%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 55.85%\n",
            "Epoch [2/10], Training Loss: 1.178, Validation Accuracy: 53.88%\n",
            "Epoch [3/10], Training Loss: 1.147, Validation Accuracy: 55.91%\n",
            "Epoch [4/10], Training Loss: 1.132, Validation Accuracy: 55.92%\n",
            "Epoch [5/10], Training Loss: 1.117, Validation Accuracy: 55.65%\n",
            "Epoch [6/10], Training Loss: 1.108, Validation Accuracy: 55.89%\n",
            "Epoch [7/10], Training Loss: 1.087, Validation Accuracy: 56.14%\n",
            "Epoch [8/10], Training Loss: 1.082, Validation Accuracy: 55.74%\n",
            "Epoch [9/10], Training Loss: 1.058, Validation Accuracy: 56.46%\n",
            "Epoch [10/10], Training Loss: 1.055, Validation Accuracy: 56.26%\n",
            "Epoch [1/10], Training Loss: 1.155, Validation Accuracy: 56.98%\n",
            "Epoch [2/10], Training Loss: 1.128, Validation Accuracy: 56.48%\n",
            "Epoch [3/10], Training Loss: 1.103, Validation Accuracy: 55.57%\n",
            "Epoch [4/10], Training Loss: 1.095, Validation Accuracy: 56.77%\n",
            "Epoch [5/10], Training Loss: 1.074, Validation Accuracy: 56.64%\n",
            "Epoch [6/10], Training Loss: 1.065, Validation Accuracy: 56.61%\n",
            "Epoch [7/10], Training Loss: 1.051, Validation Accuracy: 55.99%\n",
            "Epoch [8/10], Training Loss: 1.038, Validation Accuracy: 57.03%\n",
            "Epoch [9/10], Training Loss: 1.032, Validation Accuracy: 54.79%\n",
            "Epoch [10/10], Training Loss: 1.023, Validation Accuracy: 55.60%\n",
            "Epoch [1/10], Training Loss: 1.152, Validation Accuracy: 56.11%\n",
            "Epoch [2/10], Training Loss: 1.124, Validation Accuracy: 57.31%\n",
            "Epoch [3/10], Training Loss: 1.094, Validation Accuracy: 57.43%\n",
            "Epoch [4/10], Training Loss: 1.079, Validation Accuracy: 56.87%\n",
            "Epoch [5/10], Training Loss: 1.066, Validation Accuracy: 56.14%\n",
            "Epoch [6/10], Training Loss: 1.051, Validation Accuracy: 57.60%\n",
            "Epoch [7/10], Training Loss: 1.041, Validation Accuracy: 57.12%\n",
            "Epoch [8/10], Training Loss: 1.040, Validation Accuracy: 57.75%\n",
            "Epoch [9/10], Training Loss: 1.015, Validation Accuracy: 57.59%\n",
            "Epoch [10/10], Training Loss: 1.006, Validation Accuracy: 56.76%\n",
            "Epoch [1/10], Training Loss: 1.148, Validation Accuracy: 57.59%\n",
            "Epoch [2/10], Training Loss: 1.104, Validation Accuracy: 57.93%\n",
            "Epoch [3/10], Training Loss: 1.081, Validation Accuracy: 57.80%\n",
            "Epoch [4/10], Training Loss: 1.065, Validation Accuracy: 56.94%\n",
            "Epoch [5/10], Training Loss: 1.054, Validation Accuracy: 58.21%\n",
            "Epoch [6/10], Training Loss: 1.039, Validation Accuracy: 56.60%\n",
            "Epoch [7/10], Training Loss: 1.019, Validation Accuracy: 58.06%\n",
            "Epoch [8/10], Training Loss: 1.009, Validation Accuracy: 57.62%\n",
            "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 56.81%\n",
            "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 57.88%\n",
            "Epoch [1/10], Training Loss: 1.135, Validation Accuracy: 57.12%\n",
            "Epoch [2/10], Training Loss: 1.095, Validation Accuracy: 58.57%\n",
            "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 57.66%\n",
            "Epoch [4/10], Training Loss: 1.043, Validation Accuracy: 58.24%\n",
            "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 57.42%\n",
            "Epoch [6/10], Training Loss: 1.014, Validation Accuracy: 57.94%\n",
            "Epoch [7/10], Training Loss: 1.002, Validation Accuracy: 58.20%\n",
            "Epoch [8/10], Training Loss: 0.988, Validation Accuracy: 58.47%\n",
            "Epoch [9/10], Training Loss: 0.982, Validation Accuracy: 58.34%\n",
            "Epoch [10/10], Training Loss: 0.963, Validation Accuracy: 57.96%\n",
            "Epoch [1/10], Training Loss: 1.090, Validation Accuracy: 57.58%\n",
            "Epoch [2/10], Training Loss: 1.059, Validation Accuracy: 58.54%\n",
            "Epoch [3/10], Training Loss: 1.034, Validation Accuracy: 57.94%\n",
            "Epoch [4/10], Training Loss: 1.013, Validation Accuracy: 57.71%\n",
            "Epoch [5/10], Training Loss: 0.990, Validation Accuracy: 58.90%\n",
            "Epoch [6/10], Training Loss: 0.991, Validation Accuracy: 58.47%\n",
            "Epoch [7/10], Training Loss: 0.969, Validation Accuracy: 58.40%\n",
            "Epoch [8/10], Training Loss: 0.951, Validation Accuracy: 59.24%\n",
            "Epoch [9/10], Training Loss: 0.939, Validation Accuracy: 58.32%\n",
            "Epoch [10/10], Training Loss: 0.935, Validation Accuracy: 57.02%\n",
            "Epoch [1/10], Training Loss: 1.074, Validation Accuracy: 58.80%\n",
            "Epoch [2/10], Training Loss: 1.022, Validation Accuracy: 58.47%\n",
            "Epoch [3/10], Training Loss: 1.014, Validation Accuracy: 59.35%\n",
            "Epoch [4/10], Training Loss: 0.986, Validation Accuracy: 59.18%\n",
            "Epoch [5/10], Training Loss: 0.971, Validation Accuracy: 58.18%\n",
            "Epoch [6/10], Training Loss: 0.958, Validation Accuracy: 59.19%\n",
            "Epoch [7/10], Training Loss: 0.935, Validation Accuracy: 58.81%\n",
            "Epoch [8/10], Training Loss: 0.925, Validation Accuracy: 58.14%\n",
            "Epoch [9/10], Training Loss: 0.917, Validation Accuracy: 59.35%\n",
            "Epoch [10/10], Training Loss: 0.899, Validation Accuracy: 58.40%\n",
            "Epoch [1/10], Training Loss: 1.064, Validation Accuracy: 59.71%\n",
            "Epoch [2/10], Training Loss: 1.017, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 59.25%\n",
            "Epoch [4/10], Training Loss: 0.981, Validation Accuracy: 59.44%\n",
            "Epoch [5/10], Training Loss: 0.952, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.939, Validation Accuracy: 59.44%\n",
            "Epoch [7/10], Training Loss: 0.928, Validation Accuracy: 59.84%\n",
            "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 59.83%\n",
            "Epoch [9/10], Training Loss: 0.910, Validation Accuracy: 59.34%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 59.02%\n",
            "Epoch [1/10], Training Loss: 1.052, Validation Accuracy: 58.65%\n",
            "Epoch [2/10], Training Loss: 1.021, Validation Accuracy: 59.62%\n",
            "Epoch [3/10], Training Loss: 0.987, Validation Accuracy: 59.67%\n",
            "Epoch [4/10], Training Loss: 0.964, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.946, Validation Accuracy: 59.97%\n",
            "Epoch [6/10], Training Loss: 0.939, Validation Accuracy: 60.02%\n",
            "Epoch [7/10], Training Loss: 0.913, Validation Accuracy: 59.79%\n",
            "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 59.72%\n",
            "Epoch [9/10], Training Loss: 0.888, Validation Accuracy: 58.94%\n",
            "Epoch [10/10], Training Loss: 0.873, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 59.71%\n",
            "Epoch [2/10], Training Loss: 1.003, Validation Accuracy: 60.12%\n",
            "Epoch [3/10], Training Loss: 0.983, Validation Accuracy: 59.62%\n",
            "Epoch [4/10], Training Loss: 0.954, Validation Accuracy: 60.07%\n",
            "Epoch [5/10], Training Loss: 0.930, Validation Accuracy: 60.61%\n",
            "Epoch [6/10], Training Loss: 0.917, Validation Accuracy: 59.60%\n",
            "Epoch [7/10], Training Loss: 0.913, Validation Accuracy: 59.27%\n",
            "Epoch [8/10], Training Loss: 0.888, Validation Accuracy: 59.09%\n",
            "Epoch [9/10], Training Loss: 0.876, Validation Accuracy: 59.94%\n",
            "Epoch [10/10], Training Loss: 0.859, Validation Accuracy: 59.99%\n",
            "Epoch [1/10], Training Loss: 1.017, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.967, Validation Accuracy: 59.57%\n",
            "Epoch [3/10], Training Loss: 0.948, Validation Accuracy: 59.34%\n",
            "Epoch [4/10], Training Loss: 0.925, Validation Accuracy: 59.28%\n",
            "Epoch [5/10], Training Loss: 0.912, Validation Accuracy: 60.21%\n",
            "Epoch [6/10], Training Loss: 0.889, Validation Accuracy: 60.25%\n",
            "Epoch [7/10], Training Loss: 0.869, Validation Accuracy: 59.82%\n",
            "Epoch [8/10], Training Loss: 0.855, Validation Accuracy: 60.48%\n",
            "Epoch [9/10], Training Loss: 0.838, Validation Accuracy: 59.53%\n",
            "Epoch [10/10], Training Loss: 0.836, Validation Accuracy: 59.26%\n",
            "Epoch [1/10], Training Loss: 0.989, Validation Accuracy: 59.21%\n",
            "Epoch [2/10], Training Loss: 0.948, Validation Accuracy: 60.17%\n",
            "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 60.52%\n",
            "Epoch [4/10], Training Loss: 0.889, Validation Accuracy: 60.68%\n",
            "Epoch [5/10], Training Loss: 0.868, Validation Accuracy: 59.40%\n",
            "Epoch [6/10], Training Loss: 0.852, Validation Accuracy: 60.66%\n",
            "Epoch [7/10], Training Loss: 0.835, Validation Accuracy: 60.02%\n",
            "Epoch [8/10], Training Loss: 0.831, Validation Accuracy: 60.40%\n",
            "Epoch [9/10], Training Loss: 0.809, Validation Accuracy: 59.49%\n",
            "Epoch [10/10], Training Loss: 0.801, Validation Accuracy: 60.84%\n",
            "Epoch [1/10], Training Loss: 0.997, Validation Accuracy: 61.15%\n",
            "Epoch [2/10], Training Loss: 0.955, Validation Accuracy: 60.30%\n",
            "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 61.07%\n",
            "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 60.72%\n",
            "Epoch [5/10], Training Loss: 0.868, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.846, Validation Accuracy: 60.93%\n",
            "Epoch [7/10], Training Loss: 0.829, Validation Accuracy: 60.84%\n",
            "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 60.28%\n",
            "Epoch [9/10], Training Loss: 0.815, Validation Accuracy: 60.97%\n",
            "Epoch [10/10], Training Loss: 0.781, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.918, Validation Accuracy: 61.18%\n",
            "Epoch [3/10], Training Loss: 0.897, Validation Accuracy: 60.88%\n",
            "Epoch [4/10], Training Loss: 0.871, Validation Accuracy: 60.85%\n",
            "Epoch [5/10], Training Loss: 0.850, Validation Accuracy: 60.66%\n",
            "Epoch [6/10], Training Loss: 0.834, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.812, Validation Accuracy: 60.66%\n",
            "Epoch [8/10], Training Loss: 0.799, Validation Accuracy: 59.87%\n",
            "Epoch [9/10], Training Loss: 0.782, Validation Accuracy: 60.74%\n",
            "Epoch [10/10], Training Loss: 0.766, Validation Accuracy: 60.37%\n",
            "Epoch [1/10], Training Loss: 0.990, Validation Accuracy: 61.31%\n",
            "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 61.08%\n",
            "Epoch [3/10], Training Loss: 0.890, Validation Accuracy: 61.58%\n",
            "Epoch [4/10], Training Loss: 0.871, Validation Accuracy: 61.35%\n",
            "Epoch [5/10], Training Loss: 0.846, Validation Accuracy: 61.11%\n",
            "Epoch [6/10], Training Loss: 0.830, Validation Accuracy: 61.21%\n",
            "Epoch [7/10], Training Loss: 0.809, Validation Accuracy: 61.58%\n",
            "Epoch [8/10], Training Loss: 0.801, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.791, Validation Accuracy: 60.67%\n",
            "Epoch [10/10], Training Loss: 0.770, Validation Accuracy: 60.11%\n",
            "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 61.04%\n",
            "Epoch [2/10], Training Loss: 0.886, Validation Accuracy: 60.98%\n",
            "Epoch [3/10], Training Loss: 0.859, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 61.03%\n",
            "Epoch [5/10], Training Loss: 0.817, Validation Accuracy: 60.81%\n",
            "Epoch [6/10], Training Loss: 0.800, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 60.96%\n",
            "Epoch [8/10], Training Loss: 0.755, Validation Accuracy: 60.41%\n",
            "Epoch [9/10], Training Loss: 0.745, Validation Accuracy: 60.88%\n",
            "Epoch [10/10], Training Loss: 0.741, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 61.73%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 61.07%\n",
            "Epoch [3/10], Training Loss: 0.834, Validation Accuracy: 61.54%\n",
            "Epoch [4/10], Training Loss: 0.816, Validation Accuracy: 60.76%\n",
            "Epoch [5/10], Training Loss: 0.784, Validation Accuracy: 61.72%\n",
            "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 60.23%\n",
            "Epoch [7/10], Training Loss: 0.759, Validation Accuracy: 61.38%\n",
            "Epoch [8/10], Training Loss: 0.739, Validation Accuracy: 61.15%\n",
            "Epoch [9/10], Training Loss: 0.717, Validation Accuracy: 60.91%\n",
            "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 60.63%\n",
            "Epoch [1/10], Training Loss: 0.944, Validation Accuracy: 60.99%\n",
            "Epoch [2/10], Training Loss: 0.863, Validation Accuracy: 61.28%\n",
            "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 61.11%\n",
            "Epoch [4/10], Training Loss: 0.797, Validation Accuracy: 61.52%\n",
            "Epoch [5/10], Training Loss: 0.779, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 62.08%\n",
            "Epoch [7/10], Training Loss: 0.732, Validation Accuracy: 61.68%\n",
            "Epoch [8/10], Training Loss: 0.719, Validation Accuracy: 61.63%\n",
            "Epoch [9/10], Training Loss: 0.698, Validation Accuracy: 61.35%\n",
            "Epoch [10/10], Training Loss: 0.680, Validation Accuracy: 61.99%\n",
            "Epoch [1/10], Training Loss: 0.924, Validation Accuracy: 61.83%\n",
            "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 62.43%\n",
            "Epoch [3/10], Training Loss: 0.813, Validation Accuracy: 61.83%\n",
            "Epoch [4/10], Training Loss: 0.780, Validation Accuracy: 61.57%\n",
            "Epoch [5/10], Training Loss: 0.752, Validation Accuracy: 61.58%\n",
            "Epoch [6/10], Training Loss: 0.736, Validation Accuracy: 61.64%\n",
            "Epoch [7/10], Training Loss: 0.716, Validation Accuracy: 61.25%\n",
            "Epoch [8/10], Training Loss: 0.702, Validation Accuracy: 61.75%\n",
            "Epoch [9/10], Training Loss: 0.682, Validation Accuracy: 61.02%\n",
            "Epoch [10/10], Training Loss: 0.669, Validation Accuracy: 61.32%\n",
            "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 61.85%\n",
            "Epoch [2/10], Training Loss: 0.862, Validation Accuracy: 61.82%\n",
            "Epoch [3/10], Training Loss: 0.819, Validation Accuracy: 61.75%\n",
            "Epoch [4/10], Training Loss: 0.800, Validation Accuracy: 61.92%\n",
            "Epoch [5/10], Training Loss: 0.766, Validation Accuracy: 62.14%\n",
            "Epoch [6/10], Training Loss: 0.739, Validation Accuracy: 61.78%\n",
            "Epoch [7/10], Training Loss: 0.720, Validation Accuracy: 61.71%\n",
            "Epoch [8/10], Training Loss: 0.705, Validation Accuracy: 62.10%\n",
            "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 61.36%\n",
            "Epoch [10/10], Training Loss: 0.669, Validation Accuracy: 61.33%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 62.32%\n",
            "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 62.50%\n",
            "Epoch [3/10], Training Loss: 0.786, Validation Accuracy: 62.31%\n",
            "Epoch [4/10], Training Loss: 0.762, Validation Accuracy: 61.92%\n",
            "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 61.95%\n",
            "Epoch [6/10], Training Loss: 0.714, Validation Accuracy: 61.78%\n",
            "Epoch [7/10], Training Loss: 0.691, Validation Accuracy: 62.06%\n",
            "Epoch [8/10], Training Loss: 0.672, Validation Accuracy: 61.91%\n",
            "Epoch [9/10], Training Loss: 0.654, Validation Accuracy: 62.14%\n",
            "Epoch [10/10], Training Loss: 0.643, Validation Accuracy: 61.90%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.78%\n",
            "Epoch [2/10], Training Loss: 0.800, Validation Accuracy: 61.71%\n",
            "Epoch [3/10], Training Loss: 0.766, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.731, Validation Accuracy: 62.01%\n",
            "Epoch [5/10], Training Loss: 0.696, Validation Accuracy: 61.31%\n",
            "Epoch [6/10], Training Loss: 0.681, Validation Accuracy: 61.79%\n",
            "Epoch [7/10], Training Loss: 0.668, Validation Accuracy: 61.65%\n",
            "Epoch [8/10], Training Loss: 0.639, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.634, Validation Accuracy: 62.09%\n",
            "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 61.76%\n",
            "Epoch [1/10], Training Loss: 0.875, Validation Accuracy: 61.96%\n",
            "Epoch [2/10], Training Loss: 0.796, Validation Accuracy: 62.63%\n",
            "Epoch [3/10], Training Loss: 0.744, Validation Accuracy: 62.15%\n",
            "Epoch [4/10], Training Loss: 0.713, Validation Accuracy: 62.15%\n",
            "Epoch [5/10], Training Loss: 0.686, Validation Accuracy: 61.52%\n",
            "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 62.35%\n",
            "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 61.97%\n",
            "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 61.47%\n",
            "Epoch [9/10], Training Loss: 0.610, Validation Accuracy: 61.14%\n",
            "Epoch [10/10], Training Loss: 0.591, Validation Accuracy: 62.18%\n",
            "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 61.64%\n",
            "Epoch [2/10], Training Loss: 0.793, Validation Accuracy: 61.46%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 62.05%\n",
            "Epoch [4/10], Training Loss: 0.704, Validation Accuracy: 61.12%\n",
            "Epoch [5/10], Training Loss: 0.671, Validation Accuracy: 62.08%\n",
            "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 61.21%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 61.75%\n",
            "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 61.92%\n",
            "Epoch [9/10], Training Loss: 0.590, Validation Accuracy: 61.44%\n",
            "Epoch [10/10], Training Loss: 0.579, Validation Accuracy: 61.84%\n",
            "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.799, Validation Accuracy: 61.50%\n",
            "Epoch [3/10], Training Loss: 0.755, Validation Accuracy: 60.50%\n",
            "Epoch [4/10], Training Loss: 0.720, Validation Accuracy: 62.24%\n",
            "Epoch [5/10], Training Loss: 0.692, Validation Accuracy: 62.26%\n",
            "Epoch [6/10], Training Loss: 0.661, Validation Accuracy: 61.50%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 62.29%\n",
            "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 61.53%\n",
            "Epoch [10/10], Training Loss: 0.585, Validation Accuracy: 60.97%\n",
            "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.32%\n",
            "Epoch [2/10], Training Loss: 0.789, Validation Accuracy: 62.19%\n",
            "Epoch [3/10], Training Loss: 0.727, Validation Accuracy: 62.52%\n",
            "Epoch [4/10], Training Loss: 0.689, Validation Accuracy: 62.00%\n",
            "Epoch [5/10], Training Loss: 0.668, Validation Accuracy: 62.40%\n",
            "Epoch [6/10], Training Loss: 0.633, Validation Accuracy: 61.14%\n",
            "Epoch [7/10], Training Loss: 0.617, Validation Accuracy: 62.11%\n",
            "Epoch [8/10], Training Loss: 0.589, Validation Accuracy: 62.37%\n",
            "Epoch [9/10], Training Loss: 0.575, Validation Accuracy: 61.95%\n",
            "Epoch [10/10], Training Loss: 0.560, Validation Accuracy: 61.55%\n",
            "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 60.91%\n",
            "Epoch [2/10], Training Loss: 0.749, Validation Accuracy: 61.63%\n",
            "Epoch [3/10], Training Loss: 0.696, Validation Accuracy: 62.18%\n",
            "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 62.26%\n",
            "Epoch [5/10], Training Loss: 0.628, Validation Accuracy: 62.18%\n",
            "Epoch [6/10], Training Loss: 0.605, Validation Accuracy: 62.34%\n",
            "Epoch [7/10], Training Loss: 0.580, Validation Accuracy: 62.26%\n",
            "Epoch [8/10], Training Loss: 0.572, Validation Accuracy: 61.54%\n",
            "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.77%\n",
            "Epoch [10/10], Training Loss: 0.523, Validation Accuracy: 61.50%\n",
            "Epoch [1/10], Training Loss: 0.835, Validation Accuracy: 61.68%\n",
            "Epoch [2/10], Training Loss: 0.738, Validation Accuracy: 61.20%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 62.71%\n",
            "Epoch [4/10], Training Loss: 0.649, Validation Accuracy: 62.25%\n",
            "Epoch [5/10], Training Loss: 0.614, Validation Accuracy: 62.50%\n",
            "Epoch [6/10], Training Loss: 0.589, Validation Accuracy: 61.87%\n",
            "Epoch [7/10], Training Loss: 0.564, Validation Accuracy: 61.20%\n",
            "Epoch [8/10], Training Loss: 0.551, Validation Accuracy: 62.37%\n",
            "Epoch [9/10], Training Loss: 0.520, Validation Accuracy: 62.05%\n",
            "Epoch [10/10], Training Loss: 0.493, Validation Accuracy: 61.75%\n",
            "Epoch [1/10], Training Loss: 0.826, Validation Accuracy: 61.36%\n",
            "Epoch [2/10], Training Loss: 0.718, Validation Accuracy: 62.26%\n",
            "Epoch [3/10], Training Loss: 0.657, Validation Accuracy: 61.83%\n",
            "Epoch [4/10], Training Loss: 0.628, Validation Accuracy: 61.82%\n",
            "Epoch [5/10], Training Loss: 0.599, Validation Accuracy: 61.34%\n",
            "Epoch [6/10], Training Loss: 0.566, Validation Accuracy: 61.44%\n",
            "Epoch [7/10], Training Loss: 0.546, Validation Accuracy: 61.52%\n",
            "Epoch [8/10], Training Loss: 0.521, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.508, Validation Accuracy: 61.44%\n",
            "Epoch [10/10], Training Loss: 0.490, Validation Accuracy: 61.01%\n",
            "Epoch [1/10], Training Loss: 0.876, Validation Accuracy: 61.99%\n",
            "Epoch [2/10], Training Loss: 0.761, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.685, Validation Accuracy: 62.16%\n",
            "Epoch [4/10], Training Loss: 0.636, Validation Accuracy: 62.17%\n",
            "Epoch [5/10], Training Loss: 0.608, Validation Accuracy: 62.23%\n",
            "Epoch [6/10], Training Loss: 0.582, Validation Accuracy: 61.90%\n",
            "Epoch [7/10], Training Loss: 0.560, Validation Accuracy: 61.44%\n",
            "Epoch [8/10], Training Loss: 0.536, Validation Accuracy: 61.30%\n",
            "Epoch [9/10], Training Loss: 0.517, Validation Accuracy: 60.95%\n",
            "Epoch [10/10], Training Loss: 0.503, Validation Accuracy: 61.60%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 61.70%\n",
            "Epoch [2/10], Training Loss: 0.727, Validation Accuracy: 62.39%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.82%\n",
            "Epoch [4/10], Training Loss: 0.625, Validation Accuracy: 61.58%\n",
            "Epoch [5/10], Training Loss: 0.592, Validation Accuracy: 62.20%\n",
            "Epoch [6/10], Training Loss: 0.564, Validation Accuracy: 61.99%\n",
            "Epoch [7/10], Training Loss: 0.533, Validation Accuracy: 62.08%\n",
            "Epoch [8/10], Training Loss: 0.515, Validation Accuracy: 61.95%\n",
            "Epoch [9/10], Training Loss: 0.493, Validation Accuracy: 61.89%\n",
            "Epoch [10/10], Training Loss: 0.482, Validation Accuracy: 61.84%\n",
            "Epoch [1/10], Training Loss: 0.804, Validation Accuracy: 61.39%\n",
            "Epoch [2/10], Training Loss: 0.690, Validation Accuracy: 62.05%\n",
            "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 61.90%\n",
            "Epoch [4/10], Training Loss: 0.590, Validation Accuracy: 61.34%\n",
            "Epoch [5/10], Training Loss: 0.556, Validation Accuracy: 62.29%\n",
            "Epoch [6/10], Training Loss: 0.525, Validation Accuracy: 61.83%\n",
            "Epoch [7/10], Training Loss: 0.503, Validation Accuracy: 61.79%\n",
            "Epoch [8/10], Training Loss: 0.492, Validation Accuracy: 61.70%\n",
            "Epoch [9/10], Training Loss: 0.464, Validation Accuracy: 61.96%\n",
            "Epoch [10/10], Training Loss: 0.444, Validation Accuracy: 61.71%\n",
            "Confusion Matrix:\n",
            "[[697  28  78  23  34   5  11  14  62  48]\n",
            " [ 39 710  30  14   8  13  17  16  43 110]\n",
            " [ 76  14 536  93  94  81  53  24  18  11]\n",
            " [ 27   7 102 457  62 205  65  39  15  21]\n",
            " [ 32  10 154  90 492  58  55  86  10  13]\n",
            " [ 19   5 100 183  43 544  34  59   5   8]\n",
            " [  9  10  87  83  54  49 679  11   7  11]\n",
            " [ 14  12  54  63  55  90  10 670   5  27]\n",
            " [108  54  24  19  13  14   3  10 712  43]\n",
            " [ 38 104  19  40  10  16  15  25  38 695]]\n",
            "Test Accuracy: 61.92%\n",
            "True Positives (TP): [697 710 536 457 492 544 679 670 712 695]\n",
            "False Positives (FP): [362 244 648 608 373 531 263 284 203 292]\n",
            "True Negatives (TN): [8638 8756 8352 8392 8627 8469 8737 8716 8797 8708]\n",
            "False Negatives (FN): [303 290 464 543 508 456 321 330 288 305]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.65816808 0.7442348  0.4527027  0.42910798 0.56878613 0.50604651\n",
            " 0.72080679 0.70230608 0.77814208 0.704154  ]\n",
            "Recall: [0.697 0.71  0.536 0.457 0.492 0.544 0.679 0.67  0.712 0.695]\n",
            "F1 Score: [0.67702768 0.72671443 0.49084249 0.44261501 0.52761394 0.52433735\n",
            " 0.69927909 0.68577277 0.74360313 0.69954706]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform1 = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "    mean2 = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std2 = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform2 = torch.FloatTensor(64, vae.z_dim).uniform_(mean2 - std2, mean2 + std2)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_uniform1 + augmented_data_uniform2) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"], other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your data\n",
        "data = [\n",
        "    2.306, 2.305, 2.304, 2.303, 2.303, 2.302, 2.302, 2.301, 2.301, 2.300,\n",
        "    2.299, 2.298, 2.296, 2.294, 2.292, 2.288, 2.283, 2.276, 2.267, 2.255,\n",
        "    2.247, 2.225, 2.196, 2.165, 2.140, 2.121, 2.103, 2.085, 2.069, 2.053,\n",
        "    2.037, 2.012, 1.990, 1.966, 1.945, 1.926, 1.899, 1.880, 1.860, 1.836,\n",
        "    1.822, 1.799, 1.779, 1.756, 1.749, 1.722, 1.706, 1.686, 1.676, 1.659,\n",
        "    1.682, 1.678, 1.656, 1.639, 1.628, 1.616, 1.600, 1.596, 1.579, 1.569,\n",
        "    1.585, 1.571, 1.555, 1.544, 1.532, 1.530, 1.522, 1.506, 1.498, 1.484,\n",
        "    1.535, 1.519, 1.514, 1.502, 1.486, 1.485, 1.479, 1.470, 1.463, 1.452,\n",
        "    1.468, 1.451, 1.449, 1.442, 1.436, 1.421, 1.409, 1.413, 1.404, 1.401,\n",
        "    1.411, 1.397, 1.381, 1.371, 1.370, 1.359, 1.354, 1.345, 1.339, 1.337,\n",
        "    1.399, 1.381, 1.370, 1.366, 1.352, 1.344, 1.329, 1.325, 1.314, 1.315,\n",
        "    1.365, 1.346, 1.332, 1.316, 1.318, 1.312, 1.296, 1.296, 1.277, 1.286,\n",
        "    1.343, 1.325, 1.310, 1.297, 1.291, 1.274, 1.273, 1.259, 1.256, 1.252,\n",
        "    1.320, 1.285, 1.275, 1.267, 1.257, 1.251, 1.226, 1.224, 1.214, 1.208,\n",
        "    1.259, 1.245, 1.236, 1.218, 1.203, 1.196, 1.188, 1.172, 1.170, 1.162,\n",
        "    1.259, 1.228, 1.217, 1.208, 1.200, 1.188, 1.164, 1.161, 1.154, 1.146,\n",
        "    1.242, 1.207, 1.196, 1.176, 1.174, 1.165, 1.149, 1.137, 1.132, 1.114,\n",
        "    1.216, 1.194, 1.175, 1.165, 1.138, 1.126, 1.115, 1.120, 1.106, 1.104,\n",
        "    1.193, 1.178, 1.147, 1.132, 1.117, 1.108, 1.087, 1.082, 1.058, 1.055,\n",
        "    1.155, 1.128, 1.103, 1.095, 1.074, 1.065, 1.051, 1.038, 1.032, 1.023,\n",
        "    1.152, 1.124, 1.094, 1.079, 1.066, 1.051, 1.041, 1.040, 1.015, 1.006,\n",
        "    1.148, 1.104, 1.081, 1.065, 1.054, 1.039, 1.019, 1.009, 0.998, 0.983,\n",
        "    1.135, 1.095, 1.063, 1.043, 1.021, 1.014, 1.002, 0.988, 0.982, 0.963,\n",
        "    1.090, 1.059, 1.034, 1.013, 0.990, 0.991, 0.969, 0.951, 0.939, 0.935\n",
        "]\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(data, marker='', linestyle='-', color='b')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Line Graph of Data Points')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Value')\n",
        "\n",
        "# Display the grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KPo5SBELXwUr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "25236508-d906-4ee2-b024-f8626528bcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ08XWKsdQyR",
        "outputId": "545b91d5-8718-4cb0-aa51-8443d53af175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extracting training loss values\n",
        "loss_values = [\n",
        "    2.306, 2.305, 2.304, 2.303, 2.303, 2.302, 2.302, 2.301, 2.301, 2.300,\n",
        " 2.299, 2.298, 2.296, 2.294, 2.292, 2.288, 2.283, 2.276, 2.267, 2.255,\n",
        " 2.247, 2.225, 2.196, 2.165, 2.140, 2.121, 2.103, 2.085, 2.069, 2.053,\n",
        " 2.037, 2.012, 1.990, 1.966, 1.945, 1.926, 1.899, 1.880, 1.860, 1.836,\n",
        " 1.822, 1.799, 1.779, 1.756, 1.749, 1.722, 1.706, 1.686, 1.676, 1.659,\n",
        " 1.682, 1.678, 1.656, 1.639, 1.628, 1.616, 1.600, 1.596, 1.579, 1.569,\n",
        " 1.585, 1.571, 1.555, 1.544, 1.532, 1.530, 1.522, 1.506, 1.498, 1.484,\n",
        " 1.535, 1.519, 1.514, 1.502, 1.486, 1.485, 1.479, 1.470, 1.463, 1.452,\n",
        " 1.468, 1.451, 1.449, 1.442, 1.436, 1.421, 1.409, 1.413, 1.404, 1.401,\n",
        " 1.411, 1.397, 1.381, 1.371, 1.370, 1.359, 1.354, 1.345, 1.339, 1.337,\n",
        " 1.399, 1.381, 1.370, 1.366, 1.352, 1.344, 1.329, 1.325, 1.314, 1.315,\n",
        "    1.365, 1.346, 1.332, 1.316, 1.318, 1.312, 1.296, 1.296, 1.277, 1.286,\n",
        "    1.343, 1.325, 1.310, 1.297, 1.291, 1.274, 1.273, 1.259, 1.256, 1.252,\n",
        "    1.320, 1.285, 1.275, 1.267, 1.257, 1.251, 1.226, 1.224, 1.214, 1.208,\n",
        "    1.259, 1.245, 1.236, 1.218, 1.203, 1.196, 1.188, 1.172, 1.170, 1.162,\n",
        "    1.259, 1.228, 1.217, 1.208, 1.200, 1.188, 1.164, 1.161, 1.154, 1.146,\n",
        "    1.242, 1.207, 1.196, 1.176, 1.174, 1.165, 1.149, 1.137, 1.132, 1.114,\n",
        "    1.216, 1.194, 1.175, 1.165, 1.138, 1.126, 1.115, 1.120, 1.106, 1.104,\n",
        "    1.193, 1.178, 1.147, 1.132, 1.117, 1.108, 1.087, 1.082, 1.058, 1.055,\n",
        "    1.155, 1.128, 1.103, 1.095, 1.074, 1.065, 1.051, 1.038, 1.032, 1.023,\n",
        "    1.152, 1.124, 1.094, 1.079, 1.066, 1.051, 1.041, 1.040, 1.015, 1.006,\n",
        "    1.148, 1.104, 1.081, 1.065, 1.054, 1.039, 1.019, 1.009, 0.998, 0.983,\n",
        "    1.135, 1.095, 1.063, 1.043, 1.021, 1.014, 1.002, 0.988, 0.982, 0.963,\n",
        "\n",
        "\n",
        " 1.365, 1.346, 1.332, 1.316, 1.318, 1.312, 1.296, 1.296, 1.277, 1.286,\n",
        "    1.343, 1.325, 1.310, 1.297, 1.291, 1.274, 1.273, 1.259, 1.256, 1.252,\n",
        "    1.320, 1.285, 1.275, 1.267, 1.257, 1.251, 1.226, 1.224, 1.214, 1.208,\n",
        "    1.259, 1.245, 1.236, 1.218, 1.203, 1.196, 1.188, 1.172, 1.170, 1.162,\n",
        "    1.259, 1.228, 1.217, 1.208, 1.200, 1.188, 1.164, 1.161, 1.154, 1.146,\n",
        "    1.242, 1.207, 1.196, 1.176, 1.174, 1.165, 1.149, 1.137, 1.132, 1.114,\n",
        "    1.216, 1.194, 1.175, 1.165, 1.138, 1.126, 1.115, 1.120, 1.106, 1.104,\n",
        "    1.193, 1.178, 1.147, 1.132, 1.117, 1.108, 1.087, 1.082, 1.058, 1.055,\n",
        "    1.155, 1.128, 1.103, 1.095, 1.074, 1.065, 1.051, 1.038, 1.032, 1.023,\n",
        "    1.152, 1.124, 1.094, 1.079, 1.066, 1.051, 1.041, 1.040, 1.015, 1.006,\n",
        "    1.148, 1.104, 1.081, 1.065, 1.054, 1.039, 1.019, 1.009, 0.998, 0.983,\n",
        "    1.135, 1.095, 1.063, 1.043, 1.021, 1.014, 1.002, 0.988, 0.982, 0.963,\n",
        "    1.090, 1.059, 1.034, 1.013, 0.990, 0.991, 0.969, 0.951, 0.939, 0.935,\n",
        "    1.074, 1.022, 1.014, 0.986, 0.971, 0.958, 0.935, 0.925, 0.917, 0.899,\n",
        "\n",
        "\n",
        "     1.064, 1.017, 0.994, 0.981, 0.952, 0.939, 0.928, 0.919, 0.910, 0.889,\n",
        "    1.052, 1.021, 0.987, 0.964, 0.946, 0.939, 0.913, 0.899, 0.888, 0.873,\n",
        "    1.047, 1.003, 0.983, 0.954, 0.930, 0.917, 0.913, 0.888, 0.876, 0.859,\n",
        "    1.017, 0.967, 0.948, 0.925, 0.912, 0.889, 0.869, 0.855, 0.838, 0.836,\n",
        "    0.989, 0.948, 0.914, 0.889, 0.868, 0.852, 0.835, 0.831, 0.809, 0.801,\n",
        "    0.997, 0.955, 0.914, 0.883, 0.868, 0.846, 0.829, 0.810, 0.815, 0.781,\n",
        "    0.986, 0.918, 0.897, 0.871, 0.850, 0.834, 0.812, 0.799, 0.782, 0.766,\n",
        "    0.990, 0.938, 0.890, 0.871, 0.846, 0.830, 0.809, 0.801, 0.791, 0.770,\n",
        "    0.970, 0.886, 0.859, 0.830, 0.817, 0.800, 0.784, 0.755, 0.745, 0.741,\n",
        "    0.930, 0.873, 0.834, 0.816, 0.784, 0.761, 0.759, 0.739, 0.717, 0.702,\n",
        "    0.944, 0.863, 0.828, 0.797, 0.779, 0.758, 0.732, 0.719, 0.698, 0.680,\n",
        "    0.924, 0.857, 0.813, 0.780, 0.752, 0.736, 0.716, 0.702, 0.682, 0.669,\n",
        "    0.935, 0.862, 0.819, 0.800, 0.766, 0.739, 0.720, 0.705, 0.692, 0.669,\n",
        "    0.908, 0.832, 0.786, 0.762, 0.739, 0.714, 0.691, 0.672, 0.654, 0.643,\n",
        "    0.880, 0.800, 0.766, 0.731, 0.696, 0.681, 0.668, 0.639, 0.634, 0.608,\n",
        "    0.875, 0.796, 0.744, 0.713, 0.686, 0.666, 0.641, 0.618, 0.610, 0.591,\n",
        "    0.872, 0.793, 0.743, 0.704, 0.671, 0.647, 0.640, 0.606, 0.590, 0.579,\n",
        "    0.905, 0.799, 0.755, 0.720, 0.692, 0.661, 0.640, 0.612, 0.611, 0.585,\n",
        "    0.880, 0.789, 0.727, 0.689, 0.668, 0.633, 0.617, 0.589, 0.575, 0.560,\n",
        "    0.842, 0.749, 0.696, 0.658, 0.628, 0.605, 0.580, 0.572, 0.557, 0.523,\n",
        "    0.835, 0.738, 0.691, 0.649, 0.614, 0.589, 0.564, 0.551, 0.520, 0.493,\n",
        "    0.826, 0.718, 0.657, 0.628, 0.599, 0.566, 0.546, 0.521, 0.508, 0.490,\n",
        "    0.876, 0.761, 0.685, 0.636, 0.608, 0.582, 0.560, 0.536, 0.517, 0.503,\n",
        "    0.830, 0.727, 0.660, 0.625, 0.592, 0.564, 0.533, 0.515, 0.493, 0.482,\n",
        "    0.804, 0.690, 0.635, 0.590, 0.556, 0.525, 0.503, 0.492, 0.464, 0.444\n",
        "\n",
        "]\n",
        "\n",
        "# Epochs (assuming 100 epochs based on the number of values)\n",
        "epochs = list(range(1, len(loss_values) + 1))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, loss_values, marker='o', linestyle='-', color='b')\n",
        "plt.title('Training Loss by Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.grid(True)\n",
        "plt.xticks(range(0, 101, 10))  # Set x-ticks to every 10 epochs\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.306, Validation Accuracy: 10.49%\n",
        "Epoch [2/10], Training Loss: 2.305, Validation Accuracy: 10.52%\n",
        "Epoch [3/10], Training Loss: 2.304, Validation Accuracy: 10.44%\n",
        "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 10.51%\n",
        "Epoch [5/10], Training Loss: 2.303, Validation Accuracy: 10.61%\n",
        "Epoch [6/10], Training Loss: 2.302, Validation Accuracy: 10.67%\n",
        "Epoch [7/10], Training Loss: 2.302, Validation Accuracy: 10.97%\n",
        "Epoch [8/10], Training Loss: 2.301, Validation Accuracy: 11.34%\n",
        "Epoch [9/10], Training Loss: 2.301, Validation Accuracy: 11.72%\n",
        "Epoch [10/10], Training Loss: 2.300, Validation Accuracy: 12.03%\n",
        "Epoch [1/10], Training Loss: 2.299, Validation Accuracy: 11.57%\n",
        "Epoch [2/10], Training Loss: 2.298, Validation Accuracy: 12.37%\n",
        "Epoch [3/10], Training Loss: 2.296, Validation Accuracy: 12.71%\n",
        "Epoch [4/10], Training Loss: 2.294, Validation Accuracy: 13.19%\n",
        "Epoch [5/10], Training Loss: 2.292, Validation Accuracy: 12.54%\n",
        "Epoch [6/10], Training Loss: 2.288, Validation Accuracy: 12.56%\n",
        "Epoch [7/10], Training Loss: 2.283, Validation Accuracy: 12.75%\n",
        "Epoch [8/10], Training Loss: 2.276, Validation Accuracy: 14.95%\n",
        "Epoch [9/10], Training Loss: 2.267, Validation Accuracy: 16.23%\n",
        "Epoch [10/10], Training Loss: 2.255, Validation Accuracy: 16.99%\n",
        "Epoch [1/10], Training Loss: 2.247, Validation Accuracy: 17.97%\n",
        "Epoch [2/10], Training Loss: 2.225, Validation Accuracy: 19.50%\n",
        "Epoch [3/10], Training Loss: 2.196, Validation Accuracy: 20.81%\n",
        "Epoch [4/10], Training Loss: 2.165, Validation Accuracy: 22.00%\n",
        "Epoch [5/10], Training Loss: 2.140, Validation Accuracy: 23.25%\n",
        "Epoch [6/10], Training Loss: 2.121, Validation Accuracy: 24.05%\n",
        "Epoch [7/10], Training Loss: 2.103, Validation Accuracy: 24.85%\n",
        "Epoch [8/10], Training Loss: 2.085, Validation Accuracy: 26.23%\n",
        "Epoch [9/10], Training Loss: 2.069, Validation Accuracy: 26.35%\n",
        "Epoch [10/10], Training Loss: 2.053, Validation Accuracy: 27.20%\n",
        "Epoch [1/10], Training Loss: 2.037, Validation Accuracy: 27.86%\n",
        "Epoch [2/10], Training Loss: 2.012, Validation Accuracy: 28.99%\n",
        "Epoch [3/10], Training Loss: 1.990, Validation Accuracy: 29.53%\n",
        "Epoch [4/10], Training Loss: 1.966, Validation Accuracy: 30.15%\n",
        "Epoch [5/10], Training Loss: 1.945, Validation Accuracy: 30.40%\n",
        "Epoch [6/10], Training Loss: 1.926, Validation Accuracy: 30.96%\n",
        "Epoch [7/10], Training Loss: 1.899, Validation Accuracy: 31.20%\n",
        "Epoch [8/10], Training Loss: 1.880, Validation Accuracy: 31.91%\n",
        "Epoch [9/10], Training Loss: 1.860, Validation Accuracy: 33.12%\n",
        "Epoch [10/10], Training Loss: 1.836, Validation Accuracy: 32.13%\n",
        "Epoch [1/10], Training Loss: 1.822, Validation Accuracy: 33.98%\n",
        "Epoch [2/10], Training Loss: 1.799, Validation Accuracy: 34.74%\n",
        "Epoch [3/10], Training Loss: 1.779, Validation Accuracy: 35.68%\n",
        "Epoch [4/10], Training Loss: 1.756, Validation Accuracy: 35.30%\n",
        "Epoch [5/10], Training Loss: 1.749, Validation Accuracy: 36.68%\n",
        "Epoch [6/10], Training Loss: 1.722, Validation Accuracy: 37.59%\n",
        "Epoch [7/10], Training Loss: 1.706, Validation Accuracy: 36.94%\n",
        "Epoch [8/10], Training Loss: 1.686, Validation Accuracy: 37.78%\n",
        "Epoch [9/10], Training Loss: 1.676, Validation Accuracy: 37.58%\n",
        "Epoch [10/10], Training Loss: 1.659, Validation Accuracy: 38.83%\n",
        "Epoch [1/10], Training Loss: 1.682, Validation Accuracy: 38.68%\n",
        "Epoch [2/10], Training Loss: 1.678, Validation Accuracy: 38.65%\n",
        "Epoch [3/10], Training Loss: 1.656, Validation Accuracy: 39.73%\n",
        "Epoch [4/10], Training Loss: 1.639, Validation Accuracy: 39.57%\n",
        "Epoch [5/10], Training Loss: 1.628, Validation Accuracy: 40.25%\n",
        "Epoch [6/10], Training Loss: 1.616, Validation Accuracy: 39.94%\n",
        "Epoch [7/10], Training Loss: 1.600, Validation Accuracy: 40.80%\n",
        "Epoch [8/10], Training Loss: 1.596, Validation Accuracy: 40.33%\n",
        "Epoch [9/10], Training Loss: 1.579, Validation Accuracy: 40.61%\n",
        "Epoch [10/10], Training Loss: 1.569, Validation Accuracy: 41.58%\n",
        "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 42.38%\n",
        "Epoch [2/10], Training Loss: 1.571, Validation Accuracy: 42.63%\n",
        "Epoch [3/10], Training Loss: 1.555, Validation Accuracy: 42.52%\n",
        "Epoch [4/10], Training Loss: 1.544, Validation Accuracy: 43.12%\n",
        "Epoch [5/10], Training Loss: 1.532, Validation Accuracy: 42.77%\n",
        "Epoch [6/10], Training Loss: 1.530, Validation Accuracy: 43.20%\n",
        "Epoch [7/10], Training Loss: 1.522, Validation Accuracy: 43.12%\n",
        "Epoch [8/10], Training Loss: 1.506, Validation Accuracy: 44.00%\n",
        "Epoch [9/10], Training Loss: 1.498, Validation Accuracy: 44.28%\n",
        "Epoch [10/10], Training Loss: 1.484, Validation Accuracy: 44.14%\n",
        "Epoch [1/10], Training Loss: 1.535, Validation Accuracy: 44.21%\n",
        "Epoch [2/10], Training Loss: 1.519, Validation Accuracy: 45.17%\n",
        "Epoch [3/10], Training Loss: 1.514, Validation Accuracy: 44.41%\n",
        "Epoch [4/10], Training Loss: 1.502, Validation Accuracy: 45.62%\n",
        "Epoch [5/10], Training Loss: 1.486, Validation Accuracy: 44.69%\n",
        "Epoch [6/10], Training Loss: 1.485, Validation Accuracy: 45.73%\n",
        "Epoch [7/10], Training Loss: 1.479, Validation Accuracy: 45.51%\n",
        "Epoch [8/10], Training Loss: 1.470, Validation Accuracy: 46.17%\n",
        "Epoch [9/10], Training Loss: 1.463, Validation Accuracy: 46.50%\n",
        "Epoch [10/10], Training Loss: 1.452, Validation Accuracy: 47.02%\n",
        "Epoch [1/10], Training Loss: 1.468, Validation Accuracy: 46.63%\n",
        "Epoch [2/10], Training Loss: 1.451, Validation Accuracy: 47.07%\n",
        "Epoch [3/10], Training Loss: 1.449, Validation Accuracy: 45.33%\n",
        "Epoch [4/10], Training Loss: 1.442, Validation Accuracy: 46.67%\n",
        "Epoch [5/10], Training Loss: 1.436, Validation Accuracy: 47.09%\n",
        "Epoch [6/10], Training Loss: 1.421, Validation Accuracy: 46.71%\n",
        "Epoch [7/10], Training Loss: 1.409, Validation Accuracy: 46.51%\n",
        "Epoch [8/10], Training Loss: 1.413, Validation Accuracy: 47.63%\n",
        "Epoch [9/10], Training Loss: 1.404, Validation Accuracy: 47.40%\n",
        "Epoch [10/10], Training Loss: 1.401, Validation Accuracy: 48.20%\n",
        "Epoch [1/10], Training Loss: 1.411, Validation Accuracy: 48.03%\n",
        "Epoch [2/10], Training Loss: 1.397, Validation Accuracy: 48.47%\n",
        "Epoch [3/10], Training Loss: 1.381, Validation Accuracy: 47.91%\n",
        "Epoch [4/10], Training Loss: 1.371, Validation Accuracy: 48.12%\n",
        "Epoch [5/10], Training Loss: 1.370, Validation Accuracy: 47.96%\n",
        "Epoch [6/10], Training Loss: 1.359, Validation Accuracy: 47.87%\n",
        "Epoch [7/10], Training Loss: 1.354, Validation Accuracy: 47.74%\n",
        "Epoch [8/10], Training Loss: 1.345, Validation Accuracy: 47.72%\n",
        "Epoch [9/10], Training Loss: 1.339, Validation Accuracy: 46.46%\n",
        "Epoch [10/10], Training Loss: 1.337, Validation Accuracy: 48.78%\n",
        "Epoch [1/10], Training Loss: 1.399, Validation Accuracy: 48.96%\n",
        "Epoch [2/10], Training Loss: 1.381, Validation Accuracy: 48.91%\n",
        "Epoch [3/10], Training Loss: 1.370, Validation Accuracy: 48.90%\n",
        "Epoch [4/10], Training Loss: 1.366, Validation Accuracy: 48.57%\n",
        "Epoch [5/10], Training Loss: 1.352, Validation Accuracy: 49.47%\n",
        "Epoch [6/10], Training Loss: 1.344, Validation Accuracy: 49.99%\n",
        "Epoch [7/10], Training Loss: 1.329, Validation Accuracy: 50.10%\n",
        "Epoch [8/10], Training Loss: 1.325, Validation Accuracy: 49.88%\n",
        "Epoch [9/10], Training Loss: 1.314, Validation Accuracy: 49.33%\n",
        "Epoch [10/10], Training Loss: 1.315, Validation Accuracy: 49.96%\n",
        "Epoch [1/10], Training Loss: 1.365, Validation Accuracy: 50.60%\n",
        "Epoch [2/10], Training Loss: 1.346, Validation Accuracy: 51.01%\n",
        "Epoch [3/10], Training Loss: 1.332, Validation Accuracy: 51.06%\n",
        "Epoch [4/10], Training Loss: 1.316, Validation Accuracy: 50.49%\n",
        "Epoch [5/10], Training Loss: 1.318, Validation Accuracy: 50.70%\n",
        "Epoch [6/10], Training Loss: 1.312, Validation Accuracy: 51.46%\n",
        "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 50.65%\n",
        "Epoch [8/10], Training Loss: 1.296, Validation Accuracy: 50.12%\n",
        "Epoch [9/10], Training Loss: 1.277, Validation Accuracy: 51.25%\n",
        "Epoch [10/10], Training Loss: 1.286, Validation Accuracy: 50.83%\n",
        "Epoch [1/10], Training Loss: 1.343, Validation Accuracy: 50.40%\n",
        "Epoch [2/10], Training Loss: 1.325, Validation Accuracy: 51.50%\n",
        "Epoch [3/10], Training Loss: 1.310, Validation Accuracy: 49.52%\n",
        "Epoch [4/10], Training Loss: 1.297, Validation Accuracy: 48.86%\n",
        "Epoch [5/10], Training Loss: 1.291, Validation Accuracy: 51.82%\n",
        "Epoch [6/10], Training Loss: 1.274, Validation Accuracy: 51.79%\n",
        "Epoch [7/10], Training Loss: 1.273, Validation Accuracy: 52.08%\n",
        "Epoch [8/10], Training Loss: 1.259, Validation Accuracy: 51.80%\n",
        "Epoch [9/10], Training Loss: 1.256, Validation Accuracy: 51.76%\n",
        "Epoch [10/10], Training Loss: 1.252, Validation Accuracy: 52.18%\n",
        "Epoch [1/10], Training Loss: 1.320, Validation Accuracy: 52.28%\n",
        "Epoch [2/10], Training Loss: 1.285, Validation Accuracy: 52.01%\n",
        "Epoch [3/10], Training Loss: 1.275, Validation Accuracy: 51.91%\n",
        "Epoch [4/10], Training Loss: 1.267, Validation Accuracy: 49.96%\n",
        "Epoch [5/10], Training Loss: 1.257, Validation Accuracy: 52.47%\n",
        "Epoch [6/10], Training Loss: 1.251, Validation Accuracy: 52.60%\n",
        "Epoch [7/10], Training Loss: 1.226, Validation Accuracy: 52.56%\n",
        "Epoch [8/10], Training Loss: 1.224, Validation Accuracy: 52.46%\n",
        "Epoch [9/10], Training Loss: 1.214, Validation Accuracy: 52.67%\n",
        "Epoch [10/10], Training Loss: 1.208, Validation Accuracy: 52.96%\n",
        "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 52.77%\n",
        "Epoch [2/10], Training Loss: 1.245, Validation Accuracy: 52.32%\n",
        "Epoch [3/10], Training Loss: 1.236, Validation Accuracy: 52.67%\n",
        "Epoch [4/10], Training Loss: 1.218, Validation Accuracy: 51.33%\n",
        "Epoch [5/10], Training Loss: 1.203, Validation Accuracy: 51.99%\n",
        "Epoch [6/10], Training Loss: 1.196, Validation Accuracy: 52.84%\n",
        "Epoch [7/10], Training Loss: 1.188, Validation Accuracy: 53.13%\n",
        "Epoch [8/10], Training Loss: 1.172, Validation Accuracy: 53.78%\n",
        "Epoch [9/10], Training Loss: 1.170, Validation Accuracy: 53.08%\n",
        "Epoch [10/10], Training Loss: 1.162, Validation Accuracy: 53.75%\n",
        "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 54.47%\n",
        "Epoch [2/10], Training Loss: 1.228, Validation Accuracy: 54.18%\n",
        "Epoch [3/10], Training Loss: 1.217, Validation Accuracy: 54.16%\n",
        "Epoch [4/10], Training Loss: 1.208, Validation Accuracy: 53.96%\n",
        "Epoch [5/10], Training Loss: 1.200, Validation Accuracy: 54.41%\n",
        "Epoch [6/10], Training Loss: 1.188, Validation Accuracy: 54.32%\n",
        "Epoch [7/10], Training Loss: 1.164, Validation Accuracy: 54.21%\n",
        "Epoch [8/10], Training Loss: 1.161, Validation Accuracy: 53.22%\n",
        "Epoch [9/10], Training Loss: 1.154, Validation Accuracy: 54.33%\n",
        "Epoch [10/10], Training Loss: 1.146, Validation Accuracy: 54.57%\n",
        "Epoch [1/10], Training Loss: 1.242, Validation Accuracy: 54.89%\n",
        "Epoch [2/10], Training Loss: 1.207, Validation Accuracy: 54.26%\n",
        "Epoch [3/10], Training Loss: 1.196, Validation Accuracy: 54.76%\n",
        "Epoch [4/10], Training Loss: 1.176, Validation Accuracy: 53.44%\n",
        "Epoch [5/10], Training Loss: 1.174, Validation Accuracy: 55.41%\n",
        "Epoch [6/10], Training Loss: 1.165, Validation Accuracy: 55.47%\n",
        "Epoch [7/10], Training Loss: 1.149, Validation Accuracy: 54.07%\n",
        "Epoch [8/10], Training Loss: 1.137, Validation Accuracy: 55.06%\n",
        "Epoch [9/10], Training Loss: 1.132, Validation Accuracy: 55.37%\n",
        "Epoch [10/10], Training Loss: 1.114, Validation Accuracy: 54.68%\n",
        "Epoch [1/10], Training Loss: 1.216, Validation Accuracy: 55.13%\n",
        "Epoch [2/10], Training Loss: 1.194, Validation Accuracy: 55.78%\n",
        "Epoch [3/10], Training Loss: 1.175, Validation Accuracy: 54.68%\n",
        "Epoch [4/10], Training Loss: 1.165, Validation Accuracy: 56.29%\n",
        "Epoch [5/10], Training Loss: 1.138, Validation Accuracy: 56.06%\n",
        "Epoch [6/10], Training Loss: 1.126, Validation Accuracy: 56.28%\n",
        "Epoch [7/10], Training Loss: 1.115, Validation Accuracy: 55.22%\n",
        "Epoch [8/10], Training Loss: 1.120, Validation Accuracy: 54.97%\n",
        "Epoch [9/10], Training Loss: 1.106, Validation Accuracy: 55.87%\n",
        "Epoch [10/10], Training Loss: 1.104, Validation Accuracy: 55.51%\n",
        "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 55.85%\n",
        "Epoch [2/10], Training Loss: 1.178, Validation Accuracy: 53.88%\n",
        "Epoch [3/10], Training Loss: 1.147, Validation Accuracy: 55.91%\n",
        "Epoch [4/10], Training Loss: 1.132, Validation Accuracy: 55.92%\n",
        "Epoch [5/10], Training Loss: 1.117, Validation Accuracy: 55.65%\n",
        "Epoch [6/10], Training Loss: 1.108, Validation Accuracy: 55.89%\n",
        "Epoch [7/10], Training Loss: 1.087, Validation Accuracy: 56.14%\n",
        "Epoch [8/10], Training Loss: 1.082, Validation Accuracy: 55.74%\n",
        "Epoch [9/10], Training Loss: 1.058, Validation Accuracy: 56.46%\n",
        "Epoch [10/10], Training Loss: 1.055, Validation Accuracy: 56.26%\n",
        "Epoch [1/10], Training Loss: 1.155, Validation Accuracy: 56.98%\n",
        "Epoch [2/10], Training Loss: 1.128, Validation Accuracy: 56.48%\n",
        "Epoch [3/10], Training Loss: 1.103, Validation Accuracy: 55.57%\n",
        "Epoch [4/10], Training Loss: 1.095, Validation Accuracy: 56.77%\n",
        "Epoch [5/10], Training Loss: 1.074, Validation Accuracy: 56.64%\n",
        "Epoch [6/10], Training Loss: 1.065, Validation Accuracy: 56.61%\n",
        "Epoch [7/10], Training Loss: 1.051, Validation Accuracy: 55.99%\n",
        "Epoch [8/10], Training Loss: 1.038, Validation Accuracy: 57.03%\n",
        "Epoch [9/10], Training Loss: 1.032, Validation Accuracy: 54.79%\n",
        "Epoch [10/10], Training Loss: 1.023, Validation Accuracy: 55.60%\n",
        "Epoch [1/10], Training Loss: 1.152, Validation Accuracy: 56.11%\n",
        "Epoch [2/10], Training Loss: 1.124, Validation Accuracy: 57.31%\n",
        "Epoch [3/10], Training Loss: 1.094, Validation Accuracy: 57.43%\n",
        "Epoch [4/10], Training Loss: 1.079, Validation Accuracy: 56.87%\n",
        "Epoch [5/10], Training Loss: 1.066, Validation Accuracy: 56.14%\n",
        "Epoch [6/10], Training Loss: 1.051, Validation Accuracy: 57.60%\n",
        "Epoch [7/10], Training Loss: 1.041, Validation Accuracy: 57.12%\n",
        "Epoch [8/10], Training Loss: 1.040, Validation Accuracy: 57.75%\n",
        "Epoch [9/10], Training Loss: 1.015, Validation Accuracy: 57.59%\n",
        "Epoch [10/10], Training Loss: 1.006, Validation Accuracy: 56.76%\n",
        "Epoch [1/10], Training Loss: 1.148, Validation Accuracy: 57.59%\n",
        "Epoch [2/10], Training Loss: 1.104, Validation Accuracy: 57.93%\n",
        "Epoch [3/10], Training Loss: 1.081, Validation Accuracy: 57.80%\n",
        "Epoch [4/10], Training Loss: 1.065, Validation Accuracy: 56.94%\n",
        "Epoch [5/10], Training Loss: 1.054, Validation Accuracy: 58.21%\n",
        "Epoch [6/10], Training Loss: 1.039, Validation Accuracy: 56.60%\n",
        "Epoch [7/10], Training Loss: 1.019, Validation Accuracy: 58.06%\n",
        "Epoch [8/10], Training Loss: 1.009, Validation Accuracy: 57.62%\n",
        "Epoch [9/10], Training Loss: 0.998, Validation Accuracy: 56.81%\n",
        "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 57.88%\n",
        "Epoch [1/10], Training Loss: 1.135, Validation Accuracy: 57.12%\n",
        "Epoch [2/10], Training Loss: 1.095, Validation Accuracy: 58.57%\n",
        "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 57.66%\n",
        "Epoch [4/10], Training Loss: 1.043, Validation Accuracy: 58.24%\n",
        "Epoch [5/10], Training Loss: 1.021, Validation Accuracy: 57.42%\n",
        "Epoch [6/10], Training Loss: 1.014, Validation Accuracy: 57.94%\n",
        "Epoch [7/10], Training Loss: 1.002, Validation Accuracy: 58.20%\n",
        "Epoch [8/10], Training Loss: 0.988, Validation Accuracy: 58.47%\n",
        "Epoch [9/10], Training Loss: 0.982, Validation Accuracy: 58.34%\n",
        "Epoch [10/10], Training Loss: 0.963, Validation Accuracy: 57.96%\n",
        "Epoch [1/10], Training Loss: 1.090, Validation Accuracy: 57.58%\n",
        "Epoch [2/10], Training Loss: 1.059, Validation Accuracy: 58.54%\n",
        "Epoch [3/10], Training Loss: 1.034, Validation Accuracy: 57.94%\n",
        "Epoch [4/10], Training Loss: 1.013, Validation Accuracy: 57.71%\n",
        "Epoch [5/10], Training Loss: 0.990, Validation Accuracy: 58.90%\n",
        "Epoch [6/10], Training Loss: 0.991, Validation Accuracy: 58.47%\n",
        "Epoch [7/10], Training Loss: 0.969, Validation Accuracy: 58.40%\n",
        "Epoch [8/10], Training Loss: 0.951, Validation Accuracy: 59.24%\n",
        "Epoch [9/10], Training Loss: 0.939, Validation Accuracy: 58.32%\n",
        "Epoch [10/10], Training Loss: 0.935, Validation Accuracy: 57.02%\n",
        "Epoch [1/10], Training Loss: 1.074, Validation Accuracy: 58.80%\n",
        "Epoch [2/10], Training Loss: 1.022, Validation Accuracy: 58.47%\n",
        "Epoch [3/10], Training Loss: 1.014, Validation Accuracy: 59.35%\n",
        "Epoch [4/10], Training Loss: 0.986, Validation Accuracy: 59.18%\n",
        "Epoch [5/10], Training Loss: 0.971, Validation Accuracy: 58.18%\n",
        "Epoch [6/10], Training Loss: 0.958, Validation Accuracy: 59.19%\n",
        "Epoch [7/10], Training Loss: 0.935, Validation Accuracy: 58.81%\n",
        "Epoch [8/10], Training Loss: 0.925, Validation Accuracy: 58.14%\n",
        "Epoch [9/10], Training Loss: 0.917, Validation Accuracy: 59.35%\n",
        "Epoch [10/10], Training Loss: 0.899, Validation Accuracy: 58.40%\n",
        "Epoch [1/10], Training Loss: 1.064, Validation Accuracy: 59.71%\n",
        "Epoch [2/10], Training Loss: 1.017, Validation Accuracy: 60.02%\n",
        "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 59.25%\n",
        "Epoch [4/10], Training Loss: 0.981, Validation Accuracy: 59.44%\n",
        "Epoch [5/10], Training Loss: 0.952, Validation Accuracy: 59.93%\n",
        "Epoch [6/10], Training Loss: 0.939, Validation Accuracy: 59.44%\n",
        "Epoch [7/10], Training Loss: 0.928, Validation Accuracy: 59.84%\n",
        "Epoch [8/10], Training Loss: 0.919, Validation Accuracy: 59.83%\n",
        "Epoch [9/10], Training Loss: 0.910, Validation Accuracy: 59.34%\n",
        "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 59.02%\n",
        "Epoch [1/10], Training Loss: 1.052, Validation Accuracy: 58.65%\n",
        "Epoch [2/10], Training Loss: 1.021, Validation Accuracy: 59.62%\n",
        "Epoch [3/10], Training Loss: 0.987, Validation Accuracy: 59.67%\n",
        "Epoch [4/10], Training Loss: 0.964, Validation Accuracy: 60.31%\n",
        "Epoch [5/10], Training Loss: 0.946, Validation Accuracy: 59.97%\n",
        "Epoch [6/10], Training Loss: 0.939, Validation Accuracy: 60.02%\n",
        "Epoch [7/10], Training Loss: 0.913, Validation Accuracy: 59.79%\n",
        "Epoch [8/10], Training Loss: 0.899, Validation Accuracy: 59.72%\n",
        "Epoch [9/10], Training Loss: 0.888, Validation Accuracy: 58.94%\n",
        "Epoch [10/10], Training Loss: 0.873, Validation Accuracy: 60.48%\n",
        "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 59.71%\n",
        "Epoch [2/10], Training Loss: 1.003, Validation Accuracy: 60.12%\n",
        "Epoch [3/10], Training Loss: 0.983, Validation Accuracy: 59.62%\n",
        "Epoch [4/10], Training Loss: 0.954, Validation Accuracy: 60.07%\n",
        "Epoch [5/10], Training Loss: 0.930, Validation Accuracy: 60.61%\n",
        "Epoch [6/10], Training Loss: 0.917, Validation Accuracy: 59.60%\n",
        "Epoch [7/10], Training Loss: 0.913, Validation Accuracy: 59.27%\n",
        "Epoch [8/10], Training Loss: 0.888, Validation Accuracy: 59.09%\n",
        "Epoch [9/10], Training Loss: 0.876, Validation Accuracy: 59.94%\n",
        "Epoch [10/10], Training Loss: 0.859, Validation Accuracy: 59.99%\n",
        "Epoch [1/10], Training Loss: 1.017, Validation Accuracy: 60.59%\n",
        "Epoch [2/10], Training Loss: 0.967, Validation Accuracy: 59.57%\n",
        "Epoch [3/10], Training Loss: 0.948, Validation Accuracy: 59.34%\n",
        "Epoch [4/10], Training Loss: 0.925, Validation Accuracy: 59.28%\n",
        "Epoch [5/10], Training Loss: 0.912, Validation Accuracy: 60.21%\n",
        "Epoch [6/10], Training Loss: 0.889, Validation Accuracy: 60.25%\n",
        "Epoch [7/10], Training Loss: 0.869, Validation Accuracy: 59.82%\n",
        "Epoch [8/10], Training Loss: 0.855, Validation Accuracy: 60.48%\n",
        "Epoch [9/10], Training Loss: 0.838, Validation Accuracy: 59.53%\n",
        "Epoch [10/10], Training Loss: 0.836, Validation Accuracy: 59.26%\n",
        "Epoch [1/10], Training Loss: 0.989, Validation Accuracy: 59.21%\n",
        "Epoch [2/10], Training Loss: 0.948, Validation Accuracy: 60.17%\n",
        "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 60.52%\n",
        "Epoch [4/10], Training Loss: 0.889, Validation Accuracy: 60.68%\n",
        "Epoch [5/10], Training Loss: 0.868, Validation Accuracy: 59.40%\n",
        "Epoch [6/10], Training Loss: 0.852, Validation Accuracy: 60.66%\n",
        "Epoch [7/10], Training Loss: 0.835, Validation Accuracy: 60.02%\n",
        "Epoch [8/10], Training Loss: 0.831, Validation Accuracy: 60.40%\n",
        "Epoch [9/10], Training Loss: 0.809, Validation Accuracy: 59.49%\n",
        "Epoch [10/10], Training Loss: 0.801, Validation Accuracy: 60.84%\n",
        "Epoch [1/10], Training Loss: 0.997, Validation Accuracy: 61.15%\n",
        "Epoch [2/10], Training Loss: 0.955, Validation Accuracy: 60.30%\n",
        "Epoch [3/10], Training Loss: 0.914, Validation Accuracy: 61.07%\n",
        "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 60.72%\n",
        "Epoch [5/10], Training Loss: 0.868, Validation Accuracy: 61.41%\n",
        "Epoch [6/10], Training Loss: 0.846, Validation Accuracy: 60.93%\n",
        "Epoch [7/10], Training Loss: 0.829, Validation Accuracy: 60.84%\n",
        "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 60.28%\n",
        "Epoch [9/10], Training Loss: 0.815, Validation Accuracy: 60.97%\n",
        "Epoch [10/10], Training Loss: 0.781, Validation Accuracy: 60.86%\n",
        "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 60.51%\n",
        "Epoch [2/10], Training Loss: 0.918, Validation Accuracy: 61.18%\n",
        "Epoch [3/10], Training Loss: 0.897, Validation Accuracy: 60.88%\n",
        "Epoch [4/10], Training Loss: 0.871, Validation Accuracy: 60.85%\n",
        "Epoch [5/10], Training Loss: 0.850, Validation Accuracy: 60.66%\n",
        "Epoch [6/10], Training Loss: 0.834, Validation Accuracy: 60.59%\n",
        "Epoch [7/10], Training Loss: 0.812, Validation Accuracy: 60.66%\n",
        "Epoch [8/10], Training Loss: 0.799, Validation Accuracy: 59.87%\n",
        "Epoch [9/10], Training Loss: 0.782, Validation Accuracy: 60.74%\n",
        "Epoch [10/10], Training Loss: 0.766, Validation Accuracy: 60.37%\n",
        "Epoch [1/10], Training Loss: 0.990, Validation Accuracy: 61.31%\n",
        "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 61.08%\n",
        "Epoch [3/10], Training Loss: 0.890, Validation Accuracy: 61.58%\n",
        "Epoch [4/10], Training Loss: 0.871, Validation Accuracy: 61.35%\n",
        "Epoch [5/10], Training Loss: 0.846, Validation Accuracy: 61.11%\n",
        "Epoch [6/10], Training Loss: 0.830, Validation Accuracy: 61.21%\n",
        "Epoch [7/10], Training Loss: 0.809, Validation Accuracy: 61.58%\n",
        "Epoch [8/10], Training Loss: 0.801, Validation Accuracy: 60.99%\n",
        "Epoch [9/10], Training Loss: 0.791, Validation Accuracy: 60.67%\n",
        "Epoch [10/10], Training Loss: 0.770, Validation Accuracy: 60.11%\n",
        "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 61.04%\n",
        "Epoch [2/10], Training Loss: 0.886, Validation Accuracy: 60.98%\n",
        "Epoch [3/10], Training Loss: 0.859, Validation Accuracy: 61.33%\n",
        "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 61.03%\n",
        "Epoch [5/10], Training Loss: 0.817, Validation Accuracy: 60.81%\n",
        "Epoch [6/10], Training Loss: 0.800, Validation Accuracy: 60.91%\n",
        "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 60.96%\n",
        "Epoch [8/10], Training Loss: 0.755, Validation Accuracy: 60.41%\n",
        "Epoch [9/10], Training Loss: 0.745, Validation Accuracy: 60.88%\n",
        "Epoch [10/10], Training Loss: 0.741, Validation Accuracy: 61.21%\n",
        "Epoch [1/10], Training Loss: 0.930, Validation Accuracy: 61.73%\n",
        "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 61.07%\n",
        "Epoch [3/10], Training Loss: 0.834, Validation Accuracy: 61.54%\n",
        "Epoch [4/10], Training Loss: 0.816, Validation Accuracy: 60.76%\n",
        "Epoch [5/10], Training Loss: 0.784, Validation Accuracy: 61.72%\n",
        "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 60.23%\n",
        "Epoch [7/10], Training Loss: 0.759, Validation Accuracy: 61.38%\n",
        "Epoch [8/10], Training Loss: 0.739, Validation Accuracy: 61.15%\n",
        "Epoch [9/10], Training Loss: 0.717, Validation Accuracy: 60.91%\n",
        "Epoch [10/10], Training Loss: 0.702, Validation Accuracy: 60.63%\n",
        "Epoch [1/10], Training Loss: 0.944, Validation Accuracy: 60.99%\n",
        "Epoch [2/10], Training Loss: 0.863, Validation Accuracy: 61.28%\n",
        "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 61.11%\n",
        "Epoch [4/10], Training Loss: 0.797, Validation Accuracy: 61.52%\n",
        "Epoch [5/10], Training Loss: 0.779, Validation Accuracy: 61.80%\n",
        "Epoch [6/10], Training Loss: 0.758, Validation Accuracy: 62.08%\n",
        "Epoch [7/10], Training Loss: 0.732, Validation Accuracy: 61.68%\n",
        "Epoch [8/10], Training Loss: 0.719, Validation Accuracy: 61.63%\n",
        "Epoch [9/10], Training Loss: 0.698, Validation Accuracy: 61.35%\n",
        "Epoch [10/10], Training Loss: 0.680, Validation Accuracy: 61.99%\n",
        "Epoch [1/10], Training Loss: 0.924, Validation Accuracy: 61.83%\n",
        "Epoch [2/10], Training Loss: 0.857, Validation Accuracy: 62.43%\n",
        "Epoch [3/10], Training Loss: 0.813, Validation Accuracy: 61.83%\n",
        "Epoch [4/10], Training Loss: 0.780, Validation Accuracy: 61.57%\n",
        "Epoch [5/10], Training Loss: 0.752, Validation Accuracy: 61.58%\n",
        "Epoch [6/10], Training Loss: 0.736, Validation Accuracy: 61.64%\n",
        "Epoch [7/10], Training Loss: 0.716, Validation Accuracy: 61.25%\n",
        "Epoch [8/10], Training Loss: 0.702, Validation Accuracy: 61.75%\n",
        "Epoch [9/10], Training Loss: 0.682, Validation Accuracy: 61.02%\n",
        "Epoch [10/10], Training Loss: 0.669, Validation Accuracy: 61.32%\n",
        "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 61.85%\n",
        "Epoch [2/10], Training Loss: 0.862, Validation Accuracy: 61.82%\n",
        "Epoch [3/10], Training Loss: 0.819, Validation Accuracy: 61.75%\n",
        "Epoch [4/10], Training Loss: 0.800, Validation Accuracy: 61.92%\n",
        "Epoch [5/10], Training Loss: 0.766, Validation Accuracy: 62.14%\n",
        "Epoch [6/10], Training Loss: 0.739, Validation Accuracy: 61.78%\n",
        "Epoch [7/10], Training Loss: 0.720, Validation Accuracy: 61.71%\n",
        "Epoch [8/10], Training Loss: 0.705, Validation Accuracy: 62.10%\n",
        "Epoch [9/10], Training Loss: 0.692, Validation Accuracy: 61.36%\n",
        "Epoch [10/10], Training Loss: 0.669, Validation Accuracy: 61.33%\n",
        "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 62.32%\n",
        "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 62.50%\n",
        "Epoch [3/10], Training Loss: 0.786, Validation Accuracy: 62.31%\n",
        "Epoch [4/10], Training Loss: 0.762, Validation Accuracy: 61.92%\n",
        "Epoch [5/10], Training Loss: 0.739, Validation Accuracy: 61.95%\n",
        "Epoch [6/10], Training Loss: 0.714, Validation Accuracy: 61.78%\n",
        "Epoch [7/10], Training Loss: 0.691, Validation Accuracy: 62.06%\n",
        "Epoch [8/10], Training Loss: 0.672, Validation Accuracy: 61.91%\n",
        "Epoch [9/10], Training Loss: 0.654, Validation Accuracy: 62.14%\n",
        "Epoch [10/10], Training Loss: 0.643, Validation Accuracy: 61.90%\n",
        "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.78%\n",
        "Epoch [2/10], Training Loss: 0.800, Validation Accuracy: 61.71%\n",
        "Epoch [3/10], Training Loss: 0.766, Validation Accuracy: 61.50%\n",
        "Epoch [4/10], Training Loss: 0.731, Validation Accuracy: 62.01%\n",
        "Epoch [5/10], Training Loss: 0.696, Validation Accuracy: 61.31%\n",
        "Epoch [6/10], Training Loss: 0.681, Validation Accuracy: 61.79%\n",
        "Epoch [7/10], Training Loss: 0.668, Validation Accuracy: 61.65%\n",
        "Epoch [8/10], Training Loss: 0.639, Validation Accuracy: 61.41%\n",
        "Epoch [9/10], Training Loss: 0.634, Validation Accuracy: 62.09%\n",
        "Epoch [10/10], Training Loss: 0.608, Validation Accuracy: 61.76%\n",
        "Epoch [1/10], Training Loss: 0.875, Validation Accuracy: 61.96%\n",
        "Epoch [2/10], Training Loss: 0.796, Validation Accuracy: 62.63%\n",
        "Epoch [3/10], Training Loss: 0.744, Validation Accuracy: 62.15%\n",
        "Epoch [4/10], Training Loss: 0.713, Validation Accuracy: 62.15%\n",
        "Epoch [5/10], Training Loss: 0.686, Validation Accuracy: 61.52%\n",
        "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 62.35%\n",
        "Epoch [7/10], Training Loss: 0.641, Validation Accuracy: 61.97%\n",
        "Epoch [8/10], Training Loss: 0.618, Validation Accuracy: 61.47%\n",
        "Epoch [9/10], Training Loss: 0.610, Validation Accuracy: 61.14%\n",
        "Epoch [10/10], Training Loss: 0.591, Validation Accuracy: 62.18%\n",
        "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 61.64%\n",
        "Epoch [2/10], Training Loss: 0.793, Validation Accuracy: 61.46%\n",
        "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 62.05%\n",
        "Epoch [4/10], Training Loss: 0.704, Validation Accuracy: 61.12%\n",
        "Epoch [5/10], Training Loss: 0.671, Validation Accuracy: 62.08%\n",
        "Epoch [6/10], Training Loss: 0.647, Validation Accuracy: 61.21%\n",
        "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 61.75%\n",
        "Epoch [8/10], Training Loss: 0.606, Validation Accuracy: 61.92%\n",
        "Epoch [9/10], Training Loss: 0.590, Validation Accuracy: 61.44%\n",
        "Epoch [10/10], Training Loss: 0.579, Validation Accuracy: 61.84%\n",
        "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 61.06%\n",
        "Epoch [2/10], Training Loss: 0.799, Validation Accuracy: 61.50%\n",
        "Epoch [3/10], Training Loss: 0.755, Validation Accuracy: 60.50%\n",
        "Epoch [4/10], Training Loss: 0.720, Validation Accuracy: 62.24%\n",
        "Epoch [5/10], Training Loss: 0.692, Validation Accuracy: 62.26%\n",
        "Epoch [6/10], Training Loss: 0.661, Validation Accuracy: 61.50%\n",
        "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 62.29%\n",
        "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 61.46%\n",
        "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 61.53%\n",
        "Epoch [10/10], Training Loss: 0.585, Validation Accuracy: 60.97%\n",
        "Epoch [1/10], Training Loss: 0.880, Validation Accuracy: 61.32%\n",
        "Epoch [2/10], Training Loss: 0.789, Validation Accuracy: 62.19%\n",
        "Epoch [3/10], Training Loss: 0.727, Validation Accuracy: 62.52%\n",
        "Epoch [4/10], Training Loss: 0.689, Validation Accuracy: 62.00%\n",
        "Epoch [5/10], Training Loss: 0.668, Validation Accuracy: 62.40%\n",
        "Epoch [6/10], Training Loss: 0.633, Validation Accuracy: 61.14%\n",
        "Epoch [7/10], Training Loss: 0.617, Validation Accuracy: 62.11%\n",
        "Epoch [8/10], Training Loss: 0.589, Validation Accuracy: 62.37%\n",
        "Epoch [9/10], Training Loss: 0.575, Validation Accuracy: 61.95%\n",
        "Epoch [10/10], Training Loss: 0.560, Validation Accuracy: 61.55%\n",
        "Epoch [1/10], Training Loss: 0.842, Validation Accuracy: 60.91%\n",
        "Epoch [2/10], Training Loss: 0.749, Validation Accuracy: 61.63%\n",
        "Epoch [3/10], Training Loss: 0.696, Validation Accuracy: 62.18%\n",
        "Epoch [4/10], Training Loss: 0.658, Validation Accuracy: 62.26%\n",
        "Epoch [5/10], Training Loss: 0.628, Validation Accuracy: 62.18%\n",
        "Epoch [6/10], Training Loss: 0.605, Validation Accuracy: 62.34%\n",
        "Epoch [7/10], Training Loss: 0.580, Validation Accuracy: 62.26%\n",
        "Epoch [8/10], Training Loss: 0.572, Validation Accuracy: 61.54%\n",
        "Epoch [9/10], Training Loss: 0.557, Validation Accuracy: 61.77%\n",
        "Epoch [10/10], Training Loss: 0.523, Validation Accuracy: 61.50%\n",
        "Epoch [1/10], Training Loss: 0.835, Validation Accuracy: 61.68%\n",
        "Epoch [2/10], Training Loss: 0.738, Validation Accuracy: 61.20%\n",
        "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 62.71%\n",
        "Epoch [4/10], Training Loss: 0.649, Validation Accuracy: 62.25%\n",
        "Epoch [5/10], Training Loss: 0.614, Validation Accuracy: 62.50%\n",
        "Epoch [6/10], Training Loss: 0.589, Validation Accuracy: 61.87%\n",
        "Epoch [7/10], Training Loss: 0.564, Validation Accuracy: 61.20%\n",
        "Epoch [8/10], Training Loss: 0.551, Validation Accuracy: 62.37%\n",
        "Epoch [9/10], Training Loss: 0.520, Validation Accuracy: 62.05%\n",
        "Epoch [10/10], Training Loss: 0.493, Validation Accuracy: 61.75%\n",
        "Epoch [1/10], Training Loss: 0.826, Validation Accuracy: 61.36%\n",
        "Epoch [2/10], Training Loss: 0.718, Validation Accuracy: 62.26%\n",
        "Epoch [3/10], Training Loss: 0.657, Validation Accuracy: 61.83%\n",
        "Epoch [4/10], Training Loss: 0.628, Validation Accuracy: 61.82%\n",
        "Epoch [5/10], Training Loss: 0.599, Validation Accuracy: 61.34%\n",
        "Epoch [6/10], Training Loss: 0.566, Validation Accuracy: 61.44%\n",
        "Epoch [7/10], Training Loss: 0.546, Validation Accuracy: 61.52%\n",
        "Epoch [8/10], Training Loss: 0.521, Validation Accuracy: 61.49%\n",
        "Epoch [9/10], Training Loss: 0.508, Validation Accuracy: 61.44%\n",
        "Epoch [10/10], Training Loss: 0.490, Validation Accuracy: 61.01%\n",
        "Epoch [1/10], Training Loss: 0.876, Validation Accuracy: 61.99%\n",
        "Epoch [2/10], Training Loss: 0.761, Validation Accuracy: 61.44%\n",
        "Epoch [3/10], Training Loss: 0.685, Validation Accuracy: 62.16%\n",
        "Epoch [4/10], Training Loss: 0.636, Validation Accuracy: 62.17%\n",
        "Epoch [5/10], Training Loss: 0.608, Validation Accuracy: 62.23%\n",
        "Epoch [6/10], Training Loss: 0.582, Validation Accuracy: 61.90%\n",
        "Epoch [7/10], Training Loss: 0.560, Validation Accuracy: 61.44%\n",
        "Epoch [8/10], Training Loss: 0.536, Validation Accuracy: 61.30%\n",
        "Epoch [9/10], Training Loss: 0.517, Validation Accuracy: 60.95%\n",
        "Epoch [10/10], Training Loss: 0.503, Validation Accuracy: 61.60%\n",
        "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 61.70%\n",
        "Epoch [2/10], Training Loss: 0.727, Validation Accuracy: 62.39%\n",
        "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.82%\n",
        "Epoch [4/10], Training Loss: 0.625, Validation Accuracy: 61.58%\n",
        "Epoch [5/10], Training Loss: 0.592, Validation Accuracy: 62.20%\n",
        "Epoch [6/10], Training Loss: 0.564, Validation Accuracy: 61.99%\n",
        "Epoch [7/10], Training Loss: 0.533, Validation Accuracy: 62.08%\n",
        "Epoch [8/10], Training Loss: 0.515, Validation Accuracy: 61.95%\n",
        "Epoch [9/10], Training Loss: 0.493, Validation Accuracy: 61.89%\n",
        "Epoch [10/10], Training Loss: 0.482, Validation Accuracy: 61.84%\n",
        "Epoch [1/10], Training Loss: 0.804, Validation Accuracy: 61.39%\n",
        "Epoch [2/10], Training Loss: 0.690, Validation Accuracy: 62.05%\n",
        "Epoch [3/10], Training Loss: 0.635, Validation Accuracy: 61.90%\n",
        "Epoch [4/10], Training Loss: 0.590, Validation Accuracy: 61.34%\n",
        "Epoch [5/10], Training Loss: 0.556, Validation Accuracy: 62.29%\n",
        "Epoch [6/10], Training Loss: 0.525, Validation Accuracy: 61.83%\n",
        "Epoch [7/10], Training Loss: 0.503, Validation Accuracy: 61.79%\n",
        "Epoch [8/10], Training Loss: 0.492, Validation Accuracy: 61.70%\n",
        "Epoch [9/10], Training Loss: 0.464, Validation Accuracy: 61.96%\n",
        "Epoch [10/10], Training Loss: 0.444, Validation Accuracy: 61.71%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A5BBnpY9XAB",
        "outputId": "52096bb7-65cb-4fb9-931b-35a7f53feb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [10.49, 10.52, 10.44, 10.51, 10.61, 10.67, 10.97, 11.34, 11.72, 12.03, 11.57, 12.37, 12.71, 13.19, 12.54, 12.56, 12.75, 14.95, 16.23, 16.99, 17.97, 19.5, 20.81, 22.0, 23.25, 24.05, 24.85, 26.23, 26.35, 27.2, 27.86, 28.99, 29.53, 30.15, 30.4, 30.96, 31.2, 31.91, 33.12, 32.13, 33.98, 34.74, 35.68, 35.3, 36.68, 37.59, 36.94, 37.78, 37.58, 38.83, 38.68, 38.65, 39.73, 39.57, 40.25, 39.94, 40.8, 40.33, 40.61, 41.58, 42.38, 42.63, 42.52, 43.12, 42.77, 43.2, 43.12, 44.0, 44.28, 44.14, 44.21, 45.17, 44.41, 45.62, 44.69, 45.73, 45.51, 46.17, 46.5, 47.02, 46.63, 47.07, 45.33, 46.67, 47.09, 46.71, 46.51, 47.63, 47.4, 48.2, 48.03, 48.47, 47.91, 48.12, 47.96, 47.87, 47.74, 47.72, 46.46, 48.78, 48.96, 48.91, 48.9, 48.57, 49.47, 49.99, 50.1, 49.88, 49.33, 49.96, 50.6, 51.01, 51.06, 50.49, 50.7, 51.46, 50.65, 50.12, 51.25, 50.83, 50.4, 51.5, 49.52, 48.86, 51.82, 51.79, 52.08, 51.8, 51.76, 52.18, 52.28, 52.01, 51.91, 49.96, 52.47, 52.6, 52.56, 52.46, 52.67, 52.96, 52.77, 52.32, 52.67, 51.33, 51.99, 52.84, 53.13, 53.78, 53.08, 53.75, 54.47, 54.18, 54.16, 53.96, 54.41, 54.32, 54.21, 53.22, 54.33, 54.57, 54.89, 54.26, 54.76, 53.44, 55.41, 55.47, 54.07, 55.06, 55.37, 54.68, 55.13, 55.78, 54.68, 56.29, 56.06, 56.28, 55.22, 54.97, 55.87, 55.51, 55.85, 53.88, 55.91, 55.92, 55.65, 55.89, 56.14, 55.74, 56.46, 56.26, 56.98, 56.48, 55.57, 56.77, 56.64, 56.61, 55.99, 57.03, 54.79, 55.6, 56.11, 57.31, 57.43, 56.87, 56.14, 57.6, 57.12, 57.75, 57.59, 56.76, 57.59, 57.93, 57.8, 56.94, 58.21, 56.6, 58.06, 57.62, 56.81, 57.88, 57.12, 58.57, 57.66, 58.24, 57.42, 57.94, 58.2, 58.47, 58.34, 57.96, 57.58, 58.54, 57.94, 57.71, 58.9, 58.47, 58.4, 59.24, 58.32, 57.02, 58.8, 58.47, 59.35, 59.18, 58.18, 59.19, 58.81, 58.14, 59.35, 58.4, 59.71, 60.02, 59.25, 59.44, 59.93, 59.44, 59.84, 59.83, 59.34, 59.02, 58.65, 59.62, 59.67, 60.31, 59.97, 60.02, 59.79, 59.72, 58.94, 60.48, 59.71, 60.12, 59.62, 60.07, 60.61, 59.6, 59.27, 59.09, 59.94, 59.99, 60.59, 59.57, 59.34, 59.28, 60.21, 60.25, 59.82, 60.48, 59.53, 59.26, 59.21, 60.17, 60.52, 60.68, 59.4, 60.66, 60.02, 60.4, 59.49, 60.84, 61.15, 60.3, 61.07, 60.72, 61.41, 60.93, 60.84, 60.28, 60.97, 60.86, 60.51, 61.18, 60.88, 60.85, 60.66, 60.59, 60.66, 59.87, 60.74, 60.37, 61.31, 61.08, 61.58, 61.35, 61.11, 61.21, 61.58, 60.99, 60.67, 60.11, 61.04, 60.98, 61.33, 61.03, 60.81, 60.91, 60.96, 60.41, 60.88, 61.21, 61.73, 61.07, 61.54, 60.76, 61.72, 60.23, 61.38, 61.15, 60.91, 60.63, 60.99, 61.28, 61.11, 61.52, 61.8, 62.08, 61.68, 61.63, 61.35, 61.99, 61.83, 62.43, 61.83, 61.57, 61.58, 61.64, 61.25, 61.75, 61.02, 61.32, 61.85, 61.82, 61.75, 61.92, 62.14, 61.78, 61.71, 62.1, 61.36, 61.33, 62.32, 62.5, 62.31, 61.92, 61.95, 61.78, 62.06, 61.91, 62.14, 61.9, 61.78, 61.71, 61.5, 62.01, 61.31, 61.79, 61.65, 61.41, 62.09, 61.76, 61.96, 62.63, 62.15, 62.15, 61.52, 62.35, 61.97, 61.47, 61.14, 62.18, 61.64, 61.46, 62.05, 61.12, 62.08, 61.21, 61.75, 61.92, 61.44, 61.84, 61.06, 61.5, 60.5, 62.24, 62.26, 61.5, 62.29, 61.46, 61.53, 60.97, 61.32, 62.19, 62.52, 62.0, 62.4, 61.14, 62.11, 62.37, 61.95, 61.55, 60.91, 61.63, 62.18, 62.26, 62.18, 62.34, 62.26, 61.54, 61.77, 61.5, 61.68, 61.2, 62.71, 62.25, 62.5, 61.87, 61.2, 62.37, 62.05, 61.75, 61.36, 62.26, 61.83, 61.82, 61.34, 61.44, 61.52, 61.49, 61.44, 61.01, 61.99, 61.44, 62.16, 62.17, 62.23, 61.9, 61.44, 61.3, 60.95, 61.6, 61.7, 62.39, 61.82, 61.58, 62.2, 61.99, 62.08, 61.95, 61.89, 61.84, 61.39, 62.05, 61.9, 61.34, 62.29, 61.83, 61.79, 61.7, 61.96, 61.71]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Precision, Recall, and F1 score arrays\n",
        "\n",
        "precision = np.array([])\n",
        "recall = np.array([0.697 0.71  0.536 0.457 0.492 0.544 0.679 0.67  0.712 0.695])\n",
        "f1_score = np.array([0.67702768 0.72671443 0.49084249 0.44261501 0.52761394 0.52433735\n",
        " 0.69927909 0.68577277 0.74360313 0.69954706])\n",
        "\n",
        "# Calculate averages\n",
        "avg_precision = np.mean(precision)\n",
        "avg_recall = np.mean(recall)\n",
        "avg_f1_score = np.mean(f1_score)\n",
        "\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VJvSmv_Zc4P8",
        "outputId": "740f4409-a890-45ef-d19d-187a6ae851c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-2-9159192ec5b0>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-9159192ec5b0>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    precision = np.array([0.65816808 0.7442348  0.4527027  0.42910798 0.56878613 0.50604651\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Precision, Recall, and F1 score arrays (no commas, using split to convert from space-separated strings)\n",
        "precision = np.array(\"0.65816808 0.7442348  0.4527027  0.42910798 0.56878613 0.50604651 0.72080679 0.70230608 0.77814208 0.704154\".split(), dtype=float)\n",
        "recall = np.array(\"0.697 0.71 0.536 0.457 0.492 0.544 0.679 0.67 0.712 0.695\".split(), dtype=float)\n",
        "f1_score = np.array(\"0.67702768 0.72671443 0.49084249 0.44261501 0.52761394 0.52433735 0.69927909 0.68577277 0.74360313 0.69954706\".split(), dtype=float)\n",
        "\n",
        "# Calculate and print averages\n",
        "print(f\"Average Precision: {np.mean(precision):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(recall):.4f}\")\n",
        "print(f\"Average F1 Score: {np.mean(f1_score):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muUlt6ABe_pc",
        "outputId": "5900c7c6-2691-474e-f335-f6f44dbd088b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision: 0.6264\n",
            "Average Recall: 0.6192\n",
            "Average F1 Score: 0.6217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_uniform + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGlY6hz7bG7u",
        "outputId": "cc7993aa-2172-48d5-95af-aeb897d80d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170M/170M [00:02<00:00, 70.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6001 5832 5972 5930 6052 6002 6027 6091 6102 5991]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.88%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.93%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.28%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 11.05%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 13.70%\n",
            "Epoch [6/10], Training Loss: 2.299, Validation Accuracy: 15.66%\n",
            "Epoch [7/10], Training Loss: 2.298, Validation Accuracy: 16.33%\n",
            "Epoch [8/10], Training Loss: 2.296, Validation Accuracy: 16.77%\n",
            "Epoch [9/10], Training Loss: 2.294, Validation Accuracy: 16.84%\n",
            "Epoch [10/10], Training Loss: 2.291, Validation Accuracy: 16.83%\n",
            "Epoch [1/10], Training Loss: 2.289, Validation Accuracy: 16.75%\n",
            "Epoch [2/10], Training Loss: 2.283, Validation Accuracy: 16.62%\n",
            "Epoch [3/10], Training Loss: 2.276, Validation Accuracy: 16.61%\n",
            "Epoch [4/10], Training Loss: 2.265, Validation Accuracy: 16.59%\n",
            "Epoch [5/10], Training Loss: 2.249, Validation Accuracy: 17.43%\n",
            "Epoch [6/10], Training Loss: 2.228, Validation Accuracy: 18.31%\n",
            "Epoch [7/10], Training Loss: 2.202, Validation Accuracy: 19.27%\n",
            "Epoch [8/10], Training Loss: 2.170, Validation Accuracy: 19.68%\n",
            "Epoch [9/10], Training Loss: 2.131, Validation Accuracy: 20.40%\n",
            "Epoch [10/10], Training Loss: 2.093, Validation Accuracy: 21.08%\n",
            "Epoch [1/10], Training Loss: 2.071, Validation Accuracy: 23.44%\n",
            "Epoch [2/10], Training Loss: 2.045, Validation Accuracy: 24.81%\n",
            "Epoch [3/10], Training Loss: 2.025, Validation Accuracy: 25.75%\n",
            "Epoch [4/10], Training Loss: 2.006, Validation Accuracy: 26.83%\n",
            "Epoch [5/10], Training Loss: 1.987, Validation Accuracy: 26.70%\n",
            "Epoch [6/10], Training Loss: 1.967, Validation Accuracy: 27.23%\n",
            "Epoch [7/10], Training Loss: 1.945, Validation Accuracy: 28.10%\n",
            "Epoch [8/10], Training Loss: 1.923, Validation Accuracy: 28.71%\n",
            "Epoch [9/10], Training Loss: 1.899, Validation Accuracy: 29.94%\n",
            "Epoch [10/10], Training Loss: 1.879, Validation Accuracy: 30.37%\n",
            "Epoch [1/10], Training Loss: 1.899, Validation Accuracy: 31.39%\n",
            "Epoch [2/10], Training Loss: 1.875, Validation Accuracy: 32.38%\n",
            "Epoch [3/10], Training Loss: 1.856, Validation Accuracy: 33.12%\n",
            "Epoch [4/10], Training Loss: 1.834, Validation Accuracy: 33.68%\n",
            "Epoch [5/10], Training Loss: 1.819, Validation Accuracy: 33.75%\n",
            "Epoch [6/10], Training Loss: 1.798, Validation Accuracy: 34.31%\n",
            "Epoch [7/10], Training Loss: 1.782, Validation Accuracy: 34.74%\n",
            "Epoch [8/10], Training Loss: 1.770, Validation Accuracy: 35.10%\n",
            "Epoch [9/10], Training Loss: 1.754, Validation Accuracy: 35.44%\n",
            "Epoch [10/10], Training Loss: 1.738, Validation Accuracy: 35.70%\n",
            "Epoch [1/10], Training Loss: 1.721, Validation Accuracy: 36.08%\n",
            "Epoch [2/10], Training Loss: 1.705, Validation Accuracy: 36.42%\n",
            "Epoch [3/10], Training Loss: 1.690, Validation Accuracy: 37.31%\n",
            "Epoch [4/10], Training Loss: 1.671, Validation Accuracy: 37.38%\n",
            "Epoch [5/10], Training Loss: 1.662, Validation Accuracy: 38.41%\n",
            "Epoch [6/10], Training Loss: 1.649, Validation Accuracy: 37.41%\n",
            "Epoch [7/10], Training Loss: 1.638, Validation Accuracy: 38.96%\n",
            "Epoch [8/10], Training Loss: 1.627, Validation Accuracy: 38.34%\n",
            "Epoch [9/10], Training Loss: 1.616, Validation Accuracy: 39.01%\n",
            "Epoch [10/10], Training Loss: 1.606, Validation Accuracy: 39.36%\n",
            "Epoch [1/10], Training Loss: 1.625, Validation Accuracy: 40.29%\n",
            "Epoch [2/10], Training Loss: 1.612, Validation Accuracy: 40.10%\n",
            "Epoch [3/10], Training Loss: 1.604, Validation Accuracy: 40.41%\n",
            "Epoch [4/10], Training Loss: 1.593, Validation Accuracy: 40.79%\n",
            "Epoch [5/10], Training Loss: 1.586, Validation Accuracy: 40.59%\n",
            "Epoch [6/10], Training Loss: 1.578, Validation Accuracy: 41.29%\n",
            "Epoch [7/10], Training Loss: 1.570, Validation Accuracy: 41.24%\n",
            "Epoch [8/10], Training Loss: 1.565, Validation Accuracy: 40.11%\n",
            "Epoch [9/10], Training Loss: 1.556, Validation Accuracy: 41.71%\n",
            "Epoch [10/10], Training Loss: 1.547, Validation Accuracy: 41.89%\n",
            "Epoch [1/10], Training Loss: 1.575, Validation Accuracy: 42.47%\n",
            "Epoch [2/10], Training Loss: 1.569, Validation Accuracy: 42.29%\n",
            "Epoch [3/10], Training Loss: 1.555, Validation Accuracy: 42.77%\n",
            "Epoch [4/10], Training Loss: 1.548, Validation Accuracy: 43.17%\n",
            "Epoch [5/10], Training Loss: 1.533, Validation Accuracy: 43.45%\n",
            "Epoch [6/10], Training Loss: 1.530, Validation Accuracy: 43.10%\n",
            "Epoch [7/10], Training Loss: 1.530, Validation Accuracy: 43.51%\n",
            "Epoch [8/10], Training Loss: 1.523, Validation Accuracy: 43.44%\n",
            "Epoch [9/10], Training Loss: 1.513, Validation Accuracy: 43.99%\n",
            "Epoch [10/10], Training Loss: 1.504, Validation Accuracy: 43.44%\n",
            "Epoch [1/10], Training Loss: 1.530, Validation Accuracy: 44.04%\n",
            "Epoch [2/10], Training Loss: 1.523, Validation Accuracy: 44.93%\n",
            "Epoch [3/10], Training Loss: 1.516, Validation Accuracy: 44.45%\n",
            "Epoch [4/10], Training Loss: 1.509, Validation Accuracy: 44.87%\n",
            "Epoch [5/10], Training Loss: 1.498, Validation Accuracy: 44.82%\n",
            "Epoch [6/10], Training Loss: 1.492, Validation Accuracy: 45.16%\n",
            "Epoch [7/10], Training Loss: 1.486, Validation Accuracy: 45.68%\n",
            "Epoch [8/10], Training Loss: 1.479, Validation Accuracy: 44.63%\n",
            "Epoch [9/10], Training Loss: 1.474, Validation Accuracy: 45.78%\n",
            "Epoch [10/10], Training Loss: 1.466, Validation Accuracy: 45.94%\n",
            "Epoch [1/10], Training Loss: 1.497, Validation Accuracy: 46.23%\n",
            "Epoch [2/10], Training Loss: 1.480, Validation Accuracy: 45.96%\n",
            "Epoch [3/10], Training Loss: 1.474, Validation Accuracy: 46.52%\n",
            "Epoch [4/10], Training Loss: 1.464, Validation Accuracy: 46.14%\n",
            "Epoch [5/10], Training Loss: 1.458, Validation Accuracy: 46.45%\n",
            "Epoch [6/10], Training Loss: 1.449, Validation Accuracy: 46.93%\n",
            "Epoch [7/10], Training Loss: 1.446, Validation Accuracy: 47.22%\n",
            "Epoch [8/10], Training Loss: 1.436, Validation Accuracy: 46.25%\n",
            "Epoch [9/10], Training Loss: 1.432, Validation Accuracy: 47.62%\n",
            "Epoch [10/10], Training Loss: 1.418, Validation Accuracy: 47.96%\n",
            "Epoch [1/10], Training Loss: 1.436, Validation Accuracy: 47.12%\n",
            "Epoch [2/10], Training Loss: 1.427, Validation Accuracy: 47.82%\n",
            "Epoch [3/10], Training Loss: 1.417, Validation Accuracy: 47.49%\n",
            "Epoch [4/10], Training Loss: 1.406, Validation Accuracy: 47.43%\n",
            "Epoch [5/10], Training Loss: 1.400, Validation Accuracy: 48.27%\n",
            "Epoch [6/10], Training Loss: 1.385, Validation Accuracy: 48.77%\n",
            "Epoch [7/10], Training Loss: 1.379, Validation Accuracy: 48.93%\n",
            "Epoch [8/10], Training Loss: 1.371, Validation Accuracy: 48.89%\n",
            "Epoch [9/10], Training Loss: 1.365, Validation Accuracy: 49.32%\n",
            "Epoch [10/10], Training Loss: 1.363, Validation Accuracy: 49.19%\n",
            "Epoch [1/10], Training Loss: 1.409, Validation Accuracy: 48.79%\n",
            "Epoch [2/10], Training Loss: 1.392, Validation Accuracy: 49.40%\n",
            "Epoch [3/10], Training Loss: 1.384, Validation Accuracy: 49.15%\n",
            "Epoch [4/10], Training Loss: 1.372, Validation Accuracy: 50.06%\n",
            "Epoch [5/10], Training Loss: 1.365, Validation Accuracy: 49.88%\n",
            "Epoch [6/10], Training Loss: 1.354, Validation Accuracy: 49.93%\n",
            "Epoch [7/10], Training Loss: 1.354, Validation Accuracy: 50.28%\n",
            "Epoch [8/10], Training Loss: 1.346, Validation Accuracy: 50.00%\n",
            "Epoch [9/10], Training Loss: 1.335, Validation Accuracy: 50.44%\n",
            "Epoch [10/10], Training Loss: 1.329, Validation Accuracy: 50.57%\n",
            "Epoch [1/10], Training Loss: 1.367, Validation Accuracy: 51.31%\n",
            "Epoch [2/10], Training Loss: 1.349, Validation Accuracy: 50.69%\n",
            "Epoch [3/10], Training Loss: 1.341, Validation Accuracy: 51.15%\n",
            "Epoch [4/10], Training Loss: 1.332, Validation Accuracy: 51.48%\n",
            "Epoch [5/10], Training Loss: 1.322, Validation Accuracy: 51.54%\n",
            "Epoch [6/10], Training Loss: 1.318, Validation Accuracy: 51.42%\n",
            "Epoch [7/10], Training Loss: 1.303, Validation Accuracy: 51.75%\n",
            "Epoch [8/10], Training Loss: 1.299, Validation Accuracy: 52.30%\n",
            "Epoch [9/10], Training Loss: 1.288, Validation Accuracy: 51.62%\n",
            "Epoch [10/10], Training Loss: 1.278, Validation Accuracy: 51.24%\n",
            "Epoch [1/10], Training Loss: 1.350, Validation Accuracy: 51.54%\n",
            "Epoch [2/10], Training Loss: 1.329, Validation Accuracy: 52.10%\n",
            "Epoch [3/10], Training Loss: 1.319, Validation Accuracy: 52.50%\n",
            "Epoch [4/10], Training Loss: 1.306, Validation Accuracy: 52.13%\n",
            "Epoch [5/10], Training Loss: 1.292, Validation Accuracy: 52.81%\n",
            "Epoch [6/10], Training Loss: 1.289, Validation Accuracy: 52.30%\n",
            "Epoch [7/10], Training Loss: 1.278, Validation Accuracy: 52.81%\n",
            "Epoch [8/10], Training Loss: 1.270, Validation Accuracy: 53.06%\n",
            "Epoch [9/10], Training Loss: 1.269, Validation Accuracy: 51.99%\n",
            "Epoch [10/10], Training Loss: 1.257, Validation Accuracy: 52.94%\n",
            "Epoch [1/10], Training Loss: 1.306, Validation Accuracy: 53.62%\n",
            "Epoch [2/10], Training Loss: 1.283, Validation Accuracy: 52.67%\n",
            "Epoch [3/10], Training Loss: 1.281, Validation Accuracy: 53.19%\n",
            "Epoch [4/10], Training Loss: 1.263, Validation Accuracy: 53.52%\n",
            "Epoch [5/10], Training Loss: 1.260, Validation Accuracy: 53.46%\n",
            "Epoch [6/10], Training Loss: 1.247, Validation Accuracy: 52.70%\n",
            "Epoch [7/10], Training Loss: 1.236, Validation Accuracy: 53.35%\n",
            "Epoch [8/10], Training Loss: 1.221, Validation Accuracy: 53.76%\n",
            "Epoch [9/10], Training Loss: 1.225, Validation Accuracy: 53.41%\n",
            "Epoch [10/10], Training Loss: 1.210, Validation Accuracy: 54.27%\n",
            "Epoch [1/10], Training Loss: 1.273, Validation Accuracy: 54.11%\n",
            "Epoch [2/10], Training Loss: 1.249, Validation Accuracy: 54.67%\n",
            "Epoch [3/10], Training Loss: 1.234, Validation Accuracy: 54.31%\n",
            "Epoch [4/10], Training Loss: 1.222, Validation Accuracy: 54.40%\n",
            "Epoch [5/10], Training Loss: 1.214, Validation Accuracy: 55.15%\n",
            "Epoch [6/10], Training Loss: 1.201, Validation Accuracy: 54.97%\n",
            "Epoch [7/10], Training Loss: 1.202, Validation Accuracy: 53.88%\n",
            "Epoch [8/10], Training Loss: 1.196, Validation Accuracy: 53.80%\n",
            "Epoch [9/10], Training Loss: 1.180, Validation Accuracy: 55.03%\n",
            "Epoch [10/10], Training Loss: 1.175, Validation Accuracy: 55.11%\n",
            "Epoch [1/10], Training Loss: 1.263, Validation Accuracy: 55.20%\n",
            "Epoch [2/10], Training Loss: 1.236, Validation Accuracy: 55.16%\n",
            "Epoch [3/10], Training Loss: 1.231, Validation Accuracy: 54.85%\n",
            "Epoch [4/10], Training Loss: 1.217, Validation Accuracy: 55.29%\n",
            "Epoch [5/10], Training Loss: 1.205, Validation Accuracy: 55.15%\n",
            "Epoch [6/10], Training Loss: 1.197, Validation Accuracy: 54.68%\n",
            "Epoch [7/10], Training Loss: 1.188, Validation Accuracy: 55.16%\n",
            "Epoch [8/10], Training Loss: 1.178, Validation Accuracy: 55.55%\n",
            "Epoch [9/10], Training Loss: 1.167, Validation Accuracy: 55.65%\n",
            "Epoch [10/10], Training Loss: 1.160, Validation Accuracy: 55.79%\n",
            "Epoch [1/10], Training Loss: 1.225, Validation Accuracy: 55.84%\n",
            "Epoch [2/10], Training Loss: 1.209, Validation Accuracy: 55.43%\n",
            "Epoch [3/10], Training Loss: 1.190, Validation Accuracy: 55.95%\n",
            "Epoch [4/10], Training Loss: 1.175, Validation Accuracy: 55.52%\n",
            "Epoch [5/10], Training Loss: 1.164, Validation Accuracy: 56.11%\n",
            "Epoch [6/10], Training Loss: 1.157, Validation Accuracy: 55.41%\n",
            "Epoch [7/10], Training Loss: 1.151, Validation Accuracy: 55.15%\n",
            "Epoch [8/10], Training Loss: 1.140, Validation Accuracy: 55.93%\n",
            "Epoch [9/10], Training Loss: 1.132, Validation Accuracy: 55.83%\n",
            "Epoch [10/10], Training Loss: 1.120, Validation Accuracy: 55.84%\n",
            "Epoch [1/10], Training Loss: 1.217, Validation Accuracy: 55.84%\n",
            "Epoch [2/10], Training Loss: 1.197, Validation Accuracy: 56.41%\n",
            "Epoch [3/10], Training Loss: 1.184, Validation Accuracy: 56.66%\n",
            "Epoch [4/10], Training Loss: 1.163, Validation Accuracy: 56.17%\n",
            "Epoch [5/10], Training Loss: 1.155, Validation Accuracy: 56.44%\n",
            "Epoch [6/10], Training Loss: 1.143, Validation Accuracy: 56.59%\n",
            "Epoch [7/10], Training Loss: 1.130, Validation Accuracy: 56.55%\n",
            "Epoch [8/10], Training Loss: 1.119, Validation Accuracy: 56.36%\n",
            "Epoch [9/10], Training Loss: 1.120, Validation Accuracy: 56.50%\n",
            "Epoch [10/10], Training Loss: 1.109, Validation Accuracy: 56.45%\n",
            "Epoch [1/10], Training Loss: 1.191, Validation Accuracy: 57.35%\n",
            "Epoch [2/10], Training Loss: 1.160, Validation Accuracy: 57.19%\n",
            "Epoch [3/10], Training Loss: 1.148, Validation Accuracy: 56.78%\n",
            "Epoch [4/10], Training Loss: 1.137, Validation Accuracy: 56.42%\n",
            "Epoch [5/10], Training Loss: 1.116, Validation Accuracy: 57.45%\n",
            "Epoch [6/10], Training Loss: 1.108, Validation Accuracy: 57.21%\n",
            "Epoch [7/10], Training Loss: 1.094, Validation Accuracy: 57.34%\n",
            "Epoch [8/10], Training Loss: 1.085, Validation Accuracy: 56.66%\n",
            "Epoch [9/10], Training Loss: 1.077, Validation Accuracy: 57.20%\n",
            "Epoch [10/10], Training Loss: 1.069, Validation Accuracy: 57.39%\n",
            "Epoch [1/10], Training Loss: 1.163, Validation Accuracy: 57.77%\n",
            "Epoch [2/10], Training Loss: 1.130, Validation Accuracy: 58.19%\n",
            "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 57.77%\n",
            "Epoch [4/10], Training Loss: 1.100, Validation Accuracy: 57.66%\n",
            "Epoch [5/10], Training Loss: 1.086, Validation Accuracy: 57.82%\n",
            "Epoch [6/10], Training Loss: 1.069, Validation Accuracy: 57.78%\n",
            "Epoch [7/10], Training Loss: 1.063, Validation Accuracy: 58.13%\n",
            "Epoch [8/10], Training Loss: 1.053, Validation Accuracy: 58.31%\n",
            "Epoch [9/10], Training Loss: 1.048, Validation Accuracy: 57.38%\n",
            "Epoch [10/10], Training Loss: 1.033, Validation Accuracy: 57.80%\n",
            "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 57.92%\n",
            "Epoch [2/10], Training Loss: 1.140, Validation Accuracy: 58.21%\n",
            "Epoch [3/10], Training Loss: 1.120, Validation Accuracy: 58.49%\n",
            "Epoch [4/10], Training Loss: 1.109, Validation Accuracy: 58.80%\n",
            "Epoch [5/10], Training Loss: 1.083, Validation Accuracy: 57.66%\n",
            "Epoch [6/10], Training Loss: 1.083, Validation Accuracy: 57.00%\n",
            "Epoch [7/10], Training Loss: 1.060, Validation Accuracy: 58.31%\n",
            "Epoch [8/10], Training Loss: 1.053, Validation Accuracy: 57.64%\n",
            "Epoch [9/10], Training Loss: 1.040, Validation Accuracy: 58.25%\n",
            "Epoch [10/10], Training Loss: 1.035, Validation Accuracy: 57.95%\n",
            "Epoch [1/10], Training Loss: 1.127, Validation Accuracy: 58.27%\n",
            "Epoch [2/10], Training Loss: 1.103, Validation Accuracy: 58.27%\n",
            "Epoch [3/10], Training Loss: 1.078, Validation Accuracy: 58.17%\n",
            "Epoch [4/10], Training Loss: 1.062, Validation Accuracy: 58.07%\n",
            "Epoch [5/10], Training Loss: 1.047, Validation Accuracy: 57.60%\n",
            "Epoch [6/10], Training Loss: 1.040, Validation Accuracy: 58.95%\n",
            "Epoch [7/10], Training Loss: 1.018, Validation Accuracy: 58.42%\n",
            "Epoch [8/10], Training Loss: 1.010, Validation Accuracy: 57.70%\n",
            "Epoch [9/10], Training Loss: 1.004, Validation Accuracy: 58.87%\n",
            "Epoch [10/10], Training Loss: 0.994, Validation Accuracy: 58.47%\n",
            "Epoch [1/10], Training Loss: 1.134, Validation Accuracy: 58.54%\n",
            "Epoch [2/10], Training Loss: 1.091, Validation Accuracy: 59.13%\n",
            "Epoch [3/10], Training Loss: 1.078, Validation Accuracy: 59.20%\n",
            "Epoch [4/10], Training Loss: 1.052, Validation Accuracy: 59.10%\n",
            "Epoch [5/10], Training Loss: 1.036, Validation Accuracy: 59.15%\n",
            "Epoch [6/10], Training Loss: 1.030, Validation Accuracy: 58.62%\n",
            "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 58.78%\n",
            "Epoch [8/10], Training Loss: 1.005, Validation Accuracy: 58.42%\n",
            "Epoch [9/10], Training Loss: 0.996, Validation Accuracy: 59.12%\n",
            "Epoch [10/10], Training Loss: 0.978, Validation Accuracy: 58.76%\n",
            "Epoch [1/10], Training Loss: 1.115, Validation Accuracy: 59.73%\n",
            "Epoch [2/10], Training Loss: 1.069, Validation Accuracy: 59.87%\n",
            "Epoch [3/10], Training Loss: 1.049, Validation Accuracy: 59.94%\n",
            "Epoch [4/10], Training Loss: 1.026, Validation Accuracy: 59.82%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.47%\n",
            "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 59.92%\n",
            "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 58.99%\n",
            "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.36%\n",
            "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 59.47%\n",
            "Epoch [10/10], Training Loss: 0.951, Validation Accuracy: 59.07%\n",
            "Epoch [1/10], Training Loss: 1.075, Validation Accuracy: 60.02%\n",
            "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.82%\n",
            "Epoch [3/10], Training Loss: 1.007, Validation Accuracy: 59.11%\n",
            "Epoch [4/10], Training Loss: 0.993, Validation Accuracy: 59.97%\n",
            "Epoch [5/10], Training Loss: 0.986, Validation Accuracy: 60.15%\n",
            "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 60.07%\n",
            "Epoch [7/10], Training Loss: 0.952, Validation Accuracy: 59.65%\n",
            "Epoch [8/10], Training Loss: 0.941, Validation Accuracy: 59.51%\n",
            "Epoch [9/10], Training Loss: 0.937, Validation Accuracy: 59.50%\n",
            "Epoch [10/10], Training Loss: 0.915, Validation Accuracy: 60.01%\n",
            "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 60.30%\n",
            "Epoch [2/10], Training Loss: 1.047, Validation Accuracy: 59.63%\n",
            "Epoch [3/10], Training Loss: 1.024, Validation Accuracy: 60.04%\n",
            "Epoch [4/10], Training Loss: 0.999, Validation Accuracy: 60.22%\n",
            "Epoch [5/10], Training Loss: 0.981, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.966, Validation Accuracy: 59.96%\n",
            "Epoch [7/10], Training Loss: 0.946, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.941, Validation Accuracy: 60.07%\n",
            "Epoch [9/10], Training Loss: 0.927, Validation Accuracy: 59.91%\n",
            "Epoch [10/10], Training Loss: 0.918, Validation Accuracy: 59.80%\n",
            "Epoch [1/10], Training Loss: 1.050, Validation Accuracy: 59.83%\n",
            "Epoch [2/10], Training Loss: 1.014, Validation Accuracy: 60.36%\n",
            "Epoch [3/10], Training Loss: 0.990, Validation Accuracy: 60.44%\n",
            "Epoch [4/10], Training Loss: 0.970, Validation Accuracy: 60.11%\n",
            "Epoch [5/10], Training Loss: 0.945, Validation Accuracy: 60.34%\n",
            "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 60.02%\n",
            "Epoch [7/10], Training Loss: 0.915, Validation Accuracy: 60.21%\n",
            "Epoch [8/10], Training Loss: 0.907, Validation Accuracy: 60.67%\n",
            "Epoch [9/10], Training Loss: 0.895, Validation Accuracy: 60.33%\n",
            "Epoch [10/10], Training Loss: 0.877, Validation Accuracy: 59.97%\n",
            "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 59.91%\n",
            "Epoch [2/10], Training Loss: 1.014, Validation Accuracy: 60.15%\n",
            "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 60.34%\n",
            "Epoch [4/10], Training Loss: 0.961, Validation Accuracy: 59.85%\n",
            "Epoch [5/10], Training Loss: 0.944, Validation Accuracy: 60.46%\n",
            "Epoch [6/10], Training Loss: 0.935, Validation Accuracy: 59.03%\n",
            "Epoch [7/10], Training Loss: 0.922, Validation Accuracy: 60.38%\n",
            "Epoch [8/10], Training Loss: 0.903, Validation Accuracy: 60.19%\n",
            "Epoch [9/10], Training Loss: 0.883, Validation Accuracy: 60.21%\n",
            "Epoch [10/10], Training Loss: 0.877, Validation Accuracy: 59.30%\n",
            "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 0.994, Validation Accuracy: 60.56%\n",
            "Epoch [3/10], Training Loss: 0.968, Validation Accuracy: 60.68%\n",
            "Epoch [4/10], Training Loss: 0.941, Validation Accuracy: 59.96%\n",
            "Epoch [5/10], Training Loss: 0.924, Validation Accuracy: 59.26%\n",
            "Epoch [6/10], Training Loss: 0.912, Validation Accuracy: 60.38%\n",
            "Epoch [7/10], Training Loss: 0.899, Validation Accuracy: 60.22%\n",
            "Epoch [8/10], Training Loss: 0.881, Validation Accuracy: 60.72%\n",
            "Epoch [9/10], Training Loss: 0.860, Validation Accuracy: 61.05%\n",
            "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 60.66%\n",
            "Epoch [1/10], Training Loss: 1.014, Validation Accuracy: 60.89%\n",
            "Epoch [2/10], Training Loss: 0.959, Validation Accuracy: 60.38%\n",
            "Epoch [3/10], Training Loss: 0.929, Validation Accuracy: 61.19%\n",
            "Epoch [4/10], Training Loss: 0.899, Validation Accuracy: 61.21%\n",
            "Epoch [5/10], Training Loss: 0.888, Validation Accuracy: 61.26%\n",
            "Epoch [6/10], Training Loss: 0.864, Validation Accuracy: 60.95%\n",
            "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 60.61%\n",
            "Epoch [8/10], Training Loss: 0.835, Validation Accuracy: 60.38%\n",
            "Epoch [9/10], Training Loss: 0.823, Validation Accuracy: 60.22%\n",
            "Epoch [10/10], Training Loss: 0.799, Validation Accuracy: 60.50%\n",
            "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 59.94%\n",
            "Epoch [2/10], Training Loss: 0.971, Validation Accuracy: 61.02%\n",
            "Epoch [3/10], Training Loss: 0.937, Validation Accuracy: 59.92%\n",
            "Epoch [4/10], Training Loss: 0.915, Validation Accuracy: 61.18%\n",
            "Epoch [5/10], Training Loss: 0.888, Validation Accuracy: 61.04%\n",
            "Epoch [6/10], Training Loss: 0.872, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.858, Validation Accuracy: 61.15%\n",
            "Epoch [8/10], Training Loss: 0.844, Validation Accuracy: 59.18%\n",
            "Epoch [9/10], Training Loss: 0.835, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.817, Validation Accuracy: 61.07%\n",
            "Epoch [1/10], Training Loss: 0.997, Validation Accuracy: 60.41%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.10%\n",
            "Epoch [3/10], Training Loss: 0.907, Validation Accuracy: 60.72%\n",
            "Epoch [4/10], Training Loss: 0.879, Validation Accuracy: 60.45%\n",
            "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.848, Validation Accuracy: 60.56%\n",
            "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.06%\n",
            "Epoch [8/10], Training Loss: 0.809, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.779, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.992, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.941, Validation Accuracy: 59.74%\n",
            "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.876, Validation Accuracy: 60.77%\n",
            "Epoch [5/10], Training Loss: 0.853, Validation Accuracy: 60.21%\n",
            "Epoch [6/10], Training Loss: 0.843, Validation Accuracy: 60.25%\n",
            "Epoch [7/10], Training Loss: 0.822, Validation Accuracy: 61.32%\n",
            "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 60.81%\n",
            "Epoch [9/10], Training Loss: 0.792, Validation Accuracy: 60.50%\n",
            "Epoch [10/10], Training Loss: 0.771, Validation Accuracy: 60.91%\n",
            "Epoch [1/10], Training Loss: 0.993, Validation Accuracy: 59.71%\n",
            "Epoch [2/10], Training Loss: 0.922, Validation Accuracy: 61.05%\n",
            "Epoch [3/10], Training Loss: 0.891, Validation Accuracy: 61.31%\n",
            "Epoch [4/10], Training Loss: 0.863, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.848, Validation Accuracy: 60.95%\n",
            "Epoch [6/10], Training Loss: 0.823, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 61.08%\n",
            "Epoch [8/10], Training Loss: 0.785, Validation Accuracy: 61.13%\n",
            "Epoch [9/10], Training Loss: 0.766, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.758, Validation Accuracy: 60.25%\n",
            "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 60.56%\n",
            "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 60.92%\n",
            "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 61.01%\n",
            "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 61.00%\n",
            "Epoch [5/10], Training Loss: 0.803, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.779, Validation Accuracy: 61.05%\n",
            "Epoch [7/10], Training Loss: 0.759, Validation Accuracy: 60.35%\n",
            "Epoch [8/10], Training Loss: 0.744, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 59.98%\n",
            "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 60.41%\n",
            "Epoch [1/10], Training Loss: 0.990, Validation Accuracy: 60.83%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.24%\n",
            "Epoch [3/10], Training Loss: 0.865, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.836, Validation Accuracy: 60.99%\n",
            "Epoch [5/10], Training Loss: 0.811, Validation Accuracy: 60.56%\n",
            "Epoch [6/10], Training Loss: 0.789, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.770, Validation Accuracy: 60.72%\n",
            "Epoch [8/10], Training Loss: 0.763, Validation Accuracy: 60.58%\n",
            "Epoch [9/10], Training Loss: 0.730, Validation Accuracy: 60.85%\n",
            "Epoch [10/10], Training Loss: 0.720, Validation Accuracy: 60.83%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 60.86%\n",
            "Epoch [2/10], Training Loss: 0.869, Validation Accuracy: 60.93%\n",
            "Epoch [3/10], Training Loss: 0.827, Validation Accuracy: 60.80%\n",
            "Epoch [4/10], Training Loss: 0.802, Validation Accuracy: 61.25%\n",
            "Epoch [5/10], Training Loss: 0.780, Validation Accuracy: 60.98%\n",
            "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 60.74%\n",
            "Epoch [7/10], Training Loss: 0.736, Validation Accuracy: 60.87%\n",
            "Epoch [8/10], Training Loss: 0.724, Validation Accuracy: 60.74%\n",
            "Epoch [9/10], Training Loss: 0.704, Validation Accuracy: 61.08%\n",
            "Epoch [10/10], Training Loss: 0.686, Validation Accuracy: 60.83%\n",
            "Epoch [1/10], Training Loss: 0.934, Validation Accuracy: 60.96%\n",
            "Epoch [2/10], Training Loss: 0.871, Validation Accuracy: 60.56%\n",
            "Epoch [3/10], Training Loss: 0.826, Validation Accuracy: 60.78%\n",
            "Epoch [4/10], Training Loss: 0.793, Validation Accuracy: 60.91%\n",
            "Epoch [5/10], Training Loss: 0.771, Validation Accuracy: 60.44%\n",
            "Epoch [6/10], Training Loss: 0.754, Validation Accuracy: 60.71%\n",
            "Epoch [7/10], Training Loss: 0.725, Validation Accuracy: 60.86%\n",
            "Epoch [8/10], Training Loss: 0.710, Validation Accuracy: 61.00%\n",
            "Epoch [9/10], Training Loss: 0.695, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.670, Validation Accuracy: 60.36%\n",
            "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 61.37%\n",
            "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 59.88%\n",
            "Epoch [3/10], Training Loss: 0.832, Validation Accuracy: 61.27%\n",
            "Epoch [4/10], Training Loss: 0.791, Validation Accuracy: 61.09%\n",
            "Epoch [5/10], Training Loss: 0.773, Validation Accuracy: 60.92%\n",
            "Epoch [6/10], Training Loss: 0.740, Validation Accuracy: 61.82%\n",
            "Epoch [7/10], Training Loss: 0.718, Validation Accuracy: 61.26%\n",
            "Epoch [8/10], Training Loss: 0.712, Validation Accuracy: 60.84%\n",
            "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 61.18%\n",
            "Epoch [10/10], Training Loss: 0.671, Validation Accuracy: 61.41%\n",
            "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 60.93%\n",
            "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.785, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.748, Validation Accuracy: 61.07%\n",
            "Epoch [5/10], Training Loss: 0.722, Validation Accuracy: 60.94%\n",
            "Epoch [6/10], Training Loss: 0.691, Validation Accuracy: 61.02%\n",
            "Epoch [7/10], Training Loss: 0.675, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.650, Validation Accuracy: 60.50%\n",
            "Epoch [9/10], Training Loss: 0.643, Validation Accuracy: 60.19%\n",
            "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 60.83%\n",
            "Epoch [2/10], Training Loss: 0.847, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.794, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.762, Validation Accuracy: 60.47%\n",
            "Epoch [5/10], Training Loss: 0.729, Validation Accuracy: 60.63%\n",
            "Epoch [6/10], Training Loss: 0.706, Validation Accuracy: 60.81%\n",
            "Epoch [7/10], Training Loss: 0.678, Validation Accuracy: 61.31%\n",
            "Epoch [8/10], Training Loss: 0.668, Validation Accuracy: 60.69%\n",
            "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 60.60%\n",
            "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 60.54%\n",
            "Epoch [1/10], Training Loss: 0.891, Validation Accuracy: 60.53%\n",
            "Epoch [2/10], Training Loss: 0.812, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.766, Validation Accuracy: 60.13%\n",
            "Epoch [4/10], Training Loss: 0.729, Validation Accuracy: 61.32%\n",
            "Epoch [5/10], Training Loss: 0.698, Validation Accuracy: 61.09%\n",
            "Epoch [6/10], Training Loss: 0.672, Validation Accuracy: 61.35%\n",
            "Epoch [7/10], Training Loss: 0.652, Validation Accuracy: 61.00%\n",
            "Epoch [8/10], Training Loss: 0.639, Validation Accuracy: 59.83%\n",
            "Epoch [9/10], Training Loss: 0.608, Validation Accuracy: 60.97%\n",
            "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 59.80%\n",
            "Epoch [2/10], Training Loss: 0.809, Validation Accuracy: 60.35%\n",
            "Epoch [3/10], Training Loss: 0.747, Validation Accuracy: 60.86%\n",
            "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 60.14%\n",
            "Epoch [5/10], Training Loss: 0.688, Validation Accuracy: 60.74%\n",
            "Epoch [6/10], Training Loss: 0.665, Validation Accuracy: 60.46%\n",
            "Epoch [7/10], Training Loss: 0.645, Validation Accuracy: 60.35%\n",
            "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 60.59%\n",
            "Epoch [9/10], Training Loss: 0.607, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.578, Validation Accuracy: 59.95%\n",
            "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 60.33%\n",
            "Epoch [2/10], Training Loss: 0.811, Validation Accuracy: 59.66%\n",
            "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 59.88%\n",
            "Epoch [4/10], Training Loss: 0.728, Validation Accuracy: 61.39%\n",
            "Epoch [5/10], Training Loss: 0.690, Validation Accuracy: 60.37%\n",
            "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.633, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.617, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.592, Validation Accuracy: 60.93%\n",
            "Epoch [10/10], Training Loss: 0.576, Validation Accuracy: 60.68%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 60.09%\n",
            "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 60.78%\n",
            "Epoch [3/10], Training Loss: 0.710, Validation Accuracy: 60.29%\n",
            "Epoch [4/10], Training Loss: 0.672, Validation Accuracy: 60.72%\n",
            "Epoch [5/10], Training Loss: 0.634, Validation Accuracy: 61.22%\n",
            "Epoch [6/10], Training Loss: 0.606, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.585, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.566, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 60.27%\n",
            "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 0.899, Validation Accuracy: 59.86%\n",
            "Epoch [2/10], Training Loss: 0.782, Validation Accuracy: 60.53%\n",
            "Epoch [3/10], Training Loss: 0.715, Validation Accuracy: 60.44%\n",
            "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 60.70%\n",
            "Epoch [5/10], Training Loss: 0.645, Validation Accuracy: 59.96%\n",
            "Epoch [6/10], Training Loss: 0.616, Validation Accuracy: 60.37%\n",
            "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 60.02%\n",
            "Epoch [9/10], Training Loss: 0.552, Validation Accuracy: 59.90%\n",
            "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 60.18%\n",
            "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 59.79%\n",
            "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 60.48%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 60.16%\n",
            "Epoch [4/10], Training Loss: 0.662, Validation Accuracy: 60.48%\n",
            "Epoch [5/10], Training Loss: 0.622, Validation Accuracy: 60.52%\n",
            "Epoch [6/10], Training Loss: 0.596, Validation Accuracy: 60.75%\n",
            "Epoch [7/10], Training Loss: 0.557, Validation Accuracy: 60.32%\n",
            "Epoch [8/10], Training Loss: 0.544, Validation Accuracy: 60.41%\n",
            "Epoch [9/10], Training Loss: 0.529, Validation Accuracy: 60.05%\n",
            "Epoch [10/10], Training Loss: 0.498, Validation Accuracy: 60.13%\n",
            "Epoch [1/10], Training Loss: 0.847, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 0.741, Validation Accuracy: 60.34%\n",
            "Epoch [3/10], Training Loss: 0.675, Validation Accuracy: 60.42%\n",
            "Epoch [4/10], Training Loss: 0.637, Validation Accuracy: 60.21%\n",
            "Epoch [5/10], Training Loss: 0.602, Validation Accuracy: 59.94%\n",
            "Epoch [6/10], Training Loss: 0.583, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 59.51%\n",
            "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 60.02%\n",
            "Epoch [9/10], Training Loss: 0.508, Validation Accuracy: 59.94%\n",
            "Epoch [10/10], Training Loss: 0.487, Validation Accuracy: 60.06%\n",
            "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 60.13%\n",
            "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 60.85%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 60.92%\n",
            "Epoch [4/10], Training Loss: 0.651, Validation Accuracy: 60.11%\n",
            "Epoch [5/10], Training Loss: 0.617, Validation Accuracy: 60.46%\n",
            "Epoch [6/10], Training Loss: 0.590, Validation Accuracy: 60.52%\n",
            "Epoch [7/10], Training Loss: 0.554, Validation Accuracy: 60.90%\n",
            "Epoch [8/10], Training Loss: 0.533, Validation Accuracy: 60.77%\n",
            "Epoch [9/10], Training Loss: 0.503, Validation Accuracy: 59.93%\n",
            "Epoch [10/10], Training Loss: 0.484, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.821, Validation Accuracy: 59.58%\n",
            "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 60.77%\n",
            "Epoch [3/10], Training Loss: 0.642, Validation Accuracy: 60.67%\n",
            "Epoch [4/10], Training Loss: 0.597, Validation Accuracy: 60.59%\n",
            "Epoch [5/10], Training Loss: 0.557, Validation Accuracy: 60.47%\n",
            "Epoch [6/10], Training Loss: 0.524, Validation Accuracy: 60.60%\n",
            "Epoch [7/10], Training Loss: 0.497, Validation Accuracy: 60.25%\n",
            "Epoch [8/10], Training Loss: 0.470, Validation Accuracy: 60.13%\n",
            "Epoch [9/10], Training Loss: 0.446, Validation Accuracy: 60.02%\n",
            "Epoch [10/10], Training Loss: 0.433, Validation Accuracy: 59.70%\n",
            "Confusion Matrix:\n",
            "[[660  34  71  27  27   7  18  14  82  60]\n",
            " [ 35 749   9  16   5  10  10  10  33 123]\n",
            " [ 80  13 491 115  79  85  61  42  22  12]\n",
            " [ 19  15  88 508  65 160  55  51  17  22]\n",
            " [ 41  10 128 124 447  70  69  93   9   9]\n",
            " [ 14   6  81 247  47 482  30  77   8   8]\n",
            " [ 10  17  81 118  45  60 626  20  10  13]\n",
            " [ 17   7  36  73  52  88  13 681   6  27]\n",
            " [103  56  18  23  15  17  13   9 695  51]\n",
            " [ 56 151  12  31  10  12  13  34  45 636]]\n",
            "Test Accuracy: 59.75%\n",
            "True Positives (TP): [660 749 491 508 447 482 626 681 695 636]\n",
            "False Positives (FP): [375 309 524 774 345 509 282 350 232 325]\n",
            "True Negatives (TN): [8625 8691 8476 8226 8655 8491 8718 8650 8768 8675]\n",
            "False Negatives (FN): [340 251 509 492 553 518 374 319 305 364]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.63768116 0.70793951 0.48374384 0.39625585 0.56439394 0.4863774\n",
            " 0.68942731 0.66052376 0.74973031 0.66181061]\n",
            "Recall: [0.66  0.749 0.491 0.508 0.447 0.482 0.626 0.681 0.695 0.636]\n",
            "F1 Score: [0.64864865 0.72789116 0.48734491 0.44522349 0.49888393 0.4841788\n",
            " 0.65618449 0.67060561 0.72132849 0.64864865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvkxuplhowrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "\n",
        "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 9.88%\n",
        "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 9.93%\n",
        "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.28%\n",
        "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 11.05%\n",
        "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 13.70%\n",
        "Epoch [6/10], Training Loss: 2.299, Validation Accuracy: 15.66%\n",
        "Epoch [7/10], Training Loss: 2.298, Validation Accuracy: 16.33%\n",
        "Epoch [8/10], Training Loss: 2.296, Validation Accuracy: 16.77%\n",
        "Epoch [9/10], Training Loss: 2.294, Validation Accuracy: 16.84%\n",
        "Epoch [10/10], Training Loss: 2.291, Validation Accuracy: 16.83%\n",
        "Epoch [1/10], Training Loss: 2.289, Validation Accuracy: 16.75%\n",
        "Epoch [2/10], Training Loss: 2.283, Validation Accuracy: 16.62%\n",
        "Epoch [3/10], Training Loss: 2.276, Validation Accuracy: 16.61%\n",
        "Epoch [4/10], Training Loss: 2.265, Validation Accuracy: 16.59%\n",
        "Epoch [5/10], Training Loss: 2.249, Validation Accuracy: 17.43%\n",
        "Epoch [6/10], Training Loss: 2.228, Validation Accuracy: 18.31%\n",
        "Epoch [7/10], Training Loss: 2.202, Validation Accuracy: 19.27%\n",
        "Epoch [8/10], Training Loss: 2.170, Validation Accuracy: 19.68%\n",
        "Epoch [9/10], Training Loss: 2.131, Validation Accuracy: 20.40%\n",
        "Epoch [10/10], Training Loss: 2.093, Validation Accuracy: 21.08%\n",
        "Epoch [1/10], Training Loss: 2.071, Validation Accuracy: 23.44%\n",
        "Epoch [2/10], Training Loss: 2.045, Validation Accuracy: 24.81%\n",
        "Epoch [3/10], Training Loss: 2.025, Validation Accuracy: 25.75%\n",
        "Epoch [4/10], Training Loss: 2.006, Validation Accuracy: 26.83%\n",
        "Epoch [5/10], Training Loss: 1.987, Validation Accuracy: 26.70%\n",
        "Epoch [6/10], Training Loss: 1.967, Validation Accuracy: 27.23%\n",
        "Epoch [7/10], Training Loss: 1.945, Validation Accuracy: 28.10%\n",
        "Epoch [8/10], Training Loss: 1.923, Validation Accuracy: 28.71%\n",
        "Epoch [9/10], Training Loss: 1.899, Validation Accuracy: 29.94%\n",
        "Epoch [10/10], Training Loss: 1.879, Validation Accuracy: 30.37%\n",
        "Epoch [1/10], Training Loss: 1.899, Validation Accuracy: 31.39%\n",
        "Epoch [2/10], Training Loss: 1.875, Validation Accuracy: 32.38%\n",
        "Epoch [3/10], Training Loss: 1.856, Validation Accuracy: 33.12%\n",
        "Epoch [4/10], Training Loss: 1.834, Validation Accuracy: 33.68%\n",
        "Epoch [5/10], Training Loss: 1.819, Validation Accuracy: 33.75%\n",
        "Epoch [6/10], Training Loss: 1.798, Validation Accuracy: 34.31%\n",
        "Epoch [7/10], Training Loss: 1.782, Validation Accuracy: 34.74%\n",
        "Epoch [8/10], Training Loss: 1.770, Validation Accuracy: 35.10%\n",
        "Epoch [9/10], Training Loss: 1.754, Validation Accuracy: 35.44%\n",
        "Epoch [10/10], Training Loss: 1.738, Validation Accuracy: 35.70%\n",
        "Epoch [1/10], Training Loss: 1.721, Validation Accuracy: 36.08%\n",
        "Epoch [2/10], Training Loss: 1.705, Validation Accuracy: 36.42%\n",
        "Epoch [3/10], Training Loss: 1.690, Validation Accuracy: 37.31%\n",
        "Epoch [4/10], Training Loss: 1.671, Validation Accuracy: 37.38%\n",
        "Epoch [5/10], Training Loss: 1.662, Validation Accuracy: 38.41%\n",
        "Epoch [6/10], Training Loss: 1.649, Validation Accuracy: 37.41%\n",
        "Epoch [7/10], Training Loss: 1.638, Validation Accuracy: 38.96%\n",
        "Epoch [8/10], Training Loss: 1.627, Validation Accuracy: 38.34%\n",
        "Epoch [9/10], Training Loss: 1.616, Validation Accuracy: 39.01%\n",
        "Epoch [10/10], Training Loss: 1.606, Validation Accuracy: 39.36%\n",
        "Epoch [1/10], Training Loss: 1.625, Validation Accuracy: 40.29%\n",
        "Epoch [2/10], Training Loss: 1.612, Validation Accuracy: 40.10%\n",
        "Epoch [3/10], Training Loss: 1.604, Validation Accuracy: 40.41%\n",
        "Epoch [4/10], Training Loss: 1.593, Validation Accuracy: 40.79%\n",
        "Epoch [5/10], Training Loss: 1.586, Validation Accuracy: 40.59%\n",
        "Epoch [6/10], Training Loss: 1.578, Validation Accuracy: 41.29%\n",
        "Epoch [7/10], Training Loss: 1.570, Validation Accuracy: 41.24%\n",
        "Epoch [8/10], Training Loss: 1.565, Validation Accuracy: 40.11%\n",
        "Epoch [9/10], Training Loss: 1.556, Validation Accuracy: 41.71%\n",
        "Epoch [10/10], Training Loss: 1.547, Validation Accuracy: 41.89%\n",
        "Epoch [1/10], Training Loss: 1.575, Validation Accuracy: 42.47%\n",
        "Epoch [2/10], Training Loss: 1.569, Validation Accuracy: 42.29%\n",
        "Epoch [3/10], Training Loss: 1.555, Validation Accuracy: 42.77%\n",
        "Epoch [4/10], Training Loss: 1.548, Validation Accuracy: 43.17%\n",
        "Epoch [5/10], Training Loss: 1.533, Validation Accuracy: 43.45%\n",
        "Epoch [6/10], Training Loss: 1.530, Validation Accuracy: 43.10%\n",
        "Epoch [7/10], Training Loss: 1.530, Validation Accuracy: 43.51%\n",
        "Epoch [8/10], Training Loss: 1.523, Validation Accuracy: 43.44%\n",
        "Epoch [9/10], Training Loss: 1.513, Validation Accuracy: 43.99%\n",
        "Epoch [10/10], Training Loss: 1.504, Validation Accuracy: 43.44%\n",
        "Epoch [1/10], Training Loss: 1.530, Validation Accuracy: 44.04%\n",
        "Epoch [2/10], Training Loss: 1.523, Validation Accuracy: 44.93%\n",
        "Epoch [3/10], Training Loss: 1.516, Validation Accuracy: 44.45%\n",
        "Epoch [4/10], Training Loss: 1.509, Validation Accuracy: 44.87%\n",
        "Epoch [5/10], Training Loss: 1.498, Validation Accuracy: 44.82%\n",
        "Epoch [6/10], Training Loss: 1.492, Validation Accuracy: 45.16%\n",
        "Epoch [7/10], Training Loss: 1.486, Validation Accuracy: 45.68%\n",
        "Epoch [8/10], Training Loss: 1.479, Validation Accuracy: 44.63%\n",
        "Epoch [9/10], Training Loss: 1.474, Validation Accuracy: 45.78%\n",
        "Epoch [10/10], Training Loss: 1.466, Validation Accuracy: 45.94%\n",
        "Epoch [1/10], Training Loss: 1.497, Validation Accuracy: 46.23%\n",
        "Epoch [2/10], Training Loss: 1.480, Validation Accuracy: 45.96%\n",
        "Epoch [3/10], Training Loss: 1.474, Validation Accuracy: 46.52%\n",
        "Epoch [4/10], Training Loss: 1.464, Validation Accuracy: 46.14%\n",
        "Epoch [5/10], Training Loss: 1.458, Validation Accuracy: 46.45%\n",
        "Epoch [6/10], Training Loss: 1.449, Validation Accuracy: 46.93%\n",
        "Epoch [7/10], Training Loss: 1.446, Validation Accuracy: 47.22%\n",
        "Epoch [8/10], Training Loss: 1.436, Validation Accuracy: 46.25%\n",
        "Epoch [9/10], Training Loss: 1.432, Validation Accuracy: 47.62%\n",
        "Epoch [10/10], Training Loss: 1.418, Validation Accuracy: 47.96%\n",
        "Epoch [1/10], Training Loss: 1.436, Validation Accuracy: 47.12%\n",
        "Epoch [2/10], Training Loss: 1.427, Validation Accuracy: 47.82%\n",
        "Epoch [3/10], Training Loss: 1.417, Validation Accuracy: 47.49%\n",
        "Epoch [4/10], Training Loss: 1.406, Validation Accuracy: 47.43%\n",
        "Epoch [5/10], Training Loss: 1.400, Validation Accuracy: 48.27%\n",
        "Epoch [6/10], Training Loss: 1.385, Validation Accuracy: 48.77%\n",
        "Epoch [7/10], Training Loss: 1.379, Validation Accuracy: 48.93%\n",
        "Epoch [8/10], Training Loss: 1.371, Validation Accuracy: 48.89%\n",
        "Epoch [9/10], Training Loss: 1.365, Validation Accuracy: 49.32%\n",
        "Epoch [10/10], Training Loss: 1.363, Validation Accuracy: 49.19%\n",
        "Epoch [1/10], Training Loss: 1.409, Validation Accuracy: 48.79%\n",
        "Epoch [2/10], Training Loss: 1.392, Validation Accuracy: 49.40%\n",
        "Epoch [3/10], Training Loss: 1.384, Validation Accuracy: 49.15%\n",
        "Epoch [4/10], Training Loss: 1.372, Validation Accuracy: 50.06%\n",
        "Epoch [5/10], Training Loss: 1.365, Validation Accuracy: 49.88%\n",
        "Epoch [6/10], Training Loss: 1.354, Validation Accuracy: 49.93%\n",
        "Epoch [7/10], Training Loss: 1.354, Validation Accuracy: 50.28%\n",
        "Epoch [8/10], Training Loss: 1.346, Validation Accuracy: 50.00%\n",
        "Epoch [9/10], Training Loss: 1.335, Validation Accuracy: 50.44%\n",
        "Epoch [10/10], Training Loss: 1.329, Validation Accuracy: 50.57%\n",
        "Epoch [1/10], Training Loss: 1.367, Validation Accuracy: 51.31%\n",
        "Epoch [2/10], Training Loss: 1.349, Validation Accuracy: 50.69%\n",
        "Epoch [3/10], Training Loss: 1.341, Validation Accuracy: 51.15%\n",
        "Epoch [4/10], Training Loss: 1.332, Validation Accuracy: 51.48%\n",
        "Epoch [5/10], Training Loss: 1.322, Validation Accuracy: 51.54%\n",
        "Epoch [6/10], Training Loss: 1.318, Validation Accuracy: 51.42%\n",
        "Epoch [7/10], Training Loss: 1.303, Validation Accuracy: 51.75%\n",
        "Epoch [8/10], Training Loss: 1.299, Validation Accuracy: 52.30%\n",
        "Epoch [9/10], Training Loss: 1.288, Validation Accuracy: 51.62%\n",
        "Epoch [10/10], Training Loss: 1.278, Validation Accuracy: 51.24%\n",
        "Epoch [1/10], Training Loss: 1.350, Validation Accuracy: 51.54%\n",
        "Epoch [2/10], Training Loss: 1.329, Validation Accuracy: 52.10%\n",
        "Epoch [3/10], Training Loss: 1.319, Validation Accuracy: 52.50%\n",
        "Epoch [4/10], Training Loss: 1.306, Validation Accuracy: 52.13%\n",
        "Epoch [5/10], Training Loss: 1.292, Validation Accuracy: 52.81%\n",
        "Epoch [6/10], Training Loss: 1.289, Validation Accuracy: 52.30%\n",
        "Epoch [7/10], Training Loss: 1.278, Validation Accuracy: 52.81%\n",
        "Epoch [8/10], Training Loss: 1.270, Validation Accuracy: 53.06%\n",
        "Epoch [9/10], Training Loss: 1.269, Validation Accuracy: 51.99%\n",
        "Epoch [10/10], Training Loss: 1.257, Validation Accuracy: 52.94%\n",
        "Epoch [1/10], Training Loss: 1.306, Validation Accuracy: 53.62%\n",
        "Epoch [2/10], Training Loss: 1.283, Validation Accuracy: 52.67%\n",
        "Epoch [3/10], Training Loss: 1.281, Validation Accuracy: 53.19%\n",
        "Epoch [4/10], Training Loss: 1.263, Validation Accuracy: 53.52%\n",
        "Epoch [5/10], Training Loss: 1.260, Validation Accuracy: 53.46%\n",
        "Epoch [6/10], Training Loss: 1.247, Validation Accuracy: 52.70%\n",
        "Epoch [7/10], Training Loss: 1.236, Validation Accuracy: 53.35%\n",
        "Epoch [8/10], Training Loss: 1.221, Validation Accuracy: 53.76%\n",
        "Epoch [9/10], Training Loss: 1.225, Validation Accuracy: 53.41%\n",
        "Epoch [10/10], Training Loss: 1.210, Validation Accuracy: 54.27%\n",
        "Epoch [1/10], Training Loss: 1.273, Validation Accuracy: 54.11%\n",
        "Epoch [2/10], Training Loss: 1.249, Validation Accuracy: 54.67%\n",
        "Epoch [3/10], Training Loss: 1.234, Validation Accuracy: 54.31%\n",
        "Epoch [4/10], Training Loss: 1.222, Validation Accuracy: 54.40%\n",
        "Epoch [5/10], Training Loss: 1.214, Validation Accuracy: 55.15%\n",
        "Epoch [6/10], Training Loss: 1.201, Validation Accuracy: 54.97%\n",
        "Epoch [7/10], Training Loss: 1.202, Validation Accuracy: 53.88%\n",
        "Epoch [8/10], Training Loss: 1.196, Validation Accuracy: 53.80%\n",
        "Epoch [9/10], Training Loss: 1.180, Validation Accuracy: 55.03%\n",
        "Epoch [10/10], Training Loss: 1.175, Validation Accuracy: 55.11%\n",
        "Epoch [1/10], Training Loss: 1.263, Validation Accuracy: 55.20%\n",
        "Epoch [2/10], Training Loss: 1.236, Validation Accuracy: 55.16%\n",
        "Epoch [3/10], Training Loss: 1.231, Validation Accuracy: 54.85%\n",
        "Epoch [4/10], Training Loss: 1.217, Validation Accuracy: 55.29%\n",
        "Epoch [5/10], Training Loss: 1.205, Validation Accuracy: 55.15%\n",
        "Epoch [6/10], Training Loss: 1.197, Validation Accuracy: 54.68%\n",
        "Epoch [7/10], Training Loss: 1.188, Validation Accuracy: 55.16%\n",
        "Epoch [8/10], Training Loss: 1.178, Validation Accuracy: 55.55%\n",
        "Epoch [9/10], Training Loss: 1.167, Validation Accuracy: 55.65%\n",
        "Epoch [10/10], Training Loss: 1.160, Validation Accuracy: 55.79%\n",
        "Epoch [1/10], Training Loss: 1.225, Validation Accuracy: 55.84%\n",
        "Epoch [2/10], Training Loss: 1.209, Validation Accuracy: 55.43%\n",
        "Epoch [3/10], Training Loss: 1.190, Validation Accuracy: 55.95%\n",
        "Epoch [4/10], Training Loss: 1.175, Validation Accuracy: 55.52%\n",
        "Epoch [5/10], Training Loss: 1.164, Validation Accuracy: 56.11%\n",
        "Epoch [6/10], Training Loss: 1.157, Validation Accuracy: 55.41%\n",
        "Epoch [7/10], Training Loss: 1.151, Validation Accuracy: 55.15%\n",
        "Epoch [8/10], Training Loss: 1.140, Validation Accuracy: 55.93%\n",
        "Epoch [9/10], Training Loss: 1.132, Validation Accuracy: 55.83%\n",
        "Epoch [10/10], Training Loss: 1.120, Validation Accuracy: 55.84%\n",
        "Epoch [1/10], Training Loss: 1.217, Validation Accuracy: 55.84%\n",
        "Epoch [2/10], Training Loss: 1.197, Validation Accuracy: 56.41%\n",
        "Epoch [3/10], Training Loss: 1.184, Validation Accuracy: 56.66%\n",
        "Epoch [4/10], Training Loss: 1.163, Validation Accuracy: 56.17%\n",
        "Epoch [5/10], Training Loss: 1.155, Validation Accuracy: 56.44%\n",
        "Epoch [6/10], Training Loss: 1.143, Validation Accuracy: 56.59%\n",
        "Epoch [7/10], Training Loss: 1.130, Validation Accuracy: 56.55%\n",
        "Epoch [8/10], Training Loss: 1.119, Validation Accuracy: 56.36%\n",
        "Epoch [9/10], Training Loss: 1.120, Validation Accuracy: 56.50%\n",
        "Epoch [10/10], Training Loss: 1.109, Validation Accuracy: 56.45%\n",
        "Epoch [1/10], Training Loss: 1.191, Validation Accuracy: 57.35%\n",
        "Epoch [2/10], Training Loss: 1.160, Validation Accuracy: 57.19%\n",
        "Epoch [3/10], Training Loss: 1.148, Validation Accuracy: 56.78%\n",
        "Epoch [4/10], Training Loss: 1.137, Validation Accuracy: 56.42%\n",
        "Epoch [5/10], Training Loss: 1.116, Validation Accuracy: 57.45%\n",
        "Epoch [6/10], Training Loss: 1.108, Validation Accuracy: 57.21%\n",
        "Epoch [7/10], Training Loss: 1.094, Validation Accuracy: 57.34%\n",
        "Epoch [8/10], Training Loss: 1.085, Validation Accuracy: 56.66%\n",
        "Epoch [9/10], Training Loss: 1.077, Validation Accuracy: 57.20%\n",
        "Epoch [10/10], Training Loss: 1.069, Validation Accuracy: 57.39%\n",
        "Epoch [1/10], Training Loss: 1.163, Validation Accuracy: 57.77%\n",
        "Epoch [2/10], Training Loss: 1.130, Validation Accuracy: 58.19%\n",
        "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 57.77%\n",
        "Epoch [4/10], Training Loss: 1.100, Validation Accuracy: 57.66%\n",
        "Epoch [5/10], Training Loss: 1.086, Validation Accuracy: 57.82%\n",
        "Epoch [6/10], Training Loss: 1.069, Validation Accuracy: 57.78%\n",
        "Epoch [7/10], Training Loss: 1.063, Validation Accuracy: 58.13%\n",
        "Epoch [8/10], Training Loss: 1.053, Validation Accuracy: 58.31%\n",
        "Epoch [9/10], Training Loss: 1.048, Validation Accuracy: 57.38%\n",
        "Epoch [10/10], Training Loss: 1.033, Validation Accuracy: 57.80%\n",
        "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 57.92%\n",
        "Epoch [2/10], Training Loss: 1.140, Validation Accuracy: 58.21%\n",
        "Epoch [3/10], Training Loss: 1.120, Validation Accuracy: 58.49%\n",
        "Epoch [4/10], Training Loss: 1.109, Validation Accuracy: 58.80%\n",
        "Epoch [5/10], Training Loss: 1.083, Validation Accuracy: 57.66%\n",
        "Epoch [6/10], Training Loss: 1.083, Validation Accuracy: 57.00%\n",
        "Epoch [7/10], Training Loss: 1.060, Validation Accuracy: 58.31%\n",
        "Epoch [8/10], Training Loss: 1.053, Validation Accuracy: 57.64%\n",
        "Epoch [9/10], Training Loss: 1.040, Validation Accuracy: 58.25%\n",
        "Epoch [10/10], Training Loss: 1.035, Validation Accuracy: 57.95%\n",
        "Epoch [1/10], Training Loss: 1.127, Validation Accuracy: 58.27%\n",
        "Epoch [2/10], Training Loss: 1.103, Validation Accuracy: 58.27%\n",
        "Epoch [3/10], Training Loss: 1.078, Validation Accuracy: 58.17%\n",
        "Epoch [4/10], Training Loss: 1.062, Validation Accuracy: 58.07%\n",
        "Epoch [5/10], Training Loss: 1.047, Validation Accuracy: 57.60%\n",
        "Epoch [6/10], Training Loss: 1.040, Validation Accuracy: 58.95%\n",
        "Epoch [7/10], Training Loss: 1.018, Validation Accuracy: 58.42%\n",
        "Epoch [8/10], Training Loss: 1.010, Validation Accuracy: 57.70%\n",
        "Epoch [9/10], Training Loss: 1.004, Validation Accuracy: 58.87%\n",
        "Epoch [10/10], Training Loss: 0.994, Validation Accuracy: 58.47%\n",
        "Epoch [1/10], Training Loss: 1.134, Validation Accuracy: 58.54%\n",
        "Epoch [2/10], Training Loss: 1.091, Validation Accuracy: 59.13%\n",
        "Epoch [3/10], Training Loss: 1.078, Validation Accuracy: 59.20%\n",
        "Epoch [4/10], Training Loss: 1.052, Validation Accuracy: 59.10%\n",
        "Epoch [5/10], Training Loss: 1.036, Validation Accuracy: 59.15%\n",
        "Epoch [6/10], Training Loss: 1.030, Validation Accuracy: 58.62%\n",
        "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 58.78%\n",
        "Epoch [8/10], Training Loss: 1.005, Validation Accuracy: 58.42%\n",
        "Epoch [9/10], Training Loss: 0.996, Validation Accuracy: 59.12%\n",
        "Epoch [10/10], Training Loss: 0.978, Validation Accuracy: 58.76%\n",
        "Epoch [1/10], Training Loss: 1.115, Validation Accuracy: 59.73%\n",
        "Epoch [2/10], Training Loss: 1.069, Validation Accuracy: 59.87%\n",
        "Epoch [3/10], Training Loss: 1.049, Validation Accuracy: 59.94%\n",
        "Epoch [4/10], Training Loss: 1.026, Validation Accuracy: 59.82%\n",
        "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.47%\n",
        "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 59.92%\n",
        "Epoch [7/10], Training Loss: 0.991, Validation Accuracy: 58.99%\n",
        "Epoch [8/10], Training Loss: 0.982, Validation Accuracy: 59.36%\n",
        "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 59.47%\n",
        "Epoch [10/10], Training Loss: 0.951, Validation Accuracy: 59.07%\n",
        "Epoch [1/10], Training Loss: 1.075, Validation Accuracy: 60.02%\n",
        "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.82%\n",
        "Epoch [3/10], Training Loss: 1.007, Validation Accuracy: 59.11%\n",
        "Epoch [4/10], Training Loss: 0.993, Validation Accuracy: 59.97%\n",
        "Epoch [5/10], Training Loss: 0.986, Validation Accuracy: 60.15%\n",
        "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 60.07%\n",
        "Epoch [7/10], Training Loss: 0.952, Validation Accuracy: 59.65%\n",
        "Epoch [8/10], Training Loss: 0.941, Validation Accuracy: 59.51%\n",
        "Epoch [9/10], Training Loss: 0.937, Validation Accuracy: 59.50%\n",
        "Epoch [10/10], Training Loss: 0.915, Validation Accuracy: 60.01%\n",
        "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 60.30%\n",
        "Epoch [2/10], Training Loss: 1.047, Validation Accuracy: 59.63%\n",
        "Epoch [3/10], Training Loss: 1.024, Validation Accuracy: 60.04%\n",
        "Epoch [4/10], Training Loss: 0.999, Validation Accuracy: 60.22%\n",
        "Epoch [5/10], Training Loss: 0.981, Validation Accuracy: 60.20%\n",
        "Epoch [6/10], Training Loss: 0.966, Validation Accuracy: 59.96%\n",
        "Epoch [7/10], Training Loss: 0.946, Validation Accuracy: 60.08%\n",
        "Epoch [8/10], Training Loss: 0.941, Validation Accuracy: 60.07%\n",
        "Epoch [9/10], Training Loss: 0.927, Validation Accuracy: 59.91%\n",
        "Epoch [10/10], Training Loss: 0.918, Validation Accuracy: 59.80%\n",
        "Epoch [1/10], Training Loss: 1.050, Validation Accuracy: 59.83%\n",
        "Epoch [2/10], Training Loss: 1.014, Validation Accuracy: 60.36%\n",
        "Epoch [3/10], Training Loss: 0.990, Validation Accuracy: 60.44%\n",
        "Epoch [4/10], Training Loss: 0.970, Validation Accuracy: 60.11%\n",
        "Epoch [5/10], Training Loss: 0.945, Validation Accuracy: 60.34%\n",
        "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 60.02%\n",
        "Epoch [7/10], Training Loss: 0.915, Validation Accuracy: 60.21%\n",
        "Epoch [8/10], Training Loss: 0.907, Validation Accuracy: 60.67%\n",
        "Epoch [9/10], Training Loss: 0.895, Validation Accuracy: 60.33%\n",
        "Epoch [10/10], Training Loss: 0.877, Validation Accuracy: 59.97%\n",
        "Epoch [1/10], Training Loss: 1.047, Validation Accuracy: 59.91%\n",
        "Epoch [2/10], Training Loss: 1.014, Validation Accuracy: 60.15%\n",
        "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 60.34%\n",
        "Epoch [4/10], Training Loss: 0.961, Validation Accuracy: 59.85%\n",
        "Epoch [5/10], Training Loss: 0.944, Validation Accuracy: 60.46%\n",
        "Epoch [6/10], Training Loss: 0.935, Validation Accuracy: 59.03%\n",
        "Epoch [7/10], Training Loss: 0.922, Validation Accuracy: 60.38%\n",
        "Epoch [8/10], Training Loss: 0.903, Validation Accuracy: 60.19%\n",
        "Epoch [9/10], Training Loss: 0.883, Validation Accuracy: 60.21%\n",
        "Epoch [10/10], Training Loss: 0.877, Validation Accuracy: 59.30%\n",
        "Epoch [1/10], Training Loss: 1.038, Validation Accuracy: 60.85%\n",
        "Epoch [2/10], Training Loss: 0.994, Validation Accuracy: 60.56%\n",
        "Epoch [3/10], Training Loss: 0.968, Validation Accuracy: 60.68%\n",
        "Epoch [4/10], Training Loss: 0.941, Validation Accuracy: 59.96%\n",
        "Epoch [5/10], Training Loss: 0.924, Validation Accuracy: 59.26%\n",
        "Epoch [6/10], Training Loss: 0.912, Validation Accuracy: 60.38%\n",
        "Epoch [7/10], Training Loss: 0.899, Validation Accuracy: 60.22%\n",
        "Epoch [8/10], Training Loss: 0.881, Validation Accuracy: 60.72%\n",
        "Epoch [9/10], Training Loss: 0.860, Validation Accuracy: 61.05%\n",
        "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 60.66%\n",
        "Epoch [1/10], Training Loss: 1.014, Validation Accuracy: 60.89%\n",
        "Epoch [2/10], Training Loss: 0.959, Validation Accuracy: 60.38%\n",
        "Epoch [3/10], Training Loss: 0.929, Validation Accuracy: 61.19%\n",
        "Epoch [4/10], Training Loss: 0.899, Validation Accuracy: 61.21%\n",
        "Epoch [5/10], Training Loss: 0.888, Validation Accuracy: 61.26%\n",
        "Epoch [6/10], Training Loss: 0.864, Validation Accuracy: 60.95%\n",
        "Epoch [7/10], Training Loss: 0.854, Validation Accuracy: 60.61%\n",
        "Epoch [8/10], Training Loss: 0.835, Validation Accuracy: 60.38%\n",
        "Epoch [9/10], Training Loss: 0.823, Validation Accuracy: 60.22%\n",
        "Epoch [10/10], Training Loss: 0.799, Validation Accuracy: 60.50%\n",
        "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 59.94%\n",
        "Epoch [2/10], Training Loss: 0.971, Validation Accuracy: 61.02%\n",
        "Epoch [3/10], Training Loss: 0.937, Validation Accuracy: 59.92%\n",
        "Epoch [4/10], Training Loss: 0.915, Validation Accuracy: 61.18%\n",
        "Epoch [5/10], Training Loss: 0.888, Validation Accuracy: 61.04%\n",
        "Epoch [6/10], Training Loss: 0.872, Validation Accuracy: 61.06%\n",
        "Epoch [7/10], Training Loss: 0.858, Validation Accuracy: 61.15%\n",
        "Epoch [8/10], Training Loss: 0.844, Validation Accuracy: 59.18%\n",
        "Epoch [9/10], Training Loss: 0.835, Validation Accuracy: 60.11%\n",
        "Epoch [10/10], Training Loss: 0.817, Validation Accuracy: 61.07%\n",
        "Epoch [1/10], Training Loss: 0.997, Validation Accuracy: 60.41%\n",
        "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.10%\n",
        "Epoch [3/10], Training Loss: 0.907, Validation Accuracy: 60.72%\n",
        "Epoch [4/10], Training Loss: 0.879, Validation Accuracy: 60.45%\n",
        "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 61.22%\n",
        "Epoch [6/10], Training Loss: 0.848, Validation Accuracy: 60.56%\n",
        "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 61.06%\n",
        "Epoch [8/10], Training Loss: 0.809, Validation Accuracy: 60.90%\n",
        "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 60.31%\n",
        "Epoch [10/10], Training Loss: 0.779, Validation Accuracy: 60.75%\n",
        "Epoch [1/10], Training Loss: 0.992, Validation Accuracy: 60.63%\n",
        "Epoch [2/10], Training Loss: 0.941, Validation Accuracy: 59.74%\n",
        "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 60.49%\n",
        "Epoch [4/10], Training Loss: 0.876, Validation Accuracy: 60.77%\n",
        "Epoch [5/10], Training Loss: 0.853, Validation Accuracy: 60.21%\n",
        "Epoch [6/10], Training Loss: 0.843, Validation Accuracy: 60.25%\n",
        "Epoch [7/10], Training Loss: 0.822, Validation Accuracy: 61.32%\n",
        "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 60.81%\n",
        "Epoch [9/10], Training Loss: 0.792, Validation Accuracy: 60.50%\n",
        "Epoch [10/10], Training Loss: 0.771, Validation Accuracy: 60.91%\n",
        "Epoch [1/10], Training Loss: 0.993, Validation Accuracy: 59.71%\n",
        "Epoch [2/10], Training Loss: 0.922, Validation Accuracy: 61.05%\n",
        "Epoch [3/10], Training Loss: 0.891, Validation Accuracy: 61.31%\n",
        "Epoch [4/10], Training Loss: 0.863, Validation Accuracy: 61.31%\n",
        "Epoch [5/10], Training Loss: 0.848, Validation Accuracy: 60.95%\n",
        "Epoch [6/10], Training Loss: 0.823, Validation Accuracy: 60.91%\n",
        "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 61.08%\n",
        "Epoch [8/10], Training Loss: 0.785, Validation Accuracy: 61.13%\n",
        "Epoch [9/10], Training Loss: 0.766, Validation Accuracy: 60.83%\n",
        "Epoch [10/10], Training Loss: 0.758, Validation Accuracy: 60.25%\n",
        "Epoch [1/10], Training Loss: 0.963, Validation Accuracy: 60.56%\n",
        "Epoch [2/10], Training Loss: 0.898, Validation Accuracy: 60.92%\n",
        "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 61.01%\n",
        "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 61.00%\n",
        "Epoch [5/10], Training Loss: 0.803, Validation Accuracy: 61.24%\n",
        "Epoch [6/10], Training Loss: 0.779, Validation Accuracy: 61.05%\n",
        "Epoch [7/10], Training Loss: 0.759, Validation Accuracy: 60.35%\n",
        "Epoch [8/10], Training Loss: 0.744, Validation Accuracy: 60.52%\n",
        "Epoch [9/10], Training Loss: 0.727, Validation Accuracy: 59.98%\n",
        "Epoch [10/10], Training Loss: 0.718, Validation Accuracy: 60.41%\n",
        "Epoch [1/10], Training Loss: 0.990, Validation Accuracy: 60.83%\n",
        "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.24%\n",
        "Epoch [3/10], Training Loss: 0.865, Validation Accuracy: 61.06%\n",
        "Epoch [4/10], Training Loss: 0.836, Validation Accuracy: 60.99%\n",
        "Epoch [5/10], Training Loss: 0.811, Validation Accuracy: 60.56%\n",
        "Epoch [6/10], Training Loss: 0.789, Validation Accuracy: 61.00%\n",
        "Epoch [7/10], Training Loss: 0.770, Validation Accuracy: 60.72%\n",
        "Epoch [8/10], Training Loss: 0.763, Validation Accuracy: 60.58%\n",
        "Epoch [9/10], Training Loss: 0.730, Validation Accuracy: 60.85%\n",
        "Epoch [10/10], Training Loss: 0.720, Validation Accuracy: 60.83%\n",
        "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 60.86%\n",
        "Epoch [2/10], Training Loss: 0.869, Validation Accuracy: 60.93%\n",
        "Epoch [3/10], Training Loss: 0.827, Validation Accuracy: 60.80%\n",
        "Epoch [4/10], Training Loss: 0.802, Validation Accuracy: 61.25%\n",
        "Epoch [5/10], Training Loss: 0.780, Validation Accuracy: 60.98%\n",
        "Epoch [6/10], Training Loss: 0.761, Validation Accuracy: 60.74%\n",
        "Epoch [7/10], Training Loss: 0.736, Validation Accuracy: 60.87%\n",
        "Epoch [8/10], Training Loss: 0.724, Validation Accuracy: 60.74%\n",
        "Epoch [9/10], Training Loss: 0.704, Validation Accuracy: 61.08%\n",
        "Epoch [10/10], Training Loss: 0.686, Validation Accuracy: 60.83%\n",
        "Epoch [1/10], Training Loss: 0.934, Validation Accuracy: 60.96%\n",
        "Epoch [2/10], Training Loss: 0.871, Validation Accuracy: 60.56%\n",
        "Epoch [3/10], Training Loss: 0.826, Validation Accuracy: 60.78%\n",
        "Epoch [4/10], Training Loss: 0.793, Validation Accuracy: 60.91%\n",
        "Epoch [5/10], Training Loss: 0.771, Validation Accuracy: 60.44%\n",
        "Epoch [6/10], Training Loss: 0.754, Validation Accuracy: 60.71%\n",
        "Epoch [7/10], Training Loss: 0.725, Validation Accuracy: 60.86%\n",
        "Epoch [8/10], Training Loss: 0.710, Validation Accuracy: 61.00%\n",
        "Epoch [9/10], Training Loss: 0.695, Validation Accuracy: 60.16%\n",
        "Epoch [10/10], Training Loss: 0.670, Validation Accuracy: 60.36%\n",
        "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 61.37%\n",
        "Epoch [2/10], Training Loss: 0.873, Validation Accuracy: 59.88%\n",
        "Epoch [3/10], Training Loss: 0.832, Validation Accuracy: 61.27%\n",
        "Epoch [4/10], Training Loss: 0.791, Validation Accuracy: 61.09%\n",
        "Epoch [5/10], Training Loss: 0.773, Validation Accuracy: 60.92%\n",
        "Epoch [6/10], Training Loss: 0.740, Validation Accuracy: 61.82%\n",
        "Epoch [7/10], Training Loss: 0.718, Validation Accuracy: 61.26%\n",
        "Epoch [8/10], Training Loss: 0.712, Validation Accuracy: 60.84%\n",
        "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 61.18%\n",
        "Epoch [10/10], Training Loss: 0.671, Validation Accuracy: 61.41%\n",
        "Epoch [1/10], Training Loss: 0.908, Validation Accuracy: 60.93%\n",
        "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 61.27%\n",
        "Epoch [3/10], Training Loss: 0.785, Validation Accuracy: 61.22%\n",
        "Epoch [4/10], Training Loss: 0.748, Validation Accuracy: 61.07%\n",
        "Epoch [5/10], Training Loss: 0.722, Validation Accuracy: 60.94%\n",
        "Epoch [6/10], Training Loss: 0.691, Validation Accuracy: 61.02%\n",
        "Epoch [7/10], Training Loss: 0.675, Validation Accuracy: 60.47%\n",
        "Epoch [8/10], Training Loss: 0.650, Validation Accuracy: 60.50%\n",
        "Epoch [9/10], Training Loss: 0.643, Validation Accuracy: 60.19%\n",
        "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 60.03%\n",
        "Epoch [1/10], Training Loss: 0.938, Validation Accuracy: 60.83%\n",
        "Epoch [2/10], Training Loss: 0.847, Validation Accuracy: 60.47%\n",
        "Epoch [3/10], Training Loss: 0.794, Validation Accuracy: 60.97%\n",
        "Epoch [4/10], Training Loss: 0.762, Validation Accuracy: 60.47%\n",
        "Epoch [5/10], Training Loss: 0.729, Validation Accuracy: 60.63%\n",
        "Epoch [6/10], Training Loss: 0.706, Validation Accuracy: 60.81%\n",
        "Epoch [7/10], Training Loss: 0.678, Validation Accuracy: 61.31%\n",
        "Epoch [8/10], Training Loss: 0.668, Validation Accuracy: 60.69%\n",
        "Epoch [9/10], Training Loss: 0.640, Validation Accuracy: 60.60%\n",
        "Epoch [10/10], Training Loss: 0.621, Validation Accuracy: 60.54%\n",
        "Epoch [1/10], Training Loss: 0.891, Validation Accuracy: 60.53%\n",
        "Epoch [2/10], Training Loss: 0.812, Validation Accuracy: 60.43%\n",
        "Epoch [3/10], Training Loss: 0.766, Validation Accuracy: 60.13%\n",
        "Epoch [4/10], Training Loss: 0.729, Validation Accuracy: 61.32%\n",
        "Epoch [5/10], Training Loss: 0.698, Validation Accuracy: 61.09%\n",
        "Epoch [6/10], Training Loss: 0.672, Validation Accuracy: 61.35%\n",
        "Epoch [7/10], Training Loss: 0.652, Validation Accuracy: 61.00%\n",
        "Epoch [8/10], Training Loss: 0.639, Validation Accuracy: 59.83%\n",
        "Epoch [9/10], Training Loss: 0.608, Validation Accuracy: 60.97%\n",
        "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 60.57%\n",
        "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 59.80%\n",
        "Epoch [2/10], Training Loss: 0.809, Validation Accuracy: 60.35%\n",
        "Epoch [3/10], Training Loss: 0.747, Validation Accuracy: 60.86%\n",
        "Epoch [4/10], Training Loss: 0.721, Validation Accuracy: 60.14%\n",
        "Epoch [5/10], Training Loss: 0.688, Validation Accuracy: 60.74%\n",
        "Epoch [6/10], Training Loss: 0.665, Validation Accuracy: 60.46%\n",
        "Epoch [7/10], Training Loss: 0.645, Validation Accuracy: 60.35%\n",
        "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 60.59%\n",
        "Epoch [9/10], Training Loss: 0.607, Validation Accuracy: 60.11%\n",
        "Epoch [10/10], Training Loss: 0.578, Validation Accuracy: 59.95%\n",
        "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 60.33%\n",
        "Epoch [2/10], Training Loss: 0.811, Validation Accuracy: 59.66%\n",
        "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 59.88%\n",
        "Epoch [4/10], Training Loss: 0.728, Validation Accuracy: 61.39%\n",
        "Epoch [5/10], Training Loss: 0.690, Validation Accuracy: 60.37%\n",
        "Epoch [6/10], Training Loss: 0.666, Validation Accuracy: 61.00%\n",
        "Epoch [7/10], Training Loss: 0.633, Validation Accuracy: 60.76%\n",
        "Epoch [8/10], Training Loss: 0.617, Validation Accuracy: 61.03%\n",
        "Epoch [9/10], Training Loss: 0.592, Validation Accuracy: 60.93%\n",
        "Epoch [10/10], Training Loss: 0.576, Validation Accuracy: 60.68%\n",
        "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 60.09%\n",
        "Epoch [2/10], Training Loss: 0.762, Validation Accuracy: 60.78%\n",
        "Epoch [3/10], Training Loss: 0.710, Validation Accuracy: 60.29%\n",
        "Epoch [4/10], Training Loss: 0.672, Validation Accuracy: 60.72%\n",
        "Epoch [5/10], Training Loss: 0.634, Validation Accuracy: 61.22%\n",
        "Epoch [6/10], Training Loss: 0.606, Validation Accuracy: 60.77%\n",
        "Epoch [7/10], Training Loss: 0.585, Validation Accuracy: 60.76%\n",
        "Epoch [8/10], Training Loss: 0.566, Validation Accuracy: 60.75%\n",
        "Epoch [9/10], Training Loss: 0.539, Validation Accuracy: 60.27%\n",
        "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 59.71%\n",
        "Epoch [1/10], Training Loss: 0.899, Validation Accuracy: 59.86%\n",
        "Epoch [2/10], Training Loss: 0.782, Validation Accuracy: 60.53%\n",
        "Epoch [3/10], Training Loss: 0.715, Validation Accuracy: 60.44%\n",
        "Epoch [4/10], Training Loss: 0.681, Validation Accuracy: 60.70%\n",
        "Epoch [5/10], Training Loss: 0.645, Validation Accuracy: 59.96%\n",
        "Epoch [6/10], Training Loss: 0.616, Validation Accuracy: 60.37%\n",
        "Epoch [7/10], Training Loss: 0.589, Validation Accuracy: 60.41%\n",
        "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 60.02%\n",
        "Epoch [9/10], Training Loss: 0.552, Validation Accuracy: 59.90%\n",
        "Epoch [10/10], Training Loss: 0.530, Validation Accuracy: 60.18%\n",
        "Epoch [1/10], Training Loss: 0.859, Validation Accuracy: 59.79%\n",
        "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 60.48%\n",
        "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 60.16%\n",
        "Epoch [4/10], Training Loss: 0.662, Validation Accuracy: 60.48%\n",
        "Epoch [5/10], Training Loss: 0.622, Validation Accuracy: 60.52%\n",
        "Epoch [6/10], Training Loss: 0.596, Validation Accuracy: 60.75%\n",
        "Epoch [7/10], Training Loss: 0.557, Validation Accuracy: 60.32%\n",
        "Epoch [8/10], Training Loss: 0.544, Validation Accuracy: 60.41%\n",
        "Epoch [9/10], Training Loss: 0.529, Validation Accuracy: 60.05%\n",
        "Epoch [10/10], Training Loss: 0.498, Validation Accuracy: 60.13%\n",
        "Epoch [1/10], Training Loss: 0.847, Validation Accuracy: 59.69%\n",
        "Epoch [2/10], Training Loss: 0.741, Validation Accuracy: 60.34%\n",
        "Epoch [3/10], Training Loss: 0.675, Validation Accuracy: 60.42%\n",
        "Epoch [4/10], Training Loss: 0.637, Validation Accuracy: 60.21%\n",
        "Epoch [5/10], Training Loss: 0.602, Validation Accuracy: 59.94%\n",
        "Epoch [6/10], Training Loss: 0.583, Validation Accuracy: 60.12%\n",
        "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 59.51%\n",
        "Epoch [8/10], Training Loss: 0.530, Validation Accuracy: 60.02%\n",
        "Epoch [9/10], Training Loss: 0.508, Validation Accuracy: 59.94%\n",
        "Epoch [10/10], Training Loss: 0.487, Validation Accuracy: 60.06%\n",
        "Epoch [1/10], Training Loss: 0.893, Validation Accuracy: 60.13%\n",
        "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 60.85%\n",
        "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 60.92%\n",
        "Epoch [4/10], Training Loss: 0.651, Validation Accuracy: 60.11%\n",
        "Epoch [5/10], Training Loss: 0.617, Validation Accuracy: 60.46%\n",
        "Epoch [6/10], Training Loss: 0.590, Validation Accuracy: 60.52%\n",
        "Epoch [7/10], Training Loss: 0.554, Validation Accuracy: 60.90%\n",
        "Epoch [8/10], Training Loss: 0.533, Validation Accuracy: 60.77%\n",
        "Epoch [9/10], Training Loss: 0.503, Validation Accuracy: 59.93%\n",
        "Epoch [10/10], Training Loss: 0.484, Validation Accuracy: 60.61%\n",
        "Epoch [1/10], Training Loss: 0.821, Validation Accuracy: 59.58%\n",
        "Epoch [2/10], Training Loss: 0.698, Validation Accuracy: 60.77%\n",
        "Epoch [3/10], Training Loss: 0.642, Validation Accuracy: 60.67%\n",
        "Epoch [4/10], Training Loss: 0.597, Validation Accuracy: 60.59%\n",
        "Epoch [5/10], Training Loss: 0.557, Validation Accuracy: 60.47%\n",
        "Epoch [6/10], Training Loss: 0.524, Validation Accuracy: 60.60%\n",
        "Epoch [7/10], Training Loss: 0.497, Validation Accuracy: 60.25%\n",
        "Epoch [8/10], Training Loss: 0.470, Validation Accuracy: 60.13%\n",
        "Epoch [9/10], Training Loss: 0.446, Validation Accuracy: 60.02%\n",
        "Epoch [10/10], Training Loss: 0.433, Validation Accuracy: 59.70%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f637441d-6f58-4d6c-9ee2-f67a324d5533",
        "id": "wPdKra2qnYGr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [9.88, 9.93, 10.28, 11.05, 13.7, 15.66, 16.33, 16.77, 16.84, 16.83, 16.75, 16.62, 16.61, 16.59, 17.43, 18.31, 19.27, 19.68, 20.4, 21.08, 23.44, 24.81, 25.75, 26.83, 26.7, 27.23, 28.1, 28.71, 29.94, 30.37, 31.39, 32.38, 33.12, 33.68, 33.75, 34.31, 34.74, 35.1, 35.44, 35.7, 36.08, 36.42, 37.31, 37.38, 38.41, 37.41, 38.96, 38.34, 39.01, 39.36, 40.29, 40.1, 40.41, 40.79, 40.59, 41.29, 41.24, 40.11, 41.71, 41.89, 42.47, 42.29, 42.77, 43.17, 43.45, 43.1, 43.51, 43.44, 43.99, 43.44, 44.04, 44.93, 44.45, 44.87, 44.82, 45.16, 45.68, 44.63, 45.78, 45.94, 46.23, 45.96, 46.52, 46.14, 46.45, 46.93, 47.22, 46.25, 47.62, 47.96, 47.12, 47.82, 47.49, 47.43, 48.27, 48.77, 48.93, 48.89, 49.32, 49.19, 48.79, 49.4, 49.15, 50.06, 49.88, 49.93, 50.28, 50.0, 50.44, 50.57, 51.31, 50.69, 51.15, 51.48, 51.54, 51.42, 51.75, 52.3, 51.62, 51.24, 51.54, 52.1, 52.5, 52.13, 52.81, 52.3, 52.81, 53.06, 51.99, 52.94, 53.62, 52.67, 53.19, 53.52, 53.46, 52.7, 53.35, 53.76, 53.41, 54.27, 54.11, 54.67, 54.31, 54.4, 55.15, 54.97, 53.88, 53.8, 55.03, 55.11, 55.2, 55.16, 54.85, 55.29, 55.15, 54.68, 55.16, 55.55, 55.65, 55.79, 55.84, 55.43, 55.95, 55.52, 56.11, 55.41, 55.15, 55.93, 55.83, 55.84, 55.84, 56.41, 56.66, 56.17, 56.44, 56.59, 56.55, 56.36, 56.5, 56.45, 57.35, 57.19, 56.78, 56.42, 57.45, 57.21, 57.34, 56.66, 57.2, 57.39, 57.77, 58.19, 57.77, 57.66, 57.82, 57.78, 58.13, 58.31, 57.38, 57.8, 57.92, 58.21, 58.49, 58.8, 57.66, 57.0, 58.31, 57.64, 58.25, 57.95, 58.27, 58.27, 58.17, 58.07, 57.6, 58.95, 58.42, 57.7, 58.87, 58.47, 58.54, 59.13, 59.2, 59.1, 59.15, 58.62, 58.78, 58.42, 59.12, 58.76, 59.73, 59.87, 59.94, 59.82, 59.47, 59.92, 58.99, 59.36, 59.47, 59.07, 60.02, 59.82, 59.11, 59.97, 60.15, 60.07, 59.65, 59.51, 59.5, 60.01, 60.3, 59.63, 60.04, 60.22, 60.2, 59.96, 60.08, 60.07, 59.91, 59.8, 59.83, 60.36, 60.44, 60.11, 60.34, 60.02, 60.21, 60.67, 60.33, 59.97, 59.91, 60.15, 60.34, 59.85, 60.46, 59.03, 60.38, 60.19, 60.21, 59.3, 60.85, 60.56, 60.68, 59.96, 59.26, 60.38, 60.22, 60.72, 61.05, 60.66, 60.89, 60.38, 61.19, 61.21, 61.26, 60.95, 60.61, 60.38, 60.22, 60.5, 59.94, 61.02, 59.92, 61.18, 61.04, 61.06, 61.15, 59.18, 60.11, 61.07, 60.41, 60.1, 60.72, 60.45, 61.22, 60.56, 61.06, 60.9, 60.31, 60.75, 60.63, 59.74, 60.49, 60.77, 60.21, 60.25, 61.32, 60.81, 60.5, 60.91, 59.71, 61.05, 61.31, 61.31, 60.95, 60.91, 61.08, 61.13, 60.83, 60.25, 60.56, 60.92, 61.01, 61.0, 61.24, 61.05, 60.35, 60.52, 59.98, 60.41, 60.83, 61.24, 61.06, 60.99, 60.56, 61.0, 60.72, 60.58, 60.85, 60.83, 60.86, 60.93, 60.8, 61.25, 60.98, 60.74, 60.87, 60.74, 61.08, 60.83, 60.96, 60.56, 60.78, 60.91, 60.44, 60.71, 60.86, 61.0, 60.16, 60.36, 61.37, 59.88, 61.27, 61.09, 60.92, 61.82, 61.26, 60.84, 61.18, 61.41, 60.93, 61.27, 61.22, 61.07, 60.94, 61.02, 60.47, 60.5, 60.19, 60.03, 60.83, 60.47, 60.97, 60.47, 60.63, 60.81, 61.31, 60.69, 60.6, 60.54, 60.53, 60.43, 60.13, 61.32, 61.09, 61.35, 61.0, 59.83, 60.97, 60.57, 59.8, 60.35, 60.86, 60.14, 60.74, 60.46, 60.35, 60.59, 60.11, 59.95, 60.33, 59.66, 59.88, 61.39, 60.37, 61.0, 60.76, 61.03, 60.93, 60.68, 60.09, 60.78, 60.29, 60.72, 61.22, 60.77, 60.76, 60.75, 60.27, 59.71, 59.86, 60.53, 60.44, 60.7, 59.96, 60.37, 60.41, 60.02, 59.9, 60.18, 59.79, 60.48, 60.16, 60.48, 60.52, 60.75, 60.32, 60.41, 60.05, 60.13, 59.69, 60.34, 60.42, 60.21, 59.94, 60.12, 59.51, 60.02, 59.94, 60.06, 60.13, 60.85, 60.92, 60.11, 60.46, 60.52, 60.9, 60.77, 59.93, 60.61, 59.58, 60.77, 60.67, 60.59, 60.47, 60.6, 60.25, 60.13, 60.02, 59.7]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlNQIM_m2cIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        },\n",
        "\n",
        "        \"truncated\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict, distribution_info_truncated: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "    mean_truncated = distribution_info_truncated[\"mean\"]\n",
        "    std_truncated = distribution_info_truncated[\"std\"]\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "    # Generate augmented data from truncated normal distribution\n",
        "    a = (0 - mean_truncated) / std_truncated\n",
        "    b = np.inf\n",
        "    augmented_data_truncated = torch.from_numpy(truncnorm.rvs(a, b, loc=mean_truncated, scale=std_truncated, size=(64, vae.z_dim))).float()\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = (augmented_data_uniform + augmented_data_truncated) / 2\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"], other_distribution_info[\"truncated\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        },\n",
        "        \"truncated\": {\n",
        "            \"mean\": np.zeros(20),\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de59d44-e106-48ec-f85f-3aa2e1f251fa",
        "id": "r27KAQ5e2cld"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 170M/170M [00:12<00:00, 14.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6132 5980 6035 5957 5946 6008 5976 6051 5957 5958]\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 10.51%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 10.63%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 10.74%\n",
            "Epoch [4/10], Training Loss: 2.303, Validation Accuracy: 11.08%\n",
            "Epoch [5/10], Training Loss: 2.302, Validation Accuracy: 11.00%\n",
            "Epoch [6/10], Training Loss: 2.302, Validation Accuracy: 11.98%\n",
            "Epoch [7/10], Training Loss: 2.301, Validation Accuracy: 13.05%\n",
            "Epoch [8/10], Training Loss: 2.301, Validation Accuracy: 13.26%\n",
            "Epoch [9/10], Training Loss: 2.301, Validation Accuracy: 12.16%\n",
            "Epoch [10/10], Training Loss: 2.300, Validation Accuracy: 11.07%\n",
            "Epoch [1/10], Training Loss: 2.301, Validation Accuracy: 11.20%\n",
            "Epoch [2/10], Training Loss: 2.300, Validation Accuracy: 11.89%\n",
            "Epoch [3/10], Training Loss: 2.300, Validation Accuracy: 12.92%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 15.11%\n",
            "Epoch [5/10], Training Loss: 2.298, Validation Accuracy: 17.87%\n",
            "Epoch [6/10], Training Loss: 2.297, Validation Accuracy: 18.48%\n",
            "Epoch [7/10], Training Loss: 2.295, Validation Accuracy: 17.59%\n",
            "Epoch [8/10], Training Loss: 2.293, Validation Accuracy: 16.99%\n",
            "Epoch [9/10], Training Loss: 2.291, Validation Accuracy: 16.31%\n",
            "Epoch [10/10], Training Loss: 2.288, Validation Accuracy: 16.41%\n",
            "Epoch [1/10], Training Loss: 2.285, Validation Accuracy: 16.83%\n",
            "Epoch [2/10], Training Loss: 2.278, Validation Accuracy: 17.83%\n",
            "Epoch [3/10], Training Loss: 2.268, Validation Accuracy: 19.04%\n",
            "Epoch [4/10], Training Loss: 2.252, Validation Accuracy: 19.58%\n",
            "Epoch [5/10], Training Loss: 2.228, Validation Accuracy: 20.10%\n",
            "Epoch [6/10], Training Loss: 2.192, Validation Accuracy: 21.69%\n",
            "Epoch [7/10], Training Loss: 2.156, Validation Accuracy: 22.74%\n",
            "Epoch [8/10], Training Loss: 2.130, Validation Accuracy: 24.00%\n",
            "Epoch [9/10], Training Loss: 2.111, Validation Accuracy: 24.49%\n",
            "Epoch [10/10], Training Loss: 2.093, Validation Accuracy: 25.77%\n",
            "Epoch [1/10], Training Loss: 2.082, Validation Accuracy: 26.54%\n",
            "Epoch [2/10], Training Loss: 2.061, Validation Accuracy: 27.43%\n",
            "Epoch [3/10], Training Loss: 2.037, Validation Accuracy: 27.86%\n",
            "Epoch [4/10], Training Loss: 2.012, Validation Accuracy: 29.14%\n",
            "Epoch [5/10], Training Loss: 1.989, Validation Accuracy: 27.97%\n",
            "Epoch [6/10], Training Loss: 1.971, Validation Accuracy: 30.05%\n",
            "Epoch [7/10], Training Loss: 1.951, Validation Accuracy: 29.79%\n",
            "Epoch [8/10], Training Loss: 1.934, Validation Accuracy: 30.76%\n",
            "Epoch [9/10], Training Loss: 1.922, Validation Accuracy: 31.66%\n",
            "Epoch [10/10], Training Loss: 1.906, Validation Accuracy: 32.54%\n",
            "Epoch [1/10], Training Loss: 1.890, Validation Accuracy: 32.86%\n",
            "Epoch [2/10], Training Loss: 1.874, Validation Accuracy: 31.90%\n",
            "Epoch [3/10], Training Loss: 1.862, Validation Accuracy: 33.59%\n",
            "Epoch [4/10], Training Loss: 1.849, Validation Accuracy: 33.26%\n",
            "Epoch [5/10], Training Loss: 1.836, Validation Accuracy: 34.05%\n",
            "Epoch [6/10], Training Loss: 1.826, Validation Accuracy: 34.48%\n",
            "Epoch [7/10], Training Loss: 1.809, Validation Accuracy: 34.18%\n",
            "Epoch [8/10], Training Loss: 1.799, Validation Accuracy: 35.31%\n",
            "Epoch [9/10], Training Loss: 1.783, Validation Accuracy: 35.06%\n",
            "Epoch [10/10], Training Loss: 1.770, Validation Accuracy: 35.93%\n",
            "Epoch [1/10], Training Loss: 1.779, Validation Accuracy: 36.09%\n",
            "Epoch [2/10], Training Loss: 1.770, Validation Accuracy: 37.16%\n",
            "Epoch [3/10], Training Loss: 1.747, Validation Accuracy: 36.71%\n",
            "Epoch [4/10], Training Loss: 1.742, Validation Accuracy: 37.76%\n",
            "Epoch [5/10], Training Loss: 1.724, Validation Accuracy: 37.12%\n",
            "Epoch [6/10], Training Loss: 1.711, Validation Accuracy: 38.62%\n",
            "Epoch [7/10], Training Loss: 1.697, Validation Accuracy: 37.55%\n",
            "Epoch [8/10], Training Loss: 1.677, Validation Accuracy: 39.15%\n",
            "Epoch [9/10], Training Loss: 1.663, Validation Accuracy: 39.10%\n",
            "Epoch [10/10], Training Loss: 1.654, Validation Accuracy: 39.18%\n",
            "Epoch [1/10], Training Loss: 1.662, Validation Accuracy: 39.74%\n",
            "Epoch [2/10], Training Loss: 1.644, Validation Accuracy: 40.31%\n",
            "Epoch [3/10], Training Loss: 1.619, Validation Accuracy: 39.60%\n",
            "Epoch [4/10], Training Loss: 1.611, Validation Accuracy: 39.50%\n",
            "Epoch [5/10], Training Loss: 1.600, Validation Accuracy: 40.64%\n",
            "Epoch [6/10], Training Loss: 1.591, Validation Accuracy: 41.52%\n",
            "Epoch [7/10], Training Loss: 1.577, Validation Accuracy: 40.81%\n",
            "Epoch [8/10], Training Loss: 1.566, Validation Accuracy: 40.49%\n",
            "Epoch [9/10], Training Loss: 1.567, Validation Accuracy: 41.91%\n",
            "Epoch [10/10], Training Loss: 1.543, Validation Accuracy: 41.58%\n",
            "Epoch [1/10], Training Loss: 1.578, Validation Accuracy: 42.49%\n",
            "Epoch [2/10], Training Loss: 1.557, Validation Accuracy: 42.56%\n",
            "Epoch [3/10], Training Loss: 1.542, Validation Accuracy: 42.95%\n",
            "Epoch [4/10], Training Loss: 1.540, Validation Accuracy: 43.14%\n",
            "Epoch [5/10], Training Loss: 1.527, Validation Accuracy: 43.35%\n",
            "Epoch [6/10], Training Loss: 1.521, Validation Accuracy: 43.99%\n",
            "Epoch [7/10], Training Loss: 1.508, Validation Accuracy: 43.24%\n",
            "Epoch [8/10], Training Loss: 1.505, Validation Accuracy: 43.47%\n",
            "Epoch [9/10], Training Loss: 1.493, Validation Accuracy: 43.94%\n",
            "Epoch [10/10], Training Loss: 1.479, Validation Accuracy: 43.84%\n",
            "Epoch [1/10], Training Loss: 1.530, Validation Accuracy: 45.15%\n",
            "Epoch [2/10], Training Loss: 1.514, Validation Accuracy: 44.58%\n",
            "Epoch [3/10], Training Loss: 1.516, Validation Accuracy: 45.18%\n",
            "Epoch [4/10], Training Loss: 1.505, Validation Accuracy: 45.25%\n",
            "Epoch [5/10], Training Loss: 1.486, Validation Accuracy: 45.88%\n",
            "Epoch [6/10], Training Loss: 1.477, Validation Accuracy: 45.67%\n",
            "Epoch [7/10], Training Loss: 1.476, Validation Accuracy: 45.76%\n",
            "Epoch [8/10], Training Loss: 1.468, Validation Accuracy: 46.34%\n",
            "Epoch [9/10], Training Loss: 1.466, Validation Accuracy: 45.84%\n",
            "Epoch [10/10], Training Loss: 1.451, Validation Accuracy: 46.56%\n",
            "Epoch [1/10], Training Loss: 1.478, Validation Accuracy: 46.25%\n",
            "Epoch [2/10], Training Loss: 1.471, Validation Accuracy: 47.10%\n",
            "Epoch [3/10], Training Loss: 1.450, Validation Accuracy: 46.20%\n",
            "Epoch [4/10], Training Loss: 1.436, Validation Accuracy: 46.73%\n",
            "Epoch [5/10], Training Loss: 1.437, Validation Accuracy: 46.01%\n",
            "Epoch [6/10], Training Loss: 1.428, Validation Accuracy: 47.68%\n",
            "Epoch [7/10], Training Loss: 1.416, Validation Accuracy: 47.00%\n",
            "Epoch [8/10], Training Loss: 1.407, Validation Accuracy: 48.04%\n",
            "Epoch [9/10], Training Loss: 1.403, Validation Accuracy: 47.81%\n",
            "Epoch [10/10], Training Loss: 1.405, Validation Accuracy: 46.86%\n",
            "Epoch [1/10], Training Loss: 1.434, Validation Accuracy: 46.83%\n",
            "Epoch [2/10], Training Loss: 1.421, Validation Accuracy: 48.42%\n",
            "Epoch [3/10], Training Loss: 1.399, Validation Accuracy: 48.76%\n",
            "Epoch [4/10], Training Loss: 1.389, Validation Accuracy: 48.92%\n",
            "Epoch [5/10], Training Loss: 1.379, Validation Accuracy: 49.54%\n",
            "Epoch [6/10], Training Loss: 1.370, Validation Accuracy: 48.85%\n",
            "Epoch [7/10], Training Loss: 1.353, Validation Accuracy: 49.22%\n",
            "Epoch [8/10], Training Loss: 1.348, Validation Accuracy: 49.06%\n",
            "Epoch [9/10], Training Loss: 1.336, Validation Accuracy: 49.51%\n",
            "Epoch [10/10], Training Loss: 1.333, Validation Accuracy: 48.94%\n",
            "Epoch [1/10], Training Loss: 1.376, Validation Accuracy: 50.79%\n",
            "Epoch [2/10], Training Loss: 1.356, Validation Accuracy: 49.59%\n",
            "Epoch [3/10], Training Loss: 1.343, Validation Accuracy: 50.47%\n",
            "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 50.81%\n",
            "Epoch [5/10], Training Loss: 1.323, Validation Accuracy: 50.29%\n",
            "Epoch [6/10], Training Loss: 1.303, Validation Accuracy: 50.50%\n",
            "Epoch [7/10], Training Loss: 1.296, Validation Accuracy: 50.82%\n",
            "Epoch [8/10], Training Loss: 1.284, Validation Accuracy: 51.57%\n",
            "Epoch [9/10], Training Loss: 1.277, Validation Accuracy: 50.98%\n",
            "Epoch [10/10], Training Loss: 1.272, Validation Accuracy: 51.94%\n",
            "Epoch [1/10], Training Loss: 1.343, Validation Accuracy: 51.56%\n",
            "Epoch [2/10], Training Loss: 1.328, Validation Accuracy: 51.83%\n",
            "Epoch [3/10], Training Loss: 1.314, Validation Accuracy: 51.80%\n",
            "Epoch [4/10], Training Loss: 1.303, Validation Accuracy: 51.73%\n",
            "Epoch [5/10], Training Loss: 1.295, Validation Accuracy: 52.26%\n",
            "Epoch [6/10], Training Loss: 1.284, Validation Accuracy: 51.27%\n",
            "Epoch [7/10], Training Loss: 1.288, Validation Accuracy: 51.73%\n",
            "Epoch [8/10], Training Loss: 1.267, Validation Accuracy: 52.16%\n",
            "Epoch [9/10], Training Loss: 1.262, Validation Accuracy: 51.23%\n",
            "Epoch [10/10], Training Loss: 1.257, Validation Accuracy: 51.53%\n",
            "Epoch [1/10], Training Loss: 1.324, Validation Accuracy: 51.89%\n",
            "Epoch [2/10], Training Loss: 1.305, Validation Accuracy: 52.97%\n",
            "Epoch [3/10], Training Loss: 1.295, Validation Accuracy: 52.46%\n",
            "Epoch [4/10], Training Loss: 1.281, Validation Accuracy: 53.06%\n",
            "Epoch [5/10], Training Loss: 1.269, Validation Accuracy: 53.25%\n",
            "Epoch [6/10], Training Loss: 1.257, Validation Accuracy: 51.86%\n",
            "Epoch [7/10], Training Loss: 1.247, Validation Accuracy: 53.39%\n",
            "Epoch [8/10], Training Loss: 1.236, Validation Accuracy: 53.44%\n",
            "Epoch [9/10], Training Loss: 1.221, Validation Accuracy: 53.46%\n",
            "Epoch [10/10], Training Loss: 1.228, Validation Accuracy: 53.33%\n",
            "Epoch [1/10], Training Loss: 1.300, Validation Accuracy: 54.12%\n",
            "Epoch [2/10], Training Loss: 1.273, Validation Accuracy: 53.69%\n",
            "Epoch [3/10], Training Loss: 1.263, Validation Accuracy: 52.69%\n",
            "Epoch [4/10], Training Loss: 1.251, Validation Accuracy: 54.06%\n",
            "Epoch [5/10], Training Loss: 1.240, Validation Accuracy: 52.91%\n",
            "Epoch [6/10], Training Loss: 1.236, Validation Accuracy: 54.19%\n",
            "Epoch [7/10], Training Loss: 1.222, Validation Accuracy: 53.48%\n",
            "Epoch [8/10], Training Loss: 1.214, Validation Accuracy: 54.03%\n",
            "Epoch [9/10], Training Loss: 1.203, Validation Accuracy: 53.56%\n",
            "Epoch [10/10], Training Loss: 1.196, Validation Accuracy: 53.28%\n",
            "Epoch [1/10], Training Loss: 1.254, Validation Accuracy: 53.97%\n",
            "Epoch [2/10], Training Loss: 1.229, Validation Accuracy: 53.59%\n",
            "Epoch [3/10], Training Loss: 1.220, Validation Accuracy: 54.39%\n",
            "Epoch [4/10], Training Loss: 1.210, Validation Accuracy: 54.19%\n",
            "Epoch [5/10], Training Loss: 1.190, Validation Accuracy: 54.86%\n",
            "Epoch [6/10], Training Loss: 1.178, Validation Accuracy: 54.82%\n",
            "Epoch [7/10], Training Loss: 1.175, Validation Accuracy: 53.44%\n",
            "Epoch [8/10], Training Loss: 1.156, Validation Accuracy: 53.97%\n",
            "Epoch [9/10], Training Loss: 1.156, Validation Accuracy: 54.75%\n",
            "Epoch [10/10], Training Loss: 1.154, Validation Accuracy: 54.64%\n",
            "Epoch [1/10], Training Loss: 1.217, Validation Accuracy: 55.93%\n",
            "Epoch [2/10], Training Loss: 1.184, Validation Accuracy: 55.21%\n",
            "Epoch [3/10], Training Loss: 1.169, Validation Accuracy: 55.11%\n",
            "Epoch [4/10], Training Loss: 1.166, Validation Accuracy: 55.49%\n",
            "Epoch [5/10], Training Loss: 1.152, Validation Accuracy: 54.89%\n",
            "Epoch [6/10], Training Loss: 1.136, Validation Accuracy: 55.85%\n",
            "Epoch [7/10], Training Loss: 1.131, Validation Accuracy: 56.07%\n",
            "Epoch [8/10], Training Loss: 1.116, Validation Accuracy: 54.89%\n",
            "Epoch [9/10], Training Loss: 1.108, Validation Accuracy: 55.72%\n",
            "Epoch [10/10], Training Loss: 1.098, Validation Accuracy: 55.24%\n",
            "Epoch [1/10], Training Loss: 1.227, Validation Accuracy: 56.17%\n",
            "Epoch [2/10], Training Loss: 1.191, Validation Accuracy: 56.27%\n",
            "Epoch [3/10], Training Loss: 1.170, Validation Accuracy: 56.43%\n",
            "Epoch [4/10], Training Loss: 1.157, Validation Accuracy: 56.23%\n",
            "Epoch [5/10], Training Loss: 1.148, Validation Accuracy: 56.60%\n",
            "Epoch [6/10], Training Loss: 1.149, Validation Accuracy: 55.56%\n",
            "Epoch [7/10], Training Loss: 1.130, Validation Accuracy: 56.13%\n",
            "Epoch [8/10], Training Loss: 1.125, Validation Accuracy: 56.55%\n",
            "Epoch [9/10], Training Loss: 1.107, Validation Accuracy: 55.93%\n",
            "Epoch [10/10], Training Loss: 1.102, Validation Accuracy: 56.11%\n",
            "Epoch [1/10], Training Loss: 1.192, Validation Accuracy: 56.17%\n",
            "Epoch [2/10], Training Loss: 1.166, Validation Accuracy: 55.97%\n",
            "Epoch [3/10], Training Loss: 1.154, Validation Accuracy: 56.53%\n",
            "Epoch [4/10], Training Loss: 1.146, Validation Accuracy: 55.75%\n",
            "Epoch [5/10], Training Loss: 1.122, Validation Accuracy: 57.29%\n",
            "Epoch [6/10], Training Loss: 1.116, Validation Accuracy: 56.77%\n",
            "Epoch [7/10], Training Loss: 1.107, Validation Accuracy: 56.37%\n",
            "Epoch [8/10], Training Loss: 1.095, Validation Accuracy: 55.36%\n",
            "Epoch [9/10], Training Loss: 1.091, Validation Accuracy: 55.75%\n",
            "Epoch [10/10], Training Loss: 1.073, Validation Accuracy: 56.51%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 57.31%\n",
            "Epoch [2/10], Training Loss: 1.168, Validation Accuracy: 56.25%\n",
            "Epoch [3/10], Training Loss: 1.153, Validation Accuracy: 56.47%\n",
            "Epoch [4/10], Training Loss: 1.121, Validation Accuracy: 56.95%\n",
            "Epoch [5/10], Training Loss: 1.112, Validation Accuracy: 57.57%\n",
            "Epoch [6/10], Training Loss: 1.107, Validation Accuracy: 56.65%\n",
            "Epoch [7/10], Training Loss: 1.102, Validation Accuracy: 56.74%\n",
            "Epoch [8/10], Training Loss: 1.082, Validation Accuracy: 57.80%\n",
            "Epoch [9/10], Training Loss: 1.072, Validation Accuracy: 56.27%\n",
            "Epoch [10/10], Training Loss: 1.071, Validation Accuracy: 57.28%\n",
            "Epoch [1/10], Training Loss: 1.147, Validation Accuracy: 56.78%\n",
            "Epoch [2/10], Training Loss: 1.119, Validation Accuracy: 57.95%\n",
            "Epoch [3/10], Training Loss: 1.098, Validation Accuracy: 57.29%\n",
            "Epoch [4/10], Training Loss: 1.082, Validation Accuracy: 57.49%\n",
            "Epoch [5/10], Training Loss: 1.064, Validation Accuracy: 57.86%\n",
            "Epoch [6/10], Training Loss: 1.044, Validation Accuracy: 58.21%\n",
            "Epoch [7/10], Training Loss: 1.038, Validation Accuracy: 58.07%\n",
            "Epoch [8/10], Training Loss: 1.029, Validation Accuracy: 57.16%\n",
            "Epoch [9/10], Training Loss: 1.020, Validation Accuracy: 55.45%\n",
            "Epoch [10/10], Training Loss: 1.028, Validation Accuracy: 57.48%\n",
            "Epoch [1/10], Training Loss: 1.117, Validation Accuracy: 57.60%\n",
            "Epoch [2/10], Training Loss: 1.080, Validation Accuracy: 58.13%\n",
            "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 58.44%\n",
            "Epoch [4/10], Training Loss: 1.050, Validation Accuracy: 56.63%\n",
            "Epoch [5/10], Training Loss: 1.040, Validation Accuracy: 57.90%\n",
            "Epoch [6/10], Training Loss: 1.025, Validation Accuracy: 58.24%\n",
            "Epoch [7/10], Training Loss: 1.004, Validation Accuracy: 58.72%\n",
            "Epoch [8/10], Training Loss: 1.000, Validation Accuracy: 58.19%\n",
            "Epoch [9/10], Training Loss: 0.986, Validation Accuracy: 58.23%\n",
            "Epoch [10/10], Training Loss: 0.979, Validation Accuracy: 58.28%\n",
            "Epoch [1/10], Training Loss: 1.140, Validation Accuracy: 58.20%\n",
            "Epoch [2/10], Training Loss: 1.104, Validation Accuracy: 59.12%\n",
            "Epoch [3/10], Training Loss: 1.079, Validation Accuracy: 58.50%\n",
            "Epoch [4/10], Training Loss: 1.062, Validation Accuracy: 59.01%\n",
            "Epoch [5/10], Training Loss: 1.048, Validation Accuracy: 58.59%\n",
            "Epoch [6/10], Training Loss: 1.040, Validation Accuracy: 58.97%\n",
            "Epoch [7/10], Training Loss: 1.014, Validation Accuracy: 58.68%\n",
            "Epoch [8/10], Training Loss: 1.017, Validation Accuracy: 59.04%\n",
            "Epoch [9/10], Training Loss: 1.002, Validation Accuracy: 59.19%\n",
            "Epoch [10/10], Training Loss: 0.996, Validation Accuracy: 58.67%\n",
            "Epoch [1/10], Training Loss: 1.118, Validation Accuracy: 58.87%\n",
            "Epoch [2/10], Training Loss: 1.074, Validation Accuracy: 58.58%\n",
            "Epoch [3/10], Training Loss: 1.044, Validation Accuracy: 59.30%\n",
            "Epoch [4/10], Training Loss: 1.031, Validation Accuracy: 58.68%\n",
            "Epoch [5/10], Training Loss: 1.016, Validation Accuracy: 58.24%\n",
            "Epoch [6/10], Training Loss: 1.011, Validation Accuracy: 58.69%\n",
            "Epoch [7/10], Training Loss: 0.996, Validation Accuracy: 58.78%\n",
            "Epoch [8/10], Training Loss: 0.986, Validation Accuracy: 58.96%\n",
            "Epoch [9/10], Training Loss: 0.958, Validation Accuracy: 58.22%\n",
            "Epoch [10/10], Training Loss: 0.957, Validation Accuracy: 58.55%\n",
            "Epoch [1/10], Training Loss: 1.104, Validation Accuracy: 58.54%\n",
            "Epoch [2/10], Training Loss: 1.070, Validation Accuracy: 59.69%\n",
            "Epoch [3/10], Training Loss: 1.042, Validation Accuracy: 59.42%\n",
            "Epoch [4/10], Training Loss: 1.024, Validation Accuracy: 59.80%\n",
            "Epoch [5/10], Training Loss: 1.003, Validation Accuracy: 58.69%\n",
            "Epoch [6/10], Training Loss: 0.997, Validation Accuracy: 59.79%\n",
            "Epoch [7/10], Training Loss: 0.990, Validation Accuracy: 59.65%\n",
            "Epoch [8/10], Training Loss: 0.974, Validation Accuracy: 58.92%\n",
            "Epoch [9/10], Training Loss: 0.957, Validation Accuracy: 58.85%\n",
            "Epoch [10/10], Training Loss: 0.944, Validation Accuracy: 59.70%\n",
            "Epoch [1/10], Training Loss: 1.066, Validation Accuracy: 59.55%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 59.72%\n",
            "Epoch [3/10], Training Loss: 1.002, Validation Accuracy: 59.63%\n",
            "Epoch [4/10], Training Loss: 0.992, Validation Accuracy: 58.69%\n",
            "Epoch [5/10], Training Loss: 0.964, Validation Accuracy: 59.40%\n",
            "Epoch [6/10], Training Loss: 0.948, Validation Accuracy: 59.25%\n",
            "Epoch [7/10], Training Loss: 0.936, Validation Accuracy: 59.71%\n",
            "Epoch [8/10], Training Loss: 0.928, Validation Accuracy: 59.44%\n",
            "Epoch [9/10], Training Loss: 0.915, Validation Accuracy: 58.55%\n",
            "Epoch [10/10], Training Loss: 0.898, Validation Accuracy: 60.12%\n",
            "Epoch [1/10], Training Loss: 1.049, Validation Accuracy: 59.03%\n",
            "Epoch [2/10], Training Loss: 1.004, Validation Accuracy: 60.41%\n",
            "Epoch [3/10], Training Loss: 0.985, Validation Accuracy: 59.61%\n",
            "Epoch [4/10], Training Loss: 0.953, Validation Accuracy: 59.96%\n",
            "Epoch [5/10], Training Loss: 0.943, Validation Accuracy: 60.16%\n",
            "Epoch [6/10], Training Loss: 0.924, Validation Accuracy: 59.38%\n",
            "Epoch [7/10], Training Loss: 0.906, Validation Accuracy: 59.99%\n",
            "Epoch [8/10], Training Loss: 0.894, Validation Accuracy: 58.99%\n",
            "Epoch [9/10], Training Loss: 0.895, Validation Accuracy: 58.65%\n",
            "Epoch [10/10], Training Loss: 0.879, Validation Accuracy: 59.31%\n",
            "Epoch [1/10], Training Loss: 1.069, Validation Accuracy: 60.13%\n",
            "Epoch [2/10], Training Loss: 1.025, Validation Accuracy: 60.85%\n",
            "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 58.82%\n",
            "Epoch [4/10], Training Loss: 0.994, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.974, Validation Accuracy: 60.29%\n",
            "Epoch [6/10], Training Loss: 0.942, Validation Accuracy: 59.92%\n",
            "Epoch [7/10], Training Loss: 0.927, Validation Accuracy: 60.27%\n",
            "Epoch [8/10], Training Loss: 0.916, Validation Accuracy: 60.43%\n",
            "Epoch [9/10], Training Loss: 0.900, Validation Accuracy: 60.05%\n",
            "Epoch [10/10], Training Loss: 0.889, Validation Accuracy: 60.08%\n",
            "Epoch [1/10], Training Loss: 1.042, Validation Accuracy: 60.17%\n",
            "Epoch [2/10], Training Loss: 0.987, Validation Accuracy: 59.94%\n",
            "Epoch [3/10], Training Loss: 0.968, Validation Accuracy: 59.41%\n",
            "Epoch [4/10], Training Loss: 0.940, Validation Accuracy: 60.60%\n",
            "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 60.15%\n",
            "Epoch [6/10], Training Loss: 0.906, Validation Accuracy: 60.07%\n",
            "Epoch [7/10], Training Loss: 0.883, Validation Accuracy: 60.59%\n",
            "Epoch [8/10], Training Loss: 0.877, Validation Accuracy: 60.11%\n",
            "Epoch [9/10], Training Loss: 0.856, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.841, Validation Accuracy: 59.17%\n",
            "Epoch [1/10], Training Loss: 1.041, Validation Accuracy: 60.80%\n",
            "Epoch [2/10], Training Loss: 0.986, Validation Accuracy: 59.79%\n",
            "Epoch [3/10], Training Loss: 0.963, Validation Accuracy: 60.74%\n",
            "Epoch [4/10], Training Loss: 0.934, Validation Accuracy: 60.17%\n",
            "Epoch [5/10], Training Loss: 0.915, Validation Accuracy: 60.28%\n",
            "Epoch [6/10], Training Loss: 0.908, Validation Accuracy: 59.73%\n",
            "Epoch [7/10], Training Loss: 0.886, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.868, Validation Accuracy: 60.07%\n",
            "Epoch [9/10], Training Loss: 0.860, Validation Accuracy: 60.32%\n",
            "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 60.70%\n",
            "Epoch [1/10], Training Loss: 1.005, Validation Accuracy: 59.86%\n",
            "Epoch [2/10], Training Loss: 0.956, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 0.930, Validation Accuracy: 60.28%\n",
            "Epoch [4/10], Training Loss: 0.898, Validation Accuracy: 60.04%\n",
            "Epoch [5/10], Training Loss: 0.881, Validation Accuracy: 60.67%\n",
            "Epoch [6/10], Training Loss: 0.859, Validation Accuracy: 60.41%\n",
            "Epoch [7/10], Training Loss: 0.851, Validation Accuracy: 60.67%\n",
            "Epoch [8/10], Training Loss: 0.834, Validation Accuracy: 61.01%\n",
            "Epoch [9/10], Training Loss: 0.820, Validation Accuracy: 59.62%\n",
            "Epoch [10/10], Training Loss: 0.806, Validation Accuracy: 60.79%\n",
            "Epoch [1/10], Training Loss: 0.982, Validation Accuracy: 61.08%\n",
            "Epoch [2/10], Training Loss: 0.929, Validation Accuracy: 60.21%\n",
            "Epoch [3/10], Training Loss: 0.909, Validation Accuracy: 61.13%\n",
            "Epoch [4/10], Training Loss: 0.875, Validation Accuracy: 60.06%\n",
            "Epoch [5/10], Training Loss: 0.858, Validation Accuracy: 61.49%\n",
            "Epoch [6/10], Training Loss: 0.841, Validation Accuracy: 60.73%\n",
            "Epoch [7/10], Training Loss: 0.820, Validation Accuracy: 60.82%\n",
            "Epoch [8/10], Training Loss: 0.802, Validation Accuracy: 60.22%\n",
            "Epoch [9/10], Training Loss: 0.805, Validation Accuracy: 60.51%\n",
            "Epoch [10/10], Training Loss: 0.784, Validation Accuracy: 60.63%\n",
            "Epoch [1/10], Training Loss: 1.010, Validation Accuracy: 59.68%\n",
            "Epoch [2/10], Training Loss: 0.950, Validation Accuracy: 61.55%\n",
            "Epoch [3/10], Training Loss: 0.912, Validation Accuracy: 61.37%\n",
            "Epoch [4/10], Training Loss: 0.889, Validation Accuracy: 60.30%\n",
            "Epoch [5/10], Training Loss: 0.869, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.849, Validation Accuracy: 60.10%\n",
            "Epoch [7/10], Training Loss: 0.845, Validation Accuracy: 60.19%\n",
            "Epoch [8/10], Training Loss: 0.830, Validation Accuracy: 59.28%\n",
            "Epoch [9/10], Training Loss: 0.808, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.804, Validation Accuracy: 60.80%\n",
            "Epoch [1/10], Training Loss: 0.972, Validation Accuracy: 60.80%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 61.34%\n",
            "Epoch [3/10], Training Loss: 0.882, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.858, Validation Accuracy: 61.36%\n",
            "Epoch [5/10], Training Loss: 0.833, Validation Accuracy: 61.55%\n",
            "Epoch [6/10], Training Loss: 0.816, Validation Accuracy: 60.36%\n",
            "Epoch [7/10], Training Loss: 0.806, Validation Accuracy: 60.89%\n",
            "Epoch [8/10], Training Loss: 0.786, Validation Accuracy: 61.25%\n",
            "Epoch [9/10], Training Loss: 0.764, Validation Accuracy: 59.87%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 61.10%\n",
            "Epoch [1/10], Training Loss: 0.993, Validation Accuracy: 61.04%\n",
            "Epoch [2/10], Training Loss: 0.918, Validation Accuracy: 61.41%\n",
            "Epoch [3/10], Training Loss: 0.889, Validation Accuracy: 61.25%\n",
            "Epoch [4/10], Training Loss: 0.857, Validation Accuracy: 61.51%\n",
            "Epoch [5/10], Training Loss: 0.849, Validation Accuracy: 61.00%\n",
            "Epoch [6/10], Training Loss: 0.812, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.801, Validation Accuracy: 60.69%\n",
            "Epoch [8/10], Training Loss: 0.777, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.777, Validation Accuracy: 61.33%\n",
            "Epoch [10/10], Training Loss: 0.753, Validation Accuracy: 61.31%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 60.91%\n",
            "Epoch [2/10], Training Loss: 0.891, Validation Accuracy: 61.31%\n",
            "Epoch [3/10], Training Loss: 0.854, Validation Accuracy: 59.71%\n",
            "Epoch [4/10], Training Loss: 0.831, Validation Accuracy: 61.48%\n",
            "Epoch [5/10], Training Loss: 0.793, Validation Accuracy: 61.21%\n",
            "Epoch [6/10], Training Loss: 0.775, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.760, Validation Accuracy: 61.41%\n",
            "Epoch [8/10], Training Loss: 0.744, Validation Accuracy: 61.25%\n",
            "Epoch [9/10], Training Loss: 0.722, Validation Accuracy: 61.12%\n",
            "Epoch [10/10], Training Loss: 0.713, Validation Accuracy: 61.07%\n",
            "Epoch [1/10], Training Loss: 0.942, Validation Accuracy: 61.80%\n",
            "Epoch [2/10], Training Loss: 0.872, Validation Accuracy: 60.98%\n",
            "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.808, Validation Accuracy: 61.67%\n",
            "Epoch [5/10], Training Loss: 0.778, Validation Accuracy: 61.95%\n",
            "Epoch [6/10], Training Loss: 0.756, Validation Accuracy: 61.65%\n",
            "Epoch [7/10], Training Loss: 0.740, Validation Accuracy: 59.97%\n",
            "Epoch [8/10], Training Loss: 0.727, Validation Accuracy: 61.72%\n",
            "Epoch [9/10], Training Loss: 0.707, Validation Accuracy: 60.85%\n",
            "Epoch [10/10], Training Loss: 0.692, Validation Accuracy: 61.49%\n",
            "Epoch [1/10], Training Loss: 0.962, Validation Accuracy: 60.84%\n",
            "Epoch [2/10], Training Loss: 0.879, Validation Accuracy: 61.61%\n",
            "Epoch [3/10], Training Loss: 0.851, Validation Accuracy: 61.28%\n",
            "Epoch [4/10], Training Loss: 0.814, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.791, Validation Accuracy: 61.26%\n",
            "Epoch [6/10], Training Loss: 0.772, Validation Accuracy: 61.58%\n",
            "Epoch [7/10], Training Loss: 0.750, Validation Accuracy: 61.44%\n",
            "Epoch [8/10], Training Loss: 0.736, Validation Accuracy: 61.14%\n",
            "Epoch [9/10], Training Loss: 0.712, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.699, Validation Accuracy: 60.20%\n",
            "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 60.59%\n",
            "Epoch [2/10], Training Loss: 0.846, Validation Accuracy: 61.73%\n",
            "Epoch [3/10], Training Loss: 0.815, Validation Accuracy: 60.53%\n",
            "Epoch [4/10], Training Loss: 0.775, Validation Accuracy: 61.60%\n",
            "Epoch [5/10], Training Loss: 0.754, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.719, Validation Accuracy: 60.86%\n",
            "Epoch [7/10], Training Loss: 0.710, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.690, Validation Accuracy: 61.35%\n",
            "Epoch [9/10], Training Loss: 0.666, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.655, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.945, Validation Accuracy: 61.07%\n",
            "Epoch [2/10], Training Loss: 0.867, Validation Accuracy: 61.67%\n",
            "Epoch [3/10], Training Loss: 0.818, Validation Accuracy: 60.84%\n",
            "Epoch [4/10], Training Loss: 0.780, Validation Accuracy: 61.24%\n",
            "Epoch [5/10], Training Loss: 0.755, Validation Accuracy: 61.25%\n",
            "Epoch [6/10], Training Loss: 0.736, Validation Accuracy: 60.66%\n",
            "Epoch [7/10], Training Loss: 0.716, Validation Accuracy: 61.09%\n",
            "Epoch [8/10], Training Loss: 0.701, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.671, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.661, Validation Accuracy: 61.15%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 60.69%\n",
            "Epoch [2/10], Training Loss: 0.825, Validation Accuracy: 61.96%\n",
            "Epoch [3/10], Training Loss: 0.796, Validation Accuracy: 61.82%\n",
            "Epoch [4/10], Training Loss: 0.746, Validation Accuracy: 61.72%\n",
            "Epoch [5/10], Training Loss: 0.717, Validation Accuracy: 61.73%\n",
            "Epoch [6/10], Training Loss: 0.699, Validation Accuracy: 61.18%\n",
            "Epoch [7/10], Training Loss: 0.677, Validation Accuracy: 61.05%\n",
            "Epoch [8/10], Training Loss: 0.662, Validation Accuracy: 61.47%\n",
            "Epoch [9/10], Training Loss: 0.639, Validation Accuracy: 60.87%\n",
            "Epoch [10/10], Training Loss: 0.633, Validation Accuracy: 61.00%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 60.74%\n",
            "Epoch [2/10], Training Loss: 0.819, Validation Accuracy: 60.90%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 61.62%\n",
            "Epoch [4/10], Training Loss: 0.729, Validation Accuracy: 61.75%\n",
            "Epoch [5/10], Training Loss: 0.691, Validation Accuracy: 61.04%\n",
            "Epoch [6/10], Training Loss: 0.680, Validation Accuracy: 61.58%\n",
            "Epoch [7/10], Training Loss: 0.645, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.635, Validation Accuracy: 60.83%\n",
            "Epoch [9/10], Training Loss: 0.617, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.605, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 0.920, Validation Accuracy: 61.27%\n",
            "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.26%\n",
            "Epoch [3/10], Training Loss: 0.772, Validation Accuracy: 61.30%\n",
            "Epoch [4/10], Training Loss: 0.741, Validation Accuracy: 62.05%\n",
            "Epoch [5/10], Training Loss: 0.712, Validation Accuracy: 61.48%\n",
            "Epoch [6/10], Training Loss: 0.690, Validation Accuracy: 60.63%\n",
            "Epoch [7/10], Training Loss: 0.668, Validation Accuracy: 61.51%\n",
            "Epoch [8/10], Training Loss: 0.646, Validation Accuracy: 61.09%\n",
            "Epoch [9/10], Training Loss: 0.632, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.612, Validation Accuracy: 60.90%\n",
            "Epoch [1/10], Training Loss: 0.897, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 0.788, Validation Accuracy: 61.16%\n",
            "Epoch [3/10], Training Loss: 0.740, Validation Accuracy: 61.77%\n",
            "Epoch [4/10], Training Loss: 0.701, Validation Accuracy: 61.99%\n",
            "Epoch [5/10], Training Loss: 0.673, Validation Accuracy: 61.74%\n",
            "Epoch [6/10], Training Loss: 0.649, Validation Accuracy: 61.11%\n",
            "Epoch [7/10], Training Loss: 0.618, Validation Accuracy: 60.60%\n",
            "Epoch [8/10], Training Loss: 0.612, Validation Accuracy: 61.49%\n",
            "Epoch [9/10], Training Loss: 0.584, Validation Accuracy: 60.44%\n",
            "Epoch [10/10], Training Loss: 0.572, Validation Accuracy: 60.99%\n",
            "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 61.59%\n",
            "Epoch [2/10], Training Loss: 0.797, Validation Accuracy: 60.94%\n",
            "Epoch [3/10], Training Loss: 0.749, Validation Accuracy: 61.60%\n",
            "Epoch [4/10], Training Loss: 0.714, Validation Accuracy: 61.55%\n",
            "Epoch [5/10], Training Loss: 0.676, Validation Accuracy: 61.42%\n",
            "Epoch [6/10], Training Loss: 0.650, Validation Accuracy: 61.63%\n",
            "Epoch [7/10], Training Loss: 0.632, Validation Accuracy: 60.43%\n",
            "Epoch [8/10], Training Loss: 0.615, Validation Accuracy: 61.62%\n",
            "Epoch [9/10], Training Loss: 0.582, Validation Accuracy: 61.28%\n",
            "Epoch [10/10], Training Loss: 0.573, Validation Accuracy: 61.16%\n",
            "Epoch [1/10], Training Loss: 0.875, Validation Accuracy: 61.81%\n",
            "Epoch [2/10], Training Loss: 0.773, Validation Accuracy: 61.68%\n",
            "Epoch [3/10], Training Loss: 0.715, Validation Accuracy: 61.48%\n",
            "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 60.96%\n",
            "Epoch [5/10], Training Loss: 0.657, Validation Accuracy: 61.43%\n",
            "Epoch [6/10], Training Loss: 0.629, Validation Accuracy: 61.53%\n",
            "Epoch [7/10], Training Loss: 0.610, Validation Accuracy: 61.11%\n",
            "Epoch [8/10], Training Loss: 0.581, Validation Accuracy: 61.75%\n",
            "Epoch [9/10], Training Loss: 0.561, Validation Accuracy: 61.48%\n",
            "Epoch [10/10], Training Loss: 0.536, Validation Accuracy: 61.37%\n",
            "Epoch [1/10], Training Loss: 0.872, Validation Accuracy: 61.00%\n",
            "Epoch [2/10], Training Loss: 0.758, Validation Accuracy: 61.42%\n",
            "Epoch [3/10], Training Loss: 0.694, Validation Accuracy: 60.74%\n",
            "Epoch [4/10], Training Loss: 0.663, Validation Accuracy: 60.74%\n",
            "Epoch [5/10], Training Loss: 0.622, Validation Accuracy: 61.58%\n",
            "Epoch [6/10], Training Loss: 0.592, Validation Accuracy: 61.08%\n",
            "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.560, Validation Accuracy: 61.21%\n",
            "Epoch [9/10], Training Loss: 0.529, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.523, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.885, Validation Accuracy: 58.89%\n",
            "Epoch [2/10], Training Loss: 0.775, Validation Accuracy: 60.10%\n",
            "Epoch [3/10], Training Loss: 0.711, Validation Accuracy: 60.87%\n",
            "Epoch [4/10], Training Loss: 0.664, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.641, Validation Accuracy: 60.95%\n",
            "Epoch [6/10], Training Loss: 0.618, Validation Accuracy: 61.44%\n",
            "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 60.38%\n",
            "Epoch [8/10], Training Loss: 0.571, Validation Accuracy: 60.53%\n",
            "Epoch [9/10], Training Loss: 0.546, Validation Accuracy: 61.11%\n",
            "Epoch [10/10], Training Loss: 0.528, Validation Accuracy: 60.74%\n",
            "Epoch [1/10], Training Loss: 0.854, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.731, Validation Accuracy: 60.59%\n",
            "Epoch [3/10], Training Loss: 0.673, Validation Accuracy: 60.67%\n",
            "Epoch [4/10], Training Loss: 0.636, Validation Accuracy: 60.91%\n",
            "Epoch [5/10], Training Loss: 0.596, Validation Accuracy: 60.93%\n",
            "Epoch [6/10], Training Loss: 0.581, Validation Accuracy: 60.43%\n",
            "Epoch [7/10], Training Loss: 0.546, Validation Accuracy: 60.21%\n",
            "Epoch [8/10], Training Loss: 0.531, Validation Accuracy: 61.32%\n",
            "Epoch [9/10], Training Loss: 0.501, Validation Accuracy: 60.76%\n",
            "Epoch [10/10], Training Loss: 0.482, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.873, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.746, Validation Accuracy: 60.92%\n",
            "Epoch [3/10], Training Loss: 0.694, Validation Accuracy: 61.62%\n",
            "Epoch [4/10], Training Loss: 0.636, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 0.604, Validation Accuracy: 61.50%\n",
            "Epoch [6/10], Training Loss: 0.576, Validation Accuracy: 61.77%\n",
            "Epoch [7/10], Training Loss: 0.553, Validation Accuracy: 60.75%\n",
            "Epoch [8/10], Training Loss: 0.549, Validation Accuracy: 61.44%\n",
            "Epoch [9/10], Training Loss: 0.509, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.495, Validation Accuracy: 61.04%\n",
            "Confusion Matrix:\n",
            "[[676  34  45  22  18   4  18  22 115  46]\n",
            " [ 36 747  10   8   5   5  19   8  52 110]\n",
            " [ 91  11 460 100 113  47  95  54  23   6]\n",
            " [ 31  20  82 461  73 124  99  59  30  21]\n",
            " [ 44   5  84  73 516  43  70 138  22   5]\n",
            " [ 27  10  96 236  55 376  55 123  11  11]\n",
            " [ 14   9  52  76  44  24 740  28   6   7]\n",
            " [ 34   9  34  59  67  51  26 688   5  27]\n",
            " [ 94  50   7  19  10   6  16   3 773  22]\n",
            " [ 44 149  16  27  15   7  27  40  64 611]]\n",
            "Test Accuracy: 60.48%\n",
            "True Positives (TP): [676 747 460 461 516 376 740 688 773 611]\n",
            "False Positives (FP): [415 297 426 620 400 311 425 475 328 255]\n",
            "True Negatives (TN): [8585 8703 8574 8380 8600 8689 8575 8525 8672 8745]\n",
            "False Negatives (FN): [324 253 540 539 484 624 260 312 227 389]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.61961503 0.71551724 0.51918736 0.42645698 0.56331878 0.54730713\n",
            " 0.63519313 0.59157352 0.70208901 0.70554273]\n",
            "Recall: [0.676 0.747 0.46  0.461 0.516 0.376 0.74  0.688 0.773 0.611]\n",
            "F1 Score: [0.64658058 0.73091977 0.48780488 0.44305622 0.53862213 0.44576171\n",
            " 0.68360277 0.63615349 0.73584008 0.65487674]\n"
          ]
        }
      ]
    }
  ]
}