# KL-FedDis: A Federated Learning Approach with Distribution Information Sharing

![KL-FedDis](https://img.shields.io/badge/Federated%20Learning-KL--FedDis-blue)  
A PyTorch-based implementation of **KL-FedDis**, a federated learning framework that incorporates Kullbackâ€“Leibler divergence for effective communication and training in non-IID environments.

## ğŸ§  About

This repository provides the official code for the paper:  
**"KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data"**  
Published in *Neuroscience Informatics (Q1 Journal), Elsevier.*

**Authors:** Md. Rahad, Ruhan Shabab, Mohd. Sultan Ahammad, Md. Mahfuz Reza, Amit Karmaker, Md. Abir Hossain, PhD  
ğŸ“„ [KL-FedDis Paper (Elsevier, 2024)](https://doi.org/10.1016/j.neuri.2024.100182)


---

## ğŸ“Œ Highlights

- KL divergence-based model distribution sharing
- Improves training convergence under Non-IID settings
- Benchmarked on CIFAR-10 using PyTorch
- Simulated multi-client federated environment

---

## ğŸ—‚ï¸ Project Every Files Structure


ğŸ§± 1. Model Definitions


ğŸ“¦ 2. Data Preparation


ğŸ“š 3. Loss Functions


ğŸ‹ï¸ 4. Training Functions



ğŸ“Š 5. Evaluation


ğŸ§‘â€ğŸ¤â€ğŸ§‘ 6. Federated Client Simulation


ğŸ”„ 7. Distribution Sharing for KL-FedDis


ğŸŒ 8. Model Aggregation and Communication


ğŸ” 9. Federated Training Coordinator


ğŸ§  10. Global Orchestration





---

## ğŸ“š Citation

If you use this code, please cite:

```bibtex
@article{rahad2024klfeddis,
  title={KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data},
  author={Rahad, Md. and Shabab, Ruhan and Ahammad, Mohd. Sultan and Reza, Md. Mahfuz and Karmaker, Amit and Hossain, Md. Abir},
  journal={Neuroscience Informatics},
  year={2024},
  publisher={Elsevier}
}


