# KL-FedDis: A Federated Learning Approach with Distribution Information Sharing

![KL-FedDis](https://img.shields.io/badge/Federated%20Learning-KL--FedDis-blue)  
A PyTorch-based implementation of **KL-FedDis**, a federated learning framework that incorporates Kullback–Leibler divergence for effective communication and training in non-IID environments.

## 🧠 About

This repository provides the official code for the paper:  
**"KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data"**  
Published in *Neuroscience Informatics (Q1 Journal), Elsevier.*

**Authors:** Md. Rahad, Ruhan Shabab, Mohd. Sultan Ahammad, Md. Mahfuz Reza, Amit Karmaker, Md. Abir Hossain, PhD  
📄 [KL-FedDis Paper (Elsevier, 2024)](https://doi.org/10.1016/j.neuri.2024.100182)


---

## 📌 Highlights

- KL divergence-based model distribution sharing
- Improves training convergence under Non-IID settings
- Benchmarked on CIFAR-10 using PyTorch
- Simulated multi-client federated environment

---

## 🗂️ Project Every Files Structure


🧱 1. Model Definitions


📦 2. Data Preparation


📚 3. Loss Functions


🏋️ 4. Training Functions



📊 5. Evaluation


🧑‍🤝‍🧑 6. Federated Client Simulation


🔄 7. Distribution Sharing for KL-FedDis


🌐 8. Model Aggregation and Communication


🔁 9. Federated Training Coordinator


🧠 10. Global Orchestration





---

## 📚 Citation

If you use this code, please cite:

```bibtex
@article{rahad2024klfeddis,
  title={KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data},
  author={Rahad, Md. and Shabab, Ruhan and Ahammad, Mohd. Sultan and Reza, Md. Mahfuz and Karmaker, Amit and Hossain, Md. Abir},
  journal={Neuroscience Informatics},
  year={2024},
  publisher={Elsevier}
}


