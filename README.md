# KL-FedDis: A Federated Learning Approach with Distribution Information Sharing

![KL-FedDis](https://img.shields.io/badge/Federated%20Learning-KL--FedDis-blue)  
A PyTorch-based implementation of **KL-FedDis**, a federated learning framework that incorporates Kullbackâ€“Leibler divergence for effective communication and training in non-IID environments.

## ğŸ§  About

This repository provides the official code for the paper:  
**"KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data"**  
Published in *Neuroscience Informatics (Q1 Journal), Elsevier.*

**Authors:** Md. Rahad, Ruhan Shabab, Mohd. Sultan Ahammad, Md. Mahfuz Reza, Amit Karmaker, Md. Abir Hossain, PhD  
[[https://www.sciencedirect.com/science/article/pii/S277252862400027X](#)] 

---

## ğŸ“Œ Highlights

- KL divergence-based model distribution sharing
- Improves training convergence under Non-IID settings
- Benchmarked on CIFAR-10 using PyTorch
- Simulated multi-client federated environment

---

## ğŸ—‚ï¸ Project every file Structure

```bash
ğŸ§± 1. Model Definitions
VAE â€” Variational Autoencoder for learning data distributions

Net â€” CNN-based image classification model

ğŸ“¦ 2. Data Preparation
transform â€” Data normalization and transformation pipeline

CIFAR10 â€” Dataset loading

train_set, val_set, test_set â€” Data split

DataLoader â€” Batching and shuffling

ğŸ“š 3. Loss Functions
vae_loss(recon_x, x, mu, logvar) â€” Combines reconstruction loss + KL divergence

CrossEntropyLoss() â€” For classification model training

ğŸ‹ï¸ 4. Training Functions
vae_train(vae, trainloader, epochs) â€” Trains VAE on local client data

train(net, trainloader, valloader, epochs) â€” Trains classification model

ğŸ“Š 5. Evaluation
evaluate(net, testloader) â€” Computes test accuracy of classification model

ğŸ§‘â€ğŸ¤â€ğŸ§‘ 6. Federated Client Simulation
initialize_clients(trainset, transform, batch_size, num_clients)
â†’ Splits data and creates local data loaders per client

ğŸ”„ 7. Distribution Sharing for KL-FedDis
get_distribution_info(vae) â€” Extracts mean & std from VAE

send_distribution_info(info) â€” Stub to send info to global server

receive_distribution_info() â€” Stub to receive info from other clients

generate_augmented_data(vae, info) â€” Synthesizes new data using Truncated Normal

ğŸŒ 8. Model Aggregation and Communication
send_model_update(client_id, model_update) â€” Stub to send model to server

(Note: No aggregation logic yet, could be added later)

ğŸ” 9. Federated Training Coordinator
federated_train(...) â€” Manages full round: local training + sharing + updating

ğŸ§  10. Global Orchestration
global_server() â€” Initializes models, clients, coordinates FL rounds

if __name__ == "__main__" â€” Entry point to run the simulation



---

## ğŸ“š Citation
If you use this code, please cite:

bibtex

@article{rahad2024klfeddis,
  title={KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data},
  author={Rahad, Md. and Shabab, Ruhan and Ahammad, Mohd. Sultan and Reza, Md. Mahfuz and Karmaker, Amit and Hossain, Md. Abir},
  journal={Neuroscience Informatics},
  year={2024},
  publisher={Elsevier}
}

---
## ğŸ¤ Contact
For questions, feel free to contact:

Md. Rahad
ğŸ“§ rahad3100@gmail.com

