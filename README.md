# KL-FedDis: A Federated Learning Approach with Distribution Information Sharing

![KL-FedDis](https://img.shields.io/badge/Federated%20Learning-KL--FedDis-blue)  
A PyTorch-based implementation of **KL-FedDis**, a federated learning framework that incorporates Kullbackâ€“Leibler divergence for effective communication and training in non-IID environments.

## ğŸ§  About

This repository provides the official code for the paper:  
**"KL-FedDis: A Federated Learning Approach with Distribution Information Sharing Using Kullback-Leibler Divergence for Non-IID Data"**  
Published in *Neuroscience Informatics (Q1 Journal), Elsevier.*

**Authors:** Md. Rahad, Ruhan Shabab, Mohd. Sultan Ahammad, Md. Mahfuz Reza, Amit Karmaker, Md. Abir Hossain, PhD  
[[Link to Paper](#)] â† *(Add actual link)*

---

## ğŸ“Œ Highlights

- KL divergence-based model distribution sharing
- Improves training convergence under Non-IID settings
- Benchmarked on CIFAR-10 using PyTorch
- Simulated multi-client federated environment

---

## ğŸ—‚ï¸ Project Structure

```bash
KL-FedDis/
â”œâ”€â”€ data_loader.py        # Data loading and partitioning
â”œâ”€â”€ models.py             # VAE and CNN models
â”œâ”€â”€ train.py              # Client and server training logic
â”œâ”€â”€ utils.py              # Helper functions
â”œâ”€â”€ main.py               # Main script to run training
â”œâ”€â”€ config.yaml           # Configuration parameters
â””â”€â”€ README.md             # Project documentation
