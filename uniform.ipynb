{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahad31/Kl-FedDis-Research-/blob/main/uniform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jm9tM2UkZFx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from typing import Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnfR-vvMlGIq"
      },
      "outputs": [],
      "source": [
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIMUT8_llUGf"
      },
      "outputs": [],
      "source": [
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHX4iOponI5i"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix03Pbuslfms",
        "outputId": "bd73e1eb-5b87-426f-b934-dc934032dd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 74167062.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRM2x2Y-liUJ"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8KMuc9oln-w"
      },
      "outputs": [],
      "source": [
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX0TOxK9lrCT"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFEH0Wt-ltgk"
      },
      "outputs": [],
      "source": [
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqaNgda9lweI"
      },
      "outputs": [],
      "source": [
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0lPGYGylyoV"
      },
      "outputs": [],
      "source": [
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqPri-58l06g"
      },
      "outputs": [],
      "source": [
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "        \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65FL3lwnl3M6"
      },
      "outputs": [],
      "source": [
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGG_ofDel6oo"
      },
      "outputs": [],
      "source": [
        "def generate_augmented_data(vae: VAE, distribution_info: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using Uniform distribution\n",
        "    mean = distribution_info[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info[\"std\"].mean().item()  # Convert numpy array to float\n",
        "    augmented_data = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "    return augmented_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWRPXeAMl9RY"
      },
      "outputs": [],
      "source": [
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info)\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEKGn1qUl_4l"
      },
      "outputs": [],
      "source": [
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "        \"std\": np.ones(20)\n",
        "    }\n",
        "    return distribution_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8kfRFIXmCcW"
      },
      "outputs": [],
      "source": [
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj93gTG2mExL",
        "outputId": "be6c82ad-45e1-4ab2-d539-ce241d61538a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 12.83%\n",
            "Epoch [2/10], Training Loss: 2.301, Validation Accuracy: 13.90%\n",
            "Epoch [3/10], Training Loss: 2.299, Validation Accuracy: 13.75%\n",
            "Epoch [4/10], Training Loss: 2.296, Validation Accuracy: 13.71%\n",
            "Epoch [5/10], Training Loss: 2.294, Validation Accuracy: 13.32%\n",
            "Epoch [6/10], Training Loss: 2.290, Validation Accuracy: 13.18%\n",
            "Epoch [7/10], Training Loss: 2.286, Validation Accuracy: 13.15%\n",
            "Epoch [8/10], Training Loss: 2.280, Validation Accuracy: 14.79%\n",
            "Epoch [9/10], Training Loss: 2.273, Validation Accuracy: 17.14%\n",
            "Epoch [10/10], Training Loss: 2.263, Validation Accuracy: 18.26%\n",
            "Epoch [1/10], Training Loss: 2.255, Validation Accuracy: 20.06%\n",
            "Epoch [2/10], Training Loss: 2.239, Validation Accuracy: 21.04%\n",
            "Epoch [3/10], Training Loss: 2.218, Validation Accuracy: 21.98%\n",
            "Epoch [4/10], Training Loss: 2.189, Validation Accuracy: 23.01%\n",
            "Epoch [5/10], Training Loss: 2.158, Validation Accuracy: 23.58%\n",
            "Epoch [6/10], Training Loss: 2.129, Validation Accuracy: 24.17%\n",
            "Epoch [7/10], Training Loss: 2.106, Validation Accuracy: 24.46%\n",
            "Epoch [8/10], Training Loss: 2.088, Validation Accuracy: 25.73%\n",
            "Epoch [9/10], Training Loss: 2.068, Validation Accuracy: 26.00%\n",
            "Epoch [10/10], Training Loss: 2.051, Validation Accuracy: 26.60%\n",
            "Epoch [1/10], Training Loss: 2.033, Validation Accuracy: 26.43%\n",
            "Epoch [2/10], Training Loss: 2.015, Validation Accuracy: 27.12%\n",
            "Epoch [3/10], Training Loss: 2.002, Validation Accuracy: 28.09%\n",
            "Epoch [4/10], Training Loss: 1.985, Validation Accuracy: 28.11%\n",
            "Epoch [5/10], Training Loss: 1.969, Validation Accuracy: 27.71%\n",
            "Epoch [6/10], Training Loss: 1.957, Validation Accuracy: 28.89%\n",
            "Epoch [7/10], Training Loss: 1.944, Validation Accuracy: 29.40%\n",
            "Epoch [8/10], Training Loss: 1.929, Validation Accuracy: 30.10%\n",
            "Epoch [9/10], Training Loss: 1.917, Validation Accuracy: 29.78%\n",
            "Epoch [10/10], Training Loss: 1.906, Validation Accuracy: 30.90%\n",
            "Epoch [1/10], Training Loss: 1.909, Validation Accuracy: 31.06%\n",
            "Epoch [2/10], Training Loss: 1.895, Validation Accuracy: 32.07%\n",
            "Epoch [3/10], Training Loss: 1.878, Validation Accuracy: 31.93%\n",
            "Epoch [4/10], Training Loss: 1.868, Validation Accuracy: 32.71%\n",
            "Epoch [5/10], Training Loss: 1.847, Validation Accuracy: 33.24%\n",
            "Epoch [6/10], Training Loss: 1.832, Validation Accuracy: 33.91%\n",
            "Epoch [7/10], Training Loss: 1.818, Validation Accuracy: 34.23%\n",
            "Epoch [8/10], Training Loss: 1.807, Validation Accuracy: 33.68%\n",
            "Epoch [9/10], Training Loss: 1.783, Validation Accuracy: 35.49%\n",
            "Epoch [10/10], Training Loss: 1.769, Validation Accuracy: 35.47%\n",
            "Epoch [1/10], Training Loss: 1.758, Validation Accuracy: 35.75%\n",
            "Epoch [2/10], Training Loss: 1.745, Validation Accuracy: 36.79%\n",
            "Epoch [3/10], Training Loss: 1.728, Validation Accuracy: 36.79%\n",
            "Epoch [4/10], Training Loss: 1.709, Validation Accuracy: 37.37%\n",
            "Epoch [5/10], Training Loss: 1.697, Validation Accuracy: 38.20%\n",
            "Epoch [6/10], Training Loss: 1.689, Validation Accuracy: 37.31%\n",
            "Epoch [7/10], Training Loss: 1.675, Validation Accuracy: 38.47%\n",
            "Epoch [8/10], Training Loss: 1.663, Validation Accuracy: 38.77%\n",
            "Epoch [9/10], Training Loss: 1.649, Validation Accuracy: 38.81%\n",
            "Epoch [10/10], Training Loss: 1.643, Validation Accuracy: 39.65%\n",
            "Epoch [1/10], Training Loss: 1.657, Validation Accuracy: 38.48%\n",
            "Epoch [2/10], Training Loss: 1.644, Validation Accuracy: 39.89%\n",
            "Epoch [3/10], Training Loss: 1.625, Validation Accuracy: 39.54%\n",
            "Epoch [4/10], Training Loss: 1.616, Validation Accuracy: 40.10%\n",
            "Epoch [5/10], Training Loss: 1.607, Validation Accuracy: 39.66%\n",
            "Epoch [6/10], Training Loss: 1.597, Validation Accuracy: 41.13%\n",
            "Epoch [7/10], Training Loss: 1.585, Validation Accuracy: 41.32%\n",
            "Epoch [8/10], Training Loss: 1.571, Validation Accuracy: 41.44%\n",
            "Epoch [9/10], Training Loss: 1.569, Validation Accuracy: 41.23%\n",
            "Epoch [10/10], Training Loss: 1.560, Validation Accuracy: 42.28%\n",
            "Epoch [1/10], Training Loss: 1.592, Validation Accuracy: 41.58%\n",
            "Epoch [2/10], Training Loss: 1.573, Validation Accuracy: 42.96%\n",
            "Epoch [3/10], Training Loss: 1.564, Validation Accuracy: 42.10%\n",
            "Epoch [4/10], Training Loss: 1.558, Validation Accuracy: 42.86%\n",
            "Epoch [5/10], Training Loss: 1.542, Validation Accuracy: 42.91%\n",
            "Epoch [6/10], Training Loss: 1.538, Validation Accuracy: 43.72%\n",
            "Epoch [7/10], Training Loss: 1.521, Validation Accuracy: 44.11%\n",
            "Epoch [8/10], Training Loss: 1.515, Validation Accuracy: 43.01%\n",
            "Epoch [9/10], Training Loss: 1.506, Validation Accuracy: 44.37%\n",
            "Epoch [10/10], Training Loss: 1.497, Validation Accuracy: 44.10%\n",
            "Epoch [1/10], Training Loss: 1.523, Validation Accuracy: 45.51%\n",
            "Epoch [2/10], Training Loss: 1.499, Validation Accuracy: 45.89%\n",
            "Epoch [3/10], Training Loss: 1.497, Validation Accuracy: 45.66%\n",
            "Epoch [4/10], Training Loss: 1.489, Validation Accuracy: 45.72%\n",
            "Epoch [5/10], Training Loss: 1.473, Validation Accuracy: 45.72%\n",
            "Epoch [6/10], Training Loss: 1.476, Validation Accuracy: 46.09%\n",
            "Epoch [7/10], Training Loss: 1.458, Validation Accuracy: 46.52%\n",
            "Epoch [8/10], Training Loss: 1.448, Validation Accuracy: 45.32%\n",
            "Epoch [9/10], Training Loss: 1.440, Validation Accuracy: 46.49%\n",
            "Epoch [10/10], Training Loss: 1.434, Validation Accuracy: 46.78%\n",
            "Epoch [1/10], Training Loss: 1.487, Validation Accuracy: 47.17%\n",
            "Epoch [2/10], Training Loss: 1.468, Validation Accuracy: 46.54%\n",
            "Epoch [3/10], Training Loss: 1.464, Validation Accuracy: 46.79%\n",
            "Epoch [4/10], Training Loss: 1.455, Validation Accuracy: 46.64%\n",
            "Epoch [5/10], Training Loss: 1.445, Validation Accuracy: 47.37%\n",
            "Epoch [6/10], Training Loss: 1.431, Validation Accuracy: 47.88%\n",
            "Epoch [7/10], Training Loss: 1.434, Validation Accuracy: 47.05%\n",
            "Epoch [8/10], Training Loss: 1.419, Validation Accuracy: 46.00%\n",
            "Epoch [9/10], Training Loss: 1.411, Validation Accuracy: 48.17%\n",
            "Epoch [10/10], Training Loss: 1.408, Validation Accuracy: 47.68%\n",
            "Epoch [1/10], Training Loss: 1.418, Validation Accuracy: 48.28%\n",
            "Epoch [2/10], Training Loss: 1.397, Validation Accuracy: 48.63%\n",
            "Epoch [3/10], Training Loss: 1.385, Validation Accuracy: 48.63%\n",
            "Epoch [4/10], Training Loss: 1.388, Validation Accuracy: 49.21%\n",
            "Epoch [5/10], Training Loss: 1.371, Validation Accuracy: 48.54%\n",
            "Epoch [6/10], Training Loss: 1.361, Validation Accuracy: 49.03%\n",
            "Epoch [7/10], Training Loss: 1.353, Validation Accuracy: 49.31%\n",
            "Epoch [8/10], Training Loss: 1.348, Validation Accuracy: 48.64%\n",
            "Epoch [9/10], Training Loss: 1.336, Validation Accuracy: 49.59%\n",
            "Epoch [10/10], Training Loss: 1.332, Validation Accuracy: 48.83%\n",
            "Epoch [1/10], Training Loss: 1.389, Validation Accuracy: 49.60%\n",
            "Epoch [2/10], Training Loss: 1.379, Validation Accuracy: 49.36%\n",
            "Epoch [3/10], Training Loss: 1.364, Validation Accuracy: 49.51%\n",
            "Epoch [4/10], Training Loss: 1.346, Validation Accuracy: 50.27%\n",
            "Epoch [5/10], Training Loss: 1.339, Validation Accuracy: 49.17%\n",
            "Epoch [6/10], Training Loss: 1.334, Validation Accuracy: 50.20%\n",
            "Epoch [7/10], Training Loss: 1.323, Validation Accuracy: 50.70%\n",
            "Epoch [8/10], Training Loss: 1.314, Validation Accuracy: 50.83%\n",
            "Epoch [9/10], Training Loss: 1.317, Validation Accuracy: 51.04%\n",
            "Epoch [10/10], Training Loss: 1.295, Validation Accuracy: 50.90%\n",
            "Epoch [1/10], Training Loss: 1.353, Validation Accuracy: 51.12%\n",
            "Epoch [2/10], Training Loss: 1.334, Validation Accuracy: 50.66%\n",
            "Epoch [3/10], Training Loss: 1.323, Validation Accuracy: 50.79%\n",
            "Epoch [4/10], Training Loss: 1.305, Validation Accuracy: 50.91%\n",
            "Epoch [5/10], Training Loss: 1.309, Validation Accuracy: 51.36%\n",
            "Epoch [6/10], Training Loss: 1.290, Validation Accuracy: 51.19%\n",
            "Epoch [7/10], Training Loss: 1.285, Validation Accuracy: 51.97%\n",
            "Epoch [8/10], Training Loss: 1.264, Validation Accuracy: 51.98%\n",
            "Epoch [9/10], Training Loss: 1.258, Validation Accuracy: 51.67%\n",
            "Epoch [10/10], Training Loss: 1.245, Validation Accuracy: 52.30%\n",
            "Epoch [1/10], Training Loss: 1.306, Validation Accuracy: 52.68%\n",
            "Epoch [2/10], Training Loss: 1.282, Validation Accuracy: 52.61%\n",
            "Epoch [3/10], Training Loss: 1.273, Validation Accuracy: 51.60%\n",
            "Epoch [4/10], Training Loss: 1.261, Validation Accuracy: 52.73%\n",
            "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 52.83%\n",
            "Epoch [6/10], Training Loss: 1.237, Validation Accuracy: 52.90%\n",
            "Epoch [7/10], Training Loss: 1.230, Validation Accuracy: 53.29%\n",
            "Epoch [8/10], Training Loss: 1.217, Validation Accuracy: 53.07%\n",
            "Epoch [9/10], Training Loss: 1.207, Validation Accuracy: 53.25%\n",
            "Epoch [10/10], Training Loss: 1.197, Validation Accuracy: 53.47%\n",
            "Epoch [1/10], Training Loss: 1.303, Validation Accuracy: 53.27%\n",
            "Epoch [2/10], Training Loss: 1.279, Validation Accuracy: 53.21%\n",
            "Epoch [3/10], Training Loss: 1.260, Validation Accuracy: 53.53%\n",
            "Epoch [4/10], Training Loss: 1.261, Validation Accuracy: 53.14%\n",
            "Epoch [5/10], Training Loss: 1.235, Validation Accuracy: 54.40%\n",
            "Epoch [6/10], Training Loss: 1.228, Validation Accuracy: 52.33%\n",
            "Epoch [7/10], Training Loss: 1.221, Validation Accuracy: 54.25%\n",
            "Epoch [8/10], Training Loss: 1.211, Validation Accuracy: 54.28%\n",
            "Epoch [9/10], Training Loss: 1.217, Validation Accuracy: 54.35%\n",
            "Epoch [10/10], Training Loss: 1.187, Validation Accuracy: 54.45%\n",
            "Epoch [1/10], Training Loss: 1.229, Validation Accuracy: 54.52%\n",
            "Epoch [2/10], Training Loss: 1.203, Validation Accuracy: 53.38%\n",
            "Epoch [3/10], Training Loss: 1.196, Validation Accuracy: 54.87%\n",
            "Epoch [4/10], Training Loss: 1.183, Validation Accuracy: 54.41%\n",
            "Epoch [5/10], Training Loss: 1.170, Validation Accuracy: 54.75%\n",
            "Epoch [6/10], Training Loss: 1.157, Validation Accuracy: 54.81%\n",
            "Epoch [7/10], Training Loss: 1.152, Validation Accuracy: 54.75%\n",
            "Epoch [8/10], Training Loss: 1.146, Validation Accuracy: 54.90%\n",
            "Epoch [9/10], Training Loss: 1.127, Validation Accuracy: 55.68%\n",
            "Epoch [10/10], Training Loss: 1.112, Validation Accuracy: 54.96%\n",
            "Epoch [1/10], Training Loss: 1.226, Validation Accuracy: 54.87%\n",
            "Epoch [2/10], Training Loss: 1.205, Validation Accuracy: 56.23%\n",
            "Epoch [3/10], Training Loss: 1.180, Validation Accuracy: 56.28%\n",
            "Epoch [4/10], Training Loss: 1.177, Validation Accuracy: 55.73%\n",
            "Epoch [5/10], Training Loss: 1.161, Validation Accuracy: 55.92%\n",
            "Epoch [6/10], Training Loss: 1.150, Validation Accuracy: 55.51%\n",
            "Epoch [7/10], Training Loss: 1.136, Validation Accuracy: 56.17%\n",
            "Epoch [8/10], Training Loss: 1.122, Validation Accuracy: 55.57%\n",
            "Epoch [9/10], Training Loss: 1.133, Validation Accuracy: 55.83%\n",
            "Epoch [10/10], Training Loss: 1.119, Validation Accuracy: 55.40%\n",
            "Epoch [1/10], Training Loss: 1.192, Validation Accuracy: 55.89%\n",
            "Epoch [2/10], Training Loss: 1.165, Validation Accuracy: 56.42%\n",
            "Epoch [3/10], Training Loss: 1.147, Validation Accuracy: 56.45%\n",
            "Epoch [4/10], Training Loss: 1.139, Validation Accuracy: 56.49%\n",
            "Epoch [5/10], Training Loss: 1.127, Validation Accuracy: 56.34%\n",
            "Epoch [6/10], Training Loss: 1.115, Validation Accuracy: 56.50%\n",
            "Epoch [7/10], Training Loss: 1.096, Validation Accuracy: 56.75%\n",
            "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 56.79%\n",
            "Epoch [9/10], Training Loss: 1.074, Validation Accuracy: 56.88%\n",
            "Epoch [10/10], Training Loss: 1.068, Validation Accuracy: 56.70%\n",
            "Epoch [1/10], Training Loss: 1.167, Validation Accuracy: 56.23%\n",
            "Epoch [2/10], Training Loss: 1.139, Validation Accuracy: 56.43%\n",
            "Epoch [3/10], Training Loss: 1.125, Validation Accuracy: 56.47%\n",
            "Epoch [4/10], Training Loss: 1.110, Validation Accuracy: 56.71%\n",
            "Epoch [5/10], Training Loss: 1.086, Validation Accuracy: 57.21%\n",
            "Epoch [6/10], Training Loss: 1.077, Validation Accuracy: 56.91%\n",
            "Epoch [7/10], Training Loss: 1.071, Validation Accuracy: 57.27%\n",
            "Epoch [8/10], Training Loss: 1.056, Validation Accuracy: 57.24%\n",
            "Epoch [9/10], Training Loss: 1.044, Validation Accuracy: 57.29%\n",
            "Epoch [10/10], Training Loss: 1.034, Validation Accuracy: 57.50%\n",
            "Epoch [1/10], Training Loss: 1.173, Validation Accuracy: 57.58%\n",
            "Epoch [2/10], Training Loss: 1.142, Validation Accuracy: 57.52%\n",
            "Epoch [3/10], Training Loss: 1.129, Validation Accuracy: 57.49%\n",
            "Epoch [4/10], Training Loss: 1.106, Validation Accuracy: 56.93%\n",
            "Epoch [5/10], Training Loss: 1.097, Validation Accuracy: 57.93%\n",
            "Epoch [6/10], Training Loss: 1.082, Validation Accuracy: 57.37%\n",
            "Epoch [7/10], Training Loss: 1.064, Validation Accuracy: 58.06%\n",
            "Epoch [8/10], Training Loss: 1.061, Validation Accuracy: 57.28%\n",
            "Epoch [9/10], Training Loss: 1.051, Validation Accuracy: 57.57%\n",
            "Epoch [10/10], Training Loss: 1.037, Validation Accuracy: 58.07%\n",
            "Epoch [1/10], Training Loss: 1.112, Validation Accuracy: 58.75%\n",
            "Epoch [2/10], Training Loss: 1.077, Validation Accuracy: 58.48%\n",
            "Epoch [3/10], Training Loss: 1.068, Validation Accuracy: 58.31%\n",
            "Epoch [4/10], Training Loss: 1.040, Validation Accuracy: 58.31%\n",
            "Epoch [5/10], Training Loss: 1.031, Validation Accuracy: 58.35%\n",
            "Epoch [6/10], Training Loss: 1.017, Validation Accuracy: 58.96%\n",
            "Epoch [7/10], Training Loss: 1.007, Validation Accuracy: 58.32%\n",
            "Epoch [8/10], Training Loss: 0.996, Validation Accuracy: 57.58%\n",
            "Epoch [9/10], Training Loss: 0.991, Validation Accuracy: 57.30%\n",
            "Epoch [10/10], Training Loss: 0.970, Validation Accuracy: 58.13%\n",
            "Epoch [1/10], Training Loss: 1.117, Validation Accuracy: 59.08%\n",
            "Epoch [2/10], Training Loss: 1.086, Validation Accuracy: 58.56%\n",
            "Epoch [3/10], Training Loss: 1.064, Validation Accuracy: 58.90%\n",
            "Epoch [4/10], Training Loss: 1.045, Validation Accuracy: 58.67%\n",
            "Epoch [5/10], Training Loss: 1.031, Validation Accuracy: 58.49%\n",
            "Epoch [6/10], Training Loss: 1.025, Validation Accuracy: 58.01%\n",
            "Epoch [7/10], Training Loss: 1.011, Validation Accuracy: 58.17%\n",
            "Epoch [8/10], Training Loss: 1.001, Validation Accuracy: 58.64%\n",
            "Epoch [9/10], Training Loss: 0.989, Validation Accuracy: 58.74%\n",
            "Epoch [10/10], Training Loss: 0.972, Validation Accuracy: 58.39%\n",
            "Epoch [1/10], Training Loss: 1.095, Validation Accuracy: 58.87%\n",
            "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 58.27%\n",
            "Epoch [3/10], Training Loss: 1.045, Validation Accuracy: 58.58%\n",
            "Epoch [4/10], Training Loss: 1.017, Validation Accuracy: 58.95%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 58.32%\n",
            "Epoch [6/10], Training Loss: 0.995, Validation Accuracy: 58.73%\n",
            "Epoch [7/10], Training Loss: 0.980, Validation Accuracy: 59.02%\n",
            "Epoch [8/10], Training Loss: 0.966, Validation Accuracy: 59.24%\n",
            "Epoch [9/10], Training Loss: 0.950, Validation Accuracy: 58.19%\n",
            "Epoch [10/10], Training Loss: 0.943, Validation Accuracy: 59.16%\n",
            "Epoch [1/10], Training Loss: 1.070, Validation Accuracy: 59.55%\n",
            "Epoch [2/10], Training Loss: 1.038, Validation Accuracy: 59.81%\n",
            "Epoch [3/10], Training Loss: 1.012, Validation Accuracy: 58.97%\n",
            "Epoch [4/10], Training Loss: 1.001, Validation Accuracy: 58.43%\n",
            "Epoch [5/10], Training Loss: 0.980, Validation Accuracy: 59.32%\n",
            "Epoch [6/10], Training Loss: 0.961, Validation Accuracy: 58.98%\n",
            "Epoch [7/10], Training Loss: 0.957, Validation Accuracy: 59.76%\n",
            "Epoch [8/10], Training Loss: 0.940, Validation Accuracy: 59.42%\n",
            "Epoch [9/10], Training Loss: 0.925, Validation Accuracy: 59.14%\n",
            "Epoch [10/10], Training Loss: 0.914, Validation Accuracy: 58.97%\n",
            "Epoch [1/10], Training Loss: 1.079, Validation Accuracy: 58.29%\n",
            "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.35%\n",
            "Epoch [3/10], Training Loss: 1.015, Validation Accuracy: 58.38%\n",
            "Epoch [4/10], Training Loss: 1.000, Validation Accuracy: 59.10%\n",
            "Epoch [5/10], Training Loss: 0.981, Validation Accuracy: 60.11%\n",
            "Epoch [6/10], Training Loss: 0.959, Validation Accuracy: 59.97%\n",
            "Epoch [7/10], Training Loss: 0.971, Validation Accuracy: 59.37%\n",
            "Epoch [8/10], Training Loss: 0.938, Validation Accuracy: 59.12%\n",
            "Epoch [9/10], Training Loss: 0.929, Validation Accuracy: 59.25%\n",
            "Epoch [10/10], Training Loss: 0.913, Validation Accuracy: 58.53%\n",
            "Epoch [1/10], Training Loss: 1.032, Validation Accuracy: 60.08%\n",
            "Epoch [2/10], Training Loss: 0.989, Validation Accuracy: 60.20%\n",
            "Epoch [3/10], Training Loss: 0.962, Validation Accuracy: 60.19%\n",
            "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 60.49%\n",
            "Epoch [5/10], Training Loss: 0.917, Validation Accuracy: 60.72%\n",
            "Epoch [6/10], Training Loss: 0.901, Validation Accuracy: 60.75%\n",
            "Epoch [7/10], Training Loss: 0.891, Validation Accuracy: 59.86%\n",
            "Epoch [8/10], Training Loss: 0.883, Validation Accuracy: 60.12%\n",
            "Epoch [9/10], Training Loss: 0.882, Validation Accuracy: 59.41%\n",
            "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 59.80%\n",
            "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 60.30%\n",
            "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 59.37%\n",
            "Epoch [3/10], Training Loss: 0.977, Validation Accuracy: 60.04%\n",
            "Epoch [4/10], Training Loss: 0.940, Validation Accuracy: 60.51%\n",
            "Epoch [5/10], Training Loss: 0.938, Validation Accuracy: 60.59%\n",
            "Epoch [6/10], Training Loss: 0.913, Validation Accuracy: 60.50%\n",
            "Epoch [7/10], Training Loss: 0.892, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.883, Validation Accuracy: 60.93%\n",
            "Epoch [9/10], Training Loss: 0.865, Validation Accuracy: 59.89%\n",
            "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 0.973, Validation Accuracy: 60.16%\n",
            "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 59.19%\n",
            "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 60.77%\n",
            "Epoch [5/10], Training Loss: 0.902, Validation Accuracy: 60.83%\n",
            "Epoch [6/10], Training Loss: 0.898, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.872, Validation Accuracy: 60.36%\n",
            "Epoch [8/10], Training Loss: 0.857, Validation Accuracy: 59.89%\n",
            "Epoch [9/10], Training Loss: 0.843, Validation Accuracy: 59.98%\n",
            "Epoch [10/10], Training Loss: 0.837, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 0.996, Validation Accuracy: 61.04%\n",
            "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 60.81%\n",
            "Epoch [3/10], Training Loss: 0.927, Validation Accuracy: 60.25%\n",
            "Epoch [4/10], Training Loss: 0.899, Validation Accuracy: 59.90%\n",
            "Epoch [5/10], Training Loss: 0.876, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.861, Validation Accuracy: 60.10%\n",
            "Epoch [7/10], Training Loss: 0.848, Validation Accuracy: 60.60%\n",
            "Epoch [8/10], Training Loss: 0.834, Validation Accuracy: 60.50%\n",
            "Epoch [9/10], Training Loss: 0.818, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.805, Validation Accuracy: 60.26%\n",
            "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.41%\n",
            "Epoch [2/10], Training Loss: 0.953, Validation Accuracy: 60.18%\n",
            "Epoch [3/10], Training Loss: 0.929, Validation Accuracy: 60.56%\n",
            "Epoch [4/10], Training Loss: 0.913, Validation Accuracy: 60.99%\n",
            "Epoch [5/10], Training Loss: 0.880, Validation Accuracy: 60.22%\n",
            "Epoch [6/10], Training Loss: 0.866, Validation Accuracy: 60.36%\n",
            "Epoch [7/10], Training Loss: 0.852, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.830, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.817, Validation Accuracy: 59.99%\n",
            "Epoch [10/10], Training Loss: 0.797, Validation Accuracy: 60.52%\n",
            "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 61.42%\n",
            "Epoch [2/10], Training Loss: 0.912, Validation Accuracy: 60.86%\n",
            "Epoch [3/10], Training Loss: 0.874, Validation Accuracy: 61.14%\n",
            "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 60.90%\n",
            "Epoch [5/10], Training Loss: 0.835, Validation Accuracy: 61.34%\n",
            "Epoch [6/10], Training Loss: 0.809, Validation Accuracy: 61.17%\n",
            "Epoch [7/10], Training Loss: 0.794, Validation Accuracy: 61.31%\n",
            "Epoch [8/10], Training Loss: 0.776, Validation Accuracy: 61.38%\n",
            "Epoch [9/10], Training Loss: 0.766, Validation Accuracy: 60.25%\n",
            "Epoch [10/10], Training Loss: 0.754, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 61.25%\n",
            "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 61.02%\n",
            "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 59.60%\n",
            "Epoch [4/10], Training Loss: 0.861, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.833, Validation Accuracy: 61.65%\n",
            "Epoch [6/10], Training Loss: 0.812, Validation Accuracy: 60.80%\n",
            "Epoch [7/10], Training Loss: 0.795, Validation Accuracy: 61.03%\n",
            "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.56%\n",
            "Epoch [9/10], Training Loss: 0.768, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.745, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 59.89%\n",
            "Epoch [2/10], Training Loss: 0.893, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.867, Validation Accuracy: 60.83%\n",
            "Epoch [4/10], Training Loss: 0.841, Validation Accuracy: 60.61%\n",
            "Epoch [5/10], Training Loss: 0.815, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.794, Validation Accuracy: 60.49%\n",
            "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 61.39%\n",
            "Epoch [8/10], Training Loss: 0.756, Validation Accuracy: 61.31%\n",
            "Epoch [9/10], Training Loss: 0.740, Validation Accuracy: 60.87%\n",
            "Epoch [10/10], Training Loss: 0.730, Validation Accuracy: 60.91%\n",
            "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 60.94%\n",
            "Epoch [2/10], Training Loss: 0.869, Validation Accuracy: 61.39%\n",
            "Epoch [3/10], Training Loss: 0.850, Validation Accuracy: 61.70%\n",
            "Epoch [4/10], Training Loss: 0.809, Validation Accuracy: 61.66%\n",
            "Epoch [5/10], Training Loss: 0.790, Validation Accuracy: 61.23%\n",
            "Epoch [6/10], Training Loss: 0.764, Validation Accuracy: 60.52%\n",
            "Epoch [7/10], Training Loss: 0.745, Validation Accuracy: 61.48%\n",
            "Epoch [8/10], Training Loss: 0.727, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.714, Validation Accuracy: 61.23%\n",
            "Epoch [10/10], Training Loss: 0.697, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 0.951, Validation Accuracy: 61.22%\n",
            "Epoch [2/10], Training Loss: 0.879, Validation Accuracy: 61.37%\n",
            "Epoch [3/10], Training Loss: 0.844, Validation Accuracy: 61.61%\n",
            "Epoch [4/10], Training Loss: 0.809, Validation Accuracy: 60.42%\n",
            "Epoch [5/10], Training Loss: 0.788, Validation Accuracy: 61.59%\n",
            "Epoch [6/10], Training Loss: 0.765, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.753, Validation Accuracy: 61.23%\n",
            "Epoch [8/10], Training Loss: 0.738, Validation Accuracy: 61.23%\n",
            "Epoch [9/10], Training Loss: 0.714, Validation Accuracy: 61.06%\n",
            "Epoch [10/10], Training Loss: 0.695, Validation Accuracy: 61.02%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.44%\n",
            "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 61.63%\n",
            "Epoch [3/10], Training Loss: 0.790, Validation Accuracy: 61.93%\n",
            "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 61.38%\n",
            "Epoch [5/10], Training Loss: 0.745, Validation Accuracy: 61.81%\n",
            "Epoch [6/10], Training Loss: 0.722, Validation Accuracy: 61.75%\n",
            "Epoch [7/10], Training Loss: 0.701, Validation Accuracy: 61.79%\n",
            "Epoch [8/10], Training Loss: 0.682, Validation Accuracy: 61.84%\n",
            "Epoch [9/10], Training Loss: 0.665, Validation Accuracy: 61.51%\n",
            "Epoch [10/10], Training Loss: 0.646, Validation Accuracy: 61.23%\n",
            "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.17%\n",
            "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.795, Validation Accuracy: 60.24%\n",
            "Epoch [4/10], Training Loss: 0.766, Validation Accuracy: 61.40%\n",
            "Epoch [5/10], Training Loss: 0.738, Validation Accuracy: 61.60%\n",
            "Epoch [6/10], Training Loss: 0.715, Validation Accuracy: 61.22%\n",
            "Epoch [7/10], Training Loss: 0.699, Validation Accuracy: 61.39%\n",
            "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 60.72%\n",
            "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 61.16%\n",
            "Epoch [10/10], Training Loss: 0.648, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 61.11%\n",
            "Epoch [2/10], Training Loss: 0.831, Validation Accuracy: 61.05%\n",
            "Epoch [3/10], Training Loss: 0.781, Validation Accuracy: 61.38%\n",
            "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 61.30%\n",
            "Epoch [5/10], Training Loss: 0.730, Validation Accuracy: 61.46%\n",
            "Epoch [6/10], Training Loss: 0.707, Validation Accuracy: 61.17%\n",
            "Epoch [7/10], Training Loss: 0.681, Validation Accuracy: 60.98%\n",
            "Epoch [8/10], Training Loss: 0.670, Validation Accuracy: 61.24%\n",
            "Epoch [9/10], Training Loss: 0.644, Validation Accuracy: 60.46%\n",
            "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 60.96%\n",
            "Epoch [1/10], Training Loss: 0.890, Validation Accuracy: 61.34%\n",
            "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.754, Validation Accuracy: 61.84%\n",
            "Epoch [4/10], Training Loss: 0.731, Validation Accuracy: 61.24%\n",
            "Epoch [5/10], Training Loss: 0.700, Validation Accuracy: 61.80%\n",
            "Epoch [6/10], Training Loss: 0.676, Validation Accuracy: 61.47%\n",
            "Epoch [7/10], Training Loss: 0.657, Validation Accuracy: 61.38%\n",
            "Epoch [8/10], Training Loss: 0.633, Validation Accuracy: 60.26%\n",
            "Epoch [9/10], Training Loss: 0.624, Validation Accuracy: 61.08%\n",
            "Epoch [10/10], Training Loss: 0.606, Validation Accuracy: 60.96%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 61.35%\n",
            "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.31%\n",
            "Epoch [3/10], Training Loss: 0.781, Validation Accuracy: 61.45%\n",
            "Epoch [4/10], Training Loss: 0.739, Validation Accuracy: 60.96%\n",
            "Epoch [5/10], Training Loss: 0.703, Validation Accuracy: 61.90%\n",
            "Epoch [6/10], Training Loss: 0.691, Validation Accuracy: 61.33%\n",
            "Epoch [7/10], Training Loss: 0.665, Validation Accuracy: 61.78%\n",
            "Epoch [8/10], Training Loss: 0.645, Validation Accuracy: 61.09%\n",
            "Epoch [9/10], Training Loss: 0.626, Validation Accuracy: 61.14%\n",
            "Epoch [10/10], Training Loss: 0.604, Validation Accuracy: 60.57%\n",
            "Epoch [1/10], Training Loss: 0.852, Validation Accuracy: 61.85%\n",
            "Epoch [2/10], Training Loss: 0.757, Validation Accuracy: 61.66%\n",
            "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 61.97%\n",
            "Epoch [4/10], Training Loss: 0.695, Validation Accuracy: 61.22%\n",
            "Epoch [5/10], Training Loss: 0.660, Validation Accuracy: 61.33%\n",
            "Epoch [6/10], Training Loss: 0.631, Validation Accuracy: 61.20%\n",
            "Epoch [7/10], Training Loss: 0.614, Validation Accuracy: 61.67%\n",
            "Epoch [8/10], Training Loss: 0.586, Validation Accuracy: 61.61%\n",
            "Epoch [9/10], Training Loss: 0.565, Validation Accuracy: 61.80%\n",
            "Epoch [10/10], Training Loss: 0.558, Validation Accuracy: 61.71%\n",
            "Epoch [1/10], Training Loss: 0.854, Validation Accuracy: 60.96%\n",
            "Epoch [2/10], Training Loss: 0.770, Validation Accuracy: 62.38%\n",
            "Epoch [3/10], Training Loss: 0.726, Validation Accuracy: 61.84%\n",
            "Epoch [4/10], Training Loss: 0.688, Validation Accuracy: 62.00%\n",
            "Epoch [5/10], Training Loss: 0.655, Validation Accuracy: 61.51%\n",
            "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.17%\n",
            "Epoch [7/10], Training Loss: 0.613, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.583, Validation Accuracy: 61.53%\n",
            "Epoch [9/10], Training Loss: 0.571, Validation Accuracy: 61.34%\n",
            "Epoch [10/10], Training Loss: 0.547, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 0.843, Validation Accuracy: 60.85%\n",
            "Epoch [2/10], Training Loss: 0.752, Validation Accuracy: 61.34%\n",
            "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 61.19%\n",
            "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 61.74%\n",
            "Epoch [5/10], Training Loss: 0.640, Validation Accuracy: 61.43%\n",
            "Epoch [6/10], Training Loss: 0.617, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 61.24%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 61.96%\n",
            "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.525, Validation Accuracy: 60.73%\n",
            "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 61.52%\n",
            "Epoch [2/10], Training Loss: 0.744, Validation Accuracy: 61.81%\n",
            "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 61.12%\n",
            "Epoch [4/10], Training Loss: 0.657, Validation Accuracy: 61.17%\n",
            "Epoch [5/10], Training Loss: 0.618, Validation Accuracy: 62.16%\n",
            "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 61.44%\n",
            "Epoch [7/10], Training Loss: 0.578, Validation Accuracy: 61.45%\n",
            "Epoch [8/10], Training Loss: 0.550, Validation Accuracy: 60.98%\n",
            "Epoch [9/10], Training Loss: 0.527, Validation Accuracy: 61.04%\n",
            "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 60.98%\n",
            "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 60.35%\n",
            "Epoch [2/10], Training Loss: 0.756, Validation Accuracy: 61.40%\n",
            "Epoch [3/10], Training Loss: 0.698, Validation Accuracy: 61.11%\n",
            "Epoch [4/10], Training Loss: 0.656, Validation Accuracy: 62.16%\n",
            "Epoch [5/10], Training Loss: 0.622, Validation Accuracy: 61.48%\n",
            "Epoch [6/10], Training Loss: 0.602, Validation Accuracy: 61.32%\n",
            "Epoch [7/10], Training Loss: 0.567, Validation Accuracy: 61.46%\n",
            "Epoch [8/10], Training Loss: 0.544, Validation Accuracy: 61.39%\n",
            "Epoch [9/10], Training Loss: 0.528, Validation Accuracy: 61.07%\n",
            "Epoch [10/10], Training Loss: 0.514, Validation Accuracy: 61.23%\n",
            "Epoch [1/10], Training Loss: 0.815, Validation Accuracy: 61.70%\n",
            "Epoch [2/10], Training Loss: 0.702, Validation Accuracy: 61.94%\n",
            "Epoch [3/10], Training Loss: 0.656, Validation Accuracy: 61.65%\n",
            "Epoch [4/10], Training Loss: 0.613, Validation Accuracy: 61.22%\n",
            "Epoch [5/10], Training Loss: 0.582, Validation Accuracy: 60.32%\n",
            "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 61.66%\n",
            "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.504, Validation Accuracy: 61.12%\n",
            "Epoch [9/10], Training Loss: 0.489, Validation Accuracy: 61.75%\n",
            "Epoch [10/10], Training Loss: 0.463, Validation Accuracy: 61.75%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 61.10%\n",
            "Epoch [2/10], Training Loss: 0.703, Validation Accuracy: 60.93%\n",
            "Epoch [3/10], Training Loss: 0.655, Validation Accuracy: 60.82%\n",
            "Epoch [4/10], Training Loss: 0.611, Validation Accuracy: 60.87%\n",
            "Epoch [5/10], Training Loss: 0.581, Validation Accuracy: 61.81%\n",
            "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 61.67%\n",
            "Epoch [7/10], Training Loss: 0.516, Validation Accuracy: 61.21%\n",
            "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 61.46%\n",
            "Epoch [9/10], Training Loss: 0.470, Validation Accuracy: 61.34%\n",
            "Epoch [10/10], Training Loss: 0.456, Validation Accuracy: 61.27%\n",
            "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 61.48%\n",
            "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 60.95%\n",
            "Epoch [3/10], Training Loss: 0.634, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.585, Validation Accuracy: 61.35%\n",
            "Epoch [5/10], Training Loss: 0.555, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.521, Validation Accuracy: 61.13%\n",
            "Epoch [7/10], Training Loss: 0.502, Validation Accuracy: 61.38%\n",
            "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 60.05%\n",
            "Epoch [9/10], Training Loss: 0.461, Validation Accuracy: 61.21%\n",
            "Epoch [10/10], Training Loss: 0.433, Validation Accuracy: 61.59%\n",
            "Epoch [1/10], Training Loss: 0.807, Validation Accuracy: 61.43%\n",
            "Epoch [2/10], Training Loss: 0.687, Validation Accuracy: 61.22%\n",
            "Epoch [3/10], Training Loss: 0.609, Validation Accuracy: 61.78%\n",
            "Epoch [4/10], Training Loss: 0.567, Validation Accuracy: 62.06%\n",
            "Epoch [5/10], Training Loss: 0.533, Validation Accuracy: 60.65%\n",
            "Epoch [6/10], Training Loss: 0.498, Validation Accuracy: 60.99%\n",
            "Epoch [7/10], Training Loss: 0.479, Validation Accuracy: 61.32%\n",
            "Epoch [8/10], Training Loss: 0.447, Validation Accuracy: 60.97%\n",
            "Epoch [9/10], Training Loss: 0.441, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.415, Validation Accuracy: 60.81%\n",
            "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 60.77%\n",
            "Epoch [2/10], Training Loss: 0.699, Validation Accuracy: 61.00%\n",
            "Epoch [3/10], Training Loss: 0.629, Validation Accuracy: 60.61%\n",
            "Epoch [4/10], Training Loss: 0.579, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.538, Validation Accuracy: 60.41%\n",
            "Epoch [6/10], Training Loss: 0.515, Validation Accuracy: 61.32%\n",
            "Epoch [7/10], Training Loss: 0.479, Validation Accuracy: 61.26%\n",
            "Epoch [8/10], Training Loss: 0.453, Validation Accuracy: 60.49%\n",
            "Epoch [9/10], Training Loss: 0.439, Validation Accuracy: 60.61%\n",
            "Epoch [10/10], Training Loss: 0.425, Validation Accuracy: 60.87%\n",
            "Epoch [1/10], Training Loss: 0.789, Validation Accuracy: 60.64%\n",
            "Epoch [2/10], Training Loss: 0.645, Validation Accuracy: 61.18%\n",
            "Epoch [3/10], Training Loss: 0.568, Validation Accuracy: 61.99%\n",
            "Epoch [4/10], Training Loss: 0.523, Validation Accuracy: 61.66%\n",
            "Epoch [5/10], Training Loss: 0.497, Validation Accuracy: 61.53%\n",
            "Epoch [6/10], Training Loss: 0.462, Validation Accuracy: 61.01%\n",
            "Epoch [7/10], Training Loss: 0.439, Validation Accuracy: 61.53%\n",
            "Epoch [8/10], Training Loss: 0.417, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.394, Validation Accuracy: 61.43%\n",
            "Epoch [10/10], Training Loss: 0.375, Validation Accuracy: 61.10%\n",
            "Test Accuracy: 61.69%\n"
          ]
        }
      ],
      "source": [
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import truncnorm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> float:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "        \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "\n",
        "# Define logic to generate augmented data using Truncated Normal distribution\n",
        "def generate_augmented_data(vae: VAE, distribution_info: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using Uniform distribution\n",
        "    mean = distribution_info[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info[\"std\"].mean().item()  # Convert numpy array to float\n",
        "    augmented_data = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info)\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Implement the logic to receive the distribution information from the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to receive the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Receive the distribution information from the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to receive the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to receive the information\n",
        "    distribution_info = {\n",
        "        \"mean\": np.zeros(20),\n",
        "        \"std\": np.ones(20)\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsfTOoc_lWTW",
        "outputId": "7c1d880e-de97-4fca-99cb-e85dfbf7b0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 48402826.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.304, Validation Accuracy: 10.06%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.20%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.31%\n",
            "Epoch [4/10], Training Loss: 2.301, Validation Accuracy: 10.48%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 10.32%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 10.08%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 10.08%\n",
            "Epoch [8/10], Training Loss: 2.298, Validation Accuracy: 10.08%\n",
            "Epoch [9/10], Training Loss: 2.296, Validation Accuracy: 10.61%\n",
            "Epoch [10/10], Training Loss: 2.295, Validation Accuracy: 11.79%\n",
            "Epoch [1/10], Training Loss: 2.294, Validation Accuracy: 13.75%\n",
            "Epoch [2/10], Training Loss: 2.291, Validation Accuracy: 16.63%\n",
            "Epoch [3/10], Training Loss: 2.287, Validation Accuracy: 18.12%\n",
            "Epoch [4/10], Training Loss: 2.280, Validation Accuracy: 18.44%\n",
            "Epoch [5/10], Training Loss: 2.271, Validation Accuracy: 18.25%\n",
            "Epoch [6/10], Training Loss: 2.258, Validation Accuracy: 18.04%\n",
            "Epoch [7/10], Training Loss: 2.238, Validation Accuracy: 19.26%\n",
            "Epoch [8/10], Training Loss: 2.209, Validation Accuracy: 20.35%\n",
            "Epoch [9/10], Training Loss: 2.173, Validation Accuracy: 22.56%\n",
            "Epoch [10/10], Training Loss: 2.138, Validation Accuracy: 23.89%\n",
            "Epoch [1/10], Training Loss: 2.133, Validation Accuracy: 24.76%\n",
            "Epoch [2/10], Training Loss: 2.112, Validation Accuracy: 26.16%\n",
            "Epoch [3/10], Training Loss: 2.090, Validation Accuracy: 26.90%\n",
            "Epoch [4/10], Training Loss: 2.067, Validation Accuracy: 26.97%\n",
            "Epoch [5/10], Training Loss: 2.038, Validation Accuracy: 27.58%\n",
            "Epoch [6/10], Training Loss: 2.007, Validation Accuracy: 28.40%\n",
            "Epoch [7/10], Training Loss: 1.981, Validation Accuracy: 29.03%\n",
            "Epoch [8/10], Training Loss: 1.953, Validation Accuracy: 29.26%\n",
            "Epoch [9/10], Training Loss: 1.932, Validation Accuracy: 30.05%\n",
            "Epoch [10/10], Training Loss: 1.910, Validation Accuracy: 29.84%\n",
            "Epoch [1/10], Training Loss: 1.885, Validation Accuracy: 30.10%\n",
            "Epoch [2/10], Training Loss: 1.860, Validation Accuracy: 31.97%\n",
            "Epoch [3/10], Training Loss: 1.832, Validation Accuracy: 32.29%\n",
            "Epoch [4/10], Training Loss: 1.807, Validation Accuracy: 33.14%\n",
            "Epoch [5/10], Training Loss: 1.791, Validation Accuracy: 33.67%\n",
            "Epoch [6/10], Training Loss: 1.770, Validation Accuracy: 34.52%\n",
            "Epoch [7/10], Training Loss: 1.751, Validation Accuracy: 34.81%\n",
            "Epoch [8/10], Training Loss: 1.736, Validation Accuracy: 35.77%\n",
            "Epoch [9/10], Training Loss: 1.714, Validation Accuracy: 35.97%\n",
            "Epoch [10/10], Training Loss: 1.707, Validation Accuracy: 36.51%\n",
            "Epoch [1/10], Training Loss: 1.711, Validation Accuracy: 37.58%\n",
            "Epoch [2/10], Training Loss: 1.698, Validation Accuracy: 37.79%\n",
            "Epoch [3/10], Training Loss: 1.683, Validation Accuracy: 37.74%\n",
            "Epoch [4/10], Training Loss: 1.675, Validation Accuracy: 37.95%\n",
            "Epoch [5/10], Training Loss: 1.658, Validation Accuracy: 37.89%\n",
            "Epoch [6/10], Training Loss: 1.645, Validation Accuracy: 38.74%\n",
            "Epoch [7/10], Training Loss: 1.637, Validation Accuracy: 39.36%\n",
            "Epoch [8/10], Training Loss: 1.628, Validation Accuracy: 39.54%\n",
            "Epoch [9/10], Training Loss: 1.621, Validation Accuracy: 39.67%\n",
            "Epoch [10/10], Training Loss: 1.606, Validation Accuracy: 40.49%\n",
            "Epoch [1/10], Training Loss: 1.615, Validation Accuracy: 40.70%\n",
            "Epoch [2/10], Training Loss: 1.595, Validation Accuracy: 41.00%\n",
            "Epoch [3/10], Training Loss: 1.593, Validation Accuracy: 41.12%\n",
            "Epoch [4/10], Training Loss: 1.578, Validation Accuracy: 40.89%\n",
            "Epoch [5/10], Training Loss: 1.575, Validation Accuracy: 41.70%\n",
            "Epoch [6/10], Training Loss: 1.559, Validation Accuracy: 40.74%\n",
            "Epoch [7/10], Training Loss: 1.558, Validation Accuracy: 42.03%\n",
            "Epoch [8/10], Training Loss: 1.541, Validation Accuracy: 42.44%\n",
            "Epoch [9/10], Training Loss: 1.532, Validation Accuracy: 41.49%\n",
            "Epoch [10/10], Training Loss: 1.521, Validation Accuracy: 42.84%\n",
            "Epoch [1/10], Training Loss: 1.555, Validation Accuracy: 42.57%\n",
            "Epoch [2/10], Training Loss: 1.540, Validation Accuracy: 43.34%\n",
            "Epoch [3/10], Training Loss: 1.532, Validation Accuracy: 43.48%\n",
            "Epoch [4/10], Training Loss: 1.514, Validation Accuracy: 43.17%\n",
            "Epoch [5/10], Training Loss: 1.505, Validation Accuracy: 43.87%\n",
            "Epoch [6/10], Training Loss: 1.497, Validation Accuracy: 43.68%\n",
            "Epoch [7/10], Training Loss: 1.488, Validation Accuracy: 43.69%\n",
            "Epoch [8/10], Training Loss: 1.483, Validation Accuracy: 44.33%\n",
            "Epoch [9/10], Training Loss: 1.470, Validation Accuracy: 44.80%\n",
            "Epoch [10/10], Training Loss: 1.463, Validation Accuracy: 44.24%\n",
            "Epoch [1/10], Training Loss: 1.517, Validation Accuracy: 46.00%\n",
            "Epoch [2/10], Training Loss: 1.503, Validation Accuracy: 44.94%\n",
            "Epoch [3/10], Training Loss: 1.488, Validation Accuracy: 44.96%\n",
            "Epoch [4/10], Training Loss: 1.485, Validation Accuracy: 46.16%\n",
            "Epoch [5/10], Training Loss: 1.463, Validation Accuracy: 45.84%\n",
            "Epoch [6/10], Training Loss: 1.458, Validation Accuracy: 46.43%\n",
            "Epoch [7/10], Training Loss: 1.445, Validation Accuracy: 46.39%\n",
            "Epoch [8/10], Training Loss: 1.437, Validation Accuracy: 45.99%\n",
            "Epoch [9/10], Training Loss: 1.427, Validation Accuracy: 47.01%\n",
            "Epoch [10/10], Training Loss: 1.419, Validation Accuracy: 46.94%\n",
            "Epoch [1/10], Training Loss: 1.450, Validation Accuracy: 47.53%\n",
            "Epoch [2/10], Training Loss: 1.424, Validation Accuracy: 48.08%\n",
            "Epoch [3/10], Training Loss: 1.414, Validation Accuracy: 47.77%\n",
            "Epoch [4/10], Training Loss: 1.409, Validation Accuracy: 47.51%\n",
            "Epoch [5/10], Training Loss: 1.388, Validation Accuracy: 47.87%\n",
            "Epoch [6/10], Training Loss: 1.379, Validation Accuracy: 47.88%\n",
            "Epoch [7/10], Training Loss: 1.375, Validation Accuracy: 48.67%\n",
            "Epoch [8/10], Training Loss: 1.359, Validation Accuracy: 48.63%\n",
            "Epoch [9/10], Training Loss: 1.354, Validation Accuracy: 48.51%\n",
            "Epoch [10/10], Training Loss: 1.339, Validation Accuracy: 48.84%\n",
            "Epoch [1/10], Training Loss: 1.414, Validation Accuracy: 48.61%\n",
            "Epoch [2/10], Training Loss: 1.405, Validation Accuracy: 49.33%\n",
            "Epoch [3/10], Training Loss: 1.386, Validation Accuracy: 49.51%\n",
            "Epoch [4/10], Training Loss: 1.372, Validation Accuracy: 49.34%\n",
            "Epoch [5/10], Training Loss: 1.368, Validation Accuracy: 49.49%\n",
            "Epoch [6/10], Training Loss: 1.359, Validation Accuracy: 49.41%\n",
            "Epoch [7/10], Training Loss: 1.347, Validation Accuracy: 49.20%\n",
            "Epoch [8/10], Training Loss: 1.339, Validation Accuracy: 49.90%\n",
            "Epoch [9/10], Training Loss: 1.327, Validation Accuracy: 49.84%\n",
            "Epoch [10/10], Training Loss: 1.323, Validation Accuracy: 49.39%\n",
            "Epoch [1/10], Training Loss: 1.355, Validation Accuracy: 50.03%\n",
            "Epoch [2/10], Training Loss: 1.337, Validation Accuracy: 49.47%\n",
            "Epoch [3/10], Training Loss: 1.318, Validation Accuracy: 50.11%\n",
            "Epoch [4/10], Training Loss: 1.305, Validation Accuracy: 49.98%\n",
            "Epoch [5/10], Training Loss: 1.291, Validation Accuracy: 49.59%\n",
            "Epoch [6/10], Training Loss: 1.295, Validation Accuracy: 50.24%\n",
            "Epoch [7/10], Training Loss: 1.278, Validation Accuracy: 51.14%\n",
            "Epoch [8/10], Training Loss: 1.268, Validation Accuracy: 51.05%\n",
            "Epoch [9/10], Training Loss: 1.258, Validation Accuracy: 50.81%\n",
            "Epoch [10/10], Training Loss: 1.245, Validation Accuracy: 51.07%\n",
            "Epoch [1/10], Training Loss: 1.321, Validation Accuracy: 51.26%\n",
            "Epoch [2/10], Training Loss: 1.305, Validation Accuracy: 51.94%\n",
            "Epoch [3/10], Training Loss: 1.286, Validation Accuracy: 51.08%\n",
            "Epoch [4/10], Training Loss: 1.287, Validation Accuracy: 51.35%\n",
            "Epoch [5/10], Training Loss: 1.263, Validation Accuracy: 50.16%\n",
            "Epoch [6/10], Training Loss: 1.251, Validation Accuracy: 50.92%\n",
            "Epoch [7/10], Training Loss: 1.249, Validation Accuracy: 52.11%\n",
            "Epoch [8/10], Training Loss: 1.240, Validation Accuracy: 51.29%\n",
            "Epoch [9/10], Training Loss: 1.229, Validation Accuracy: 51.93%\n",
            "Epoch [10/10], Training Loss: 1.218, Validation Accuracy: 51.89%\n",
            "Epoch [1/10], Training Loss: 1.313, Validation Accuracy: 52.49%\n",
            "Epoch [2/10], Training Loss: 1.288, Validation Accuracy: 52.83%\n",
            "Epoch [3/10], Training Loss: 1.269, Validation Accuracy: 52.99%\n",
            "Epoch [4/10], Training Loss: 1.262, Validation Accuracy: 53.12%\n",
            "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 50.56%\n",
            "Epoch [6/10], Training Loss: 1.241, Validation Accuracy: 53.25%\n",
            "Epoch [7/10], Training Loss: 1.229, Validation Accuracy: 53.06%\n",
            "Epoch [8/10], Training Loss: 1.215, Validation Accuracy: 53.74%\n",
            "Epoch [9/10], Training Loss: 1.209, Validation Accuracy: 52.57%\n",
            "Epoch [10/10], Training Loss: 1.201, Validation Accuracy: 53.51%\n",
            "Epoch [1/10], Training Loss: 1.259, Validation Accuracy: 53.23%\n",
            "Epoch [2/10], Training Loss: 1.236, Validation Accuracy: 53.46%\n",
            "Epoch [3/10], Training Loss: 1.214, Validation Accuracy: 54.14%\n",
            "Epoch [4/10], Training Loss: 1.203, Validation Accuracy: 54.31%\n",
            "Epoch [5/10], Training Loss: 1.187, Validation Accuracy: 53.21%\n",
            "Epoch [6/10], Training Loss: 1.179, Validation Accuracy: 54.31%\n",
            "Epoch [7/10], Training Loss: 1.169, Validation Accuracy: 54.33%\n",
            "Epoch [8/10], Training Loss: 1.159, Validation Accuracy: 54.02%\n",
            "Epoch [9/10], Training Loss: 1.151, Validation Accuracy: 53.83%\n",
            "Epoch [10/10], Training Loss: 1.141, Validation Accuracy: 54.35%\n",
            "Epoch [1/10], Training Loss: 1.265, Validation Accuracy: 54.24%\n",
            "Epoch [2/10], Training Loss: 1.239, Validation Accuracy: 54.67%\n",
            "Epoch [3/10], Training Loss: 1.223, Validation Accuracy: 54.10%\n",
            "Epoch [4/10], Training Loss: 1.210, Validation Accuracy: 54.50%\n",
            "Epoch [5/10], Training Loss: 1.192, Validation Accuracy: 54.23%\n",
            "Epoch [6/10], Training Loss: 1.188, Validation Accuracy: 55.07%\n",
            "Epoch [7/10], Training Loss: 1.171, Validation Accuracy: 54.05%\n",
            "Epoch [8/10], Training Loss: 1.166, Validation Accuracy: 55.03%\n",
            "Epoch [9/10], Training Loss: 1.151, Validation Accuracy: 54.10%\n",
            "Epoch [10/10], Training Loss: 1.139, Validation Accuracy: 54.89%\n",
            "Epoch [1/10], Training Loss: 1.210, Validation Accuracy: 54.51%\n",
            "Epoch [2/10], Training Loss: 1.186, Validation Accuracy: 54.26%\n",
            "Epoch [3/10], Training Loss: 1.169, Validation Accuracy: 54.93%\n",
            "Epoch [4/10], Training Loss: 1.148, Validation Accuracy: 55.15%\n",
            "Epoch [5/10], Training Loss: 1.134, Validation Accuracy: 54.39%\n",
            "Epoch [6/10], Training Loss: 1.127, Validation Accuracy: 55.25%\n",
            "Epoch [7/10], Training Loss: 1.112, Validation Accuracy: 55.35%\n",
            "Epoch [8/10], Training Loss: 1.097, Validation Accuracy: 54.71%\n",
            "Epoch [9/10], Training Loss: 1.093, Validation Accuracy: 54.70%\n",
            "Epoch [10/10], Training Loss: 1.086, Validation Accuracy: 54.59%\n",
            "Epoch [1/10], Training Loss: 1.193, Validation Accuracy: 55.36%\n",
            "Epoch [2/10], Training Loss: 1.164, Validation Accuracy: 55.80%\n",
            "Epoch [3/10], Training Loss: 1.144, Validation Accuracy: 56.02%\n",
            "Epoch [4/10], Training Loss: 1.131, Validation Accuracy: 55.56%\n",
            "Epoch [5/10], Training Loss: 1.121, Validation Accuracy: 54.81%\n",
            "Epoch [6/10], Training Loss: 1.114, Validation Accuracy: 55.92%\n",
            "Epoch [7/10], Training Loss: 1.099, Validation Accuracy: 56.44%\n",
            "Epoch [8/10], Training Loss: 1.083, Validation Accuracy: 56.57%\n",
            "Epoch [9/10], Training Loss: 1.076, Validation Accuracy: 56.15%\n",
            "Epoch [10/10], Training Loss: 1.070, Validation Accuracy: 56.41%\n",
            "Epoch [1/10], Training Loss: 1.200, Validation Accuracy: 55.66%\n",
            "Epoch [2/10], Training Loss: 1.163, Validation Accuracy: 55.99%\n",
            "Epoch [3/10], Training Loss: 1.142, Validation Accuracy: 57.06%\n",
            "Epoch [4/10], Training Loss: 1.120, Validation Accuracy: 56.75%\n",
            "Epoch [5/10], Training Loss: 1.110, Validation Accuracy: 56.06%\n",
            "Epoch [6/10], Training Loss: 1.103, Validation Accuracy: 56.52%\n",
            "Epoch [7/10], Training Loss: 1.080, Validation Accuracy: 56.39%\n",
            "Epoch [8/10], Training Loss: 1.069, Validation Accuracy: 57.02%\n",
            "Epoch [9/10], Training Loss: 1.056, Validation Accuracy: 56.73%\n",
            "Epoch [10/10], Training Loss: 1.043, Validation Accuracy: 55.57%\n",
            "Epoch [1/10], Training Loss: 1.148, Validation Accuracy: 56.69%\n",
            "Epoch [2/10], Training Loss: 1.115, Validation Accuracy: 56.73%\n",
            "Epoch [3/10], Training Loss: 1.090, Validation Accuracy: 57.57%\n",
            "Epoch [4/10], Training Loss: 1.078, Validation Accuracy: 57.55%\n",
            "Epoch [5/10], Training Loss: 1.053, Validation Accuracy: 57.12%\n",
            "Epoch [6/10], Training Loss: 1.042, Validation Accuracy: 57.68%\n",
            "Epoch [7/10], Training Loss: 1.028, Validation Accuracy: 57.00%\n",
            "Epoch [8/10], Training Loss: 1.016, Validation Accuracy: 57.42%\n",
            "Epoch [9/10], Training Loss: 1.004, Validation Accuracy: 57.26%\n",
            "Epoch [10/10], Training Loss: 0.997, Validation Accuracy: 56.99%\n",
            "Epoch [1/10], Training Loss: 1.155, Validation Accuracy: 57.16%\n",
            "Epoch [2/10], Training Loss: 1.123, Validation Accuracy: 57.41%\n",
            "Epoch [3/10], Training Loss: 1.094, Validation Accuracy: 57.81%\n",
            "Epoch [4/10], Training Loss: 1.081, Validation Accuracy: 57.82%\n",
            "Epoch [5/10], Training Loss: 1.068, Validation Accuracy: 56.98%\n",
            "Epoch [6/10], Training Loss: 1.050, Validation Accuracy: 57.46%\n",
            "Epoch [7/10], Training Loss: 1.040, Validation Accuracy: 56.52%\n",
            "Epoch [8/10], Training Loss: 1.026, Validation Accuracy: 57.61%\n",
            "Epoch [9/10], Training Loss: 1.014, Validation Accuracy: 56.58%\n",
            "Epoch [10/10], Training Loss: 1.002, Validation Accuracy: 57.64%\n",
            "Epoch [1/10], Training Loss: 1.112, Validation Accuracy: 57.48%\n",
            "Epoch [2/10], Training Loss: 1.071, Validation Accuracy: 57.62%\n",
            "Epoch [3/10], Training Loss: 1.052, Validation Accuracy: 57.03%\n",
            "Epoch [4/10], Training Loss: 1.034, Validation Accuracy: 58.32%\n",
            "Epoch [5/10], Training Loss: 1.022, Validation Accuracy: 57.92%\n",
            "Epoch [6/10], Training Loss: 0.991, Validation Accuracy: 58.23%\n",
            "Epoch [7/10], Training Loss: 0.982, Validation Accuracy: 57.94%\n",
            "Epoch [8/10], Training Loss: 0.971, Validation Accuracy: 58.23%\n",
            "Epoch [9/10], Training Loss: 0.960, Validation Accuracy: 58.26%\n",
            "Epoch [10/10], Training Loss: 0.951, Validation Accuracy: 57.69%\n",
            "Epoch [1/10], Training Loss: 1.100, Validation Accuracy: 58.83%\n",
            "Epoch [2/10], Training Loss: 1.057, Validation Accuracy: 58.66%\n",
            "Epoch [3/10], Training Loss: 1.039, Validation Accuracy: 58.40%\n",
            "Epoch [4/10], Training Loss: 1.027, Validation Accuracy: 58.36%\n",
            "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 58.59%\n",
            "Epoch [6/10], Training Loss: 0.986, Validation Accuracy: 59.05%\n",
            "Epoch [7/10], Training Loss: 0.981, Validation Accuracy: 57.92%\n",
            "Epoch [8/10], Training Loss: 0.961, Validation Accuracy: 58.45%\n",
            "Epoch [9/10], Training Loss: 0.946, Validation Accuracy: 58.63%\n",
            "Epoch [10/10], Training Loss: 0.936, Validation Accuracy: 58.53%\n",
            "Epoch [1/10], Training Loss: 1.103, Validation Accuracy: 58.53%\n",
            "Epoch [2/10], Training Loss: 1.060, Validation Accuracy: 58.45%\n",
            "Epoch [3/10], Training Loss: 1.032, Validation Accuracy: 59.27%\n",
            "Epoch [4/10], Training Loss: 1.008, Validation Accuracy: 59.06%\n",
            "Epoch [5/10], Training Loss: 0.996, Validation Accuracy: 58.49%\n",
            "Epoch [6/10], Training Loss: 0.981, Validation Accuracy: 58.81%\n",
            "Epoch [7/10], Training Loss: 0.965, Validation Accuracy: 58.45%\n",
            "Epoch [8/10], Training Loss: 0.944, Validation Accuracy: 59.25%\n",
            "Epoch [9/10], Training Loss: 0.934, Validation Accuracy: 58.72%\n",
            "Epoch [10/10], Training Loss: 0.926, Validation Accuracy: 58.74%\n",
            "Epoch [1/10], Training Loss: 1.057, Validation Accuracy: 58.89%\n",
            "Epoch [2/10], Training Loss: 1.003, Validation Accuracy: 59.27%\n",
            "Epoch [3/10], Training Loss: 0.979, Validation Accuracy: 59.30%\n",
            "Epoch [4/10], Training Loss: 0.964, Validation Accuracy: 59.36%\n",
            "Epoch [5/10], Training Loss: 0.939, Validation Accuracy: 59.37%\n",
            "Epoch [6/10], Training Loss: 0.923, Validation Accuracy: 59.24%\n",
            "Epoch [7/10], Training Loss: 0.899, Validation Accuracy: 58.94%\n",
            "Epoch [8/10], Training Loss: 0.892, Validation Accuracy: 58.69%\n",
            "Epoch [9/10], Training Loss: 0.870, Validation Accuracy: 59.06%\n",
            "Epoch [10/10], Training Loss: 0.868, Validation Accuracy: 58.84%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 58.97%\n",
            "Epoch [2/10], Training Loss: 1.032, Validation Accuracy: 59.39%\n",
            "Epoch [3/10], Training Loss: 1.006, Validation Accuracy: 58.76%\n",
            "Epoch [4/10], Training Loss: 0.976, Validation Accuracy: 58.77%\n",
            "Epoch [5/10], Training Loss: 0.960, Validation Accuracy: 59.46%\n",
            "Epoch [6/10], Training Loss: 0.942, Validation Accuracy: 59.39%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 59.52%\n",
            "Epoch [8/10], Training Loss: 0.913, Validation Accuracy: 59.06%\n",
            "Epoch [9/10], Training Loss: 0.897, Validation Accuracy: 59.85%\n",
            "Epoch [10/10], Training Loss: 0.887, Validation Accuracy: 57.85%\n",
            "Epoch [1/10], Training Loss: 1.039, Validation Accuracy: 59.28%\n",
            "Epoch [2/10], Training Loss: 0.988, Validation Accuracy: 59.94%\n",
            "Epoch [3/10], Training Loss: 0.960, Validation Accuracy: 58.55%\n",
            "Epoch [4/10], Training Loss: 0.938, Validation Accuracy: 59.79%\n",
            "Epoch [5/10], Training Loss: 0.910, Validation Accuracy: 60.19%\n",
            "Epoch [6/10], Training Loss: 0.892, Validation Accuracy: 59.44%\n",
            "Epoch [7/10], Training Loss: 0.885, Validation Accuracy: 59.89%\n",
            "Epoch [8/10], Training Loss: 0.857, Validation Accuracy: 59.40%\n",
            "Epoch [9/10], Training Loss: 0.846, Validation Accuracy: 59.47%\n",
            "Epoch [10/10], Training Loss: 0.836, Validation Accuracy: 59.33%\n",
            "Epoch [1/10], Training Loss: 1.034, Validation Accuracy: 60.32%\n",
            "Epoch [2/10], Training Loss: 0.975, Validation Accuracy: 59.58%\n",
            "Epoch [3/10], Training Loss: 0.952, Validation Accuracy: 59.93%\n",
            "Epoch [4/10], Training Loss: 0.918, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.899, Validation Accuracy: 60.52%\n",
            "Epoch [6/10], Training Loss: 0.884, Validation Accuracy: 59.35%\n",
            "Epoch [7/10], Training Loss: 0.874, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.851, Validation Accuracy: 59.87%\n",
            "Epoch [9/10], Training Loss: 0.831, Validation Accuracy: 60.55%\n",
            "Epoch [10/10], Training Loss: 0.826, Validation Accuracy: 59.73%\n",
            "Epoch [1/10], Training Loss: 1.024, Validation Accuracy: 60.03%\n",
            "Epoch [2/10], Training Loss: 0.968, Validation Accuracy: 61.06%\n",
            "Epoch [3/10], Training Loss: 0.933, Validation Accuracy: 60.15%\n",
            "Epoch [4/10], Training Loss: 0.904, Validation Accuracy: 60.44%\n",
            "Epoch [5/10], Training Loss: 0.884, Validation Accuracy: 59.56%\n",
            "Epoch [6/10], Training Loss: 0.865, Validation Accuracy: 60.37%\n",
            "Epoch [7/10], Training Loss: 0.845, Validation Accuracy: 60.17%\n",
            "Epoch [8/10], Training Loss: 0.835, Validation Accuracy: 60.00%\n",
            "Epoch [9/10], Training Loss: 0.814, Validation Accuracy: 60.44%\n",
            "Epoch [10/10], Training Loss: 0.800, Validation Accuracy: 59.71%\n",
            "Epoch [1/10], Training Loss: 0.985, Validation Accuracy: 59.53%\n",
            "Epoch [2/10], Training Loss: 0.924, Validation Accuracy: 60.61%\n",
            "Epoch [3/10], Training Loss: 0.881, Validation Accuracy: 60.68%\n",
            "Epoch [4/10], Training Loss: 0.861, Validation Accuracy: 60.06%\n",
            "Epoch [5/10], Training Loss: 0.841, Validation Accuracy: 61.18%\n",
            "Epoch [6/10], Training Loss: 0.818, Validation Accuracy: 60.27%\n",
            "Epoch [7/10], Training Loss: 0.804, Validation Accuracy: 60.48%\n",
            "Epoch [8/10], Training Loss: 0.784, Validation Accuracy: 60.45%\n",
            "Epoch [9/10], Training Loss: 0.767, Validation Accuracy: 60.31%\n",
            "Epoch [10/10], Training Loss: 0.756, Validation Accuracy: 59.39%\n",
            "Epoch [1/10], Training Loss: 1.012, Validation Accuracy: 59.15%\n",
            "Epoch [2/10], Training Loss: 0.957, Validation Accuracy: 60.12%\n",
            "Epoch [3/10], Training Loss: 0.909, Validation Accuracy: 59.86%\n",
            "Epoch [4/10], Training Loss: 0.881, Validation Accuracy: 60.30%\n",
            "Epoch [5/10], Training Loss: 0.858, Validation Accuracy: 60.28%\n",
            "Epoch [6/10], Training Loss: 0.835, Validation Accuracy: 60.44%\n",
            "Epoch [7/10], Training Loss: 0.822, Validation Accuracy: 60.40%\n",
            "Epoch [8/10], Training Loss: 0.806, Validation Accuracy: 60.15%\n",
            "Epoch [9/10], Training Loss: 0.790, Validation Accuracy: 59.19%\n",
            "Epoch [10/10], Training Loss: 0.772, Validation Accuracy: 59.80%\n",
            "Epoch [1/10], Training Loss: 0.962, Validation Accuracy: 59.99%\n",
            "Epoch [2/10], Training Loss: 0.901, Validation Accuracy: 60.38%\n",
            "Epoch [3/10], Training Loss: 0.864, Validation Accuracy: 60.97%\n",
            "Epoch [4/10], Training Loss: 0.834, Validation Accuracy: 60.52%\n",
            "Epoch [5/10], Training Loss: 0.813, Validation Accuracy: 59.82%\n",
            "Epoch [6/10], Training Loss: 0.798, Validation Accuracy: 60.67%\n",
            "Epoch [7/10], Training Loss: 0.771, Validation Accuracy: 60.89%\n",
            "Epoch [8/10], Training Loss: 0.751, Validation Accuracy: 60.82%\n",
            "Epoch [9/10], Training Loss: 0.744, Validation Accuracy: 60.47%\n",
            "Epoch [10/10], Training Loss: 0.711, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.967, Validation Accuracy: 60.60%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 60.59%\n",
            "Epoch [3/10], Training Loss: 0.857, Validation Accuracy: 61.35%\n",
            "Epoch [4/10], Training Loss: 0.823, Validation Accuracy: 60.71%\n",
            "Epoch [5/10], Training Loss: 0.822, Validation Accuracy: 60.42%\n",
            "Epoch [6/10], Training Loss: 0.781, Validation Accuracy: 60.54%\n",
            "Epoch [7/10], Training Loss: 0.775, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.753, Validation Accuracy: 60.27%\n",
            "Epoch [9/10], Training Loss: 0.735, Validation Accuracy: 59.75%\n",
            "Epoch [10/10], Training Loss: 0.723, Validation Accuracy: 60.62%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 60.64%\n",
            "Epoch [2/10], Training Loss: 0.887, Validation Accuracy: 61.07%\n",
            "Epoch [3/10], Training Loss: 0.840, Validation Accuracy: 60.65%\n",
            "Epoch [4/10], Training Loss: 0.812, Validation Accuracy: 60.80%\n",
            "Epoch [5/10], Training Loss: 0.789, Validation Accuracy: 60.89%\n",
            "Epoch [6/10], Training Loss: 0.765, Validation Accuracy: 59.95%\n",
            "Epoch [7/10], Training Loss: 0.747, Validation Accuracy: 60.31%\n",
            "Epoch [8/10], Training Loss: 0.721, Validation Accuracy: 60.08%\n",
            "Epoch [9/10], Training Loss: 0.723, Validation Accuracy: 61.03%\n",
            "Epoch [10/10], Training Loss: 0.691, Validation Accuracy: 60.17%\n",
            "Epoch [1/10], Training Loss: 0.931, Validation Accuracy: 59.90%\n",
            "Epoch [2/10], Training Loss: 0.850, Validation Accuracy: 60.99%\n",
            "Epoch [3/10], Training Loss: 0.808, Validation Accuracy: 61.05%\n",
            "Epoch [4/10], Training Loss: 0.771, Validation Accuracy: 61.07%\n",
            "Epoch [5/10], Training Loss: 0.750, Validation Accuracy: 60.43%\n",
            "Epoch [6/10], Training Loss: 0.722, Validation Accuracy: 60.50%\n",
            "Epoch [7/10], Training Loss: 0.705, Validation Accuracy: 60.82%\n",
            "Epoch [8/10], Training Loss: 0.691, Validation Accuracy: 60.95%\n",
            "Epoch [9/10], Training Loss: 0.658, Validation Accuracy: 60.63%\n",
            "Epoch [10/10], Training Loss: 0.648, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.968, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.888, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.830, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.795, Validation Accuracy: 60.96%\n",
            "Epoch [5/10], Training Loss: 0.774, Validation Accuracy: 60.84%\n",
            "Epoch [6/10], Training Loss: 0.743, Validation Accuracy: 60.66%\n",
            "Epoch [7/10], Training Loss: 0.727, Validation Accuracy: 60.84%\n",
            "Epoch [8/10], Training Loss: 0.700, Validation Accuracy: 60.47%\n",
            "Epoch [9/10], Training Loss: 0.685, Validation Accuracy: 60.50%\n",
            "Epoch [10/10], Training Loss: 0.665, Validation Accuracy: 60.13%\n",
            "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 60.62%\n",
            "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 60.67%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.739, Validation Accuracy: 61.62%\n",
            "Epoch [5/10], Training Loss: 0.720, Validation Accuracy: 60.91%\n",
            "Epoch [6/10], Training Loss: 0.692, Validation Accuracy: 61.07%\n",
            "Epoch [7/10], Training Loss: 0.668, Validation Accuracy: 59.79%\n",
            "Epoch [8/10], Training Loss: 0.645, Validation Accuracy: 60.84%\n",
            "Epoch [9/10], Training Loss: 0.625, Validation Accuracy: 60.72%\n",
            "Epoch [10/10], Training Loss: 0.611, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 0.922, Validation Accuracy: 60.67%\n",
            "Epoch [2/10], Training Loss: 0.829, Validation Accuracy: 61.51%\n",
            "Epoch [3/10], Training Loss: 0.782, Validation Accuracy: 60.62%\n",
            "Epoch [4/10], Training Loss: 0.741, Validation Accuracy: 60.97%\n",
            "Epoch [5/10], Training Loss: 0.715, Validation Accuracy: 60.68%\n",
            "Epoch [6/10], Training Loss: 0.689, Validation Accuracy: 60.77%\n",
            "Epoch [7/10], Training Loss: 0.675, Validation Accuracy: 60.44%\n",
            "Epoch [8/10], Training Loss: 0.655, Validation Accuracy: 59.86%\n",
            "Epoch [9/10], Training Loss: 0.635, Validation Accuracy: 60.34%\n",
            "Epoch [10/10], Training Loss: 0.613, Validation Accuracy: 60.86%\n",
            "Epoch [1/10], Training Loss: 0.920, Validation Accuracy: 60.97%\n",
            "Epoch [2/10], Training Loss: 0.825, Validation Accuracy: 61.29%\n",
            "Epoch [3/10], Training Loss: 0.772, Validation Accuracy: 60.93%\n",
            "Epoch [4/10], Training Loss: 0.740, Validation Accuracy: 61.31%\n",
            "Epoch [5/10], Training Loss: 0.708, Validation Accuracy: 61.86%\n",
            "Epoch [6/10], Training Loss: 0.672, Validation Accuracy: 60.80%\n",
            "Epoch [7/10], Training Loss: 0.659, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.627, Validation Accuracy: 60.40%\n",
            "Epoch [9/10], Training Loss: 0.623, Validation Accuracy: 60.13%\n",
            "Epoch [10/10], Training Loss: 0.595, Validation Accuracy: 60.74%\n",
            "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 61.07%\n",
            "Epoch [2/10], Training Loss: 0.786, Validation Accuracy: 60.97%\n",
            "Epoch [3/10], Training Loss: 0.731, Validation Accuracy: 60.73%\n",
            "Epoch [4/10], Training Loss: 0.686, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.662, Validation Accuracy: 60.96%\n",
            "Epoch [6/10], Training Loss: 0.641, Validation Accuracy: 61.07%\n",
            "Epoch [7/10], Training Loss: 0.615, Validation Accuracy: 61.30%\n",
            "Epoch [8/10], Training Loss: 0.595, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.576, Validation Accuracy: 61.03%\n",
            "Epoch [10/10], Training Loss: 0.558, Validation Accuracy: 60.55%\n",
            "Epoch [1/10], Training Loss: 0.924, Validation Accuracy: 61.26%\n",
            "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 61.44%\n",
            "Epoch [3/10], Training Loss: 0.751, Validation Accuracy: 60.80%\n",
            "Epoch [4/10], Training Loss: 0.716, Validation Accuracy: 60.55%\n",
            "Epoch [5/10], Training Loss: 0.684, Validation Accuracy: 60.60%\n",
            "Epoch [6/10], Training Loss: 0.651, Validation Accuracy: 60.73%\n",
            "Epoch [7/10], Training Loss: 0.622, Validation Accuracy: 60.83%\n",
            "Epoch [8/10], Training Loss: 0.605, Validation Accuracy: 60.87%\n",
            "Epoch [9/10], Training Loss: 0.591, Validation Accuracy: 60.01%\n",
            "Epoch [10/10], Training Loss: 0.569, Validation Accuracy: 60.79%\n",
            "Epoch [1/10], Training Loss: 0.867, Validation Accuracy: 60.26%\n",
            "Epoch [2/10], Training Loss: 0.753, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 0.712, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.661, Validation Accuracy: 60.87%\n",
            "Epoch [5/10], Training Loss: 0.617, Validation Accuracy: 60.49%\n",
            "Epoch [6/10], Training Loss: 0.596, Validation Accuracy: 61.11%\n",
            "Epoch [7/10], Training Loss: 0.574, Validation Accuracy: 61.28%\n",
            "Epoch [8/10], Training Loss: 0.550, Validation Accuracy: 61.01%\n",
            "Epoch [9/10], Training Loss: 0.541, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.515, Validation Accuracy: 59.86%\n",
            "Epoch [1/10], Training Loss: 0.888, Validation Accuracy: 59.67%\n",
            "Epoch [2/10], Training Loss: 0.766, Validation Accuracy: 61.01%\n",
            "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 61.72%\n",
            "Epoch [4/10], Training Loss: 0.669, Validation Accuracy: 60.76%\n",
            "Epoch [5/10], Training Loss: 0.631, Validation Accuracy: 61.45%\n",
            "Epoch [6/10], Training Loss: 0.603, Validation Accuracy: 61.31%\n",
            "Epoch [7/10], Training Loss: 0.590, Validation Accuracy: 60.66%\n",
            "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 61.34%\n",
            "Epoch [9/10], Training Loss: 0.537, Validation Accuracy: 60.43%\n",
            "Epoch [10/10], Training Loss: 0.521, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 59.61%\n",
            "Epoch [2/10], Training Loss: 0.763, Validation Accuracy: 59.57%\n",
            "Epoch [3/10], Training Loss: 0.714, Validation Accuracy: 60.87%\n",
            "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.615, Validation Accuracy: 61.21%\n",
            "Epoch [6/10], Training Loss: 0.590, Validation Accuracy: 60.65%\n",
            "Epoch [7/10], Training Loss: 0.569, Validation Accuracy: 61.02%\n",
            "Epoch [8/10], Training Loss: 0.536, Validation Accuracy: 60.53%\n",
            "Epoch [9/10], Training Loss: 0.516, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.509, Validation Accuracy: 60.14%\n",
            "Epoch [1/10], Training Loss: 0.847, Validation Accuracy: 60.42%\n",
            "Epoch [2/10], Training Loss: 0.740, Validation Accuracy: 60.55%\n",
            "Epoch [3/10], Training Loss: 0.660, Validation Accuracy: 61.50%\n",
            "Epoch [4/10], Training Loss: 0.618, Validation Accuracy: 61.98%\n",
            "Epoch [5/10], Training Loss: 0.582, Validation Accuracy: 61.25%\n",
            "Epoch [6/10], Training Loss: 0.549, Validation Accuracy: 60.97%\n",
            "Epoch [7/10], Training Loss: 0.529, Validation Accuracy: 61.08%\n",
            "Epoch [8/10], Training Loss: 0.508, Validation Accuracy: 61.13%\n",
            "Epoch [9/10], Training Loss: 0.485, Validation Accuracy: 60.61%\n",
            "Epoch [10/10], Training Loss: 0.466, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.873, Validation Accuracy: 60.43%\n",
            "Epoch [2/10], Training Loss: 0.731, Validation Accuracy: 61.30%\n",
            "Epoch [3/10], Training Loss: 0.670, Validation Accuracy: 61.09%\n",
            "Epoch [4/10], Training Loss: 0.628, Validation Accuracy: 60.73%\n",
            "Epoch [5/10], Training Loss: 0.601, Validation Accuracy: 60.56%\n",
            "Epoch [6/10], Training Loss: 0.568, Validation Accuracy: 60.20%\n",
            "Epoch [7/10], Training Loss: 0.532, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.512, Validation Accuracy: 60.31%\n",
            "Epoch [9/10], Training Loss: 0.484, Validation Accuracy: 60.56%\n",
            "Epoch [10/10], Training Loss: 0.467, Validation Accuracy: 60.08%\n",
            "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 59.98%\n",
            "Epoch [2/10], Training Loss: 0.696, Validation Accuracy: 61.09%\n",
            "Epoch [3/10], Training Loss: 0.619, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.586, Validation Accuracy: 60.66%\n",
            "Epoch [5/10], Training Loss: 0.544, Validation Accuracy: 59.82%\n",
            "Epoch [6/10], Training Loss: 0.520, Validation Accuracy: 61.33%\n",
            "Epoch [7/10], Training Loss: 0.491, Validation Accuracy: 59.95%\n",
            "Epoch [8/10], Training Loss: 0.464, Validation Accuracy: 60.65%\n",
            "Epoch [9/10], Training Loss: 0.441, Validation Accuracy: 60.41%\n",
            "Epoch [10/10], Training Loss: 0.428, Validation Accuracy: 60.52%\n",
            "Epoch [1/10], Training Loss: 0.837, Validation Accuracy: 60.11%\n",
            "Epoch [2/10], Training Loss: 0.701, Validation Accuracy: 60.91%\n",
            "Epoch [3/10], Training Loss: 0.639, Validation Accuracy: 60.98%\n",
            "Epoch [4/10], Training Loss: 0.593, Validation Accuracy: 60.69%\n",
            "Epoch [5/10], Training Loss: 0.558, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.521, Validation Accuracy: 59.87%\n",
            "Epoch [7/10], Training Loss: 0.511, Validation Accuracy: 60.70%\n",
            "Epoch [8/10], Training Loss: 0.473, Validation Accuracy: 60.68%\n",
            "Epoch [9/10], Training Loss: 0.460, Validation Accuracy: 60.54%\n",
            "Epoch [10/10], Training Loss: 0.444, Validation Accuracy: 60.21%\n",
            "Epoch [1/10], Training Loss: 0.841, Validation Accuracy: 60.36%\n",
            "Epoch [2/10], Training Loss: 0.707, Validation Accuracy: 60.22%\n",
            "Epoch [3/10], Training Loss: 0.620, Validation Accuracy: 60.77%\n",
            "Epoch [4/10], Training Loss: 0.577, Validation Accuracy: 61.38%\n",
            "Epoch [5/10], Training Loss: 0.540, Validation Accuracy: 60.42%\n",
            "Epoch [6/10], Training Loss: 0.512, Validation Accuracy: 60.76%\n",
            "Epoch [7/10], Training Loss: 0.478, Validation Accuracy: 60.60%\n",
            "Epoch [8/10], Training Loss: 0.448, Validation Accuracy: 60.70%\n",
            "Epoch [9/10], Training Loss: 0.432, Validation Accuracy: 60.41%\n",
            "Epoch [10/10], Training Loss: 0.411, Validation Accuracy: 60.31%\n",
            "Epoch [1/10], Training Loss: 0.817, Validation Accuracy: 59.72%\n",
            "Epoch [2/10], Training Loss: 0.663, Validation Accuracy: 60.98%\n",
            "Epoch [3/10], Training Loss: 0.591, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.550, Validation Accuracy: 60.88%\n",
            "Epoch [5/10], Training Loss: 0.511, Validation Accuracy: 61.13%\n",
            "Epoch [6/10], Training Loss: 0.470, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.447, Validation Accuracy: 60.71%\n",
            "Epoch [8/10], Training Loss: 0.427, Validation Accuracy: 60.81%\n",
            "Epoch [9/10], Training Loss: 0.407, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.387, Validation Accuracy: 60.46%\n",
            "Epoch [1/10], Training Loss: 0.832, Validation Accuracy: 60.17%\n",
            "Epoch [2/10], Training Loss: 0.673, Validation Accuracy: 60.75%\n",
            "Epoch [3/10], Training Loss: 0.591, Validation Accuracy: 60.35%\n",
            "Epoch [4/10], Training Loss: 0.551, Validation Accuracy: 60.38%\n",
            "Epoch [5/10], Training Loss: 0.515, Validation Accuracy: 60.55%\n",
            "Epoch [6/10], Training Loss: 0.480, Validation Accuracy: 60.46%\n",
            "Epoch [7/10], Training Loss: 0.460, Validation Accuracy: 59.97%\n",
            "Epoch [8/10], Training Loss: 0.431, Validation Accuracy: 60.27%\n",
            "Epoch [9/10], Training Loss: 0.406, Validation Accuracy: 60.14%\n",
            "Epoch [10/10], Training Loss: 0.385, Validation Accuracy: 59.97%\n",
            "Test Accuracy: 61.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to get the distribution information from the VAE model\n",
        "    # This can involve extracting the mean and standard deviation of the latent space\n",
        "    # and sending this information to the global server for use in generating augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "        \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define logic to generate augmented data using Truncated Normal distribution\n",
        "def generate_augmented_data(vae: VAE, distribution_info: Dict) -> torch.Tensor:\n",
        "    # Generate augmented data using Uniform distribution\n",
        "    mean = distribution_info[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info[\"std\"].mean().item()  # Convert numpy array to float\n",
        "    augmented_data = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "    return augmented_data\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info)\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Implement the logic to receive the distribution information from the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to receive the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Receive the distribution information from the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to receive the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to receive the information\n",
        "    distribution_info = {\n",
        "        \"mean\": np.zeros(20),\n",
        "        \"std\": np.ones(20)\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1VAJn_5PdhT",
        "outputId": "ffb5e727-25a0-4278-c394-4fb0cf80a013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16168347.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Training Loss: 2.305, Validation Accuracy: 9.72%\n",
            "Epoch [2/10], Training Loss: 2.304, Validation Accuracy: 9.72%\n",
            "Epoch [3/10], Training Loss: 2.303, Validation Accuracy: 9.72%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 9.73%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 9.86%\n",
            "Epoch [6/10], Training Loss: 2.301, Validation Accuracy: 10.91%\n",
            "Epoch [7/10], Training Loss: 2.300, Validation Accuracy: 12.53%\n",
            "Epoch [8/10], Training Loss: 2.299, Validation Accuracy: 14.40%\n",
            "Epoch [9/10], Training Loss: 2.297, Validation Accuracy: 14.19%\n",
            "Epoch [10/10], Training Loss: 2.296, Validation Accuracy: 13.73%\n",
            "Epoch [1/10], Training Loss: 2.296, Validation Accuracy: 14.16%\n",
            "Epoch [2/10], Training Loss: 2.294, Validation Accuracy: 15.16%\n",
            "Epoch [3/10], Training Loss: 2.291, Validation Accuracy: 16.58%\n",
            "Epoch [4/10], Training Loss: 2.288, Validation Accuracy: 18.28%\n",
            "Epoch [5/10], Training Loss: 2.284, Validation Accuracy: 19.51%\n",
            "Epoch [6/10], Training Loss: 2.279, Validation Accuracy: 20.15%\n",
            "Epoch [7/10], Training Loss: 2.270, Validation Accuracy: 20.31%\n",
            "Epoch [8/10], Training Loss: 2.257, Validation Accuracy: 20.52%\n",
            "Epoch [9/10], Training Loss: 2.237, Validation Accuracy: 20.52%\n",
            "Epoch [10/10], Training Loss: 2.208, Validation Accuracy: 21.41%\n",
            "Epoch [1/10], Training Loss: 2.172, Validation Accuracy: 24.51%\n",
            "Epoch [2/10], Training Loss: 2.119, Validation Accuracy: 25.01%\n",
            "Epoch [3/10], Training Loss: 2.077, Validation Accuracy: 26.30%\n",
            "Epoch [4/10], Training Loss: 2.048, Validation Accuracy: 26.53%\n",
            "Epoch [5/10], Training Loss: 2.027, Validation Accuracy: 27.89%\n",
            "Epoch [6/10], Training Loss: 2.008, Validation Accuracy: 28.61%\n",
            "Epoch [7/10], Training Loss: 1.986, Validation Accuracy: 29.12%\n",
            "Epoch [8/10], Training Loss: 1.967, Validation Accuracy: 29.69%\n",
            "Epoch [9/10], Training Loss: 1.947, Validation Accuracy: 30.65%\n",
            "Epoch [10/10], Training Loss: 1.931, Validation Accuracy: 31.13%\n",
            "Epoch [1/10], Training Loss: 1.935, Validation Accuracy: 31.28%\n",
            "Epoch [2/10], Training Loss: 1.923, Validation Accuracy: 31.29%\n",
            "Epoch [3/10], Training Loss: 1.912, Validation Accuracy: 31.68%\n",
            "Epoch [4/10], Training Loss: 1.900, Validation Accuracy: 32.28%\n",
            "Epoch [5/10], Training Loss: 1.888, Validation Accuracy: 33.17%\n",
            "Epoch [6/10], Training Loss: 1.876, Validation Accuracy: 33.00%\n",
            "Epoch [7/10], Training Loss: 1.862, Validation Accuracy: 33.75%\n",
            "Epoch [8/10], Training Loss: 1.850, Validation Accuracy: 34.17%\n",
            "Epoch [9/10], Training Loss: 1.831, Validation Accuracy: 35.14%\n",
            "Epoch [10/10], Training Loss: 1.812, Validation Accuracy: 34.94%\n",
            "Epoch [1/10], Training Loss: 1.820, Validation Accuracy: 35.44%\n",
            "Epoch [2/10], Training Loss: 1.800, Validation Accuracy: 36.75%\n",
            "Epoch [3/10], Training Loss: 1.778, Validation Accuracy: 36.53%\n",
            "Epoch [4/10], Training Loss: 1.758, Validation Accuracy: 37.59%\n",
            "Epoch [5/10], Training Loss: 1.733, Validation Accuracy: 37.66%\n",
            "Epoch [6/10], Training Loss: 1.716, Validation Accuracy: 38.06%\n",
            "Epoch [7/10], Training Loss: 1.695, Validation Accuracy: 38.43%\n",
            "Epoch [8/10], Training Loss: 1.680, Validation Accuracy: 39.27%\n",
            "Epoch [9/10], Training Loss: 1.663, Validation Accuracy: 39.53%\n",
            "Epoch [10/10], Training Loss: 1.650, Validation Accuracy: 39.64%\n",
            "Epoch [1/10], Training Loss: 1.664, Validation Accuracy: 39.79%\n",
            "Epoch [2/10], Training Loss: 1.645, Validation Accuracy: 39.93%\n",
            "Epoch [3/10], Training Loss: 1.633, Validation Accuracy: 39.58%\n",
            "Epoch [4/10], Training Loss: 1.619, Validation Accuracy: 41.28%\n",
            "Epoch [5/10], Training Loss: 1.605, Validation Accuracy: 41.49%\n",
            "Epoch [6/10], Training Loss: 1.590, Validation Accuracy: 41.25%\n",
            "Epoch [7/10], Training Loss: 1.588, Validation Accuracy: 41.51%\n",
            "Epoch [8/10], Training Loss: 1.579, Validation Accuracy: 41.88%\n",
            "Epoch [9/10], Training Loss: 1.575, Validation Accuracy: 42.54%\n",
            "Epoch [10/10], Training Loss: 1.554, Validation Accuracy: 42.68%\n",
            "Epoch [1/10], Training Loss: 1.585, Validation Accuracy: 43.36%\n",
            "Epoch [2/10], Training Loss: 1.571, Validation Accuracy: 43.38%\n",
            "Epoch [3/10], Training Loss: 1.558, Validation Accuracy: 42.88%\n",
            "Epoch [4/10], Training Loss: 1.544, Validation Accuracy: 43.74%\n",
            "Epoch [5/10], Training Loss: 1.534, Validation Accuracy: 44.46%\n",
            "Epoch [6/10], Training Loss: 1.528, Validation Accuracy: 43.47%\n",
            "Epoch [7/10], Training Loss: 1.515, Validation Accuracy: 44.04%\n",
            "Epoch [8/10], Training Loss: 1.504, Validation Accuracy: 44.94%\n",
            "Epoch [9/10], Training Loss: 1.496, Validation Accuracy: 44.24%\n",
            "Epoch [10/10], Training Loss: 1.493, Validation Accuracy: 45.40%\n",
            "Epoch [1/10], Training Loss: 1.512, Validation Accuracy: 45.95%\n",
            "Epoch [2/10], Training Loss: 1.493, Validation Accuracy: 45.92%\n",
            "Epoch [3/10], Training Loss: 1.481, Validation Accuracy: 46.15%\n",
            "Epoch [4/10], Training Loss: 1.474, Validation Accuracy: 45.44%\n",
            "Epoch [5/10], Training Loss: 1.466, Validation Accuracy: 45.88%\n",
            "Epoch [6/10], Training Loss: 1.459, Validation Accuracy: 46.50%\n",
            "Epoch [7/10], Training Loss: 1.443, Validation Accuracy: 47.14%\n",
            "Epoch [8/10], Training Loss: 1.436, Validation Accuracy: 46.99%\n",
            "Epoch [9/10], Training Loss: 1.434, Validation Accuracy: 47.31%\n",
            "Epoch [10/10], Training Loss: 1.430, Validation Accuracy: 46.67%\n",
            "Epoch [1/10], Training Loss: 1.481, Validation Accuracy: 47.32%\n",
            "Epoch [2/10], Training Loss: 1.462, Validation Accuracy: 47.35%\n",
            "Epoch [3/10], Training Loss: 1.456, Validation Accuracy: 46.75%\n",
            "Epoch [4/10], Training Loss: 1.453, Validation Accuracy: 47.37%\n",
            "Epoch [5/10], Training Loss: 1.436, Validation Accuracy: 47.85%\n",
            "Epoch [6/10], Training Loss: 1.427, Validation Accuracy: 48.21%\n",
            "Epoch [7/10], Training Loss: 1.415, Validation Accuracy: 48.00%\n",
            "Epoch [8/10], Training Loss: 1.419, Validation Accuracy: 47.61%\n",
            "Epoch [9/10], Training Loss: 1.405, Validation Accuracy: 48.50%\n",
            "Epoch [10/10], Training Loss: 1.396, Validation Accuracy: 47.39%\n",
            "Epoch [1/10], Training Loss: 1.431, Validation Accuracy: 49.25%\n",
            "Epoch [2/10], Training Loss: 1.424, Validation Accuracy: 48.32%\n",
            "Epoch [3/10], Training Loss: 1.403, Validation Accuracy: 49.17%\n",
            "Epoch [4/10], Training Loss: 1.395, Validation Accuracy: 49.60%\n",
            "Epoch [5/10], Training Loss: 1.380, Validation Accuracy: 48.87%\n",
            "Epoch [6/10], Training Loss: 1.375, Validation Accuracy: 49.30%\n",
            "Epoch [7/10], Training Loss: 1.369, Validation Accuracy: 49.52%\n",
            "Epoch [8/10], Training Loss: 1.347, Validation Accuracy: 49.89%\n",
            "Epoch [9/10], Training Loss: 1.344, Validation Accuracy: 49.63%\n",
            "Epoch [10/10], Training Loss: 1.335, Validation Accuracy: 50.12%\n",
            "Epoch [1/10], Training Loss: 1.400, Validation Accuracy: 49.96%\n",
            "Epoch [2/10], Training Loss: 1.381, Validation Accuracy: 50.16%\n",
            "Epoch [3/10], Training Loss: 1.372, Validation Accuracy: 49.51%\n",
            "Epoch [4/10], Training Loss: 1.364, Validation Accuracy: 50.81%\n",
            "Epoch [5/10], Training Loss: 1.347, Validation Accuracy: 50.05%\n",
            "Epoch [6/10], Training Loss: 1.341, Validation Accuracy: 51.30%\n",
            "Epoch [7/10], Training Loss: 1.332, Validation Accuracy: 51.20%\n",
            "Epoch [8/10], Training Loss: 1.326, Validation Accuracy: 51.21%\n",
            "Epoch [9/10], Training Loss: 1.315, Validation Accuracy: 49.99%\n",
            "Epoch [10/10], Training Loss: 1.312, Validation Accuracy: 50.43%\n",
            "Epoch [1/10], Training Loss: 1.373, Validation Accuracy: 51.26%\n",
            "Epoch [2/10], Training Loss: 1.358, Validation Accuracy: 50.34%\n",
            "Epoch [3/10], Training Loss: 1.341, Validation Accuracy: 51.84%\n",
            "Epoch [4/10], Training Loss: 1.323, Validation Accuracy: 52.36%\n",
            "Epoch [5/10], Training Loss: 1.314, Validation Accuracy: 51.56%\n",
            "Epoch [6/10], Training Loss: 1.307, Validation Accuracy: 51.42%\n",
            "Epoch [7/10], Training Loss: 1.299, Validation Accuracy: 51.77%\n",
            "Epoch [8/10], Training Loss: 1.292, Validation Accuracy: 51.36%\n",
            "Epoch [9/10], Training Loss: 1.283, Validation Accuracy: 51.72%\n",
            "Epoch [10/10], Training Loss: 1.272, Validation Accuracy: 52.45%\n",
            "Epoch [1/10], Training Loss: 1.327, Validation Accuracy: 52.54%\n",
            "Epoch [2/10], Training Loss: 1.307, Validation Accuracy: 52.40%\n",
            "Epoch [3/10], Training Loss: 1.293, Validation Accuracy: 52.75%\n",
            "Epoch [4/10], Training Loss: 1.280, Validation Accuracy: 52.23%\n",
            "Epoch [5/10], Training Loss: 1.275, Validation Accuracy: 51.92%\n",
            "Epoch [6/10], Training Loss: 1.259, Validation Accuracy: 51.71%\n",
            "Epoch [7/10], Training Loss: 1.263, Validation Accuracy: 53.28%\n",
            "Epoch [8/10], Training Loss: 1.251, Validation Accuracy: 53.36%\n",
            "Epoch [9/10], Training Loss: 1.231, Validation Accuracy: 53.77%\n",
            "Epoch [10/10], Training Loss: 1.221, Validation Accuracy: 53.12%\n",
            "Epoch [1/10], Training Loss: 1.314, Validation Accuracy: 53.35%\n",
            "Epoch [2/10], Training Loss: 1.287, Validation Accuracy: 54.11%\n",
            "Epoch [3/10], Training Loss: 1.271, Validation Accuracy: 54.35%\n",
            "Epoch [4/10], Training Loss: 1.265, Validation Accuracy: 53.61%\n",
            "Epoch [5/10], Training Loss: 1.249, Validation Accuracy: 54.36%\n",
            "Epoch [6/10], Training Loss: 1.235, Validation Accuracy: 54.31%\n",
            "Epoch [7/10], Training Loss: 1.231, Validation Accuracy: 54.09%\n",
            "Epoch [8/10], Training Loss: 1.220, Validation Accuracy: 52.53%\n",
            "Epoch [9/10], Training Loss: 1.217, Validation Accuracy: 53.92%\n",
            "Epoch [10/10], Training Loss: 1.206, Validation Accuracy: 53.91%\n",
            "Epoch [1/10], Training Loss: 1.262, Validation Accuracy: 54.13%\n",
            "Epoch [2/10], Training Loss: 1.253, Validation Accuracy: 53.36%\n",
            "Epoch [3/10], Training Loss: 1.236, Validation Accuracy: 53.16%\n",
            "Epoch [4/10], Training Loss: 1.218, Validation Accuracy: 54.53%\n",
            "Epoch [5/10], Training Loss: 1.201, Validation Accuracy: 54.59%\n",
            "Epoch [6/10], Training Loss: 1.181, Validation Accuracy: 54.66%\n",
            "Epoch [7/10], Training Loss: 1.174, Validation Accuracy: 54.71%\n",
            "Epoch [8/10], Training Loss: 1.161, Validation Accuracy: 54.94%\n",
            "Epoch [9/10], Training Loss: 1.151, Validation Accuracy: 54.82%\n",
            "Epoch [10/10], Training Loss: 1.144, Validation Accuracy: 55.30%\n",
            "Epoch [1/10], Training Loss: 1.258, Validation Accuracy: 54.48%\n",
            "Epoch [2/10], Training Loss: 1.222, Validation Accuracy: 54.56%\n",
            "Epoch [3/10], Training Loss: 1.205, Validation Accuracy: 55.29%\n",
            "Epoch [4/10], Training Loss: 1.189, Validation Accuracy: 53.60%\n",
            "Epoch [5/10], Training Loss: 1.178, Validation Accuracy: 55.44%\n",
            "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 55.02%\n",
            "Epoch [7/10], Training Loss: 1.156, Validation Accuracy: 55.22%\n",
            "Epoch [8/10], Training Loss: 1.149, Validation Accuracy: 55.54%\n",
            "Epoch [9/10], Training Loss: 1.135, Validation Accuracy: 56.07%\n",
            "Epoch [10/10], Training Loss: 1.119, Validation Accuracy: 55.74%\n",
            "Epoch [1/10], Training Loss: 1.232, Validation Accuracy: 56.30%\n",
            "Epoch [2/10], Training Loss: 1.204, Validation Accuracy: 56.78%\n",
            "Epoch [3/10], Training Loss: 1.183, Validation Accuracy: 56.80%\n",
            "Epoch [4/10], Training Loss: 1.174, Validation Accuracy: 55.63%\n",
            "Epoch [5/10], Training Loss: 1.166, Validation Accuracy: 56.26%\n",
            "Epoch [6/10], Training Loss: 1.146, Validation Accuracy: 57.65%\n",
            "Epoch [7/10], Training Loss: 1.134, Validation Accuracy: 57.16%\n",
            "Epoch [8/10], Training Loss: 1.125, Validation Accuracy: 56.38%\n",
            "Epoch [9/10], Training Loss: 1.112, Validation Accuracy: 57.16%\n",
            "Epoch [10/10], Training Loss: 1.096, Validation Accuracy: 56.51%\n",
            "Epoch [1/10], Training Loss: 1.194, Validation Accuracy: 56.83%\n",
            "Epoch [2/10], Training Loss: 1.164, Validation Accuracy: 56.78%\n",
            "Epoch [3/10], Training Loss: 1.146, Validation Accuracy: 56.48%\n",
            "Epoch [4/10], Training Loss: 1.135, Validation Accuracy: 57.77%\n",
            "Epoch [5/10], Training Loss: 1.121, Validation Accuracy: 57.39%\n",
            "Epoch [6/10], Training Loss: 1.103, Validation Accuracy: 56.77%\n",
            "Epoch [7/10], Training Loss: 1.098, Validation Accuracy: 56.37%\n",
            "Epoch [8/10], Training Loss: 1.095, Validation Accuracy: 57.58%\n",
            "Epoch [9/10], Training Loss: 1.075, Validation Accuracy: 57.55%\n",
            "Epoch [10/10], Training Loss: 1.064, Validation Accuracy: 57.76%\n",
            "Epoch [1/10], Training Loss: 1.196, Validation Accuracy: 57.76%\n",
            "Epoch [2/10], Training Loss: 1.160, Validation Accuracy: 57.51%\n",
            "Epoch [3/10], Training Loss: 1.141, Validation Accuracy: 57.85%\n",
            "Epoch [4/10], Training Loss: 1.128, Validation Accuracy: 56.69%\n",
            "Epoch [5/10], Training Loss: 1.110, Validation Accuracy: 57.73%\n",
            "Epoch [6/10], Training Loss: 1.094, Validation Accuracy: 57.71%\n",
            "Epoch [7/10], Training Loss: 1.084, Validation Accuracy: 58.28%\n",
            "Epoch [8/10], Training Loss: 1.076, Validation Accuracy: 58.35%\n",
            "Epoch [9/10], Training Loss: 1.061, Validation Accuracy: 57.02%\n",
            "Epoch [10/10], Training Loss: 1.058, Validation Accuracy: 57.19%\n",
            "Epoch [1/10], Training Loss: 1.152, Validation Accuracy: 57.86%\n",
            "Epoch [2/10], Training Loss: 1.113, Validation Accuracy: 57.73%\n",
            "Epoch [3/10], Training Loss: 1.101, Validation Accuracy: 57.46%\n",
            "Epoch [4/10], Training Loss: 1.080, Validation Accuracy: 57.85%\n",
            "Epoch [5/10], Training Loss: 1.058, Validation Accuracy: 57.36%\n",
            "Epoch [6/10], Training Loss: 1.047, Validation Accuracy: 58.23%\n",
            "Epoch [7/10], Training Loss: 1.034, Validation Accuracy: 57.95%\n",
            "Epoch [8/10], Training Loss: 1.028, Validation Accuracy: 58.14%\n",
            "Epoch [9/10], Training Loss: 1.007, Validation Accuracy: 58.11%\n",
            "Epoch [10/10], Training Loss: 0.999, Validation Accuracy: 57.47%\n",
            "Epoch [1/10], Training Loss: 1.145, Validation Accuracy: 58.13%\n",
            "Epoch [2/10], Training Loss: 1.105, Validation Accuracy: 58.50%\n",
            "Epoch [3/10], Training Loss: 1.081, Validation Accuracy: 57.63%\n",
            "Epoch [4/10], Training Loss: 1.064, Validation Accuracy: 58.83%\n",
            "Epoch [5/10], Training Loss: 1.044, Validation Accuracy: 58.09%\n",
            "Epoch [6/10], Training Loss: 1.034, Validation Accuracy: 58.78%\n",
            "Epoch [7/10], Training Loss: 1.021, Validation Accuracy: 58.73%\n",
            "Epoch [8/10], Training Loss: 1.009, Validation Accuracy: 58.76%\n",
            "Epoch [9/10], Training Loss: 0.997, Validation Accuracy: 58.34%\n",
            "Epoch [10/10], Training Loss: 0.977, Validation Accuracy: 58.60%\n",
            "Epoch [1/10], Training Loss: 1.142, Validation Accuracy: 58.42%\n",
            "Epoch [2/10], Training Loss: 1.099, Validation Accuracy: 58.19%\n",
            "Epoch [3/10], Training Loss: 1.071, Validation Accuracy: 58.77%\n",
            "Epoch [4/10], Training Loss: 1.054, Validation Accuracy: 56.26%\n",
            "Epoch [5/10], Training Loss: 1.038, Validation Accuracy: 58.84%\n",
            "Epoch [6/10], Training Loss: 1.031, Validation Accuracy: 59.05%\n",
            "Epoch [7/10], Training Loss: 1.011, Validation Accuracy: 58.91%\n",
            "Epoch [8/10], Training Loss: 0.989, Validation Accuracy: 58.70%\n",
            "Epoch [9/10], Training Loss: 0.984, Validation Accuracy: 59.33%\n",
            "Epoch [10/10], Training Loss: 0.982, Validation Accuracy: 58.03%\n",
            "Epoch [1/10], Training Loss: 1.103, Validation Accuracy: 58.41%\n",
            "Epoch [2/10], Training Loss: 1.057, Validation Accuracy: 59.96%\n",
            "Epoch [3/10], Training Loss: 1.037, Validation Accuracy: 58.63%\n",
            "Epoch [4/10], Training Loss: 1.020, Validation Accuracy: 58.33%\n",
            "Epoch [5/10], Training Loss: 1.012, Validation Accuracy: 59.27%\n",
            "Epoch [6/10], Training Loss: 0.990, Validation Accuracy: 59.57%\n",
            "Epoch [7/10], Training Loss: 0.971, Validation Accuracy: 58.19%\n",
            "Epoch [8/10], Training Loss: 0.965, Validation Accuracy: 59.01%\n",
            "Epoch [9/10], Training Loss: 0.955, Validation Accuracy: 59.15%\n",
            "Epoch [10/10], Training Loss: 0.936, Validation Accuracy: 58.43%\n",
            "Epoch [1/10], Training Loss: 1.107, Validation Accuracy: 58.96%\n",
            "Epoch [2/10], Training Loss: 1.066, Validation Accuracy: 59.03%\n",
            "Epoch [3/10], Training Loss: 1.034, Validation Accuracy: 59.01%\n",
            "Epoch [4/10], Training Loss: 1.018, Validation Accuracy: 59.40%\n",
            "Epoch [5/10], Training Loss: 0.996, Validation Accuracy: 59.37%\n",
            "Epoch [6/10], Training Loss: 0.983, Validation Accuracy: 58.75%\n",
            "Epoch [7/10], Training Loss: 0.965, Validation Accuracy: 59.44%\n",
            "Epoch [8/10], Training Loss: 0.948, Validation Accuracy: 59.55%\n",
            "Epoch [9/10], Training Loss: 0.952, Validation Accuracy: 59.49%\n",
            "Epoch [10/10], Training Loss: 0.938, Validation Accuracy: 59.56%\n",
            "Epoch [1/10], Training Loss: 1.069, Validation Accuracy: 59.81%\n",
            "Epoch [2/10], Training Loss: 1.022, Validation Accuracy: 60.06%\n",
            "Epoch [3/10], Training Loss: 1.001, Validation Accuracy: 59.21%\n",
            "Epoch [4/10], Training Loss: 0.979, Validation Accuracy: 60.01%\n",
            "Epoch [5/10], Training Loss: 0.954, Validation Accuracy: 60.25%\n",
            "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 59.30%\n",
            "Epoch [7/10], Training Loss: 0.930, Validation Accuracy: 59.02%\n",
            "Epoch [8/10], Training Loss: 0.908, Validation Accuracy: 59.73%\n",
            "Epoch [9/10], Training Loss: 0.903, Validation Accuracy: 58.16%\n",
            "Epoch [10/10], Training Loss: 0.888, Validation Accuracy: 59.09%\n",
            "Epoch [1/10], Training Loss: 1.067, Validation Accuracy: 58.78%\n",
            "Epoch [2/10], Training Loss: 1.023, Validation Accuracy: 60.19%\n",
            "Epoch [3/10], Training Loss: 0.991, Validation Accuracy: 59.80%\n",
            "Epoch [4/10], Training Loss: 0.969, Validation Accuracy: 60.52%\n",
            "Epoch [5/10], Training Loss: 0.945, Validation Accuracy: 59.92%\n",
            "Epoch [6/10], Training Loss: 0.929, Validation Accuracy: 60.28%\n",
            "Epoch [7/10], Training Loss: 0.915, Validation Accuracy: 60.08%\n",
            "Epoch [8/10], Training Loss: 0.894, Validation Accuracy: 59.51%\n",
            "Epoch [9/10], Training Loss: 0.883, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.874, Validation Accuracy: 59.91%\n",
            "Epoch [1/10], Training Loss: 1.073, Validation Accuracy: 59.45%\n",
            "Epoch [2/10], Training Loss: 1.016, Validation Accuracy: 59.41%\n",
            "Epoch [3/10], Training Loss: 0.986, Validation Accuracy: 60.22%\n",
            "Epoch [4/10], Training Loss: 0.965, Validation Accuracy: 60.23%\n",
            "Epoch [5/10], Training Loss: 0.946, Validation Accuracy: 60.29%\n",
            "Epoch [6/10], Training Loss: 0.928, Validation Accuracy: 60.67%\n",
            "Epoch [7/10], Training Loss: 0.915, Validation Accuracy: 59.54%\n",
            "Epoch [8/10], Training Loss: 0.897, Validation Accuracy: 59.74%\n",
            "Epoch [9/10], Training Loss: 0.886, Validation Accuracy: 60.11%\n",
            "Epoch [10/10], Training Loss: 0.866, Validation Accuracy: 60.35%\n",
            "Epoch [1/10], Training Loss: 1.027, Validation Accuracy: 59.46%\n",
            "Epoch [2/10], Training Loss: 0.985, Validation Accuracy: 59.51%\n",
            "Epoch [3/10], Training Loss: 0.951, Validation Accuracy: 59.84%\n",
            "Epoch [4/10], Training Loss: 0.928, Validation Accuracy: 60.54%\n",
            "Epoch [5/10], Training Loss: 0.904, Validation Accuracy: 60.33%\n",
            "Epoch [6/10], Training Loss: 0.882, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.873, Validation Accuracy: 60.57%\n",
            "Epoch [8/10], Training Loss: 0.866, Validation Accuracy: 60.18%\n",
            "Epoch [9/10], Training Loss: 0.861, Validation Accuracy: 60.56%\n",
            "Epoch [10/10], Training Loss: 0.833, Validation Accuracy: 60.79%\n",
            "Epoch [1/10], Training Loss: 1.033, Validation Accuracy: 60.35%\n",
            "Epoch [2/10], Training Loss: 0.976, Validation Accuracy: 60.63%\n",
            "Epoch [3/10], Training Loss: 0.949, Validation Accuracy: 60.54%\n",
            "Epoch [4/10], Training Loss: 0.923, Validation Accuracy: 60.51%\n",
            "Epoch [5/10], Training Loss: 0.912, Validation Accuracy: 59.97%\n",
            "Epoch [6/10], Training Loss: 0.895, Validation Accuracy: 59.64%\n",
            "Epoch [7/10], Training Loss: 0.876, Validation Accuracy: 59.90%\n",
            "Epoch [8/10], Training Loss: 0.851, Validation Accuracy: 59.71%\n",
            "Epoch [9/10], Training Loss: 0.842, Validation Accuracy: 59.98%\n",
            "Epoch [10/10], Training Loss: 0.841, Validation Accuracy: 59.64%\n",
            "Epoch [1/10], Training Loss: 1.003, Validation Accuracy: 59.58%\n",
            "Epoch [2/10], Training Loss: 0.948, Validation Accuracy: 60.12%\n",
            "Epoch [3/10], Training Loss: 0.918, Validation Accuracy: 60.18%\n",
            "Epoch [4/10], Training Loss: 0.888, Validation Accuracy: 61.02%\n",
            "Epoch [5/10], Training Loss: 0.863, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.845, Validation Accuracy: 60.90%\n",
            "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.90%\n",
            "Epoch [8/10], Training Loss: 0.812, Validation Accuracy: 60.88%\n",
            "Epoch [9/10], Training Loss: 0.795, Validation Accuracy: 59.75%\n",
            "Epoch [10/10], Training Loss: 0.792, Validation Accuracy: 60.27%\n",
            "Epoch [1/10], Training Loss: 0.999, Validation Accuracy: 60.13%\n",
            "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 60.02%\n",
            "Epoch [3/10], Training Loss: 0.906, Validation Accuracy: 60.21%\n",
            "Epoch [4/10], Training Loss: 0.875, Validation Accuracy: 59.99%\n",
            "Epoch [5/10], Training Loss: 0.855, Validation Accuracy: 61.17%\n",
            "Epoch [6/10], Training Loss: 0.836, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.814, Validation Accuracy: 60.36%\n",
            "Epoch [8/10], Training Loss: 0.802, Validation Accuracy: 60.42%\n",
            "Epoch [9/10], Training Loss: 0.775, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.764, Validation Accuracy: 60.21%\n",
            "Epoch [1/10], Training Loss: 1.021, Validation Accuracy: 60.68%\n",
            "Epoch [2/10], Training Loss: 0.948, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.911, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.882, Validation Accuracy: 60.81%\n",
            "Epoch [5/10], Training Loss: 0.860, Validation Accuracy: 61.06%\n",
            "Epoch [6/10], Training Loss: 0.837, Validation Accuracy: 60.16%\n",
            "Epoch [7/10], Training Loss: 0.825, Validation Accuracy: 60.54%\n",
            "Epoch [8/10], Training Loss: 0.810, Validation Accuracy: 60.63%\n",
            "Epoch [9/10], Training Loss: 0.792, Validation Accuracy: 60.07%\n",
            "Epoch [10/10], Training Loss: 0.771, Validation Accuracy: 60.35%\n",
            "Epoch [1/10], Training Loss: 0.980, Validation Accuracy: 61.00%\n",
            "Epoch [2/10], Training Loss: 0.909, Validation Accuracy: 60.54%\n",
            "Epoch [3/10], Training Loss: 0.872, Validation Accuracy: 61.33%\n",
            "Epoch [4/10], Training Loss: 0.844, Validation Accuracy: 61.25%\n",
            "Epoch [5/10], Training Loss: 0.819, Validation Accuracy: 60.61%\n",
            "Epoch [6/10], Training Loss: 0.793, Validation Accuracy: 60.78%\n",
            "Epoch [7/10], Training Loss: 0.781, Validation Accuracy: 60.53%\n",
            "Epoch [8/10], Training Loss: 0.759, Validation Accuracy: 60.95%\n",
            "Epoch [9/10], Training Loss: 0.746, Validation Accuracy: 60.73%\n",
            "Epoch [10/10], Training Loss: 0.738, Validation Accuracy: 59.50%\n",
            "Epoch [1/10], Training Loss: 0.986, Validation Accuracy: 60.84%\n",
            "Epoch [2/10], Training Loss: 0.909, Validation Accuracy: 61.12%\n",
            "Epoch [3/10], Training Loss: 0.871, Validation Accuracy: 60.78%\n",
            "Epoch [4/10], Training Loss: 0.844, Validation Accuracy: 61.43%\n",
            "Epoch [5/10], Training Loss: 0.821, Validation Accuracy: 61.21%\n",
            "Epoch [6/10], Training Loss: 0.802, Validation Accuracy: 60.35%\n",
            "Epoch [7/10], Training Loss: 0.772, Validation Accuracy: 60.93%\n",
            "Epoch [8/10], Training Loss: 0.761, Validation Accuracy: 60.39%\n",
            "Epoch [9/10], Training Loss: 0.752, Validation Accuracy: 60.50%\n",
            "Epoch [10/10], Training Loss: 0.725, Validation Accuracy: 60.66%\n",
            "Epoch [1/10], Training Loss: 0.943, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.863, Validation Accuracy: 60.64%\n",
            "Epoch [3/10], Training Loss: 0.832, Validation Accuracy: 60.85%\n",
            "Epoch [4/10], Training Loss: 0.794, Validation Accuracy: 60.66%\n",
            "Epoch [5/10], Training Loss: 0.779, Validation Accuracy: 60.59%\n",
            "Epoch [6/10], Training Loss: 0.759, Validation Accuracy: 61.01%\n",
            "Epoch [7/10], Training Loss: 0.727, Validation Accuracy: 61.11%\n",
            "Epoch [8/10], Training Loss: 0.714, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.695, Validation Accuracy: 60.59%\n",
            "Epoch [10/10], Training Loss: 0.679, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.949, Validation Accuracy: 59.88%\n",
            "Epoch [2/10], Training Loss: 0.876, Validation Accuracy: 61.17%\n",
            "Epoch [3/10], Training Loss: 0.821, Validation Accuracy: 60.99%\n",
            "Epoch [4/10], Training Loss: 0.790, Validation Accuracy: 59.95%\n",
            "Epoch [5/10], Training Loss: 0.767, Validation Accuracy: 60.65%\n",
            "Epoch [6/10], Training Loss: 0.752, Validation Accuracy: 60.50%\n",
            "Epoch [7/10], Training Loss: 0.729, Validation Accuracy: 60.64%\n",
            "Epoch [8/10], Training Loss: 0.703, Validation Accuracy: 60.64%\n",
            "Epoch [9/10], Training Loss: 0.688, Validation Accuracy: 60.57%\n",
            "Epoch [10/10], Training Loss: 0.667, Validation Accuracy: 60.22%\n",
            "Epoch [1/10], Training Loss: 0.958, Validation Accuracy: 60.78%\n",
            "Epoch [2/10], Training Loss: 0.888, Validation Accuracy: 60.74%\n",
            "Epoch [3/10], Training Loss: 0.837, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.799, Validation Accuracy: 60.48%\n",
            "Epoch [5/10], Training Loss: 0.781, Validation Accuracy: 60.81%\n",
            "Epoch [6/10], Training Loss: 0.752, Validation Accuracy: 60.36%\n",
            "Epoch [7/10], Training Loss: 0.732, Validation Accuracy: 60.04%\n",
            "Epoch [8/10], Training Loss: 0.710, Validation Accuracy: 60.67%\n",
            "Epoch [9/10], Training Loss: 0.688, Validation Accuracy: 60.25%\n",
            "Epoch [10/10], Training Loss: 0.677, Validation Accuracy: 60.45%\n",
            "Epoch [1/10], Training Loss: 0.932, Validation Accuracy: 59.93%\n",
            "Epoch [2/10], Training Loss: 0.858, Validation Accuracy: 60.47%\n",
            "Epoch [3/10], Training Loss: 0.804, Validation Accuracy: 61.42%\n",
            "Epoch [4/10], Training Loss: 0.770, Validation Accuracy: 61.24%\n",
            "Epoch [5/10], Training Loss: 0.741, Validation Accuracy: 61.29%\n",
            "Epoch [6/10], Training Loss: 0.718, Validation Accuracy: 61.39%\n",
            "Epoch [7/10], Training Loss: 0.689, Validation Accuracy: 60.42%\n",
            "Epoch [8/10], Training Loss: 0.682, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.657, Validation Accuracy: 60.83%\n",
            "Epoch [10/10], Training Loss: 0.642, Validation Accuracy: 61.62%\n",
            "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.844, Validation Accuracy: 60.82%\n",
            "Epoch [3/10], Training Loss: 0.800, Validation Accuracy: 61.41%\n",
            "Epoch [4/10], Training Loss: 0.762, Validation Accuracy: 60.27%\n",
            "Epoch [5/10], Training Loss: 0.741, Validation Accuracy: 61.51%\n",
            "Epoch [6/10], Training Loss: 0.707, Validation Accuracy: 61.06%\n",
            "Epoch [7/10], Training Loss: 0.689, Validation Accuracy: 61.45%\n",
            "Epoch [8/10], Training Loss: 0.672, Validation Accuracy: 60.48%\n",
            "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 60.57%\n",
            "Epoch [10/10], Training Loss: 0.643, Validation Accuracy: 61.00%\n",
            "Epoch [1/10], Training Loss: 0.891, Validation Accuracy: 60.76%\n",
            "Epoch [2/10], Training Loss: 0.806, Validation Accuracy: 61.00%\n",
            "Epoch [3/10], Training Loss: 0.763, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.726, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.696, Validation Accuracy: 60.78%\n",
            "Epoch [6/10], Training Loss: 0.675, Validation Accuracy: 60.95%\n",
            "Epoch [7/10], Training Loss: 0.654, Validation Accuracy: 60.84%\n",
            "Epoch [8/10], Training Loss: 0.631, Validation Accuracy: 59.90%\n",
            "Epoch [9/10], Training Loss: 0.615, Validation Accuracy: 60.98%\n",
            "Epoch [10/10], Training Loss: 0.598, Validation Accuracy: 60.60%\n",
            "Epoch [1/10], Training Loss: 0.906, Validation Accuracy: 59.98%\n",
            "Epoch [2/10], Training Loss: 0.811, Validation Accuracy: 61.46%\n",
            "Epoch [3/10], Training Loss: 0.753, Validation Accuracy: 60.49%\n",
            "Epoch [4/10], Training Loss: 0.717, Validation Accuracy: 60.34%\n",
            "Epoch [5/10], Training Loss: 0.684, Validation Accuracy: 60.94%\n",
            "Epoch [6/10], Training Loss: 0.656, Validation Accuracy: 61.00%\n",
            "Epoch [7/10], Training Loss: 0.653, Validation Accuracy: 61.18%\n",
            "Epoch [8/10], Training Loss: 0.619, Validation Accuracy: 59.80%\n",
            "Epoch [9/10], Training Loss: 0.600, Validation Accuracy: 60.64%\n",
            "Epoch [10/10], Training Loss: 0.568, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 0.918, Validation Accuracy: 60.22%\n",
            "Epoch [2/10], Training Loss: 0.826, Validation Accuracy: 60.34%\n",
            "Epoch [3/10], Training Loss: 0.772, Validation Accuracy: 59.78%\n",
            "Epoch [4/10], Training Loss: 0.730, Validation Accuracy: 60.68%\n",
            "Epoch [5/10], Training Loss: 0.697, Validation Accuracy: 60.55%\n",
            "Epoch [6/10], Training Loss: 0.668, Validation Accuracy: 59.87%\n",
            "Epoch [7/10], Training Loss: 0.653, Validation Accuracy: 60.53%\n",
            "Epoch [8/10], Training Loss: 0.619, Validation Accuracy: 60.23%\n",
            "Epoch [9/10], Training Loss: 0.602, Validation Accuracy: 60.54%\n",
            "Epoch [10/10], Training Loss: 0.580, Validation Accuracy: 60.30%\n",
            "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 60.55%\n",
            "Epoch [2/10], Training Loss: 0.798, Validation Accuracy: 61.05%\n",
            "Epoch [3/10], Training Loss: 0.743, Validation Accuracy: 61.47%\n",
            "Epoch [4/10], Training Loss: 0.700, Validation Accuracy: 61.28%\n",
            "Epoch [5/10], Training Loss: 0.663, Validation Accuracy: 61.71%\n",
            "Epoch [6/10], Training Loss: 0.634, Validation Accuracy: 61.34%\n",
            "Epoch [7/10], Training Loss: 0.615, Validation Accuracy: 61.11%\n",
            "Epoch [8/10], Training Loss: 0.593, Validation Accuracy: 61.69%\n",
            "Epoch [9/10], Training Loss: 0.572, Validation Accuracy: 60.90%\n",
            "Epoch [10/10], Training Loss: 0.555, Validation Accuracy: 61.35%\n",
            "Epoch [1/10], Training Loss: 0.889, Validation Accuracy: 60.42%\n",
            "Epoch [2/10], Training Loss: 0.787, Validation Accuracy: 60.21%\n",
            "Epoch [3/10], Training Loss: 0.725, Validation Accuracy: 60.62%\n",
            "Epoch [4/10], Training Loss: 0.700, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.663, Validation Accuracy: 60.73%\n",
            "Epoch [6/10], Training Loss: 0.630, Validation Accuracy: 61.14%\n",
            "Epoch [7/10], Training Loss: 0.611, Validation Accuracy: 60.32%\n",
            "Epoch [8/10], Training Loss: 0.588, Validation Accuracy: 60.67%\n",
            "Epoch [9/10], Training Loss: 0.571, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.551, Validation Accuracy: 60.69%\n",
            "Epoch [1/10], Training Loss: 0.847, Validation Accuracy: 61.06%\n",
            "Epoch [2/10], Training Loss: 0.744, Validation Accuracy: 61.27%\n",
            "Epoch [3/10], Training Loss: 0.698, Validation Accuracy: 61.04%\n",
            "Epoch [4/10], Training Loss: 0.652, Validation Accuracy: 61.45%\n",
            "Epoch [5/10], Training Loss: 0.625, Validation Accuracy: 61.12%\n",
            "Epoch [6/10], Training Loss: 0.596, Validation Accuracy: 60.66%\n",
            "Epoch [7/10], Training Loss: 0.575, Validation Accuracy: 61.01%\n",
            "Epoch [8/10], Training Loss: 0.544, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.520, Validation Accuracy: 61.21%\n",
            "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 60.20%\n",
            "Epoch [1/10], Training Loss: 0.866, Validation Accuracy: 60.54%\n",
            "Epoch [2/10], Training Loss: 0.748, Validation Accuracy: 60.85%\n",
            "Epoch [3/10], Training Loss: 0.695, Validation Accuracy: 61.08%\n",
            "Epoch [4/10], Training Loss: 0.661, Validation Accuracy: 61.20%\n",
            "Epoch [5/10], Training Loss: 0.608, Validation Accuracy: 61.24%\n",
            "Epoch [6/10], Training Loss: 0.576, Validation Accuracy: 60.61%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 60.76%\n",
            "Epoch [8/10], Training Loss: 0.531, Validation Accuracy: 59.85%\n",
            "Epoch [9/10], Training Loss: 0.513, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.493, Validation Accuracy: 60.01%\n",
            "Epoch [1/10], Training Loss: 0.873, Validation Accuracy: 59.73%\n",
            "Epoch [2/10], Training Loss: 0.756, Validation Accuracy: 60.59%\n",
            "Epoch [3/10], Training Loss: 0.691, Validation Accuracy: 60.30%\n",
            "Epoch [4/10], Training Loss: 0.656, Validation Accuracy: 60.12%\n",
            "Epoch [5/10], Training Loss: 0.615, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.589, Validation Accuracy: 60.27%\n",
            "Epoch [7/10], Training Loss: 0.562, Validation Accuracy: 60.03%\n",
            "Epoch [8/10], Training Loss: 0.531, Validation Accuracy: 60.39%\n",
            "Epoch [9/10], Training Loss: 0.512, Validation Accuracy: 60.07%\n",
            "Epoch [10/10], Training Loss: 0.501, Validation Accuracy: 60.26%\n",
            "Epoch [1/10], Training Loss: 0.874, Validation Accuracy: 60.34%\n",
            "Epoch [2/10], Training Loss: 0.734, Validation Accuracy: 60.55%\n",
            "Epoch [3/10], Training Loss: 0.670, Validation Accuracy: 61.07%\n",
            "Epoch [4/10], Training Loss: 0.621, Validation Accuracy: 60.88%\n",
            "Epoch [5/10], Training Loss: 0.591, Validation Accuracy: 60.95%\n",
            "Epoch [6/10], Training Loss: 0.567, Validation Accuracy: 61.57%\n",
            "Epoch [7/10], Training Loss: 0.530, Validation Accuracy: 61.04%\n",
            "Epoch [8/10], Training Loss: 0.506, Validation Accuracy: 61.17%\n",
            "Epoch [9/10], Training Loss: 0.487, Validation Accuracy: 60.71%\n",
            "Epoch [10/10], Training Loss: 0.468, Validation Accuracy: 61.04%\n",
            "Epoch [1/10], Training Loss: 0.825, Validation Accuracy: 60.70%\n",
            "Epoch [2/10], Training Loss: 0.719, Validation Accuracy: 60.39%\n",
            "Epoch [3/10], Training Loss: 0.663, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.618, Validation Accuracy: 61.00%\n",
            "Epoch [5/10], Training Loss: 0.570, Validation Accuracy: 60.34%\n",
            "Epoch [6/10], Training Loss: 0.553, Validation Accuracy: 60.72%\n",
            "Epoch [7/10], Training Loss: 0.521, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.503, Validation Accuracy: 60.26%\n",
            "Epoch [9/10], Training Loss: 0.487, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.459, Validation Accuracy: 60.16%\n",
            "Epoch [1/10], Training Loss: 0.830, Validation Accuracy: 60.08%\n",
            "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 61.15%\n",
            "Epoch [3/10], Training Loss: 0.641, Validation Accuracy: 60.31%\n",
            "Epoch [4/10], Training Loss: 0.586, Validation Accuracy: 60.76%\n",
            "Epoch [5/10], Training Loss: 0.552, Validation Accuracy: 60.74%\n",
            "Epoch [6/10], Training Loss: 0.522, Validation Accuracy: 60.87%\n",
            "Epoch [7/10], Training Loss: 0.500, Validation Accuracy: 60.81%\n",
            "Epoch [8/10], Training Loss: 0.469, Validation Accuracy: 60.53%\n",
            "Epoch [9/10], Training Loss: 0.448, Validation Accuracy: 60.50%\n",
            "Epoch [10/10], Training Loss: 0.432, Validation Accuracy: 60.03%\n",
            "Confusion Matrix:\n",
            "[[615  38  61  27  29  18  21   8 123  60]\n",
            " [ 28 704   9  15   8  13  11   9  58 145]\n",
            " [ 68  15 401 113 131 108  83  30  24  27]\n",
            " [ 21  17  52 438  72 233  67  37  24  39]\n",
            " [ 26  10  70  72 555  98  68  77  13  11]\n",
            " [ 11  11  47 188  80 562  36  39  13  13]\n",
            " [  8  14  31  92  78  65 684   4  13  11]\n",
            " [ 12   5  34  58 112 143  14 578   2  42]\n",
            " [ 61  51   8  24  10  14   8   7 760  57]\n",
            " [ 33 109   5  24  13  29  15  16  47 709]]\n",
            "Test Accuracy: 60.06%\n",
            "True Positives (TP): [615 704 401 438 555 562 684 578 760 709]\n",
            "False Positives (FP): [268 270 317 613 533 721 323 227 317 405]\n",
            "True Negatives (TN): [8732 8730 8683 8387 8467 8279 8677 8773 8683 8595]\n",
            "False Negatives (FN): [385 296 599 562 445 438 316 422 240 291]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.69648924 0.72279261 0.55849582 0.41674596 0.51011029 0.43803585\n",
            " 0.67924528 0.71801242 0.70566388 0.63644524]\n",
            "Recall: [0.615 0.704 0.401 0.438 0.555 0.562 0.684 0.578 0.76  0.709]\n",
            "F1 Score: [0.65321296 0.71327254 0.46682189 0.42710873 0.5316092  0.49233465\n",
            " 0.68161435 0.64044321 0.73182475 0.67076632]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 12.83%\n",
        "Epoch [2/10], Training Loss: 2.301, Validation Accuracy: 13.90%\n",
        "Epoch [3/10], Training Loss: 2.299, Validation Accuracy: 13.75%\n",
        "Epoch [4/10], Training Loss: 2.296, Validation Accuracy: 13.71%\n",
        "Epoch [5/10], Training Loss: 2.294, Validation Accuracy: 13.32%\n",
        "Epoch [6/10], Training Loss: 2.290, Validation Accuracy: 13.18%\n",
        "Epoch [7/10], Training Loss: 2.286, Validation Accuracy: 13.15%\n",
        "Epoch [8/10], Training Loss: 2.280, Validation Accuracy: 14.79%\n",
        "Epoch [9/10], Training Loss: 2.273, Validation Accuracy: 17.14%\n",
        "Epoch [10/10], Training Loss: 2.263, Validation Accuracy: 18.26%\n",
        "Epoch [1/10], Training Loss: 2.255, Validation Accuracy: 20.06%\n",
        "Epoch [2/10], Training Loss: 2.239, Validation Accuracy: 21.04%\n",
        "Epoch [3/10], Training Loss: 2.218, Validation Accuracy: 21.98%\n",
        "Epoch [4/10], Training Loss: 2.189, Validation Accuracy: 23.01%\n",
        "Epoch [5/10], Training Loss: 2.158, Validation Accuracy: 23.58%\n",
        "Epoch [6/10], Training Loss: 2.129, Validation Accuracy: 24.17%\n",
        "Epoch [7/10], Training Loss: 2.106, Validation Accuracy: 24.46%\n",
        "Epoch [8/10], Training Loss: 2.088, Validation Accuracy: 25.73%\n",
        "Epoch [9/10], Training Loss: 2.068, Validation Accuracy: 26.00%\n",
        "Epoch [10/10], Training Loss: 2.051, Validation Accuracy: 26.60%\n",
        "Epoch [1/10], Training Loss: 2.033, Validation Accuracy: 26.43%\n",
        "Epoch [2/10], Training Loss: 2.015, Validation Accuracy: 27.12%\n",
        "Epoch [3/10], Training Loss: 2.002, Validation Accuracy: 28.09%\n",
        "Epoch [4/10], Training Loss: 1.985, Validation Accuracy: 28.11%\n",
        "Epoch [5/10], Training Loss: 1.969, Validation Accuracy: 27.71%\n",
        "Epoch [6/10], Training Loss: 1.957, Validation Accuracy: 28.89%\n",
        "Epoch [7/10], Training Loss: 1.944, Validation Accuracy: 29.40%\n",
        "Epoch [8/10], Training Loss: 1.929, Validation Accuracy: 30.10%\n",
        "Epoch [9/10], Training Loss: 1.917, Validation Accuracy: 29.78%\n",
        "Epoch [10/10], Training Loss: 1.906, Validation Accuracy: 30.90%\n",
        "Epoch [1/10], Training Loss: 1.909, Validation Accuracy: 31.06%\n",
        "Epoch [2/10], Training Loss: 1.895, Validation Accuracy: 32.07%\n",
        "Epoch [3/10], Training Loss: 1.878, Validation Accuracy: 31.93%\n",
        "Epoch [4/10], Training Loss: 1.868, Validation Accuracy: 32.71%\n",
        "Epoch [5/10], Training Loss: 1.847, Validation Accuracy: 33.24%\n",
        "Epoch [6/10], Training Loss: 1.832, Validation Accuracy: 33.91%\n",
        "Epoch [7/10], Training Loss: 1.818, Validation Accuracy: 34.23%\n",
        "Epoch [8/10], Training Loss: 1.807, Validation Accuracy: 33.68%\n",
        "Epoch [9/10], Training Loss: 1.783, Validation Accuracy: 35.49%\n",
        "Epoch [10/10], Training Loss: 1.769, Validation Accuracy: 35.47%\n",
        "Epoch [1/10], Training Loss: 1.758, Validation Accuracy: 35.75%\n",
        "Epoch [2/10], Training Loss: 1.745, Validation Accuracy: 36.79%\n",
        "Epoch [3/10], Training Loss: 1.728, Validation Accuracy: 36.79%\n",
        "Epoch [4/10], Training Loss: 1.709, Validation Accuracy: 37.37%\n",
        "Epoch [5/10], Training Loss: 1.697, Validation Accuracy: 38.20%\n",
        "Epoch [6/10], Training Loss: 1.689, Validation Accuracy: 37.31%\n",
        "Epoch [7/10], Training Loss: 1.675, Validation Accuracy: 38.47%\n",
        "Epoch [8/10], Training Loss: 1.663, Validation Accuracy: 38.77%\n",
        "Epoch [9/10], Training Loss: 1.649, Validation Accuracy: 38.81%\n",
        "Epoch [10/10], Training Loss: 1.643, Validation Accuracy: 39.65%\n",
        "Epoch [1/10], Training Loss: 1.657, Validation Accuracy: 38.48%\n",
        "Epoch [2/10], Training Loss: 1.644, Validation Accuracy: 39.89%\n",
        "Epoch [3/10], Training Loss: 1.625, Validation Accuracy: 39.54%\n",
        "Epoch [4/10], Training Loss: 1.616, Validation Accuracy: 40.10%\n",
        "Epoch [5/10], Training Loss: 1.607, Validation Accuracy: 39.66%\n",
        "Epoch [6/10], Training Loss: 1.597, Validation Accuracy: 41.13%\n",
        "Epoch [7/10], Training Loss: 1.585, Validation Accuracy: 41.32%\n",
        "Epoch [8/10], Training Loss: 1.571, Validation Accuracy: 41.44%\n",
        "Epoch [9/10], Training Loss: 1.569, Validation Accuracy: 41.23%\n",
        "Epoch [10/10], Training Loss: 1.560, Validation Accuracy: 42.28%\n",
        "Epoch [1/10], Training Loss: 1.592, Validation Accuracy: 41.58%\n",
        "Epoch [2/10], Training Loss: 1.573, Validation Accuracy: 42.96%\n",
        "Epoch [3/10], Training Loss: 1.564, Validation Accuracy: 42.10%\n",
        "Epoch [4/10], Training Loss: 1.558, Validation Accuracy: 42.86%\n",
        "Epoch [5/10], Training Loss: 1.542, Validation Accuracy: 42.91%\n",
        "Epoch [6/10], Training Loss: 1.538, Validation Accuracy: 43.72%\n",
        "Epoch [7/10], Training Loss: 1.521, Validation Accuracy: 44.11%\n",
        "Epoch [8/10], Training Loss: 1.515, Validation Accuracy: 43.01%\n",
        "Epoch [9/10], Training Loss: 1.506, Validation Accuracy: 44.37%\n",
        "Epoch [10/10], Training Loss: 1.497, Validation Accuracy: 44.10%\n",
        "Epoch [1/10], Training Loss: 1.523, Validation Accuracy: 45.51%\n",
        "Epoch [2/10], Training Loss: 1.499, Validation Accuracy: 45.89%\n",
        "Epoch [3/10], Training Loss: 1.497, Validation Accuracy: 45.66%\n",
        "Epoch [4/10], Training Loss: 1.489, Validation Accuracy: 45.72%\n",
        "Epoch [5/10], Training Loss: 1.473, Validation Accuracy: 45.72%\n",
        "Epoch [6/10], Training Loss: 1.476, Validation Accuracy: 46.09%\n",
        "Epoch [7/10], Training Loss: 1.458, Validation Accuracy: 46.52%\n",
        "Epoch [8/10], Training Loss: 1.448, Validation Accuracy: 45.32%\n",
        "Epoch [9/10], Training Loss: 1.440, Validation Accuracy: 46.49%\n",
        "Epoch [10/10], Training Loss: 1.434, Validation Accuracy: 46.78%\n",
        "Epoch [1/10], Training Loss: 1.487, Validation Accuracy: 47.17%\n",
        "Epoch [2/10], Training Loss: 1.468, Validation Accuracy: 46.54%\n",
        "Epoch [3/10], Training Loss: 1.464, Validation Accuracy: 46.79%\n",
        "Epoch [4/10], Training Loss: 1.455, Validation Accuracy: 46.64%\n",
        "Epoch [5/10], Training Loss: 1.445, Validation Accuracy: 47.37%\n",
        "Epoch [6/10], Training Loss: 1.431, Validation Accuracy: 47.88%\n",
        "Epoch [7/10], Training Loss: 1.434, Validation Accuracy: 47.05%\n",
        "Epoch [8/10], Training Loss: 1.419, Validation Accuracy: 46.00%\n",
        "Epoch [9/10], Training Loss: 1.411, Validation Accuracy: 48.17%\n",
        "Epoch [10/10], Training Loss: 1.408, Validation Accuracy: 47.68%\n",
        "Epoch [1/10], Training Loss: 1.418, Validation Accuracy: 48.28%\n",
        "Epoch [2/10], Training Loss: 1.397, Validation Accuracy: 48.63%\n",
        "Epoch [3/10], Training Loss: 1.385, Validation Accuracy: 48.63%\n",
        "Epoch [4/10], Training Loss: 1.388, Validation Accuracy: 49.21%\n",
        "Epoch [5/10], Training Loss: 1.371, Validation Accuracy: 48.54%\n",
        "Epoch [6/10], Training Loss: 1.361, Validation Accuracy: 49.03%\n",
        "Epoch [7/10], Training Loss: 1.353, Validation Accuracy: 49.31%\n",
        "Epoch [8/10], Training Loss: 1.348, Validation Accuracy: 48.64%\n",
        "Epoch [9/10], Training Loss: 1.336, Validation Accuracy: 49.59%\n",
        "Epoch [10/10], Training Loss: 1.332, Validation Accuracy: 48.83%\n",
        "Epoch [1/10], Training Loss: 1.389, Validation Accuracy: 49.60%\n",
        "Epoch [2/10], Training Loss: 1.379, Validation Accuracy: 49.36%\n",
        "Epoch [3/10], Training Loss: 1.364, Validation Accuracy: 49.51%\n",
        "Epoch [4/10], Training Loss: 1.346, Validation Accuracy: 50.27%\n",
        "Epoch [5/10], Training Loss: 1.339, Validation Accuracy: 49.17%\n",
        "Epoch [6/10], Training Loss: 1.334, Validation Accuracy: 50.20%\n",
        "Epoch [7/10], Training Loss: 1.323, Validation Accuracy: 50.70%\n",
        "Epoch [8/10], Training Loss: 1.314, Validation Accuracy: 50.83%\n",
        "Epoch [9/10], Training Loss: 1.317, Validation Accuracy: 51.04%\n",
        "Epoch [10/10], Training Loss: 1.295, Validation Accuracy: 50.90%\n",
        "Epoch [1/10], Training Loss: 1.353, Validation Accuracy: 51.12%\n",
        "Epoch [2/10], Training Loss: 1.334, Validation Accuracy: 50.66%\n",
        "Epoch [3/10], Training Loss: 1.323, Validation Accuracy: 50.79%\n",
        "Epoch [4/10], Training Loss: 1.305, Validation Accuracy: 50.91%\n",
        "Epoch [5/10], Training Loss: 1.309, Validation Accuracy: 51.36%\n",
        "Epoch [6/10], Training Loss: 1.290, Validation Accuracy: 51.19%\n",
        "Epoch [7/10], Training Loss: 1.285, Validation Accuracy: 51.97%\n",
        "Epoch [8/10], Training Loss: 1.264, Validation Accuracy: 51.98%\n",
        "Epoch [9/10], Training Loss: 1.258, Validation Accuracy: 51.67%\n",
        "Epoch [10/10], Training Loss: 1.245, Validation Accuracy: 52.30%\n",
        "Epoch [1/10], Training Loss: 1.306, Validation Accuracy: 52.68%\n",
        "Epoch [2/10], Training Loss: 1.282, Validation Accuracy: 52.61%\n",
        "Epoch [3/10], Training Loss: 1.273, Validation Accuracy: 51.60%\n",
        "Epoch [4/10], Training Loss: 1.261, Validation Accuracy: 52.73%\n",
        "Epoch [5/10], Training Loss: 1.252, Validation Accuracy: 52.83%\n",
        "Epoch [6/10], Training Loss: 1.237, Validation Accuracy: 52.90%\n",
        "Epoch [7/10], Training Loss: 1.230, Validation Accuracy: 53.29%\n",
        "Epoch [8/10], Training Loss: 1.217, Validation Accuracy: 53.07%\n",
        "Epoch [9/10], Training Loss: 1.207, Validation Accuracy: 53.25%\n",
        "Epoch [10/10], Training Loss: 1.197, Validation Accuracy: 53.47%\n",
        "Epoch [1/10], Training Loss: 1.303, Validation Accuracy: 53.27%\n",
        "Epoch [2/10], Training Loss: 1.279, Validation Accuracy: 53.21%\n",
        "Epoch [3/10], Training Loss: 1.260, Validation Accuracy: 53.53%\n",
        "Epoch [4/10], Training Loss: 1.261, Validation Accuracy: 53.14%\n",
        "Epoch [5/10], Training Loss: 1.235, Validation Accuracy: 54.40%\n",
        "Epoch [6/10], Training Loss: 1.228, Validation Accuracy: 52.33%\n",
        "Epoch [7/10], Training Loss: 1.221, Validation Accuracy: 54.25%\n",
        "Epoch [8/10], Training Loss: 1.211, Validation Accuracy: 54.28%\n",
        "Epoch [9/10], Training Loss: 1.217, Validation Accuracy: 54.35%\n",
        "Epoch [10/10], Training Loss: 1.187, Validation Accuracy: 54.45%\n",
        "Epoch [1/10], Training Loss: 1.229, Validation Accuracy: 54.52%\n",
        "Epoch [2/10], Training Loss: 1.203, Validation Accuracy: 53.38%\n",
        "Epoch [3/10], Training Loss: 1.196, Validation Accuracy: 54.87%\n",
        "Epoch [4/10], Training Loss: 1.183, Validation Accuracy: 54.41%\n",
        "Epoch [5/10], Training Loss: 1.170, Validation Accuracy: 54.75%\n",
        "Epoch [6/10], Training Loss: 1.157, Validation Accuracy: 54.81%\n",
        "Epoch [7/10], Training Loss: 1.152, Validation Accuracy: 54.75%\n",
        "Epoch [8/10], Training Loss: 1.146, Validation Accuracy: 54.90%\n",
        "Epoch [9/10], Training Loss: 1.127, Validation Accuracy: 55.68%\n",
        "Epoch [10/10], Training Loss: 1.112, Validation Accuracy: 54.96%\n",
        "Epoch [1/10], Training Loss: 1.226, Validation Accuracy: 54.87%\n",
        "Epoch [2/10], Training Loss: 1.205, Validation Accuracy: 56.23%\n",
        "Epoch [3/10], Training Loss: 1.180, Validation Accuracy: 56.28%\n",
        "Epoch [4/10], Training Loss: 1.177, Validation Accuracy: 55.73%\n",
        "Epoch [5/10], Training Loss: 1.161, Validation Accuracy: 55.92%\n",
        "Epoch [6/10], Training Loss: 1.150, Validation Accuracy: 55.51%\n",
        "Epoch [7/10], Training Loss: 1.136, Validation Accuracy: 56.17%\n",
        "Epoch [8/10], Training Loss: 1.122, Validation Accuracy: 55.57%\n",
        "Epoch [9/10], Training Loss: 1.133, Validation Accuracy: 55.83%\n",
        "Epoch [10/10], Training Loss: 1.119, Validation Accuracy: 55.40%\n",
        "Epoch [1/10], Training Loss: 1.192, Validation Accuracy: 55.89%\n",
        "Epoch [2/10], Training Loss: 1.165, Validation Accuracy: 56.42%\n",
        "Epoch [3/10], Training Loss: 1.147, Validation Accuracy: 56.45%\n",
        "Epoch [4/10], Training Loss: 1.139, Validation Accuracy: 56.49%\n",
        "Epoch [5/10], Training Loss: 1.127, Validation Accuracy: 56.34%\n",
        "Epoch [6/10], Training Loss: 1.115, Validation Accuracy: 56.50%\n",
        "Epoch [7/10], Training Loss: 1.096, Validation Accuracy: 56.75%\n",
        "Epoch [8/10], Training Loss: 1.094, Validation Accuracy: 56.79%\n",
        "Epoch [9/10], Training Loss: 1.074, Validation Accuracy: 56.88%\n",
        "Epoch [10/10], Training Loss: 1.068, Validation Accuracy: 56.70%\n",
        "Epoch [1/10], Training Loss: 1.167, Validation Accuracy: 56.23%\n",
        "Epoch [2/10], Training Loss: 1.139, Validation Accuracy: 56.43%\n",
        "Epoch [3/10], Training Loss: 1.125, Validation Accuracy: 56.47%\n",
        "Epoch [4/10], Training Loss: 1.110, Validation Accuracy: 56.71%\n",
        "Epoch [5/10], Training Loss: 1.086, Validation Accuracy: 57.21%\n",
        "Epoch [6/10], Training Loss: 1.077, Validation Accuracy: 56.91%\n",
        "Epoch [7/10], Training Loss: 1.071, Validation Accuracy: 57.27%\n",
        "Epoch [8/10], Training Loss: 1.056, Validation Accuracy: 57.24%\n",
        "Epoch [9/10], Training Loss: 1.044, Validation Accuracy: 57.29%\n",
        "Epoch [10/10], Training Loss: 1.034, Validation Accuracy: 57.50%\n",
        "Epoch [1/10], Training Loss: 1.173, Validation Accuracy: 57.58%\n",
        "Epoch [2/10], Training Loss: 1.142, Validation Accuracy: 57.52%\n",
        "Epoch [3/10], Training Loss: 1.129, Validation Accuracy: 57.49%\n",
        "Epoch [4/10], Training Loss: 1.106, Validation Accuracy: 56.93%\n",
        "Epoch [5/10], Training Loss: 1.097, Validation Accuracy: 57.93%\n",
        "Epoch [6/10], Training Loss: 1.082, Validation Accuracy: 57.37%\n",
        "Epoch [7/10], Training Loss: 1.064, Validation Accuracy: 58.06%\n",
        "Epoch [8/10], Training Loss: 1.061, Validation Accuracy: 57.28%\n",
        "Epoch [9/10], Training Loss: 1.051, Validation Accuracy: 57.57%\n",
        "Epoch [10/10], Training Loss: 1.037, Validation Accuracy: 58.07%\n",
        "Epoch [1/10], Training Loss: 1.112, Validation Accuracy: 58.75%\n",
        "Epoch [2/10], Training Loss: 1.077, Validation Accuracy: 58.48%\n",
        "Epoch [3/10], Training Loss: 1.068, Validation Accuracy: 58.31%\n",
        "Epoch [4/10], Training Loss: 1.040, Validation Accuracy: 58.31%\n",
        "Epoch [5/10], Training Loss: 1.031, Validation Accuracy: 58.35%\n",
        "Epoch [6/10], Training Loss: 1.017, Validation Accuracy: 58.96%\n",
        "Epoch [7/10], Training Loss: 1.007, Validation Accuracy: 58.32%\n",
        "Epoch [8/10], Training Loss: 0.996, Validation Accuracy: 57.58%\n",
        "Epoch [9/10], Training Loss: 0.991, Validation Accuracy: 57.30%\n",
        "Epoch [10/10], Training Loss: 0.970, Validation Accuracy: 58.13%\n",
        "Epoch [1/10], Training Loss: 1.117, Validation Accuracy: 59.08%\n",
        "Epoch [2/10], Training Loss: 1.086, Validation Accuracy: 58.56%\n",
        "Epoch [3/10], Training Loss: 1.064, Validation Accuracy: 58.90%\n",
        "Epoch [4/10], Training Loss: 1.045, Validation Accuracy: 58.67%\n",
        "Epoch [5/10], Training Loss: 1.031, Validation Accuracy: 58.49%\n",
        "Epoch [6/10], Training Loss: 1.025, Validation Accuracy: 58.01%\n",
        "Epoch [7/10], Training Loss: 1.011, Validation Accuracy: 58.17%\n",
        "Epoch [8/10], Training Loss: 1.001, Validation Accuracy: 58.64%\n",
        "Epoch [9/10], Training Loss: 0.989, Validation Accuracy: 58.74%\n",
        "Epoch [10/10], Training Loss: 0.972, Validation Accuracy: 58.39%\n",
        "Epoch [1/10], Training Loss: 1.095, Validation Accuracy: 58.87%\n",
        "Epoch [2/10], Training Loss: 1.065, Validation Accuracy: 58.27%\n",
        "Epoch [3/10], Training Loss: 1.045, Validation Accuracy: 58.58%\n",
        "Epoch [4/10], Training Loss: 1.017, Validation Accuracy: 58.95%\n",
        "Epoch [5/10], Training Loss: 1.004, Validation Accuracy: 58.32%\n",
        "Epoch [6/10], Training Loss: 0.995, Validation Accuracy: 58.73%\n",
        "Epoch [7/10], Training Loss: 0.980, Validation Accuracy: 59.02%\n",
        "Epoch [8/10], Training Loss: 0.966, Validation Accuracy: 59.24%\n",
        "Epoch [9/10], Training Loss: 0.950, Validation Accuracy: 58.19%\n",
        "Epoch [10/10], Training Loss: 0.943, Validation Accuracy: 59.16%\n",
        "Epoch [1/10], Training Loss: 1.070, Validation Accuracy: 59.55%\n",
        "Epoch [2/10], Training Loss: 1.038, Validation Accuracy: 59.81%\n",
        "Epoch [3/10], Training Loss: 1.012, Validation Accuracy: 58.97%\n",
        "Epoch [4/10], Training Loss: 1.001, Validation Accuracy: 58.43%\n",
        "Epoch [5/10], Training Loss: 0.980, Validation Accuracy: 59.32%\n",
        "Epoch [6/10], Training Loss: 0.961, Validation Accuracy: 58.98%\n",
        "Epoch [7/10], Training Loss: 0.957, Validation Accuracy: 59.76%\n",
        "Epoch [8/10], Training Loss: 0.940, Validation Accuracy: 59.42%\n",
        "Epoch [9/10], Training Loss: 0.925, Validation Accuracy: 59.14%\n",
        "Epoch [10/10], Training Loss: 0.914, Validation Accuracy: 58.97%\n",
        "Epoch [1/10], Training Loss: 1.079, Validation Accuracy: 58.29%\n",
        "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.35%\n",
        "Epoch [3/10], Training Loss: 1.015, Validation Accuracy: 58.38%\n",
        "Epoch [4/10], Training Loss: 1.000, Validation Accuracy: 59.10%\n",
        "Epoch [5/10], Training Loss: 0.981, Validation Accuracy: 60.11%\n",
        "Epoch [6/10], Training Loss: 0.959, Validation Accuracy: 59.97%\n",
        "Epoch [7/10], Training Loss: 0.971, Validation Accuracy: 59.37%\n",
        "Epoch [8/10], Training Loss: 0.938, Validation Accuracy: 59.12%\n",
        "Epoch [9/10], Training Loss: 0.929, Validation Accuracy: 59.25%\n",
        "Epoch [10/10], Training Loss: 0.913, Validation Accuracy: 58.53%\n",
        "Epoch [1/10], Training Loss: 1.032, Validation Accuracy: 60.08%\n",
        "Epoch [2/10], Training Loss: 0.989, Validation Accuracy: 60.20%\n",
        "Epoch [3/10], Training Loss: 0.962, Validation Accuracy: 60.19%\n",
        "Epoch [4/10], Training Loss: 0.944, Validation Accuracy: 60.49%\n",
        "Epoch [5/10], Training Loss: 0.917, Validation Accuracy: 60.72%\n",
        "Epoch [6/10], Training Loss: 0.901, Validation Accuracy: 60.75%\n",
        "Epoch [7/10], Training Loss: 0.891, Validation Accuracy: 59.86%\n",
        "Epoch [8/10], Training Loss: 0.883, Validation Accuracy: 60.12%\n",
        "Epoch [9/10], Training Loss: 0.882, Validation Accuracy: 59.41%\n",
        "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 59.80%\n",
        "Epoch [1/10], Training Loss: 1.037, Validation Accuracy: 60.30%\n",
        "Epoch [2/10], Training Loss: 0.992, Validation Accuracy: 59.37%\n",
        "Epoch [3/10], Training Loss: 0.977, Validation Accuracy: 60.04%\n",
        "Epoch [4/10], Training Loss: 0.940, Validation Accuracy: 60.51%\n",
        "Epoch [5/10], Training Loss: 0.938, Validation Accuracy: 60.59%\n",
        "Epoch [6/10], Training Loss: 0.913, Validation Accuracy: 60.50%\n",
        "Epoch [7/10], Training Loss: 0.892, Validation Accuracy: 60.47%\n",
        "Epoch [8/10], Training Loss: 0.883, Validation Accuracy: 60.93%\n",
        "Epoch [9/10], Training Loss: 0.865, Validation Accuracy: 59.89%\n",
        "Epoch [10/10], Training Loss: 0.856, Validation Accuracy: 60.48%\n",
        "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 59.69%\n",
        "Epoch [2/10], Training Loss: 0.973, Validation Accuracy: 60.16%\n",
        "Epoch [3/10], Training Loss: 0.943, Validation Accuracy: 59.19%\n",
        "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 60.77%\n",
        "Epoch [5/10], Training Loss: 0.902, Validation Accuracy: 60.83%\n",
        "Epoch [6/10], Training Loss: 0.898, Validation Accuracy: 60.72%\n",
        "Epoch [7/10], Training Loss: 0.872, Validation Accuracy: 60.36%\n",
        "Epoch [8/10], Training Loss: 0.857, Validation Accuracy: 59.89%\n",
        "Epoch [9/10], Training Loss: 0.843, Validation Accuracy: 59.98%\n",
        "Epoch [10/10], Training Loss: 0.837, Validation Accuracy: 60.78%\n",
        "Epoch [1/10], Training Loss: 0.996, Validation Accuracy: 61.04%\n",
        "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 60.81%\n",
        "Epoch [3/10], Training Loss: 0.927, Validation Accuracy: 60.25%\n",
        "Epoch [4/10], Training Loss: 0.899, Validation Accuracy: 59.90%\n",
        "Epoch [5/10], Training Loss: 0.876, Validation Accuracy: 60.73%\n",
        "Epoch [6/10], Training Loss: 0.861, Validation Accuracy: 60.10%\n",
        "Epoch [7/10], Training Loss: 0.848, Validation Accuracy: 60.60%\n",
        "Epoch [8/10], Training Loss: 0.834, Validation Accuracy: 60.50%\n",
        "Epoch [9/10], Training Loss: 0.818, Validation Accuracy: 60.37%\n",
        "Epoch [10/10], Training Loss: 0.805, Validation Accuracy: 60.26%\n",
        "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.41%\n",
        "Epoch [2/10], Training Loss: 0.953, Validation Accuracy: 60.18%\n",
        "Epoch [3/10], Training Loss: 0.929, Validation Accuracy: 60.56%\n",
        "Epoch [4/10], Training Loss: 0.913, Validation Accuracy: 60.99%\n",
        "Epoch [5/10], Training Loss: 0.880, Validation Accuracy: 60.22%\n",
        "Epoch [6/10], Training Loss: 0.866, Validation Accuracy: 60.36%\n",
        "Epoch [7/10], Training Loss: 0.852, Validation Accuracy: 60.31%\n",
        "Epoch [8/10], Training Loss: 0.830, Validation Accuracy: 60.62%\n",
        "Epoch [9/10], Training Loss: 0.817, Validation Accuracy: 59.99%\n",
        "Epoch [10/10], Training Loss: 0.797, Validation Accuracy: 60.52%\n",
        "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 61.42%\n",
        "Epoch [2/10], Training Loss: 0.912, Validation Accuracy: 60.86%\n",
        "Epoch [3/10], Training Loss: 0.874, Validation Accuracy: 61.14%\n",
        "Epoch [4/10], Training Loss: 0.843, Validation Accuracy: 60.90%\n",
        "Epoch [5/10], Training Loss: 0.835, Validation Accuracy: 61.34%\n",
        "Epoch [6/10], Training Loss: 0.809, Validation Accuracy: 61.17%\n",
        "Epoch [7/10], Training Loss: 0.794, Validation Accuracy: 61.31%\n",
        "Epoch [8/10], Training Loss: 0.776, Validation Accuracy: 61.38%\n",
        "Epoch [9/10], Training Loss: 0.766, Validation Accuracy: 60.25%\n",
        "Epoch [10/10], Training Loss: 0.754, Validation Accuracy: 60.75%\n",
        "Epoch [1/10], Training Loss: 0.970, Validation Accuracy: 61.25%\n",
        "Epoch [2/10], Training Loss: 0.903, Validation Accuracy: 61.02%\n",
        "Epoch [3/10], Training Loss: 0.884, Validation Accuracy: 59.60%\n",
        "Epoch [4/10], Training Loss: 0.861, Validation Accuracy: 60.75%\n",
        "Epoch [5/10], Training Loss: 0.833, Validation Accuracy: 61.65%\n",
        "Epoch [6/10], Training Loss: 0.812, Validation Accuracy: 60.80%\n",
        "Epoch [7/10], Training Loss: 0.795, Validation Accuracy: 61.03%\n",
        "Epoch [8/10], Training Loss: 0.778, Validation Accuracy: 60.56%\n",
        "Epoch [9/10], Training Loss: 0.768, Validation Accuracy: 60.80%\n",
        "Epoch [10/10], Training Loss: 0.745, Validation Accuracy: 60.57%\n",
        "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 59.89%\n",
        "Epoch [2/10], Training Loss: 0.893, Validation Accuracy: 60.47%\n",
        "Epoch [3/10], Training Loss: 0.867, Validation Accuracy: 60.83%\n",
        "Epoch [4/10], Training Loss: 0.841, Validation Accuracy: 60.61%\n",
        "Epoch [5/10], Training Loss: 0.815, Validation Accuracy: 61.53%\n",
        "Epoch [6/10], Training Loss: 0.794, Validation Accuracy: 60.49%\n",
        "Epoch [7/10], Training Loss: 0.784, Validation Accuracy: 61.39%\n",
        "Epoch [8/10], Training Loss: 0.756, Validation Accuracy: 61.31%\n",
        "Epoch [9/10], Training Loss: 0.740, Validation Accuracy: 60.87%\n",
        "Epoch [10/10], Training Loss: 0.730, Validation Accuracy: 60.91%\n",
        "Epoch [1/10], Training Loss: 0.935, Validation Accuracy: 60.94%\n",
        "Epoch [2/10], Training Loss: 0.869, Validation Accuracy: 61.39%\n",
        "Epoch [3/10], Training Loss: 0.850, Validation Accuracy: 61.70%\n",
        "Epoch [4/10], Training Loss: 0.809, Validation Accuracy: 61.66%\n",
        "Epoch [5/10], Training Loss: 0.790, Validation Accuracy: 61.23%\n",
        "Epoch [6/10], Training Loss: 0.764, Validation Accuracy: 60.52%\n",
        "Epoch [7/10], Training Loss: 0.745, Validation Accuracy: 61.48%\n",
        "Epoch [8/10], Training Loss: 0.727, Validation Accuracy: 61.02%\n",
        "Epoch [9/10], Training Loss: 0.714, Validation Accuracy: 61.23%\n",
        "Epoch [10/10], Training Loss: 0.697, Validation Accuracy: 60.57%\n",
        "Epoch [1/10], Training Loss: 0.951, Validation Accuracy: 61.22%\n",
        "Epoch [2/10], Training Loss: 0.879, Validation Accuracy: 61.37%\n",
        "Epoch [3/10], Training Loss: 0.844, Validation Accuracy: 61.61%\n",
        "Epoch [4/10], Training Loss: 0.809, Validation Accuracy: 60.42%\n",
        "Epoch [5/10], Training Loss: 0.788, Validation Accuracy: 61.59%\n",
        "Epoch [6/10], Training Loss: 0.765, Validation Accuracy: 60.78%\n",
        "Epoch [7/10], Training Loss: 0.753, Validation Accuracy: 61.23%\n",
        "Epoch [8/10], Training Loss: 0.738, Validation Accuracy: 61.23%\n",
        "Epoch [9/10], Training Loss: 0.714, Validation Accuracy: 61.06%\n",
        "Epoch [10/10], Training Loss: 0.695, Validation Accuracy: 61.02%\n",
        "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.44%\n",
        "Epoch [2/10], Training Loss: 0.832, Validation Accuracy: 61.63%\n",
        "Epoch [3/10], Training Loss: 0.790, Validation Accuracy: 61.93%\n",
        "Epoch [4/10], Training Loss: 0.764, Validation Accuracy: 61.38%\n",
        "Epoch [5/10], Training Loss: 0.745, Validation Accuracy: 61.81%\n",
        "Epoch [6/10], Training Loss: 0.722, Validation Accuracy: 61.75%\n",
        "Epoch [7/10], Training Loss: 0.701, Validation Accuracy: 61.79%\n",
        "Epoch [8/10], Training Loss: 0.682, Validation Accuracy: 61.84%\n",
        "Epoch [9/10], Training Loss: 0.665, Validation Accuracy: 61.51%\n",
        "Epoch [10/10], Training Loss: 0.646, Validation Accuracy: 61.23%\n",
        "Epoch [1/10], Training Loss: 0.907, Validation Accuracy: 61.17%\n",
        "Epoch [2/10], Training Loss: 0.841, Validation Accuracy: 61.27%\n",
        "Epoch [3/10], Training Loss: 0.795, Validation Accuracy: 60.24%\n",
        "Epoch [4/10], Training Loss: 0.766, Validation Accuracy: 61.40%\n",
        "Epoch [5/10], Training Loss: 0.738, Validation Accuracy: 61.60%\n",
        "Epoch [6/10], Training Loss: 0.715, Validation Accuracy: 61.22%\n",
        "Epoch [7/10], Training Loss: 0.699, Validation Accuracy: 61.39%\n",
        "Epoch [8/10], Training Loss: 0.669, Validation Accuracy: 60.72%\n",
        "Epoch [9/10], Training Loss: 0.659, Validation Accuracy: 61.16%\n",
        "Epoch [10/10], Training Loss: 0.648, Validation Accuracy: 61.21%\n",
        "Epoch [1/10], Training Loss: 0.904, Validation Accuracy: 61.11%\n",
        "Epoch [2/10], Training Loss: 0.831, Validation Accuracy: 61.05%\n",
        "Epoch [3/10], Training Loss: 0.781, Validation Accuracy: 61.38%\n",
        "Epoch [4/10], Training Loss: 0.756, Validation Accuracy: 61.30%\n",
        "Epoch [5/10], Training Loss: 0.730, Validation Accuracy: 61.46%\n",
        "Epoch [6/10], Training Loss: 0.707, Validation Accuracy: 61.17%\n",
        "Epoch [7/10], Training Loss: 0.681, Validation Accuracy: 60.98%\n",
        "Epoch [8/10], Training Loss: 0.670, Validation Accuracy: 61.24%\n",
        "Epoch [9/10], Training Loss: 0.644, Validation Accuracy: 60.46%\n",
        "Epoch [10/10], Training Loss: 0.625, Validation Accuracy: 60.96%\n",
        "Epoch [1/10], Training Loss: 0.890, Validation Accuracy: 61.34%\n",
        "Epoch [2/10], Training Loss: 0.807, Validation Accuracy: 60.97%\n",
        "Epoch [3/10], Training Loss: 0.754, Validation Accuracy: 61.84%\n",
        "Epoch [4/10], Training Loss: 0.731, Validation Accuracy: 61.24%\n",
        "Epoch [5/10], Training Loss: 0.700, Validation Accuracy: 61.80%\n",
        "Epoch [6/10], Training Loss: 0.676, Validation Accuracy: 61.47%\n",
        "Epoch [7/10], Training Loss: 0.657, Validation Accuracy: 61.38%\n",
        "Epoch [8/10], Training Loss: 0.633, Validation Accuracy: 60.26%\n",
        "Epoch [9/10], Training Loss: 0.624, Validation Accuracy: 61.08%\n",
        "Epoch [10/10], Training Loss: 0.606, Validation Accuracy: 60.96%\n",
        "Epoch [1/10], Training Loss: 0.898, Validation Accuracy: 61.35%\n",
        "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.31%\n",
        "Epoch [3/10], Training Loss: 0.781, Validation Accuracy: 61.45%\n",
        "Epoch [4/10], Training Loss: 0.739, Validation Accuracy: 60.96%\n",
        "Epoch [5/10], Training Loss: 0.703, Validation Accuracy: 61.90%\n",
        "Epoch [6/10], Training Loss: 0.691, Validation Accuracy: 61.33%\n",
        "Epoch [7/10], Training Loss: 0.665, Validation Accuracy: 61.78%\n",
        "Epoch [8/10], Training Loss: 0.645, Validation Accuracy: 61.09%\n",
        "Epoch [9/10], Training Loss: 0.626, Validation Accuracy: 61.14%\n",
        "Epoch [10/10], Training Loss: 0.604, Validation Accuracy: 60.57%\n",
        "Epoch [1/10], Training Loss: 0.852, Validation Accuracy: 61.85%\n",
        "Epoch [2/10], Training Loss: 0.757, Validation Accuracy: 61.66%\n",
        "Epoch [3/10], Training Loss: 0.720, Validation Accuracy: 61.97%\n",
        "Epoch [4/10], Training Loss: 0.695, Validation Accuracy: 61.22%\n",
        "Epoch [5/10], Training Loss: 0.660, Validation Accuracy: 61.33%\n",
        "Epoch [6/10], Training Loss: 0.631, Validation Accuracy: 61.20%\n",
        "Epoch [7/10], Training Loss: 0.614, Validation Accuracy: 61.67%\n",
        "Epoch [8/10], Training Loss: 0.586, Validation Accuracy: 61.61%\n",
        "Epoch [9/10], Training Loss: 0.565, Validation Accuracy: 61.80%\n",
        "Epoch [10/10], Training Loss: 0.558, Validation Accuracy: 61.71%\n",
        "Epoch [1/10], Training Loss: 0.854, Validation Accuracy: 60.96%\n",
        "Epoch [2/10], Training Loss: 0.770, Validation Accuracy: 62.38%\n",
        "Epoch [3/10], Training Loss: 0.726, Validation Accuracy: 61.84%\n",
        "Epoch [4/10], Training Loss: 0.688, Validation Accuracy: 62.00%\n",
        "Epoch [5/10], Training Loss: 0.655, Validation Accuracy: 61.51%\n",
        "Epoch [6/10], Training Loss: 0.632, Validation Accuracy: 61.17%\n",
        "Epoch [7/10], Training Loss: 0.613, Validation Accuracy: 61.35%\n",
        "Epoch [8/10], Training Loss: 0.583, Validation Accuracy: 61.53%\n",
        "Epoch [9/10], Training Loss: 0.571, Validation Accuracy: 61.34%\n",
        "Epoch [10/10], Training Loss: 0.547, Validation Accuracy: 61.11%\n",
        "Epoch [1/10], Training Loss: 0.843, Validation Accuracy: 60.85%\n",
        "Epoch [2/10], Training Loss: 0.752, Validation Accuracy: 61.34%\n",
        "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 61.19%\n",
        "Epoch [4/10], Training Loss: 0.673, Validation Accuracy: 61.74%\n",
        "Epoch [5/10], Training Loss: 0.640, Validation Accuracy: 61.43%\n",
        "Epoch [6/10], Training Loss: 0.617, Validation Accuracy: 61.28%\n",
        "Epoch [7/10], Training Loss: 0.587, Validation Accuracy: 61.24%\n",
        "Epoch [8/10], Training Loss: 0.570, Validation Accuracy: 61.96%\n",
        "Epoch [9/10], Training Loss: 0.548, Validation Accuracy: 60.80%\n",
        "Epoch [10/10], Training Loss: 0.525, Validation Accuracy: 60.73%\n",
        "Epoch [1/10], Training Loss: 0.846, Validation Accuracy: 61.52%\n",
        "Epoch [2/10], Training Loss: 0.744, Validation Accuracy: 61.81%\n",
        "Epoch [3/10], Training Loss: 0.686, Validation Accuracy: 61.12%\n",
        "Epoch [4/10], Training Loss: 0.657, Validation Accuracy: 61.17%\n",
        "Epoch [5/10], Training Loss: 0.618, Validation Accuracy: 62.16%\n",
        "Epoch [6/10], Training Loss: 0.580, Validation Accuracy: 61.44%\n",
        "Epoch [7/10], Training Loss: 0.578, Validation Accuracy: 61.45%\n",
        "Epoch [8/10], Training Loss: 0.550, Validation Accuracy: 60.98%\n",
        "Epoch [9/10], Training Loss: 0.527, Validation Accuracy: 61.04%\n",
        "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 60.98%\n",
        "Epoch [1/10], Training Loss: 0.865, Validation Accuracy: 60.35%\n",
        "Epoch [2/10], Training Loss: 0.756, Validation Accuracy: 61.40%\n",
        "Epoch [3/10], Training Loss: 0.698, Validation Accuracy: 61.11%\n",
        "Epoch [4/10], Training Loss: 0.656, Validation Accuracy: 62.16%\n",
        "Epoch [5/10], Training Loss: 0.622, Validation Accuracy: 61.48%\n",
        "Epoch [6/10], Training Loss: 0.602, Validation Accuracy: 61.32%\n",
        "Epoch [7/10], Training Loss: 0.567, Validation Accuracy: 61.46%\n",
        "Epoch [8/10], Training Loss: 0.544, Validation Accuracy: 61.39%\n",
        "Epoch [9/10], Training Loss: 0.528, Validation Accuracy: 61.07%\n",
        "Epoch [10/10], Training Loss: 0.514, Validation Accuracy: 61.23%\n",
        "Epoch [1/10], Training Loss: 0.815, Validation Accuracy: 61.70%\n",
        "Epoch [2/10], Training Loss: 0.702, Validation Accuracy: 61.94%\n",
        "Epoch [3/10], Training Loss: 0.656, Validation Accuracy: 61.65%\n",
        "Epoch [4/10], Training Loss: 0.613, Validation Accuracy: 61.22%\n",
        "Epoch [5/10], Training Loss: 0.582, Validation Accuracy: 60.32%\n",
        "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 61.66%\n",
        "Epoch [7/10], Training Loss: 0.519, Validation Accuracy: 60.73%\n",
        "Epoch [8/10], Training Loss: 0.504, Validation Accuracy: 61.12%\n",
        "Epoch [9/10], Training Loss: 0.489, Validation Accuracy: 61.75%\n",
        "Epoch [10/10], Training Loss: 0.463, Validation Accuracy: 61.75%\n",
        "Epoch [1/10], Training Loss: 0.812, Validation Accuracy: 61.10%\n",
        "Epoch [2/10], Training Loss: 0.703, Validation Accuracy: 60.93%\n",
        "Epoch [3/10], Training Loss: 0.655, Validation Accuracy: 60.82%\n",
        "Epoch [4/10], Training Loss: 0.611, Validation Accuracy: 60.87%\n",
        "Epoch [5/10], Training Loss: 0.581, Validation Accuracy: 61.81%\n",
        "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 61.67%\n",
        "Epoch [7/10], Training Loss: 0.516, Validation Accuracy: 61.21%\n",
        "Epoch [8/10], Training Loss: 0.497, Validation Accuracy: 61.46%\n",
        "Epoch [9/10], Training Loss: 0.470, Validation Accuracy: 61.34%\n",
        "Epoch [10/10], Training Loss: 0.456, Validation Accuracy: 61.27%\n",
        "Epoch [1/10], Training Loss: 0.813, Validation Accuracy: 61.48%\n",
        "Epoch [2/10], Training Loss: 0.692, Validation Accuracy: 60.95%\n",
        "Epoch [3/10], Training Loss: 0.634, Validation Accuracy: 61.09%\n",
        "Epoch [4/10], Training Loss: 0.585, Validation Accuracy: 61.35%\n",
        "Epoch [5/10], Training Loss: 0.555, Validation Accuracy: 61.41%\n",
        "Epoch [6/10], Training Loss: 0.521, Validation Accuracy: 61.13%\n",
        "Epoch [7/10], Training Loss: 0.502, Validation Accuracy: 61.38%\n",
        "Epoch [8/10], Training Loss: 0.472, Validation Accuracy: 60.05%\n",
        "Epoch [9/10], Training Loss: 0.461, Validation Accuracy: 61.21%\n",
        "Epoch [10/10], Training Loss: 0.433, Validation Accuracy: 61.59%\n",
        "Epoch [1/10], Training Loss: 0.807, Validation Accuracy: 61.43%\n",
        "Epoch [2/10], Training Loss: 0.687, Validation Accuracy: 61.22%\n",
        "Epoch [3/10], Training Loss: 0.609, Validation Accuracy: 61.78%\n",
        "Epoch [4/10], Training Loss: 0.567, Validation Accuracy: 62.06%\n",
        "Epoch [5/10], Training Loss: 0.533, Validation Accuracy: 60.65%\n",
        "Epoch [6/10], Training Loss: 0.498, Validation Accuracy: 60.99%\n",
        "Epoch [7/10], Training Loss: 0.479, Validation Accuracy: 61.32%\n",
        "Epoch [8/10], Training Loss: 0.447, Validation Accuracy: 60.97%\n",
        "Epoch [9/10], Training Loss: 0.441, Validation Accuracy: 60.80%\n",
        "Epoch [10/10], Training Loss: 0.415, Validation Accuracy: 60.81%\n",
        "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 60.77%\n",
        "Epoch [2/10], Training Loss: 0.699, Validation Accuracy: 61.00%\n",
        "Epoch [3/10], Training Loss: 0.629, Validation Accuracy: 60.61%\n",
        "Epoch [4/10], Training Loss: 0.579, Validation Accuracy: 60.94%\n",
        "Epoch [5/10], Training Loss: 0.538, Validation Accuracy: 60.41%\n",
        "Epoch [6/10], Training Loss: 0.515, Validation Accuracy: 61.32%\n",
        "Epoch [7/10], Training Loss: 0.479, Validation Accuracy: 61.26%\n",
        "Epoch [8/10], Training Loss: 0.453, Validation Accuracy: 60.49%\n",
        "Epoch [9/10], Training Loss: 0.439, Validation Accuracy: 60.61%\n",
        "Epoch [10/10], Training Loss: 0.425, Validation Accuracy: 60.87%\n",
        "Epoch [1/10], Training Loss: 0.789, Validation Accuracy: 60.64%\n",
        "Epoch [2/10], Training Loss: 0.645, Validation Accuracy: 61.18%\n",
        "Epoch [3/10], Training Loss: 0.568, Validation Accuracy: 61.99%\n",
        "Epoch [4/10], Training Loss: 0.523, Validation Accuracy: 61.66%\n",
        "Epoch [5/10], Training Loss: 0.497, Validation Accuracy: 61.53%\n",
        "Epoch [6/10], Training Loss: 0.462, Validation Accuracy: 61.01%\n",
        "Epoch [7/10], Training Loss: 0.439, Validation Accuracy: 61.53%\n",
        "Epoch [8/10], Training Loss: 0.417, Validation Accuracy: 60.99%\n",
        "Epoch [9/10], Training Loss: 0.394, Validation Accuracy: 61.43%\n",
        "Epoch [10/10], Training Loss: 0.375, Validation Accuracy: 61.10%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_toe-2YRsAlA",
        "outputId": "f53848c9-d9c6-44c7-b6bd-9adfe138f968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [12.83, 13.9, 13.75, 13.71, 13.32, 13.18, 13.15, 14.79, 17.14, 18.26, 20.06, 21.04, 21.98, 23.01, 23.58, 24.17, 24.46, 25.73, 26.0, 26.6, 26.43, 27.12, 28.09, 28.11, 27.71, 28.89, 29.4, 30.1, 29.78, 30.9, 31.06, 32.07, 31.93, 32.71, 33.24, 33.91, 34.23, 33.68, 35.49, 35.47, 35.75, 36.79, 36.79, 37.37, 38.2, 37.31, 38.47, 38.77, 38.81, 39.65, 38.48, 39.89, 39.54, 40.1, 39.66, 41.13, 41.32, 41.44, 41.23, 42.28, 41.58, 42.96, 42.1, 42.86, 42.91, 43.72, 44.11, 43.01, 44.37, 44.1, 45.51, 45.89, 45.66, 45.72, 45.72, 46.09, 46.52, 45.32, 46.49, 46.78, 47.17, 46.54, 46.79, 46.64, 47.37, 47.88, 47.05, 46.0, 48.17, 47.68, 48.28, 48.63, 48.63, 49.21, 48.54, 49.03, 49.31, 48.64, 49.59, 48.83, 49.6, 49.36, 49.51, 50.27, 49.17, 50.2, 50.7, 50.83, 51.04, 50.9, 51.12, 50.66, 50.79, 50.91, 51.36, 51.19, 51.97, 51.98, 51.67, 52.3, 52.68, 52.61, 51.6, 52.73, 52.83, 52.9, 53.29, 53.07, 53.25, 53.47, 53.27, 53.21, 53.53, 53.14, 54.4, 52.33, 54.25, 54.28, 54.35, 54.45, 54.52, 53.38, 54.87, 54.41, 54.75, 54.81, 54.75, 54.9, 55.68, 54.96, 54.87, 56.23, 56.28, 55.73, 55.92, 55.51, 56.17, 55.57, 55.83, 55.4, 55.89, 56.42, 56.45, 56.49, 56.34, 56.5, 56.75, 56.79, 56.88, 56.7, 56.23, 56.43, 56.47, 56.71, 57.21, 56.91, 57.27, 57.24, 57.29, 57.5, 57.58, 57.52, 57.49, 56.93, 57.93, 57.37, 58.06, 57.28, 57.57, 58.07, 58.75, 58.48, 58.31, 58.31, 58.35, 58.96, 58.32, 57.58, 57.3, 58.13, 59.08, 58.56, 58.9, 58.67, 58.49, 58.01, 58.17, 58.64, 58.74, 58.39, 58.87, 58.27, 58.58, 58.95, 58.32, 58.73, 59.02, 59.24, 58.19, 59.16, 59.55, 59.81, 58.97, 58.43, 59.32, 58.98, 59.76, 59.42, 59.14, 58.97, 58.29, 59.35, 58.38, 59.1, 60.11, 59.97, 59.37, 59.12, 59.25, 58.53, 60.08, 60.2, 60.19, 60.49, 60.72, 60.75, 59.86, 60.12, 59.41, 59.8, 60.3, 59.37, 60.04, 60.51, 60.59, 60.5, 60.47, 60.93, 59.89, 60.48, 59.69, 60.16, 59.19, 60.77, 60.83, 60.72, 60.36, 59.89, 59.98, 60.78, 61.04, 60.81, 60.25, 59.9, 60.73, 60.1, 60.6, 60.5, 60.37, 60.26, 60.41, 60.18, 60.56, 60.99, 60.22, 60.36, 60.31, 60.62, 59.99, 60.52, 61.42, 60.86, 61.14, 60.9, 61.34, 61.17, 61.31, 61.38, 60.25, 60.75, 61.25, 61.02, 59.6, 60.75, 61.65, 60.8, 61.03, 60.56, 60.8, 60.57, 59.89, 60.47, 60.83, 60.61, 61.53, 60.49, 61.39, 61.31, 60.87, 60.91, 60.94, 61.39, 61.7, 61.66, 61.23, 60.52, 61.48, 61.02, 61.23, 60.57, 61.22, 61.37, 61.61, 60.42, 61.59, 60.78, 61.23, 61.23, 61.06, 61.02, 61.44, 61.63, 61.93, 61.38, 61.81, 61.75, 61.79, 61.84, 61.51, 61.23, 61.17, 61.27, 60.24, 61.4, 61.6, 61.22, 61.39, 60.72, 61.16, 61.21, 61.11, 61.05, 61.38, 61.3, 61.46, 61.17, 60.98, 61.24, 60.46, 60.96, 61.34, 60.97, 61.84, 61.24, 61.8, 61.47, 61.38, 60.26, 61.08, 60.96, 61.35, 61.31, 61.45, 60.96, 61.9, 61.33, 61.78, 61.09, 61.14, 60.57, 61.85, 61.66, 61.97, 61.22, 61.33, 61.2, 61.67, 61.61, 61.8, 61.71, 60.96, 62.38, 61.84, 62.0, 61.51, 61.17, 61.35, 61.53, 61.34, 61.11, 60.85, 61.34, 61.19, 61.74, 61.43, 61.28, 61.24, 61.96, 60.8, 60.73, 61.52, 61.81, 61.12, 61.17, 62.16, 61.44, 61.45, 60.98, 61.04, 60.98, 60.35, 61.4, 61.11, 62.16, 61.48, 61.32, 61.46, 61.39, 61.07, 61.23, 61.7, 61.94, 61.65, 61.22, 60.32, 61.66, 60.73, 61.12, 61.75, 61.75, 61.1, 60.93, 60.82, 60.87, 61.81, 61.67, 61.21, 61.46, 61.34, 61.27, 61.48, 60.95, 61.09, 61.35, 61.41, 61.13, 61.38, 60.05, 61.21, 61.59, 61.43, 61.22, 61.78, 62.06, 60.65, 60.99, 61.32, 60.97, 60.8, 60.81, 60.77, 61.0, 60.61, 60.94, 60.41, 61.32, 61.26, 60.49, 60.61, 60.87, 60.64, 61.18, 61.99, 61.66, 61.53, 61.01, 61.53, 60.99, 61.43, 61.1]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X2bY0HpPpF1",
        "outputId": "645de767-d3d5-42ab-d6de-2d22c9b26fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6091 6023 6006 5961 5847 6033 6188 5968 5976 5907]\n",
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.22%\n",
            "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.22%\n",
            "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.22%\n",
            "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 10.22%\n",
            "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 10.22%\n",
            "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 10.27%\n",
            "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 10.44%\n",
            "Epoch [8/10], Training Loss: 2.297, Validation Accuracy: 11.48%\n",
            "Epoch [9/10], Training Loss: 2.295, Validation Accuracy: 12.79%\n",
            "Epoch [10/10], Training Loss: 2.293, Validation Accuracy: 16.07%\n",
            "Epoch [1/10], Training Loss: 2.290, Validation Accuracy: 16.34%\n",
            "Epoch [2/10], Training Loss: 2.284, Validation Accuracy: 16.02%\n",
            "Epoch [3/10], Training Loss: 2.274, Validation Accuracy: 15.54%\n",
            "Epoch [4/10], Training Loss: 2.261, Validation Accuracy: 15.61%\n",
            "Epoch [5/10], Training Loss: 2.248, Validation Accuracy: 16.07%\n",
            "Epoch [6/10], Training Loss: 2.234, Validation Accuracy: 16.72%\n",
            "Epoch [7/10], Training Loss: 2.220, Validation Accuracy: 18.66%\n",
            "Epoch [8/10], Training Loss: 2.206, Validation Accuracy: 20.07%\n",
            "Epoch [9/10], Training Loss: 2.192, Validation Accuracy: 20.51%\n",
            "Epoch [10/10], Training Loss: 2.179, Validation Accuracy: 21.34%\n",
            "Epoch [1/10], Training Loss: 2.177, Validation Accuracy: 21.24%\n",
            "Epoch [2/10], Training Loss: 2.163, Validation Accuracy: 21.46%\n",
            "Epoch [3/10], Training Loss: 2.150, Validation Accuracy: 23.00%\n",
            "Epoch [4/10], Training Loss: 2.139, Validation Accuracy: 23.20%\n",
            "Epoch [5/10], Training Loss: 2.129, Validation Accuracy: 23.93%\n",
            "Epoch [6/10], Training Loss: 2.119, Validation Accuracy: 24.03%\n",
            "Epoch [7/10], Training Loss: 2.108, Validation Accuracy: 24.67%\n",
            "Epoch [8/10], Training Loss: 2.097, Validation Accuracy: 25.00%\n",
            "Epoch [9/10], Training Loss: 2.085, Validation Accuracy: 25.54%\n",
            "Epoch [10/10], Training Loss: 2.073, Validation Accuracy: 25.19%\n",
            "Epoch [1/10], Training Loss: 2.058, Validation Accuracy: 26.08%\n",
            "Epoch [2/10], Training Loss: 2.045, Validation Accuracy: 26.84%\n",
            "Epoch [3/10], Training Loss: 2.033, Validation Accuracy: 27.22%\n",
            "Epoch [4/10], Training Loss: 2.021, Validation Accuracy: 27.54%\n",
            "Epoch [5/10], Training Loss: 2.009, Validation Accuracy: 27.62%\n",
            "Epoch [6/10], Training Loss: 1.998, Validation Accuracy: 27.70%\n",
            "Epoch [7/10], Training Loss: 1.986, Validation Accuracy: 28.27%\n",
            "Epoch [8/10], Training Loss: 1.975, Validation Accuracy: 28.65%\n",
            "Epoch [9/10], Training Loss: 1.966, Validation Accuracy: 27.64%\n",
            "Epoch [10/10], Training Loss: 1.954, Validation Accuracy: 29.48%\n",
            "Epoch [1/10], Training Loss: 1.941, Validation Accuracy: 28.98%\n",
            "Epoch [2/10], Training Loss: 1.926, Validation Accuracy: 29.39%\n",
            "Epoch [3/10], Training Loss: 1.911, Validation Accuracy: 29.50%\n",
            "Epoch [4/10], Training Loss: 1.898, Validation Accuracy: 30.02%\n",
            "Epoch [5/10], Training Loss: 1.884, Validation Accuracy: 30.38%\n",
            "Epoch [6/10], Training Loss: 1.869, Validation Accuracy: 30.97%\n",
            "Epoch [7/10], Training Loss: 1.854, Validation Accuracy: 31.25%\n",
            "Epoch [8/10], Training Loss: 1.838, Validation Accuracy: 31.10%\n",
            "Epoch [9/10], Training Loss: 1.822, Validation Accuracy: 31.92%\n",
            "Epoch [10/10], Training Loss: 1.806, Validation Accuracy: 32.82%\n",
            "Epoch [1/10], Training Loss: 1.812, Validation Accuracy: 33.25%\n",
            "Epoch [2/10], Training Loss: 1.793, Validation Accuracy: 34.31%\n",
            "Epoch [3/10], Training Loss: 1.774, Validation Accuracy: 34.87%\n",
            "Epoch [4/10], Training Loss: 1.757, Validation Accuracy: 35.19%\n",
            "Epoch [5/10], Training Loss: 1.740, Validation Accuracy: 35.66%\n",
            "Epoch [6/10], Training Loss: 1.723, Validation Accuracy: 36.75%\n",
            "Epoch [7/10], Training Loss: 1.711, Validation Accuracy: 36.15%\n",
            "Epoch [8/10], Training Loss: 1.690, Validation Accuracy: 37.58%\n",
            "Epoch [9/10], Training Loss: 1.679, Validation Accuracy: 38.20%\n",
            "Epoch [10/10], Training Loss: 1.660, Validation Accuracy: 38.61%\n",
            "Epoch [1/10], Training Loss: 1.679, Validation Accuracy: 38.25%\n",
            "Epoch [2/10], Training Loss: 1.666, Validation Accuracy: 38.87%\n",
            "Epoch [3/10], Training Loss: 1.645, Validation Accuracy: 39.57%\n",
            "Epoch [4/10], Training Loss: 1.633, Validation Accuracy: 39.27%\n",
            "Epoch [5/10], Training Loss: 1.616, Validation Accuracy: 39.75%\n",
            "Epoch [6/10], Training Loss: 1.604, Validation Accuracy: 40.70%\n",
            "Epoch [7/10], Training Loss: 1.589, Validation Accuracy: 40.68%\n",
            "Epoch [8/10], Training Loss: 1.582, Validation Accuracy: 41.86%\n",
            "Epoch [9/10], Training Loss: 1.571, Validation Accuracy: 42.18%\n",
            "Epoch [10/10], Training Loss: 1.557, Validation Accuracy: 42.33%\n",
            "Epoch [1/10], Training Loss: 1.605, Validation Accuracy: 42.83%\n",
            "Epoch [2/10], Training Loss: 1.587, Validation Accuracy: 42.52%\n",
            "Epoch [3/10], Training Loss: 1.573, Validation Accuracy: 42.18%\n",
            "Epoch [4/10], Training Loss: 1.564, Validation Accuracy: 43.16%\n",
            "Epoch [5/10], Training Loss: 1.555, Validation Accuracy: 43.46%\n",
            "Epoch [6/10], Training Loss: 1.544, Validation Accuracy: 44.19%\n",
            "Epoch [7/10], Training Loss: 1.530, Validation Accuracy: 43.57%\n",
            "Epoch [8/10], Training Loss: 1.525, Validation Accuracy: 44.77%\n",
            "Epoch [9/10], Training Loss: 1.512, Validation Accuracy: 44.80%\n",
            "Epoch [10/10], Training Loss: 1.503, Validation Accuracy: 45.07%\n",
            "Epoch [1/10], Training Loss: 1.503, Validation Accuracy: 44.45%\n",
            "Epoch [2/10], Training Loss: 1.491, Validation Accuracy: 45.18%\n",
            "Epoch [3/10], Training Loss: 1.472, Validation Accuracy: 44.21%\n",
            "Epoch [4/10], Training Loss: 1.474, Validation Accuracy: 45.92%\n",
            "Epoch [5/10], Training Loss: 1.453, Validation Accuracy: 46.38%\n",
            "Epoch [6/10], Training Loss: 1.443, Validation Accuracy: 46.53%\n",
            "Epoch [7/10], Training Loss: 1.434, Validation Accuracy: 46.71%\n",
            "Epoch [8/10], Training Loss: 1.427, Validation Accuracy: 45.55%\n",
            "Epoch [9/10], Training Loss: 1.425, Validation Accuracy: 46.73%\n",
            "Epoch [10/10], Training Loss: 1.411, Validation Accuracy: 47.01%\n",
            "Epoch [1/10], Training Loss: 1.437, Validation Accuracy: 47.37%\n",
            "Epoch [2/10], Training Loss: 1.420, Validation Accuracy: 48.08%\n",
            "Epoch [3/10], Training Loss: 1.411, Validation Accuracy: 47.86%\n",
            "Epoch [4/10], Training Loss: 1.401, Validation Accuracy: 48.37%\n",
            "Epoch [5/10], Training Loss: 1.396, Validation Accuracy: 47.69%\n",
            "Epoch [6/10], Training Loss: 1.382, Validation Accuracy: 48.61%\n",
            "Epoch [7/10], Training Loss: 1.374, Validation Accuracy: 48.93%\n",
            "Epoch [8/10], Training Loss: 1.365, Validation Accuracy: 49.18%\n",
            "Epoch [9/10], Training Loss: 1.357, Validation Accuracy: 48.80%\n",
            "Epoch [10/10], Training Loss: 1.344, Validation Accuracy: 49.24%\n",
            "Epoch [1/10], Training Loss: 1.401, Validation Accuracy: 49.76%\n",
            "Epoch [2/10], Training Loss: 1.376, Validation Accuracy: 49.80%\n",
            "Epoch [3/10], Training Loss: 1.368, Validation Accuracy: 48.89%\n",
            "Epoch [4/10], Training Loss: 1.362, Validation Accuracy: 48.65%\n",
            "Epoch [5/10], Training Loss: 1.348, Validation Accuracy: 48.97%\n",
            "Epoch [6/10], Training Loss: 1.344, Validation Accuracy: 49.77%\n",
            "Epoch [7/10], Training Loss: 1.339, Validation Accuracy: 49.89%\n",
            "Epoch [8/10], Training Loss: 1.324, Validation Accuracy: 50.12%\n",
            "Epoch [9/10], Training Loss: 1.320, Validation Accuracy: 49.70%\n",
            "Epoch [10/10], Training Loss: 1.314, Validation Accuracy: 49.97%\n",
            "Epoch [1/10], Training Loss: 1.373, Validation Accuracy: 49.57%\n",
            "Epoch [2/10], Training Loss: 1.361, Validation Accuracy: 50.33%\n",
            "Epoch [3/10], Training Loss: 1.347, Validation Accuracy: 50.78%\n",
            "Epoch [4/10], Training Loss: 1.332, Validation Accuracy: 50.65%\n",
            "Epoch [5/10], Training Loss: 1.327, Validation Accuracy: 51.66%\n",
            "Epoch [6/10], Training Loss: 1.318, Validation Accuracy: 50.50%\n",
            "Epoch [7/10], Training Loss: 1.304, Validation Accuracy: 50.83%\n",
            "Epoch [8/10], Training Loss: 1.297, Validation Accuracy: 51.72%\n",
            "Epoch [9/10], Training Loss: 1.292, Validation Accuracy: 51.48%\n",
            "Epoch [10/10], Training Loss: 1.285, Validation Accuracy: 51.55%\n",
            "Epoch [1/10], Training Loss: 1.372, Validation Accuracy: 52.04%\n",
            "Epoch [2/10], Training Loss: 1.357, Validation Accuracy: 50.80%\n",
            "Epoch [3/10], Training Loss: 1.342, Validation Accuracy: 51.47%\n",
            "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 52.32%\n",
            "Epoch [5/10], Training Loss: 1.316, Validation Accuracy: 51.89%\n",
            "Epoch [6/10], Training Loss: 1.305, Validation Accuracy: 52.09%\n",
            "Epoch [7/10], Training Loss: 1.300, Validation Accuracy: 52.64%\n",
            "Epoch [8/10], Training Loss: 1.293, Validation Accuracy: 51.79%\n",
            "Epoch [9/10], Training Loss: 1.281, Validation Accuracy: 52.73%\n",
            "Epoch [10/10], Training Loss: 1.273, Validation Accuracy: 52.31%\n",
            "Epoch [1/10], Training Loss: 1.307, Validation Accuracy: 52.50%\n",
            "Epoch [2/10], Training Loss: 1.296, Validation Accuracy: 52.46%\n",
            "Epoch [3/10], Training Loss: 1.279, Validation Accuracy: 53.36%\n",
            "Epoch [4/10], Training Loss: 1.269, Validation Accuracy: 53.10%\n",
            "Epoch [5/10], Training Loss: 1.258, Validation Accuracy: 53.25%\n",
            "Epoch [6/10], Training Loss: 1.248, Validation Accuracy: 53.31%\n",
            "Epoch [7/10], Training Loss: 1.237, Validation Accuracy: 53.36%\n",
            "Epoch [8/10], Training Loss: 1.227, Validation Accuracy: 53.26%\n",
            "Epoch [9/10], Training Loss: 1.219, Validation Accuracy: 53.60%\n",
            "Epoch [10/10], Training Loss: 1.209, Validation Accuracy: 53.26%\n",
            "Epoch [1/10], Training Loss: 1.273, Validation Accuracy: 54.31%\n",
            "Epoch [2/10], Training Loss: 1.265, Validation Accuracy: 53.59%\n",
            "Epoch [3/10], Training Loss: 1.238, Validation Accuracy: 53.57%\n",
            "Epoch [4/10], Training Loss: 1.229, Validation Accuracy: 52.78%\n",
            "Epoch [5/10], Training Loss: 1.212, Validation Accuracy: 54.09%\n",
            "Epoch [6/10], Training Loss: 1.198, Validation Accuracy: 54.12%\n",
            "Epoch [7/10], Training Loss: 1.192, Validation Accuracy: 54.07%\n",
            "Epoch [8/10], Training Loss: 1.187, Validation Accuracy: 54.17%\n",
            "Epoch [9/10], Training Loss: 1.178, Validation Accuracy: 53.90%\n",
            "Epoch [10/10], Training Loss: 1.172, Validation Accuracy: 54.34%\n",
            "Epoch [1/10], Training Loss: 1.254, Validation Accuracy: 53.63%\n",
            "Epoch [2/10], Training Loss: 1.235, Validation Accuracy: 53.90%\n",
            "Epoch [3/10], Training Loss: 1.233, Validation Accuracy: 54.70%\n",
            "Epoch [4/10], Training Loss: 1.212, Validation Accuracy: 54.55%\n",
            "Epoch [5/10], Training Loss: 1.198, Validation Accuracy: 54.01%\n",
            "Epoch [6/10], Training Loss: 1.198, Validation Accuracy: 54.83%\n",
            "Epoch [7/10], Training Loss: 1.177, Validation Accuracy: 54.63%\n",
            "Epoch [8/10], Training Loss: 1.172, Validation Accuracy: 53.69%\n",
            "Epoch [9/10], Training Loss: 1.170, Validation Accuracy: 55.36%\n",
            "Epoch [10/10], Training Loss: 1.156, Validation Accuracy: 55.04%\n",
            "Epoch [1/10], Training Loss: 1.241, Validation Accuracy: 54.65%\n",
            "Epoch [2/10], Training Loss: 1.223, Validation Accuracy: 55.58%\n",
            "Epoch [3/10], Training Loss: 1.201, Validation Accuracy: 55.52%\n",
            "Epoch [4/10], Training Loss: 1.190, Validation Accuracy: 55.71%\n",
            "Epoch [5/10], Training Loss: 1.175, Validation Accuracy: 55.39%\n",
            "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 55.39%\n",
            "Epoch [7/10], Training Loss: 1.156, Validation Accuracy: 55.80%\n",
            "Epoch [8/10], Training Loss: 1.143, Validation Accuracy: 55.59%\n",
            "Epoch [9/10], Training Loss: 1.131, Validation Accuracy: 55.73%\n",
            "Epoch [10/10], Training Loss: 1.125, Validation Accuracy: 55.11%\n",
            "Epoch [1/10], Training Loss: 1.244, Validation Accuracy: 55.91%\n",
            "Epoch [2/10], Training Loss: 1.218, Validation Accuracy: 56.35%\n",
            "Epoch [3/10], Training Loss: 1.204, Validation Accuracy: 55.29%\n",
            "Epoch [4/10], Training Loss: 1.186, Validation Accuracy: 55.67%\n",
            "Epoch [5/10], Training Loss: 1.171, Validation Accuracy: 55.44%\n",
            "Epoch [6/10], Training Loss: 1.158, Validation Accuracy: 56.12%\n",
            "Epoch [7/10], Training Loss: 1.145, Validation Accuracy: 55.47%\n",
            "Epoch [8/10], Training Loss: 1.141, Validation Accuracy: 56.12%\n",
            "Epoch [9/10], Training Loss: 1.130, Validation Accuracy: 55.27%\n",
            "Epoch [10/10], Training Loss: 1.123, Validation Accuracy: 56.23%\n",
            "Epoch [1/10], Training Loss: 1.198, Validation Accuracy: 56.08%\n",
            "Epoch [2/10], Training Loss: 1.174, Validation Accuracy: 56.88%\n",
            "Epoch [3/10], Training Loss: 1.154, Validation Accuracy: 56.74%\n",
            "Epoch [4/10], Training Loss: 1.139, Validation Accuracy: 56.61%\n",
            "Epoch [5/10], Training Loss: 1.129, Validation Accuracy: 56.43%\n",
            "Epoch [6/10], Training Loss: 1.111, Validation Accuracy: 56.95%\n",
            "Epoch [7/10], Training Loss: 1.107, Validation Accuracy: 56.38%\n",
            "Epoch [8/10], Training Loss: 1.086, Validation Accuracy: 57.09%\n",
            "Epoch [9/10], Training Loss: 1.086, Validation Accuracy: 57.26%\n",
            "Epoch [10/10], Training Loss: 1.070, Validation Accuracy: 56.72%\n",
            "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 57.23%\n",
            "Epoch [2/10], Training Loss: 1.141, Validation Accuracy: 57.44%\n",
            "Epoch [3/10], Training Loss: 1.120, Validation Accuracy: 57.23%\n",
            "Epoch [4/10], Training Loss: 1.107, Validation Accuracy: 56.99%\n",
            "Epoch [5/10], Training Loss: 1.089, Validation Accuracy: 57.16%\n",
            "Epoch [6/10], Training Loss: 1.077, Validation Accuracy: 57.10%\n",
            "Epoch [7/10], Training Loss: 1.067, Validation Accuracy: 57.01%\n",
            "Epoch [8/10], Training Loss: 1.056, Validation Accuracy: 56.99%\n",
            "Epoch [9/10], Training Loss: 1.045, Validation Accuracy: 57.78%\n",
            "Epoch [10/10], Training Loss: 1.031, Validation Accuracy: 57.30%\n",
            "Epoch [1/10], Training Loss: 1.166, Validation Accuracy: 57.42%\n",
            "Epoch [2/10], Training Loss: 1.124, Validation Accuracy: 57.84%\n",
            "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 57.43%\n",
            "Epoch [4/10], Training Loss: 1.094, Validation Accuracy: 57.56%\n",
            "Epoch [5/10], Training Loss: 1.076, Validation Accuracy: 58.04%\n",
            "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.21%\n",
            "Epoch [7/10], Training Loss: 1.054, Validation Accuracy: 58.12%\n",
            "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 57.86%\n",
            "Epoch [9/10], Training Loss: 1.032, Validation Accuracy: 57.62%\n",
            "Epoch [10/10], Training Loss: 1.023, Validation Accuracy: 57.36%\n",
            "Epoch [1/10], Training Loss: 1.147, Validation Accuracy: 58.14%\n",
            "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 57.97%\n",
            "Epoch [3/10], Training Loss: 1.089, Validation Accuracy: 58.24%\n",
            "Epoch [4/10], Training Loss: 1.068, Validation Accuracy: 58.15%\n",
            "Epoch [5/10], Training Loss: 1.062, Validation Accuracy: 58.05%\n",
            "Epoch [6/10], Training Loss: 1.048, Validation Accuracy: 58.34%\n",
            "Epoch [7/10], Training Loss: 1.031, Validation Accuracy: 57.45%\n",
            "Epoch [8/10], Training Loss: 1.028, Validation Accuracy: 58.09%\n",
            "Epoch [9/10], Training Loss: 1.011, Validation Accuracy: 58.78%\n",
            "Epoch [10/10], Training Loss: 0.997, Validation Accuracy: 57.85%\n",
            "Epoch [1/10], Training Loss: 1.161, Validation Accuracy: 57.93%\n",
            "Epoch [2/10], Training Loss: 1.113, Validation Accuracy: 58.49%\n",
            "Epoch [3/10], Training Loss: 1.093, Validation Accuracy: 57.52%\n",
            "Epoch [4/10], Training Loss: 1.074, Validation Accuracy: 57.97%\n",
            "Epoch [5/10], Training Loss: 1.052, Validation Accuracy: 58.40%\n",
            "Epoch [6/10], Training Loss: 1.043, Validation Accuracy: 59.01%\n",
            "Epoch [7/10], Training Loss: 1.027, Validation Accuracy: 58.55%\n",
            "Epoch [8/10], Training Loss: 1.020, Validation Accuracy: 58.04%\n",
            "Epoch [9/10], Training Loss: 1.005, Validation Accuracy: 58.28%\n",
            "Epoch [10/10], Training Loss: 0.999, Validation Accuracy: 58.60%\n",
            "Epoch [1/10], Training Loss: 1.110, Validation Accuracy: 58.98%\n",
            "Epoch [2/10], Training Loss: 1.068, Validation Accuracy: 59.28%\n",
            "Epoch [3/10], Training Loss: 1.056, Validation Accuracy: 58.76%\n",
            "Epoch [4/10], Training Loss: 1.028, Validation Accuracy: 59.36%\n",
            "Epoch [5/10], Training Loss: 1.006, Validation Accuracy: 58.62%\n",
            "Epoch [6/10], Training Loss: 0.998, Validation Accuracy: 58.78%\n",
            "Epoch [7/10], Training Loss: 0.985, Validation Accuracy: 59.40%\n",
            "Epoch [8/10], Training Loss: 0.967, Validation Accuracy: 59.32%\n",
            "Epoch [9/10], Training Loss: 0.953, Validation Accuracy: 58.93%\n",
            "Epoch [10/10], Training Loss: 0.945, Validation Accuracy: 58.94%\n",
            "Epoch [1/10], Training Loss: 1.092, Validation Accuracy: 58.97%\n",
            "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.30%\n",
            "Epoch [3/10], Training Loss: 1.019, Validation Accuracy: 58.81%\n",
            "Epoch [4/10], Training Loss: 1.011, Validation Accuracy: 59.17%\n",
            "Epoch [5/10], Training Loss: 0.988, Validation Accuracy: 59.51%\n",
            "Epoch [6/10], Training Loss: 0.966, Validation Accuracy: 59.50%\n",
            "Epoch [7/10], Training Loss: 0.953, Validation Accuracy: 59.45%\n",
            "Epoch [8/10], Training Loss: 0.935, Validation Accuracy: 59.39%\n",
            "Epoch [9/10], Training Loss: 0.932, Validation Accuracy: 59.49%\n",
            "Epoch [10/10], Training Loss: 0.918, Validation Accuracy: 58.92%\n",
            "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.24%\n",
            "Epoch [2/10], Training Loss: 1.027, Validation Accuracy: 59.17%\n",
            "Epoch [3/10], Training Loss: 1.009, Validation Accuracy: 59.61%\n",
            "Epoch [4/10], Training Loss: 0.983, Validation Accuracy: 59.28%\n",
            "Epoch [5/10], Training Loss: 0.974, Validation Accuracy: 58.87%\n",
            "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 59.42%\n",
            "Epoch [7/10], Training Loss: 0.938, Validation Accuracy: 59.14%\n",
            "Epoch [8/10], Training Loss: 0.923, Validation Accuracy: 59.03%\n",
            "Epoch [9/10], Training Loss: 0.919, Validation Accuracy: 59.28%\n",
            "Epoch [10/10], Training Loss: 0.907, Validation Accuracy: 59.20%\n",
            "Epoch [1/10], Training Loss: 1.064, Validation Accuracy: 60.01%\n",
            "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 60.28%\n",
            "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 60.32%\n",
            "Epoch [4/10], Training Loss: 0.980, Validation Accuracy: 59.92%\n",
            "Epoch [5/10], Training Loss: 0.953, Validation Accuracy: 60.24%\n",
            "Epoch [6/10], Training Loss: 0.938, Validation Accuracy: 60.16%\n",
            "Epoch [7/10], Training Loss: 0.923, Validation Accuracy: 59.10%\n",
            "Epoch [8/10], Training Loss: 0.912, Validation Accuracy: 58.91%\n",
            "Epoch [9/10], Training Loss: 0.893, Validation Accuracy: 58.84%\n",
            "Epoch [10/10], Training Loss: 0.887, Validation Accuracy: 59.99%\n",
            "Epoch [1/10], Training Loss: 1.071, Validation Accuracy: 59.95%\n",
            "Epoch [2/10], Training Loss: 1.023, Validation Accuracy: 60.28%\n",
            "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 59.95%\n",
            "Epoch [4/10], Training Loss: 0.976, Validation Accuracy: 59.97%\n",
            "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.12%\n",
            "Epoch [6/10], Training Loss: 0.953, Validation Accuracy: 60.32%\n",
            "Epoch [7/10], Training Loss: 0.927, Validation Accuracy: 59.70%\n",
            "Epoch [8/10], Training Loss: 0.910, Validation Accuracy: 60.23%\n",
            "Epoch [9/10], Training Loss: 0.897, Validation Accuracy: 59.99%\n",
            "Epoch [10/10], Training Loss: 0.884, Validation Accuracy: 58.65%\n",
            "Epoch [1/10], Training Loss: 1.035, Validation Accuracy: 60.19%\n",
            "Epoch [2/10], Training Loss: 0.983, Validation Accuracy: 60.50%\n",
            "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 60.30%\n",
            "Epoch [4/10], Training Loss: 0.926, Validation Accuracy: 61.11%\n",
            "Epoch [5/10], Training Loss: 0.913, Validation Accuracy: 60.52%\n",
            "Epoch [6/10], Training Loss: 0.901, Validation Accuracy: 60.11%\n",
            "Epoch [7/10], Training Loss: 0.879, Validation Accuracy: 60.59%\n",
            "Epoch [8/10], Training Loss: 0.865, Validation Accuracy: 60.77%\n",
            "Epoch [9/10], Training Loss: 0.847, Validation Accuracy: 59.72%\n",
            "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 60.56%\n",
            "Epoch [1/10], Training Loss: 1.019, Validation Accuracy: 60.72%\n",
            "Epoch [2/10], Training Loss: 0.959, Validation Accuracy: 59.96%\n",
            "Epoch [3/10], Training Loss: 0.937, Validation Accuracy: 60.29%\n",
            "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 60.11%\n",
            "Epoch [5/10], Training Loss: 0.891, Validation Accuracy: 59.82%\n",
            "Epoch [6/10], Training Loss: 0.879, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.851, Validation Accuracy: 59.45%\n",
            "Epoch [8/10], Training Loss: 0.845, Validation Accuracy: 60.61%\n",
            "Epoch [9/10], Training Loss: 0.819, Validation Accuracy: 60.35%\n",
            "Epoch [10/10], Training Loss: 0.814, Validation Accuracy: 60.03%\n",
            "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 60.41%\n",
            "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 60.52%\n",
            "Epoch [3/10], Training Loss: 0.919, Validation Accuracy: 60.78%\n",
            "Epoch [4/10], Training Loss: 0.897, Validation Accuracy: 59.77%\n",
            "Epoch [5/10], Training Loss: 0.872, Validation Accuracy: 60.94%\n",
            "Epoch [6/10], Training Loss: 0.868, Validation Accuracy: 60.39%\n",
            "Epoch [7/10], Training Loss: 0.843, Validation Accuracy: 60.73%\n",
            "Epoch [8/10], Training Loss: 0.825, Validation Accuracy: 60.22%\n",
            "Epoch [9/10], Training Loss: 0.818, Validation Accuracy: 60.43%\n",
            "Epoch [10/10], Training Loss: 0.794, Validation Accuracy: 60.61%\n",
            "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.49%\n",
            "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 61.50%\n",
            "Epoch [3/10], Training Loss: 0.919, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.885, Validation Accuracy: 60.93%\n",
            "Epoch [5/10], Training Loss: 0.860, Validation Accuracy: 61.15%\n",
            "Epoch [6/10], Training Loss: 0.850, Validation Accuracy: 60.79%\n",
            "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.49%\n",
            "Epoch [8/10], Training Loss: 0.808, Validation Accuracy: 60.96%\n",
            "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 60.60%\n",
            "Epoch [10/10], Training Loss: 0.785, Validation Accuracy: 60.42%\n",
            "Epoch [1/10], Training Loss: 1.011, Validation Accuracy: 60.63%\n",
            "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 61.38%\n",
            "Epoch [3/10], Training Loss: 0.918, Validation Accuracy: 61.08%\n",
            "Epoch [4/10], Training Loss: 0.881, Validation Accuracy: 60.46%\n",
            "Epoch [5/10], Training Loss: 0.869, Validation Accuracy: 60.21%\n",
            "Epoch [6/10], Training Loss: 0.847, Validation Accuracy: 60.59%\n",
            "Epoch [7/10], Training Loss: 0.828, Validation Accuracy: 60.93%\n",
            "Epoch [8/10], Training Loss: 0.816, Validation Accuracy: 60.90%\n",
            "Epoch [9/10], Training Loss: 0.800, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.780, Validation Accuracy: 60.75%\n",
            "Epoch [1/10], Training Loss: 0.971, Validation Accuracy: 61.08%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.22%\n",
            "Epoch [3/10], Training Loss: 0.873, Validation Accuracy: 61.04%\n",
            "Epoch [4/10], Training Loss: 0.853, Validation Accuracy: 60.78%\n",
            "Epoch [5/10], Training Loss: 0.830, Validation Accuracy: 61.20%\n",
            "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 60.84%\n",
            "Epoch [7/10], Training Loss: 0.792, Validation Accuracy: 59.53%\n",
            "Epoch [8/10], Training Loss: 0.776, Validation Accuracy: 61.10%\n",
            "Epoch [9/10], Training Loss: 0.756, Validation Accuracy: 60.57%\n",
            "Epoch [10/10], Training Loss: 0.740, Validation Accuracy: 61.06%\n",
            "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 60.35%\n",
            "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.11%\n",
            "Epoch [3/10], Training Loss: 0.860, Validation Accuracy: 61.29%\n",
            "Epoch [4/10], Training Loss: 0.829, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.804, Validation Accuracy: 61.19%\n",
            "Epoch [6/10], Training Loss: 0.786, Validation Accuracy: 60.87%\n",
            "Epoch [7/10], Training Loss: 0.765, Validation Accuracy: 60.01%\n",
            "Epoch [8/10], Training Loss: 0.749, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.730, Validation Accuracy: 61.28%\n",
            "Epoch [10/10], Training Loss: 0.715, Validation Accuracy: 61.21%\n",
            "Epoch [1/10], Training Loss: 0.960, Validation Accuracy: 61.26%\n",
            "Epoch [2/10], Training Loss: 0.884, Validation Accuracy: 60.28%\n",
            "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 60.33%\n",
            "Epoch [4/10], Training Loss: 0.825, Validation Accuracy: 61.44%\n",
            "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 60.29%\n",
            "Epoch [6/10], Training Loss: 0.786, Validation Accuracy: 61.16%\n",
            "Epoch [7/10], Training Loss: 0.752, Validation Accuracy: 60.70%\n",
            "Epoch [8/10], Training Loss: 0.736, Validation Accuracy: 61.28%\n",
            "Epoch [9/10], Training Loss: 0.725, Validation Accuracy: 60.86%\n",
            "Epoch [10/10], Training Loss: 0.703, Validation Accuracy: 60.65%\n",
            "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 60.52%\n",
            "Epoch [2/10], Training Loss: 0.880, Validation Accuracy: 61.13%\n",
            "Epoch [3/10], Training Loss: 0.837, Validation Accuracy: 61.02%\n",
            "Epoch [4/10], Training Loss: 0.801, Validation Accuracy: 60.74%\n",
            "Epoch [5/10], Training Loss: 0.776, Validation Accuracy: 61.03%\n",
            "Epoch [6/10], Training Loss: 0.760, Validation Accuracy: 60.71%\n",
            "Epoch [7/10], Training Loss: 0.738, Validation Accuracy: 61.42%\n",
            "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 61.41%\n",
            "Epoch [9/10], Training Loss: 0.710, Validation Accuracy: 60.98%\n",
            "Epoch [10/10], Training Loss: 0.682, Validation Accuracy: 60.33%\n",
            "Epoch [1/10], Training Loss: 0.955, Validation Accuracy: 61.37%\n",
            "Epoch [2/10], Training Loss: 0.885, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 61.90%\n",
            "Epoch [4/10], Training Loss: 0.802, Validation Accuracy: 60.58%\n",
            "Epoch [5/10], Training Loss: 0.782, Validation Accuracy: 61.31%\n",
            "Epoch [6/10], Training Loss: 0.759, Validation Accuracy: 61.21%\n",
            "Epoch [7/10], Training Loss: 0.733, Validation Accuracy: 60.95%\n",
            "Epoch [8/10], Training Loss: 0.723, Validation Accuracy: 60.70%\n",
            "Epoch [9/10], Training Loss: 0.705, Validation Accuracy: 60.68%\n",
            "Epoch [10/10], Training Loss: 0.694, Validation Accuracy: 60.80%\n",
            "Epoch [1/10], Training Loss: 0.916, Validation Accuracy: 61.00%\n",
            "Epoch [2/10], Training Loss: 0.845, Validation Accuracy: 61.24%\n",
            "Epoch [3/10], Training Loss: 0.806, Validation Accuracy: 61.42%\n",
            "Epoch [4/10], Training Loss: 0.765, Validation Accuracy: 61.49%\n",
            "Epoch [5/10], Training Loss: 0.742, Validation Accuracy: 60.33%\n",
            "Epoch [6/10], Training Loss: 0.722, Validation Accuracy: 61.26%\n",
            "Epoch [7/10], Training Loss: 0.704, Validation Accuracy: 61.32%\n",
            "Epoch [8/10], Training Loss: 0.680, Validation Accuracy: 60.75%\n",
            "Epoch [9/10], Training Loss: 0.664, Validation Accuracy: 60.80%\n",
            "Epoch [10/10], Training Loss: 0.646, Validation Accuracy: 61.09%\n",
            "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 60.94%\n",
            "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.18%\n",
            "Epoch [3/10], Training Loss: 0.773, Validation Accuracy: 60.82%\n",
            "Epoch [4/10], Training Loss: 0.750, Validation Accuracy: 60.96%\n",
            "Epoch [5/10], Training Loss: 0.725, Validation Accuracy: 60.93%\n",
            "Epoch [6/10], Training Loss: 0.695, Validation Accuracy: 61.28%\n",
            "Epoch [7/10], Training Loss: 0.680, Validation Accuracy: 60.41%\n",
            "Epoch [8/10], Training Loss: 0.660, Validation Accuracy: 61.03%\n",
            "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 60.37%\n",
            "Epoch [10/10], Training Loss: 0.630, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 0.901, Validation Accuracy: 60.82%\n",
            "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 61.90%\n",
            "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 60.88%\n",
            "Epoch [4/10], Training Loss: 0.745, Validation Accuracy: 60.90%\n",
            "Epoch [5/10], Training Loss: 0.713, Validation Accuracy: 61.35%\n",
            "Epoch [6/10], Training Loss: 0.696, Validation Accuracy: 61.29%\n",
            "Epoch [7/10], Training Loss: 0.676, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.653, Validation Accuracy: 61.05%\n",
            "Epoch [9/10], Training Loss: 0.636, Validation Accuracy: 61.32%\n",
            "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 60.51%\n",
            "Epoch [1/10], Training Loss: 0.901, Validation Accuracy: 61.22%\n",
            "Epoch [2/10], Training Loss: 0.818, Validation Accuracy: 60.65%\n",
            "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 61.43%\n",
            "Epoch [4/10], Training Loss: 0.727, Validation Accuracy: 61.26%\n",
            "Epoch [5/10], Training Loss: 0.699, Validation Accuracy: 61.89%\n",
            "Epoch [6/10], Training Loss: 0.681, Validation Accuracy: 61.45%\n",
            "Epoch [7/10], Training Loss: 0.646, Validation Accuracy: 61.35%\n",
            "Epoch [8/10], Training Loss: 0.630, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.618, Validation Accuracy: 60.98%\n",
            "Epoch [10/10], Training Loss: 0.597, Validation Accuracy: 61.27%\n",
            "Epoch [1/10], Training Loss: 0.919, Validation Accuracy: 60.88%\n",
            "Epoch [2/10], Training Loss: 0.818, Validation Accuracy: 60.84%\n",
            "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 61.43%\n",
            "Epoch [4/10], Training Loss: 0.729, Validation Accuracy: 60.86%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.36%\n",
            "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 60.98%\n",
            "Epoch [7/10], Training Loss: 0.667, Validation Accuracy: 60.80%\n",
            "Epoch [8/10], Training Loss: 0.633, Validation Accuracy: 60.87%\n",
            "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 60.74%\n",
            "Epoch [10/10], Training Loss: 0.600, Validation Accuracy: 60.46%\n",
            "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.48%\n",
            "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 60.71%\n",
            "Epoch [3/10], Training Loss: 0.737, Validation Accuracy: 61.46%\n",
            "Epoch [4/10], Training Loss: 0.694, Validation Accuracy: 61.42%\n",
            "Epoch [5/10], Training Loss: 0.657, Validation Accuracy: 61.03%\n",
            "Epoch [6/10], Training Loss: 0.641, Validation Accuracy: 60.06%\n",
            "Epoch [7/10], Training Loss: 0.615, Validation Accuracy: 60.91%\n",
            "Epoch [8/10], Training Loss: 0.596, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.575, Validation Accuracy: 60.86%\n",
            "Epoch [10/10], Training Loss: 0.558, Validation Accuracy: 60.66%\n",
            "Epoch [1/10], Training Loss: 0.866, Validation Accuracy: 60.21%\n",
            "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 61.07%\n",
            "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 60.28%\n",
            "Epoch [4/10], Training Loss: 0.683, Validation Accuracy: 60.10%\n",
            "Epoch [5/10], Training Loss: 0.652, Validation Accuracy: 61.27%\n",
            "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 60.50%\n",
            "Epoch [7/10], Training Loss: 0.600, Validation Accuracy: 61.06%\n",
            "Epoch [8/10], Training Loss: 0.572, Validation Accuracy: 60.99%\n",
            "Epoch [9/10], Training Loss: 0.555, Validation Accuracy: 61.03%\n",
            "Epoch [10/10], Training Loss: 0.535, Validation Accuracy: 60.59%\n",
            "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 61.03%\n",
            "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 60.72%\n",
            "Epoch [3/10], Training Loss: 0.711, Validation Accuracy: 61.22%\n",
            "Epoch [4/10], Training Loss: 0.663, Validation Accuracy: 59.98%\n",
            "Epoch [5/10], Training Loss: 0.649, Validation Accuracy: 60.82%\n",
            "Epoch [6/10], Training Loss: 0.611, Validation Accuracy: 60.84%\n",
            "Epoch [7/10], Training Loss: 0.581, Validation Accuracy: 60.79%\n",
            "Epoch [8/10], Training Loss: 0.564, Validation Accuracy: 60.55%\n",
            "Epoch [9/10], Training Loss: 0.552, Validation Accuracy: 60.82%\n",
            "Epoch [10/10], Training Loss: 0.526, Validation Accuracy: 60.48%\n",
            "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 60.37%\n",
            "Epoch [2/10], Training Loss: 0.755, Validation Accuracy: 60.76%\n",
            "Epoch [3/10], Training Loss: 0.697, Validation Accuracy: 61.37%\n",
            "Epoch [4/10], Training Loss: 0.657, Validation Accuracy: 60.86%\n",
            "Epoch [5/10], Training Loss: 0.620, Validation Accuracy: 61.50%\n",
            "Epoch [6/10], Training Loss: 0.588, Validation Accuracy: 60.81%\n",
            "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.76%\n",
            "Epoch [8/10], Training Loss: 0.550, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 60.90%\n",
            "Epoch [10/10], Training Loss: 0.518, Validation Accuracy: 61.11%\n",
            "Epoch [1/10], Training Loss: 0.876, Validation Accuracy: 61.50%\n",
            "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 61.59%\n",
            "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 60.73%\n",
            "Epoch [4/10], Training Loss: 0.663, Validation Accuracy: 60.94%\n",
            "Epoch [5/10], Training Loss: 0.630, Validation Accuracy: 60.23%\n",
            "Epoch [6/10], Training Loss: 0.592, Validation Accuracy: 60.85%\n",
            "Epoch [7/10], Training Loss: 0.567, Validation Accuracy: 60.74%\n",
            "Epoch [8/10], Training Loss: 0.558, Validation Accuracy: 59.98%\n",
            "Epoch [9/10], Training Loss: 0.526, Validation Accuracy: 59.97%\n",
            "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 60.22%\n",
            "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 60.60%\n",
            "Epoch [2/10], Training Loss: 0.725, Validation Accuracy: 60.98%\n",
            "Epoch [3/10], Training Loss: 0.663, Validation Accuracy: 61.04%\n",
            "Epoch [4/10], Training Loss: 0.619, Validation Accuracy: 60.43%\n",
            "Epoch [5/10], Training Loss: 0.589, Validation Accuracy: 61.41%\n",
            "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 60.80%\n",
            "Epoch [7/10], Training Loss: 0.536, Validation Accuracy: 61.42%\n",
            "Epoch [8/10], Training Loss: 0.509, Validation Accuracy: 61.40%\n",
            "Epoch [9/10], Training Loss: 0.493, Validation Accuracy: 60.61%\n",
            "Epoch [10/10], Training Loss: 0.467, Validation Accuracy: 60.28%\n",
            "Epoch [1/10], Training Loss: 0.827, Validation Accuracy: 60.57%\n",
            "Epoch [2/10], Training Loss: 0.697, Validation Accuracy: 60.26%\n",
            "Epoch [3/10], Training Loss: 0.639, Validation Accuracy: 60.11%\n",
            "Epoch [4/10], Training Loss: 0.592, Validation Accuracy: 60.82%\n",
            "Epoch [5/10], Training Loss: 0.564, Validation Accuracy: 60.84%\n",
            "Epoch [6/10], Training Loss: 0.535, Validation Accuracy: 60.83%\n",
            "Epoch [7/10], Training Loss: 0.511, Validation Accuracy: 59.87%\n",
            "Epoch [8/10], Training Loss: 0.489, Validation Accuracy: 61.02%\n",
            "Epoch [9/10], Training Loss: 0.464, Validation Accuracy: 60.50%\n",
            "Epoch [10/10], Training Loss: 0.450, Validation Accuracy: 59.91%\n",
            "Confusion Matrix:\n",
            "[[683  43  51  17  18   6  17  33  67  65]\n",
            " [ 22 762   5   7   5   2  11  13  24 149]\n",
            " [ 68  15 491  63  72  65  95  74  30  27]\n",
            " [ 25  19  67 342  47 190 121 110  24  55]\n",
            " [ 39  17 112  48 396  55 136 170  18   9]\n",
            " [ 25  11  61 144  48 498  49 123  18  23]\n",
            " [ 12  13  47  51  34  22 759  24  11  27]\n",
            " [ 17  13  25  27  45  53  23 753   3  41]\n",
            " [121  74  27  17   9   5  13  11 641  82]\n",
            " [ 29 154  14  23   4   7  11  39  18 701]]\n",
            "Test Accuracy: 60.26%\n",
            "True Positives (TP): [683 762 491 342 396 498 759 753 641 701]\n",
            "False Positives (FP): [358 359 409 397 282 405 476 597 213 478]\n",
            "True Negatives (TN): [8642 8641 8591 8603 8718 8595 8524 8403 8787 8522]\n",
            "False Negatives (FN): [317 238 509 658 604 502 241 247 359 299]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.6560999  0.67975022 0.54555556 0.46278755 0.5840708  0.55149502\n",
            " 0.6145749  0.55777778 0.75058548 0.59457167]\n",
            "Recall: [0.683 0.762 0.491 0.342 0.396 0.498 0.759 0.753 0.641 0.701]\n",
            "F1 Score: [0.66927976 0.718529   0.51684211 0.3933295  0.47199046 0.52338413\n",
            " 0.67919463 0.64085106 0.69147789 0.64341441]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XusxNycsosQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your provided text\n",
        "log = \"\"\"\n",
        "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.22%\n",
        "Epoch [2/10], Training Loss: 2.303, Validation Accuracy: 10.22%\n",
        "Epoch [3/10], Training Loss: 2.302, Validation Accuracy: 10.22%\n",
        "Epoch [4/10], Training Loss: 2.302, Validation Accuracy: 10.22%\n",
        "Epoch [5/10], Training Loss: 2.301, Validation Accuracy: 10.22%\n",
        "Epoch [6/10], Training Loss: 2.300, Validation Accuracy: 10.27%\n",
        "Epoch [7/10], Training Loss: 2.299, Validation Accuracy: 10.44%\n",
        "Epoch [8/10], Training Loss: 2.297, Validation Accuracy: 11.48%\n",
        "Epoch [9/10], Training Loss: 2.295, Validation Accuracy: 12.79%\n",
        "Epoch [10/10], Training Loss: 2.293, Validation Accuracy: 16.07%\n",
        "Epoch [1/10], Training Loss: 2.290, Validation Accuracy: 16.34%\n",
        "Epoch [2/10], Training Loss: 2.284, Validation Accuracy: 16.02%\n",
        "Epoch [3/10], Training Loss: 2.274, Validation Accuracy: 15.54%\n",
        "Epoch [4/10], Training Loss: 2.261, Validation Accuracy: 15.61%\n",
        "Epoch [5/10], Training Loss: 2.248, Validation Accuracy: 16.07%\n",
        "Epoch [6/10], Training Loss: 2.234, Validation Accuracy: 16.72%\n",
        "Epoch [7/10], Training Loss: 2.220, Validation Accuracy: 18.66%\n",
        "Epoch [8/10], Training Loss: 2.206, Validation Accuracy: 20.07%\n",
        "Epoch [9/10], Training Loss: 2.192, Validation Accuracy: 20.51%\n",
        "Epoch [10/10], Training Loss: 2.179, Validation Accuracy: 21.34%\n",
        "Epoch [1/10], Training Loss: 2.177, Validation Accuracy: 21.24%\n",
        "Epoch [2/10], Training Loss: 2.163, Validation Accuracy: 21.46%\n",
        "Epoch [3/10], Training Loss: 2.150, Validation Accuracy: 23.00%\n",
        "Epoch [4/10], Training Loss: 2.139, Validation Accuracy: 23.20%\n",
        "Epoch [5/10], Training Loss: 2.129, Validation Accuracy: 23.93%\n",
        "Epoch [6/10], Training Loss: 2.119, Validation Accuracy: 24.03%\n",
        "Epoch [7/10], Training Loss: 2.108, Validation Accuracy: 24.67%\n",
        "Epoch [8/10], Training Loss: 2.097, Validation Accuracy: 25.00%\n",
        "Epoch [9/10], Training Loss: 2.085, Validation Accuracy: 25.54%\n",
        "Epoch [10/10], Training Loss: 2.073, Validation Accuracy: 25.19%\n",
        "Epoch [1/10], Training Loss: 2.058, Validation Accuracy: 26.08%\n",
        "Epoch [2/10], Training Loss: 2.045, Validation Accuracy: 26.84%\n",
        "Epoch [3/10], Training Loss: 2.033, Validation Accuracy: 27.22%\n",
        "Epoch [4/10], Training Loss: 2.021, Validation Accuracy: 27.54%\n",
        "Epoch [5/10], Training Loss: 2.009, Validation Accuracy: 27.62%\n",
        "Epoch [6/10], Training Loss: 1.998, Validation Accuracy: 27.70%\n",
        "Epoch [7/10], Training Loss: 1.986, Validation Accuracy: 28.27%\n",
        "Epoch [8/10], Training Loss: 1.975, Validation Accuracy: 28.65%\n",
        "Epoch [9/10], Training Loss: 1.966, Validation Accuracy: 27.64%\n",
        "Epoch [10/10], Training Loss: 1.954, Validation Accuracy: 29.48%\n",
        "Epoch [1/10], Training Loss: 1.941, Validation Accuracy: 28.98%\n",
        "Epoch [2/10], Training Loss: 1.926, Validation Accuracy: 29.39%\n",
        "Epoch [3/10], Training Loss: 1.911, Validation Accuracy: 29.50%\n",
        "Epoch [4/10], Training Loss: 1.898, Validation Accuracy: 30.02%\n",
        "Epoch [5/10], Training Loss: 1.884, Validation Accuracy: 30.38%\n",
        "Epoch [6/10], Training Loss: 1.869, Validation Accuracy: 30.97%\n",
        "Epoch [7/10], Training Loss: 1.854, Validation Accuracy: 31.25%\n",
        "Epoch [8/10], Training Loss: 1.838, Validation Accuracy: 31.10%\n",
        "Epoch [9/10], Training Loss: 1.822, Validation Accuracy: 31.92%\n",
        "Epoch [10/10], Training Loss: 1.806, Validation Accuracy: 32.82%\n",
        "Epoch [1/10], Training Loss: 1.812, Validation Accuracy: 33.25%\n",
        "Epoch [2/10], Training Loss: 1.793, Validation Accuracy: 34.31%\n",
        "Epoch [3/10], Training Loss: 1.774, Validation Accuracy: 34.87%\n",
        "Epoch [4/10], Training Loss: 1.757, Validation Accuracy: 35.19%\n",
        "Epoch [5/10], Training Loss: 1.740, Validation Accuracy: 35.66%\n",
        "Epoch [6/10], Training Loss: 1.723, Validation Accuracy: 36.75%\n",
        "Epoch [7/10], Training Loss: 1.711, Validation Accuracy: 36.15%\n",
        "Epoch [8/10], Training Loss: 1.690, Validation Accuracy: 37.58%\n",
        "Epoch [9/10], Training Loss: 1.679, Validation Accuracy: 38.20%\n",
        "Epoch [10/10], Training Loss: 1.660, Validation Accuracy: 38.61%\n",
        "Epoch [1/10], Training Loss: 1.679, Validation Accuracy: 38.25%\n",
        "Epoch [2/10], Training Loss: 1.666, Validation Accuracy: 38.87%\n",
        "Epoch [3/10], Training Loss: 1.645, Validation Accuracy: 39.57%\n",
        "Epoch [4/10], Training Loss: 1.633, Validation Accuracy: 39.27%\n",
        "Epoch [5/10], Training Loss: 1.616, Validation Accuracy: 39.75%\n",
        "Epoch [6/10], Training Loss: 1.604, Validation Accuracy: 40.70%\n",
        "Epoch [7/10], Training Loss: 1.589, Validation Accuracy: 40.68%\n",
        "Epoch [8/10], Training Loss: 1.582, Validation Accuracy: 41.86%\n",
        "Epoch [9/10], Training Loss: 1.571, Validation Accuracy: 42.18%\n",
        "Epoch [10/10], Training Loss: 1.557, Validation Accuracy: 42.33%\n",
        "Epoch [1/10], Training Loss: 1.605, Validation Accuracy: 42.83%\n",
        "Epoch [2/10], Training Loss: 1.587, Validation Accuracy: 42.52%\n",
        "Epoch [3/10], Training Loss: 1.573, Validation Accuracy: 42.18%\n",
        "Epoch [4/10], Training Loss: 1.564, Validation Accuracy: 43.16%\n",
        "Epoch [5/10], Training Loss: 1.555, Validation Accuracy: 43.46%\n",
        "Epoch [6/10], Training Loss: 1.544, Validation Accuracy: 44.19%\n",
        "Epoch [7/10], Training Loss: 1.530, Validation Accuracy: 43.57%\n",
        "Epoch [8/10], Training Loss: 1.525, Validation Accuracy: 44.77%\n",
        "Epoch [9/10], Training Loss: 1.512, Validation Accuracy: 44.80%\n",
        "Epoch [10/10], Training Loss: 1.503, Validation Accuracy: 45.07%\n",
        "Epoch [1/10], Training Loss: 1.503, Validation Accuracy: 44.45%\n",
        "Epoch [2/10], Training Loss: 1.491, Validation Accuracy: 45.18%\n",
        "Epoch [3/10], Training Loss: 1.472, Validation Accuracy: 44.21%\n",
        "Epoch [4/10], Training Loss: 1.474, Validation Accuracy: 45.92%\n",
        "Epoch [5/10], Training Loss: 1.453, Validation Accuracy: 46.38%\n",
        "Epoch [6/10], Training Loss: 1.443, Validation Accuracy: 46.53%\n",
        "Epoch [7/10], Training Loss: 1.434, Validation Accuracy: 46.71%\n",
        "Epoch [8/10], Training Loss: 1.427, Validation Accuracy: 45.55%\n",
        "Epoch [9/10], Training Loss: 1.425, Validation Accuracy: 46.73%\n",
        "Epoch [10/10], Training Loss: 1.411, Validation Accuracy: 47.01%\n",
        "Epoch [1/10], Training Loss: 1.437, Validation Accuracy: 47.37%\n",
        "Epoch [2/10], Training Loss: 1.420, Validation Accuracy: 48.08%\n",
        "Epoch [3/10], Training Loss: 1.411, Validation Accuracy: 47.86%\n",
        "Epoch [4/10], Training Loss: 1.401, Validation Accuracy: 48.37%\n",
        "Epoch [5/10], Training Loss: 1.396, Validation Accuracy: 47.69%\n",
        "Epoch [6/10], Training Loss: 1.382, Validation Accuracy: 48.61%\n",
        "Epoch [7/10], Training Loss: 1.374, Validation Accuracy: 48.93%\n",
        "Epoch [8/10], Training Loss: 1.365, Validation Accuracy: 49.18%\n",
        "Epoch [9/10], Training Loss: 1.357, Validation Accuracy: 48.80%\n",
        "Epoch [10/10], Training Loss: 1.344, Validation Accuracy: 49.24%\n",
        "Epoch [1/10], Training Loss: 1.401, Validation Accuracy: 49.76%\n",
        "Epoch [2/10], Training Loss: 1.376, Validation Accuracy: 49.80%\n",
        "Epoch [3/10], Training Loss: 1.368, Validation Accuracy: 48.89%\n",
        "Epoch [4/10], Training Loss: 1.362, Validation Accuracy: 48.65%\n",
        "Epoch [5/10], Training Loss: 1.348, Validation Accuracy: 48.97%\n",
        "Epoch [6/10], Training Loss: 1.344, Validation Accuracy: 49.77%\n",
        "Epoch [7/10], Training Loss: 1.339, Validation Accuracy: 49.89%\n",
        "Epoch [8/10], Training Loss: 1.324, Validation Accuracy: 50.12%\n",
        "Epoch [9/10], Training Loss: 1.320, Validation Accuracy: 49.70%\n",
        "Epoch [10/10], Training Loss: 1.314, Validation Accuracy: 49.97%\n",
        "Epoch [1/10], Training Loss: 1.373, Validation Accuracy: 49.57%\n",
        "Epoch [2/10], Training Loss: 1.361, Validation Accuracy: 50.33%\n",
        "Epoch [3/10], Training Loss: 1.347, Validation Accuracy: 50.78%\n",
        "Epoch [4/10], Training Loss: 1.332, Validation Accuracy: 50.65%\n",
        "Epoch [5/10], Training Loss: 1.327, Validation Accuracy: 51.66%\n",
        "Epoch [6/10], Training Loss: 1.318, Validation Accuracy: 50.50%\n",
        "Epoch [7/10], Training Loss: 1.304, Validation Accuracy: 50.83%\n",
        "Epoch [8/10], Training Loss: 1.297, Validation Accuracy: 51.72%\n",
        "Epoch [9/10], Training Loss: 1.292, Validation Accuracy: 51.48%\n",
        "Epoch [10/10], Training Loss: 1.285, Validation Accuracy: 51.55%\n",
        "Epoch [1/10], Training Loss: 1.372, Validation Accuracy: 52.04%\n",
        "Epoch [2/10], Training Loss: 1.357, Validation Accuracy: 50.80%\n",
        "Epoch [3/10], Training Loss: 1.342, Validation Accuracy: 51.47%\n",
        "Epoch [4/10], Training Loss: 1.327, Validation Accuracy: 52.32%\n",
        "Epoch [5/10], Training Loss: 1.316, Validation Accuracy: 51.89%\n",
        "Epoch [6/10], Training Loss: 1.305, Validation Accuracy: 52.09%\n",
        "Epoch [7/10], Training Loss: 1.300, Validation Accuracy: 52.64%\n",
        "Epoch [8/10], Training Loss: 1.293, Validation Accuracy: 51.79%\n",
        "Epoch [9/10], Training Loss: 1.281, Validation Accuracy: 52.73%\n",
        "Epoch [10/10], Training Loss: 1.273, Validation Accuracy: 52.31%\n",
        "Epoch [1/10], Training Loss: 1.307, Validation Accuracy: 52.50%\n",
        "Epoch [2/10], Training Loss: 1.296, Validation Accuracy: 52.46%\n",
        "Epoch [3/10], Training Loss: 1.279, Validation Accuracy: 53.36%\n",
        "Epoch [4/10], Training Loss: 1.269, Validation Accuracy: 53.10%\n",
        "Epoch [5/10], Training Loss: 1.258, Validation Accuracy: 53.25%\n",
        "Epoch [6/10], Training Loss: 1.248, Validation Accuracy: 53.31%\n",
        "Epoch [7/10], Training Loss: 1.237, Validation Accuracy: 53.36%\n",
        "Epoch [8/10], Training Loss: 1.227, Validation Accuracy: 53.26%\n",
        "Epoch [9/10], Training Loss: 1.219, Validation Accuracy: 53.60%\n",
        "Epoch [10/10], Training Loss: 1.209, Validation Accuracy: 53.26%\n",
        "Epoch [1/10], Training Loss: 1.273, Validation Accuracy: 54.31%\n",
        "Epoch [2/10], Training Loss: 1.265, Validation Accuracy: 53.59%\n",
        "Epoch [3/10], Training Loss: 1.238, Validation Accuracy: 53.57%\n",
        "Epoch [4/10], Training Loss: 1.229, Validation Accuracy: 52.78%\n",
        "Epoch [5/10], Training Loss: 1.212, Validation Accuracy: 54.09%\n",
        "Epoch [6/10], Training Loss: 1.198, Validation Accuracy: 54.12%\n",
        "Epoch [7/10], Training Loss: 1.192, Validation Accuracy: 54.07%\n",
        "Epoch [8/10], Training Loss: 1.187, Validation Accuracy: 54.17%\n",
        "Epoch [9/10], Training Loss: 1.178, Validation Accuracy: 53.90%\n",
        "Epoch [10/10], Training Loss: 1.172, Validation Accuracy: 54.34%\n",
        "Epoch [1/10], Training Loss: 1.254, Validation Accuracy: 53.63%\n",
        "Epoch [2/10], Training Loss: 1.235, Validation Accuracy: 53.90%\n",
        "Epoch [3/10], Training Loss: 1.233, Validation Accuracy: 54.70%\n",
        "Epoch [4/10], Training Loss: 1.212, Validation Accuracy: 54.55%\n",
        "Epoch [5/10], Training Loss: 1.198, Validation Accuracy: 54.01%\n",
        "Epoch [6/10], Training Loss: 1.198, Validation Accuracy: 54.83%\n",
        "Epoch [7/10], Training Loss: 1.177, Validation Accuracy: 54.63%\n",
        "Epoch [8/10], Training Loss: 1.172, Validation Accuracy: 53.69%\n",
        "Epoch [9/10], Training Loss: 1.170, Validation Accuracy: 55.36%\n",
        "Epoch [10/10], Training Loss: 1.156, Validation Accuracy: 55.04%\n",
        "Epoch [1/10], Training Loss: 1.241, Validation Accuracy: 54.65%\n",
        "Epoch [2/10], Training Loss: 1.223, Validation Accuracy: 55.58%\n",
        "Epoch [3/10], Training Loss: 1.201, Validation Accuracy: 55.52%\n",
        "Epoch [4/10], Training Loss: 1.190, Validation Accuracy: 55.71%\n",
        "Epoch [5/10], Training Loss: 1.175, Validation Accuracy: 55.39%\n",
        "Epoch [6/10], Training Loss: 1.169, Validation Accuracy: 55.39%\n",
        "Epoch [7/10], Training Loss: 1.156, Validation Accuracy: 55.80%\n",
        "Epoch [8/10], Training Loss: 1.143, Validation Accuracy: 55.59%\n",
        "Epoch [9/10], Training Loss: 1.131, Validation Accuracy: 55.73%\n",
        "Epoch [10/10], Training Loss: 1.125, Validation Accuracy: 55.11%\n",
        "Epoch [1/10], Training Loss: 1.244, Validation Accuracy: 55.91%\n",
        "Epoch [2/10], Training Loss: 1.218, Validation Accuracy: 56.35%\n",
        "Epoch [3/10], Training Loss: 1.204, Validation Accuracy: 55.29%\n",
        "Epoch [4/10], Training Loss: 1.186, Validation Accuracy: 55.67%\n",
        "Epoch [5/10], Training Loss: 1.171, Validation Accuracy: 55.44%\n",
        "Epoch [6/10], Training Loss: 1.158, Validation Accuracy: 56.12%\n",
        "Epoch [7/10], Training Loss: 1.145, Validation Accuracy: 55.47%\n",
        "Epoch [8/10], Training Loss: 1.141, Validation Accuracy: 56.12%\n",
        "Epoch [9/10], Training Loss: 1.130, Validation Accuracy: 55.27%\n",
        "Epoch [10/10], Training Loss: 1.123, Validation Accuracy: 56.23%\n",
        "Epoch [1/10], Training Loss: 1.198, Validation Accuracy: 56.08%\n",
        "Epoch [2/10], Training Loss: 1.174, Validation Accuracy: 56.88%\n",
        "Epoch [3/10], Training Loss: 1.154, Validation Accuracy: 56.74%\n",
        "Epoch [4/10], Training Loss: 1.139, Validation Accuracy: 56.61%\n",
        "Epoch [5/10], Training Loss: 1.129, Validation Accuracy: 56.43%\n",
        "Epoch [6/10], Training Loss: 1.111, Validation Accuracy: 56.95%\n",
        "Epoch [7/10], Training Loss: 1.107, Validation Accuracy: 56.38%\n",
        "Epoch [8/10], Training Loss: 1.086, Validation Accuracy: 57.09%\n",
        "Epoch [9/10], Training Loss: 1.086, Validation Accuracy: 57.26%\n",
        "Epoch [10/10], Training Loss: 1.070, Validation Accuracy: 56.72%\n",
        "Epoch [1/10], Training Loss: 1.170, Validation Accuracy: 57.23%\n",
        "Epoch [2/10], Training Loss: 1.141, Validation Accuracy: 57.44%\n",
        "Epoch [3/10], Training Loss: 1.120, Validation Accuracy: 57.23%\n",
        "Epoch [4/10], Training Loss: 1.107, Validation Accuracy: 56.99%\n",
        "Epoch [5/10], Training Loss: 1.089, Validation Accuracy: 57.16%\n",
        "Epoch [6/10], Training Loss: 1.077, Validation Accuracy: 57.10%\n",
        "Epoch [7/10], Training Loss: 1.067, Validation Accuracy: 57.01%\n",
        "Epoch [8/10], Training Loss: 1.056, Validation Accuracy: 56.99%\n",
        "Epoch [9/10], Training Loss: 1.045, Validation Accuracy: 57.78%\n",
        "Epoch [10/10], Training Loss: 1.031, Validation Accuracy: 57.30%\n",
        "Epoch [1/10], Training Loss: 1.166, Validation Accuracy: 57.42%\n",
        "Epoch [2/10], Training Loss: 1.124, Validation Accuracy: 57.84%\n",
        "Epoch [3/10], Training Loss: 1.108, Validation Accuracy: 57.43%\n",
        "Epoch [4/10], Training Loss: 1.094, Validation Accuracy: 57.56%\n",
        "Epoch [5/10], Training Loss: 1.076, Validation Accuracy: 58.04%\n",
        "Epoch [6/10], Training Loss: 1.064, Validation Accuracy: 58.21%\n",
        "Epoch [7/10], Training Loss: 1.054, Validation Accuracy: 58.12%\n",
        "Epoch [8/10], Training Loss: 1.036, Validation Accuracy: 57.86%\n",
        "Epoch [9/10], Training Loss: 1.032, Validation Accuracy: 57.62%\n",
        "Epoch [10/10], Training Loss: 1.023, Validation Accuracy: 57.36%\n",
        "Epoch [1/10], Training Loss: 1.147, Validation Accuracy: 58.14%\n",
        "Epoch [2/10], Training Loss: 1.111, Validation Accuracy: 57.97%\n",
        "Epoch [3/10], Training Loss: 1.089, Validation Accuracy: 58.24%\n",
        "Epoch [4/10], Training Loss: 1.068, Validation Accuracy: 58.15%\n",
        "Epoch [5/10], Training Loss: 1.062, Validation Accuracy: 58.05%\n",
        "Epoch [6/10], Training Loss: 1.048, Validation Accuracy: 58.34%\n",
        "Epoch [7/10], Training Loss: 1.031, Validation Accuracy: 57.45%\n",
        "Epoch [8/10], Training Loss: 1.028, Validation Accuracy: 58.09%\n",
        "Epoch [9/10], Training Loss: 1.011, Validation Accuracy: 58.78%\n",
        "Epoch [10/10], Training Loss: 0.997, Validation Accuracy: 57.85%\n",
        "Epoch [1/10], Training Loss: 1.161, Validation Accuracy: 57.93%\n",
        "Epoch [2/10], Training Loss: 1.113, Validation Accuracy: 58.49%\n",
        "Epoch [3/10], Training Loss: 1.093, Validation Accuracy: 57.52%\n",
        "Epoch [4/10], Training Loss: 1.074, Validation Accuracy: 57.97%\n",
        "Epoch [5/10], Training Loss: 1.052, Validation Accuracy: 58.40%\n",
        "Epoch [6/10], Training Loss: 1.043, Validation Accuracy: 59.01%\n",
        "Epoch [7/10], Training Loss: 1.027, Validation Accuracy: 58.55%\n",
        "Epoch [8/10], Training Loss: 1.020, Validation Accuracy: 58.04%\n",
        "Epoch [9/10], Training Loss: 1.005, Validation Accuracy: 58.28%\n",
        "Epoch [10/10], Training Loss: 0.999, Validation Accuracy: 58.60%\n",
        "Epoch [1/10], Training Loss: 1.110, Validation Accuracy: 58.98%\n",
        "Epoch [2/10], Training Loss: 1.068, Validation Accuracy: 59.28%\n",
        "Epoch [3/10], Training Loss: 1.056, Validation Accuracy: 58.76%\n",
        "Epoch [4/10], Training Loss: 1.028, Validation Accuracy: 59.36%\n",
        "Epoch [5/10], Training Loss: 1.006, Validation Accuracy: 58.62%\n",
        "Epoch [6/10], Training Loss: 0.998, Validation Accuracy: 58.78%\n",
        "Epoch [7/10], Training Loss: 0.985, Validation Accuracy: 59.40%\n",
        "Epoch [8/10], Training Loss: 0.967, Validation Accuracy: 59.32%\n",
        "Epoch [9/10], Training Loss: 0.953, Validation Accuracy: 58.93%\n",
        "Epoch [10/10], Training Loss: 0.945, Validation Accuracy: 58.94%\n",
        "Epoch [1/10], Training Loss: 1.092, Validation Accuracy: 58.97%\n",
        "Epoch [2/10], Training Loss: 1.041, Validation Accuracy: 59.30%\n",
        "Epoch [3/10], Training Loss: 1.019, Validation Accuracy: 58.81%\n",
        "Epoch [4/10], Training Loss: 1.011, Validation Accuracy: 59.17%\n",
        "Epoch [5/10], Training Loss: 0.988, Validation Accuracy: 59.51%\n",
        "Epoch [6/10], Training Loss: 0.966, Validation Accuracy: 59.50%\n",
        "Epoch [7/10], Training Loss: 0.953, Validation Accuracy: 59.45%\n",
        "Epoch [8/10], Training Loss: 0.935, Validation Accuracy: 59.39%\n",
        "Epoch [9/10], Training Loss: 0.932, Validation Accuracy: 59.49%\n",
        "Epoch [10/10], Training Loss: 0.918, Validation Accuracy: 58.92%\n",
        "Epoch [1/10], Training Loss: 1.082, Validation Accuracy: 59.24%\n",
        "Epoch [2/10], Training Loss: 1.027, Validation Accuracy: 59.17%\n",
        "Epoch [3/10], Training Loss: 1.009, Validation Accuracy: 59.61%\n",
        "Epoch [4/10], Training Loss: 0.983, Validation Accuracy: 59.28%\n",
        "Epoch [5/10], Training Loss: 0.974, Validation Accuracy: 58.87%\n",
        "Epoch [6/10], Training Loss: 0.965, Validation Accuracy: 59.42%\n",
        "Epoch [7/10], Training Loss: 0.938, Validation Accuracy: 59.14%\n",
        "Epoch [8/10], Training Loss: 0.923, Validation Accuracy: 59.03%\n",
        "Epoch [9/10], Training Loss: 0.919, Validation Accuracy: 59.28%\n",
        "Epoch [10/10], Training Loss: 0.907, Validation Accuracy: 59.20%\n",
        "Epoch [1/10], Training Loss: 1.064, Validation Accuracy: 60.01%\n",
        "Epoch [2/10], Training Loss: 1.024, Validation Accuracy: 60.28%\n",
        "Epoch [3/10], Training Loss: 0.994, Validation Accuracy: 60.32%\n",
        "Epoch [4/10], Training Loss: 0.980, Validation Accuracy: 59.92%\n",
        "Epoch [5/10], Training Loss: 0.953, Validation Accuracy: 60.24%\n",
        "Epoch [6/10], Training Loss: 0.938, Validation Accuracy: 60.16%\n",
        "Epoch [7/10], Training Loss: 0.923, Validation Accuracy: 59.10%\n",
        "Epoch [8/10], Training Loss: 0.912, Validation Accuracy: 58.91%\n",
        "Epoch [9/10], Training Loss: 0.893, Validation Accuracy: 58.84%\n",
        "Epoch [10/10], Training Loss: 0.887, Validation Accuracy: 59.99%\n",
        "Epoch [1/10], Training Loss: 1.071, Validation Accuracy: 59.95%\n",
        "Epoch [2/10], Training Loss: 1.023, Validation Accuracy: 60.28%\n",
        "Epoch [3/10], Training Loss: 0.996, Validation Accuracy: 59.95%\n",
        "Epoch [4/10], Training Loss: 0.976, Validation Accuracy: 59.97%\n",
        "Epoch [5/10], Training Loss: 0.961, Validation Accuracy: 60.12%\n",
        "Epoch [6/10], Training Loss: 0.953, Validation Accuracy: 60.32%\n",
        "Epoch [7/10], Training Loss: 0.927, Validation Accuracy: 59.70%\n",
        "Epoch [8/10], Training Loss: 0.910, Validation Accuracy: 60.23%\n",
        "Epoch [9/10], Training Loss: 0.897, Validation Accuracy: 59.99%\n",
        "Epoch [10/10], Training Loss: 0.884, Validation Accuracy: 58.65%\n",
        "Epoch [1/10], Training Loss: 1.035, Validation Accuracy: 60.19%\n",
        "Epoch [2/10], Training Loss: 0.983, Validation Accuracy: 60.50%\n",
        "Epoch [3/10], Training Loss: 0.955, Validation Accuracy: 60.30%\n",
        "Epoch [4/10], Training Loss: 0.926, Validation Accuracy: 61.11%\n",
        "Epoch [5/10], Training Loss: 0.913, Validation Accuracy: 60.52%\n",
        "Epoch [6/10], Training Loss: 0.901, Validation Accuracy: 60.11%\n",
        "Epoch [7/10], Training Loss: 0.879, Validation Accuracy: 60.59%\n",
        "Epoch [8/10], Training Loss: 0.865, Validation Accuracy: 60.77%\n",
        "Epoch [9/10], Training Loss: 0.847, Validation Accuracy: 59.72%\n",
        "Epoch [10/10], Training Loss: 0.842, Validation Accuracy: 60.56%\n",
        "Epoch [1/10], Training Loss: 1.019, Validation Accuracy: 60.72%\n",
        "Epoch [2/10], Training Loss: 0.959, Validation Accuracy: 59.96%\n",
        "Epoch [3/10], Training Loss: 0.937, Validation Accuracy: 60.29%\n",
        "Epoch [4/10], Training Loss: 0.922, Validation Accuracy: 60.11%\n",
        "Epoch [5/10], Training Loss: 0.891, Validation Accuracy: 59.82%\n",
        "Epoch [6/10], Training Loss: 0.879, Validation Accuracy: 60.96%\n",
        "Epoch [7/10], Training Loss: 0.851, Validation Accuracy: 59.45%\n",
        "Epoch [8/10], Training Loss: 0.845, Validation Accuracy: 60.61%\n",
        "Epoch [9/10], Training Loss: 0.819, Validation Accuracy: 60.35%\n",
        "Epoch [10/10], Training Loss: 0.814, Validation Accuracy: 60.03%\n",
        "Epoch [1/10], Training Loss: 1.016, Validation Accuracy: 60.41%\n",
        "Epoch [2/10], Training Loss: 0.951, Validation Accuracy: 60.52%\n",
        "Epoch [3/10], Training Loss: 0.919, Validation Accuracy: 60.78%\n",
        "Epoch [4/10], Training Loss: 0.897, Validation Accuracy: 59.77%\n",
        "Epoch [5/10], Training Loss: 0.872, Validation Accuracy: 60.94%\n",
        "Epoch [6/10], Training Loss: 0.868, Validation Accuracy: 60.39%\n",
        "Epoch [7/10], Training Loss: 0.843, Validation Accuracy: 60.73%\n",
        "Epoch [8/10], Training Loss: 0.825, Validation Accuracy: 60.22%\n",
        "Epoch [9/10], Training Loss: 0.818, Validation Accuracy: 60.43%\n",
        "Epoch [10/10], Training Loss: 0.794, Validation Accuracy: 60.61%\n",
        "Epoch [1/10], Training Loss: 1.013, Validation Accuracy: 60.49%\n",
        "Epoch [2/10], Training Loss: 0.945, Validation Accuracy: 61.50%\n",
        "Epoch [3/10], Training Loss: 0.919, Validation Accuracy: 60.64%\n",
        "Epoch [4/10], Training Loss: 0.885, Validation Accuracy: 60.93%\n",
        "Epoch [5/10], Training Loss: 0.860, Validation Accuracy: 61.15%\n",
        "Epoch [6/10], Training Loss: 0.850, Validation Accuracy: 60.79%\n",
        "Epoch [7/10], Training Loss: 0.831, Validation Accuracy: 60.49%\n",
        "Epoch [8/10], Training Loss: 0.808, Validation Accuracy: 60.96%\n",
        "Epoch [9/10], Training Loss: 0.799, Validation Accuracy: 60.60%\n",
        "Epoch [10/10], Training Loss: 0.785, Validation Accuracy: 60.42%\n",
        "Epoch [1/10], Training Loss: 1.011, Validation Accuracy: 60.63%\n",
        "Epoch [2/10], Training Loss: 0.938, Validation Accuracy: 61.38%\n",
        "Epoch [3/10], Training Loss: 0.918, Validation Accuracy: 61.08%\n",
        "Epoch [4/10], Training Loss: 0.881, Validation Accuracy: 60.46%\n",
        "Epoch [5/10], Training Loss: 0.869, Validation Accuracy: 60.21%\n",
        "Epoch [6/10], Training Loss: 0.847, Validation Accuracy: 60.59%\n",
        "Epoch [7/10], Training Loss: 0.828, Validation Accuracy: 60.93%\n",
        "Epoch [8/10], Training Loss: 0.816, Validation Accuracy: 60.90%\n",
        "Epoch [9/10], Training Loss: 0.800, Validation Accuracy: 60.37%\n",
        "Epoch [10/10], Training Loss: 0.780, Validation Accuracy: 60.75%\n",
        "Epoch [1/10], Training Loss: 0.971, Validation Accuracy: 61.08%\n",
        "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.22%\n",
        "Epoch [3/10], Training Loss: 0.873, Validation Accuracy: 61.04%\n",
        "Epoch [4/10], Training Loss: 0.853, Validation Accuracy: 60.78%\n",
        "Epoch [5/10], Training Loss: 0.830, Validation Accuracy: 61.20%\n",
        "Epoch [6/10], Training Loss: 0.803, Validation Accuracy: 60.84%\n",
        "Epoch [7/10], Training Loss: 0.792, Validation Accuracy: 59.53%\n",
        "Epoch [8/10], Training Loss: 0.776, Validation Accuracy: 61.10%\n",
        "Epoch [9/10], Training Loss: 0.756, Validation Accuracy: 60.57%\n",
        "Epoch [10/10], Training Loss: 0.740, Validation Accuracy: 61.06%\n",
        "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 60.35%\n",
        "Epoch [2/10], Training Loss: 0.905, Validation Accuracy: 61.11%\n",
        "Epoch [3/10], Training Loss: 0.860, Validation Accuracy: 61.29%\n",
        "Epoch [4/10], Training Loss: 0.829, Validation Accuracy: 60.95%\n",
        "Epoch [5/10], Training Loss: 0.804, Validation Accuracy: 61.19%\n",
        "Epoch [6/10], Training Loss: 0.786, Validation Accuracy: 60.87%\n",
        "Epoch [7/10], Training Loss: 0.765, Validation Accuracy: 60.01%\n",
        "Epoch [8/10], Training Loss: 0.749, Validation Accuracy: 61.41%\n",
        "Epoch [9/10], Training Loss: 0.730, Validation Accuracy: 61.28%\n",
        "Epoch [10/10], Training Loss: 0.715, Validation Accuracy: 61.21%\n",
        "Epoch [1/10], Training Loss: 0.960, Validation Accuracy: 61.26%\n",
        "Epoch [2/10], Training Loss: 0.884, Validation Accuracy: 60.28%\n",
        "Epoch [3/10], Training Loss: 0.841, Validation Accuracy: 60.33%\n",
        "Epoch [4/10], Training Loss: 0.825, Validation Accuracy: 61.44%\n",
        "Epoch [5/10], Training Loss: 0.794, Validation Accuracy: 60.29%\n",
        "Epoch [6/10], Training Loss: 0.786, Validation Accuracy: 61.16%\n",
        "Epoch [7/10], Training Loss: 0.752, Validation Accuracy: 60.70%\n",
        "Epoch [8/10], Training Loss: 0.736, Validation Accuracy: 61.28%\n",
        "Epoch [9/10], Training Loss: 0.725, Validation Accuracy: 60.86%\n",
        "Epoch [10/10], Training Loss: 0.703, Validation Accuracy: 60.65%\n",
        "Epoch [1/10], Training Loss: 0.964, Validation Accuracy: 60.52%\n",
        "Epoch [2/10], Training Loss: 0.880, Validation Accuracy: 61.13%\n",
        "Epoch [3/10], Training Loss: 0.837, Validation Accuracy: 61.02%\n",
        "Epoch [4/10], Training Loss: 0.801, Validation Accuracy: 60.74%\n",
        "Epoch [5/10], Training Loss: 0.776, Validation Accuracy: 61.03%\n",
        "Epoch [6/10], Training Loss: 0.760, Validation Accuracy: 60.71%\n",
        "Epoch [7/10], Training Loss: 0.738, Validation Accuracy: 61.42%\n",
        "Epoch [8/10], Training Loss: 0.725, Validation Accuracy: 61.41%\n",
        "Epoch [9/10], Training Loss: 0.710, Validation Accuracy: 60.98%\n",
        "Epoch [10/10], Training Loss: 0.682, Validation Accuracy: 60.33%\n",
        "Epoch [1/10], Training Loss: 0.955, Validation Accuracy: 61.37%\n",
        "Epoch [2/10], Training Loss: 0.885, Validation Accuracy: 60.84%\n",
        "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 61.90%\n",
        "Epoch [4/10], Training Loss: 0.802, Validation Accuracy: 60.58%\n",
        "Epoch [5/10], Training Loss: 0.782, Validation Accuracy: 61.31%\n",
        "Epoch [6/10], Training Loss: 0.759, Validation Accuracy: 61.21%\n",
        "Epoch [7/10], Training Loss: 0.733, Validation Accuracy: 60.95%\n",
        "Epoch [8/10], Training Loss: 0.723, Validation Accuracy: 60.70%\n",
        "Epoch [9/10], Training Loss: 0.705, Validation Accuracy: 60.68%\n",
        "Epoch [10/10], Training Loss: 0.694, Validation Accuracy: 60.80%\n",
        "Epoch [1/10], Training Loss: 0.916, Validation Accuracy: 61.00%\n",
        "Epoch [2/10], Training Loss: 0.845, Validation Accuracy: 61.24%\n",
        "Epoch [3/10], Training Loss: 0.806, Validation Accuracy: 61.42%\n",
        "Epoch [4/10], Training Loss: 0.765, Validation Accuracy: 61.49%\n",
        "Epoch [5/10], Training Loss: 0.742, Validation Accuracy: 60.33%\n",
        "Epoch [6/10], Training Loss: 0.722, Validation Accuracy: 61.26%\n",
        "Epoch [7/10], Training Loss: 0.704, Validation Accuracy: 61.32%\n",
        "Epoch [8/10], Training Loss: 0.680, Validation Accuracy: 60.75%\n",
        "Epoch [9/10], Training Loss: 0.664, Validation Accuracy: 60.80%\n",
        "Epoch [10/10], Training Loss: 0.646, Validation Accuracy: 61.09%\n",
        "Epoch [1/10], Training Loss: 0.905, Validation Accuracy: 60.94%\n",
        "Epoch [2/10], Training Loss: 0.821, Validation Accuracy: 61.18%\n",
        "Epoch [3/10], Training Loss: 0.773, Validation Accuracy: 60.82%\n",
        "Epoch [4/10], Training Loss: 0.750, Validation Accuracy: 60.96%\n",
        "Epoch [5/10], Training Loss: 0.725, Validation Accuracy: 60.93%\n",
        "Epoch [6/10], Training Loss: 0.695, Validation Accuracy: 61.28%\n",
        "Epoch [7/10], Training Loss: 0.680, Validation Accuracy: 60.41%\n",
        "Epoch [8/10], Training Loss: 0.660, Validation Accuracy: 61.03%\n",
        "Epoch [9/10], Training Loss: 0.633, Validation Accuracy: 60.37%\n",
        "Epoch [10/10], Training Loss: 0.630, Validation Accuracy: 61.11%\n",
        "Epoch [1/10], Training Loss: 0.901, Validation Accuracy: 60.82%\n",
        "Epoch [2/10], Training Loss: 0.816, Validation Accuracy: 61.90%\n",
        "Epoch [3/10], Training Loss: 0.777, Validation Accuracy: 60.88%\n",
        "Epoch [4/10], Training Loss: 0.745, Validation Accuracy: 60.90%\n",
        "Epoch [5/10], Training Loss: 0.713, Validation Accuracy: 61.35%\n",
        "Epoch [6/10], Training Loss: 0.696, Validation Accuracy: 61.29%\n",
        "Epoch [7/10], Training Loss: 0.676, Validation Accuracy: 61.35%\n",
        "Epoch [8/10], Training Loss: 0.653, Validation Accuracy: 61.05%\n",
        "Epoch [9/10], Training Loss: 0.636, Validation Accuracy: 61.32%\n",
        "Epoch [10/10], Training Loss: 0.609, Validation Accuracy: 60.51%\n",
        "Epoch [1/10], Training Loss: 0.901, Validation Accuracy: 61.22%\n",
        "Epoch [2/10], Training Loss: 0.818, Validation Accuracy: 60.65%\n",
        "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 61.43%\n",
        "Epoch [4/10], Training Loss: 0.727, Validation Accuracy: 61.26%\n",
        "Epoch [5/10], Training Loss: 0.699, Validation Accuracy: 61.89%\n",
        "Epoch [6/10], Training Loss: 0.681, Validation Accuracy: 61.45%\n",
        "Epoch [7/10], Training Loss: 0.646, Validation Accuracy: 61.35%\n",
        "Epoch [8/10], Training Loss: 0.630, Validation Accuracy: 60.99%\n",
        "Epoch [9/10], Training Loss: 0.618, Validation Accuracy: 60.98%\n",
        "Epoch [10/10], Training Loss: 0.597, Validation Accuracy: 61.27%\n",
        "Epoch [1/10], Training Loss: 0.919, Validation Accuracy: 60.88%\n",
        "Epoch [2/10], Training Loss: 0.818, Validation Accuracy: 60.84%\n",
        "Epoch [3/10], Training Loss: 0.764, Validation Accuracy: 61.43%\n",
        "Epoch [4/10], Training Loss: 0.729, Validation Accuracy: 60.86%\n",
        "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.36%\n",
        "Epoch [6/10], Training Loss: 0.679, Validation Accuracy: 60.98%\n",
        "Epoch [7/10], Training Loss: 0.667, Validation Accuracy: 60.80%\n",
        "Epoch [8/10], Training Loss: 0.633, Validation Accuracy: 60.87%\n",
        "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 60.74%\n",
        "Epoch [10/10], Training Loss: 0.600, Validation Accuracy: 60.46%\n",
        "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.48%\n",
        "Epoch [2/10], Training Loss: 0.776, Validation Accuracy: 60.71%\n",
        "Epoch [3/10], Training Loss: 0.737, Validation Accuracy: 61.46%\n",
        "Epoch [4/10], Training Loss: 0.694, Validation Accuracy: 61.42%\n",
        "Epoch [5/10], Training Loss: 0.657, Validation Accuracy: 61.03%\n",
        "Epoch [6/10], Training Loss: 0.641, Validation Accuracy: 60.06%\n",
        "Epoch [7/10], Training Loss: 0.615, Validation Accuracy: 60.91%\n",
        "Epoch [8/10], Training Loss: 0.596, Validation Accuracy: 60.62%\n",
        "Epoch [9/10], Training Loss: 0.575, Validation Accuracy: 60.86%\n",
        "Epoch [10/10], Training Loss: 0.558, Validation Accuracy: 60.66%\n",
        "Epoch [1/10], Training Loss: 0.866, Validation Accuracy: 60.21%\n",
        "Epoch [2/10], Training Loss: 0.767, Validation Accuracy: 61.07%\n",
        "Epoch [3/10], Training Loss: 0.709, Validation Accuracy: 60.28%\n",
        "Epoch [4/10], Training Loss: 0.683, Validation Accuracy: 60.10%\n",
        "Epoch [5/10], Training Loss: 0.652, Validation Accuracy: 61.27%\n",
        "Epoch [6/10], Training Loss: 0.625, Validation Accuracy: 60.50%\n",
        "Epoch [7/10], Training Loss: 0.600, Validation Accuracy: 61.06%\n",
        "Epoch [8/10], Training Loss: 0.572, Validation Accuracy: 60.99%\n",
        "Epoch [9/10], Training Loss: 0.555, Validation Accuracy: 61.03%\n",
        "Epoch [10/10], Training Loss: 0.535, Validation Accuracy: 60.59%\n",
        "Epoch [1/10], Training Loss: 0.857, Validation Accuracy: 61.03%\n",
        "Epoch [2/10], Training Loss: 0.760, Validation Accuracy: 60.72%\n",
        "Epoch [3/10], Training Loss: 0.711, Validation Accuracy: 61.22%\n",
        "Epoch [4/10], Training Loss: 0.663, Validation Accuracy: 59.98%\n",
        "Epoch [5/10], Training Loss: 0.649, Validation Accuracy: 60.82%\n",
        "Epoch [6/10], Training Loss: 0.611, Validation Accuracy: 60.84%\n",
        "Epoch [7/10], Training Loss: 0.581, Validation Accuracy: 60.79%\n",
        "Epoch [8/10], Training Loss: 0.564, Validation Accuracy: 60.55%\n",
        "Epoch [9/10], Training Loss: 0.552, Validation Accuracy: 60.82%\n",
        "Epoch [10/10], Training Loss: 0.526, Validation Accuracy: 60.48%\n",
        "Epoch [1/10], Training Loss: 0.881, Validation Accuracy: 60.37%\n",
        "Epoch [2/10], Training Loss: 0.755, Validation Accuracy: 60.76%\n",
        "Epoch [3/10], Training Loss: 0.697, Validation Accuracy: 61.37%\n",
        "Epoch [4/10], Training Loss: 0.657, Validation Accuracy: 60.86%\n",
        "Epoch [5/10], Training Loss: 0.620, Validation Accuracy: 61.50%\n",
        "Epoch [6/10], Training Loss: 0.588, Validation Accuracy: 60.81%\n",
        "Epoch [7/10], Training Loss: 0.576, Validation Accuracy: 61.76%\n",
        "Epoch [8/10], Training Loss: 0.550, Validation Accuracy: 60.62%\n",
        "Epoch [9/10], Training Loss: 0.534, Validation Accuracy: 60.90%\n",
        "Epoch [10/10], Training Loss: 0.518, Validation Accuracy: 61.11%\n",
        "Epoch [1/10], Training Loss: 0.876, Validation Accuracy: 61.50%\n",
        "Epoch [2/10], Training Loss: 0.751, Validation Accuracy: 61.59%\n",
        "Epoch [3/10], Training Loss: 0.701, Validation Accuracy: 60.73%\n",
        "Epoch [4/10], Training Loss: 0.663, Validation Accuracy: 60.94%\n",
        "Epoch [5/10], Training Loss: 0.630, Validation Accuracy: 60.23%\n",
        "Epoch [6/10], Training Loss: 0.592, Validation Accuracy: 60.85%\n",
        "Epoch [7/10], Training Loss: 0.567, Validation Accuracy: 60.74%\n",
        "Epoch [8/10], Training Loss: 0.558, Validation Accuracy: 59.98%\n",
        "Epoch [9/10], Training Loss: 0.526, Validation Accuracy: 59.97%\n",
        "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 60.22%\n",
        "Epoch [1/10], Training Loss: 0.836, Validation Accuracy: 60.60%\n",
        "Epoch [2/10], Training Loss: 0.725, Validation Accuracy: 60.98%\n",
        "Epoch [3/10], Training Loss: 0.663, Validation Accuracy: 61.04%\n",
        "Epoch [4/10], Training Loss: 0.619, Validation Accuracy: 60.43%\n",
        "Epoch [5/10], Training Loss: 0.589, Validation Accuracy: 61.41%\n",
        "Epoch [6/10], Training Loss: 0.551, Validation Accuracy: 60.80%\n",
        "Epoch [7/10], Training Loss: 0.536, Validation Accuracy: 61.42%\n",
        "Epoch [8/10], Training Loss: 0.509, Validation Accuracy: 61.40%\n",
        "Epoch [9/10], Training Loss: 0.493, Validation Accuracy: 60.61%\n",
        "Epoch [10/10], Training Loss: 0.467, Validation Accuracy: 60.28%\n",
        "Epoch [1/10], Training Loss: 0.827, Validation Accuracy: 60.57%\n",
        "Epoch [2/10], Training Loss: 0.697, Validation Accuracy: 60.26%\n",
        "Epoch [3/10], Training Loss: 0.639, Validation Accuracy: 60.11%\n",
        "Epoch [4/10], Training Loss: 0.592, Validation Accuracy: 60.82%\n",
        "Epoch [5/10], Training Loss: 0.564, Validation Accuracy: 60.84%\n",
        "Epoch [6/10], Training Loss: 0.535, Validation Accuracy: 60.83%\n",
        "Epoch [7/10], Training Loss: 0.511, Validation Accuracy: 59.87%\n",
        "Epoch [8/10], Training Loss: 0.489, Validation Accuracy: 61.02%\n",
        "Epoch [9/10], Training Loss: 0.464, Validation Accuracy: 60.50%\n",
        "Epoch [10/10], Training Loss: 0.450, Validation Accuracy: 59.91%\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to find validation accuracies\n",
        "accuracies = re.findall(r'Validation Accuracy: (\\d+\\.\\d+)%', log)\n",
        "\n",
        "# Convert accuracies from string to float\n",
        "accuracies = [float(acc) for acc in accuracies]\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracies:\", accuracies)\n",
        "\n",
        "# Print size of the array\n",
        "print(\"Size of array:\", len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebdf5d0-03ed-4dfa-b59f-391d76ca633f",
        "id": "wPdKra2qnYGr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [10.22, 10.22, 10.22, 10.22, 10.22, 10.27, 10.44, 11.48, 12.79, 16.07, 16.34, 16.02, 15.54, 15.61, 16.07, 16.72, 18.66, 20.07, 20.51, 21.34, 21.24, 21.46, 23.0, 23.2, 23.93, 24.03, 24.67, 25.0, 25.54, 25.19, 26.08, 26.84, 27.22, 27.54, 27.62, 27.7, 28.27, 28.65, 27.64, 29.48, 28.98, 29.39, 29.5, 30.02, 30.38, 30.97, 31.25, 31.1, 31.92, 32.82, 33.25, 34.31, 34.87, 35.19, 35.66, 36.75, 36.15, 37.58, 38.2, 38.61, 38.25, 38.87, 39.57, 39.27, 39.75, 40.7, 40.68, 41.86, 42.18, 42.33, 42.83, 42.52, 42.18, 43.16, 43.46, 44.19, 43.57, 44.77, 44.8, 45.07, 44.45, 45.18, 44.21, 45.92, 46.38, 46.53, 46.71, 45.55, 46.73, 47.01, 47.37, 48.08, 47.86, 48.37, 47.69, 48.61, 48.93, 49.18, 48.8, 49.24, 49.76, 49.8, 48.89, 48.65, 48.97, 49.77, 49.89, 50.12, 49.7, 49.97, 49.57, 50.33, 50.78, 50.65, 51.66, 50.5, 50.83, 51.72, 51.48, 51.55, 52.04, 50.8, 51.47, 52.32, 51.89, 52.09, 52.64, 51.79, 52.73, 52.31, 52.5, 52.46, 53.36, 53.1, 53.25, 53.31, 53.36, 53.26, 53.6, 53.26, 54.31, 53.59, 53.57, 52.78, 54.09, 54.12, 54.07, 54.17, 53.9, 54.34, 53.63, 53.9, 54.7, 54.55, 54.01, 54.83, 54.63, 53.69, 55.36, 55.04, 54.65, 55.58, 55.52, 55.71, 55.39, 55.39, 55.8, 55.59, 55.73, 55.11, 55.91, 56.35, 55.29, 55.67, 55.44, 56.12, 55.47, 56.12, 55.27, 56.23, 56.08, 56.88, 56.74, 56.61, 56.43, 56.95, 56.38, 57.09, 57.26, 56.72, 57.23, 57.44, 57.23, 56.99, 57.16, 57.1, 57.01, 56.99, 57.78, 57.3, 57.42, 57.84, 57.43, 57.56, 58.04, 58.21, 58.12, 57.86, 57.62, 57.36, 58.14, 57.97, 58.24, 58.15, 58.05, 58.34, 57.45, 58.09, 58.78, 57.85, 57.93, 58.49, 57.52, 57.97, 58.4, 59.01, 58.55, 58.04, 58.28, 58.6, 58.98, 59.28, 58.76, 59.36, 58.62, 58.78, 59.4, 59.32, 58.93, 58.94, 58.97, 59.3, 58.81, 59.17, 59.51, 59.5, 59.45, 59.39, 59.49, 58.92, 59.24, 59.17, 59.61, 59.28, 58.87, 59.42, 59.14, 59.03, 59.28, 59.2, 60.01, 60.28, 60.32, 59.92, 60.24, 60.16, 59.1, 58.91, 58.84, 59.99, 59.95, 60.28, 59.95, 59.97, 60.12, 60.32, 59.7, 60.23, 59.99, 58.65, 60.19, 60.5, 60.3, 61.11, 60.52, 60.11, 60.59, 60.77, 59.72, 60.56, 60.72, 59.96, 60.29, 60.11, 59.82, 60.96, 59.45, 60.61, 60.35, 60.03, 60.41, 60.52, 60.78, 59.77, 60.94, 60.39, 60.73, 60.22, 60.43, 60.61, 60.49, 61.5, 60.64, 60.93, 61.15, 60.79, 60.49, 60.96, 60.6, 60.42, 60.63, 61.38, 61.08, 60.46, 60.21, 60.59, 60.93, 60.9, 60.37, 60.75, 61.08, 61.22, 61.04, 60.78, 61.2, 60.84, 59.53, 61.1, 60.57, 61.06, 60.35, 61.11, 61.29, 60.95, 61.19, 60.87, 60.01, 61.41, 61.28, 61.21, 61.26, 60.28, 60.33, 61.44, 60.29, 61.16, 60.7, 61.28, 60.86, 60.65, 60.52, 61.13, 61.02, 60.74, 61.03, 60.71, 61.42, 61.41, 60.98, 60.33, 61.37, 60.84, 61.9, 60.58, 61.31, 61.21, 60.95, 60.7, 60.68, 60.8, 61.0, 61.24, 61.42, 61.49, 60.33, 61.26, 61.32, 60.75, 60.8, 61.09, 60.94, 61.18, 60.82, 60.96, 60.93, 61.28, 60.41, 61.03, 60.37, 61.11, 60.82, 61.9, 60.88, 60.9, 61.35, 61.29, 61.35, 61.05, 61.32, 60.51, 61.22, 60.65, 61.43, 61.26, 61.89, 61.45, 61.35, 60.99, 60.98, 61.27, 60.88, 60.84, 61.43, 60.86, 61.36, 60.98, 60.8, 60.87, 60.74, 60.46, 60.48, 60.71, 61.46, 61.42, 61.03, 60.06, 60.91, 60.62, 60.86, 60.66, 60.21, 61.07, 60.28, 60.1, 61.27, 60.5, 61.06, 60.99, 61.03, 60.59, 61.03, 60.72, 61.22, 59.98, 60.82, 60.84, 60.79, 60.55, 60.82, 60.48, 60.37, 60.76, 61.37, 60.86, 61.5, 60.81, 61.76, 60.62, 60.9, 61.11, 61.5, 61.59, 60.73, 60.94, 60.23, 60.85, 60.74, 59.98, 59.97, 60.22, 60.6, 60.98, 61.04, 60.43, 61.41, 60.8, 61.42, 61.4, 60.61, 60.28, 60.57, 60.26, 60.11, 60.82, 60.84, 60.83, 59.87, 61.02, 60.5, 59.91]\n",
            "Size of array: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lynhb80Wm3oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import random\n",
        "\n",
        "# Define VAE model\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 4x4\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2*z_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 4, 4)),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),  # 32x32\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = h[:, :self.z_dim], h[:, self.z_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Define VAE training procedure\n",
        "def vae_train(vae: VAE, trainloader: DataLoader, epochs: int) -> None:\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        vae.train()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, _ = data\n",
        "            optimizer.zero_grad()\n",
        "            recon_x, mu, logvar = vae(inputs)\n",
        "            loss = vae_loss(recon_x, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Define classification model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "full_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Separate dataset by class\n",
        "class_indices = {i: [] for i in range(10)}  # CIFAR-10 has 10 classes\n",
        "for idx, (_, label) in enumerate(full_dataset):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "# Define target count per class, summing to 60,000 with random distribution\n",
        "class_counts = np.random.multinomial(60000, [0.1] * 10)  # Adjust probabilities if you want specific class biases\n",
        "print(\"Random Images per Class:\", class_counts)\n",
        "\n",
        "# Sample indices based on the specified class counts\n",
        "indices = []\n",
        "for class_id, count in enumerate(class_counts):\n",
        "    # Ensure count does not exceed available images\n",
        "    count = min(count, len(class_indices[class_id]))\n",
        "    selected_indices = random.sample(class_indices[class_id], count)\n",
        "    indices.extend(selected_indices)\n",
        "\n",
        "# Create a custom CIFAR-10 dataset with the sampled indices\n",
        "custom_dataset = Subset(full_dataset, indices)\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_set, val_set = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create training and validation loaders\n",
        "trainloader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define training procedure for classification model\n",
        "def train(net: nn.Module, trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation loop\n",
        "        net.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data\n",
        "                outputs = net(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / total\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {running_loss / (i+1):.3f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# Define evaluation procedure\n",
        "def evaluate(net: nn.Module, testloader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    # Calculate TP, FP, TN, FN for each class\n",
        "    tp = np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "    tn = cm.sum() - (fp + fn + tp)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each class\n",
        "    precision = precision_score(all_labels, all_predictions, average=None)\n",
        "    recall = recall_score(all_labels, all_predictions, average=None)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
        "\n",
        "    return accuracy, tp, fp, tn, fn, precision, recall, f1\n",
        "\n",
        "# Initialize clients\n",
        "def initialize_clients(trainset, transform, batch_size, num_clients):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "        client_trainset = torch.utils.data.Subset(trainset, range(i * len(trainset) // num_clients, (i + 1) * len(trainset) // num_clients))\n",
        "        client_trainloader = torch.utils.data.DataLoader(client_trainset, batch_size=batch_size, shuffle=True)\n",
        "        clients[f\"client_{i}\"] = client_trainloader\n",
        "    return clients\n",
        "\n",
        "def get_distribution_info(vae: VAE) -> Dict:\n",
        "    # Implement the logic to extract distribution information from the VAE\n",
        "    # This can involve computing statistics, parameters, or any other relevant information\n",
        "    # that can be used to generate augmented data\n",
        "\n",
        "    # Example implementation:\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": vae.encoder[-1].bias.data.cpu().numpy(),\n",
        "            \"std\": torch.exp(0.5 * vae.encoder[-1].weight.data).cpu().numpy()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return distribution_info\n",
        "\n",
        "def send_distribution_info(distribution_info: Dict) -> None:\n",
        "    # Implement the logic to send the distribution information to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the information\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the distribution information to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "def generate_augmented_data(vae: VAE, distribution_info_uniform: Dict ) -> torch.Tensor:\n",
        "    # Generate augmented data using both uniform and truncated uniform distributions\n",
        "\n",
        "    mean = distribution_info_uniform[\"mean\"].mean().item()  # Convert numpy array to float\n",
        "    std = distribution_info_uniform[\"std\"].mean().item()  # Convert numpy array to float\n",
        "\n",
        "\n",
        "\n",
        "     # Generate augmented data using Uniform distribution\n",
        "    augmented_data_uniform = torch.FloatTensor(64, vae.z_dim).uniform_(mean - std, mean + std)\n",
        "\n",
        "\n",
        "    # Calculate the average of augmented data from both distributions\n",
        "    augmented_data_average = augmented_data_uniform\n",
        "\n",
        "    return augmented_data_average\n",
        "\n",
        "def federated_train(net: nn.Module, vae: VAE, trainloaders: Dict[str, DataLoader], trainloader: DataLoader, valloader: DataLoader, epochs: int) -> None:\n",
        "    for epoch in range(epochs):\n",
        "        for client_id, client_trainloader in trainloaders.items():\n",
        "            # Train VAE on client data\n",
        "            vae_train(vae, client_trainloader, epochs=10)\n",
        "\n",
        "            # Share distribution information with global server\n",
        "            distribution_info = get_distribution_info(vae)\n",
        "            send_distribution_info(distribution_info)\n",
        "\n",
        "            # Receive distribution information from other clients\n",
        "            other_distribution_info = receive_distribution_info()\n",
        "\n",
        "            # Generate augmented data using received distribution information\n",
        "            augmented_data = generate_augmented_data(vae, other_distribution_info[\"uniform\"])\n",
        "\n",
        "            # Train classification model using local, augmented, and validation data\n",
        "            train(net, client_trainloader, valloader, epochs=10)\n",
        "\n",
        "            # Send model updates to global server\n",
        "            send_model_update(client_id, net.state_dict())\n",
        "\n",
        "# Define logic to receive distribution information from global server\n",
        "def receive_distribution_info() -> Dict:\n",
        "    # Receive distribution information logic\n",
        "    distribution_info = {\n",
        "        \"uniform\": {\n",
        "            \"mean\": np.zeros(20),  # Adjust the size based on your latent space dimension\n",
        "            \"std\": np.ones(20)\n",
        "        }\n",
        "    }\n",
        "    return distribution_info\n",
        "\n",
        "def send_model_update(client_id: str, model_update: Dict) -> None:\n",
        "    # Implement the logic to send the model update to the global server\n",
        "    # This can involve using a network protocol, a message queue, or any other communication mechanism\n",
        "    # to send the model update\n",
        "\n",
        "    # Example implementation:\n",
        "    # Send the model update to the global server using a network protocol\n",
        "    # For example, you can use the `socket` module to send the information over a network\n",
        "    # or use a message queue like `RabbitMQ` to send the information\n",
        "    pass\n",
        "\n",
        "# Define global server procedure\n",
        "def global_server() -> None:\n",
        "    net = Net()\n",
        "    x_dim = 3 * 32 * 32  # CIFAR-10 input size\n",
        "    h_dim = 400\n",
        "    z_dim = 20\n",
        "    vae = VAE(x_dim, h_dim, z_dim)  # Initialize VAE object with required arguments\n",
        "\n",
        "    # Initialize clients\n",
        "    num_clients = 5  # Define the number of clients\n",
        "    clients = initialize_clients(train_set, transform, batch_size=128, num_clients=num_clients)\n",
        "\n",
        "    # Train model using FedDIS\n",
        "    federated_train(net, vae, clients, trainloader, valloader, epochs=10)\n",
        "\n",
        "    # Evaluate final model\n",
        "    test_accuracy, tp, fp, tn, fn, precision, recall, f1 = evaluate(net, testloader)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"True Positives (TP):\", tp)\n",
        "    print(\"False Positives (FP):\", fp)\n",
        "    print(\"True Negatives (TN):\", tn)\n",
        "    print(\"False Negatives (FN):\", fn)\n",
        "    print(\":\", fn+tn+tp+fp)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310f5a43-979c-485d-f51b-dd84e42bc823",
        "id": "vpKCzIbdm4gQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 80.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Random Images per Class: [6023 5956 5990 6006 5979 6044 6106 5926 5927 6043]\n",
            "Epoch [1/10], Training Loss: 2.303, Validation Accuracy: 10.51%\n",
            "Epoch [2/10], Training Loss: 2.302, Validation Accuracy: 10.51%\n",
            "Epoch [3/10], Training Loss: 2.301, Validation Accuracy: 10.51%\n",
            "Epoch [4/10], Training Loss: 2.299, Validation Accuracy: 10.51%\n",
            "Epoch [5/10], Training Loss: 2.297, Validation Accuracy: 10.51%\n",
            "Epoch [6/10], Training Loss: 2.295, Validation Accuracy: 10.54%\n",
            "Epoch [7/10], Training Loss: 2.292, Validation Accuracy: 10.92%\n",
            "Epoch [8/10], Training Loss: 2.287, Validation Accuracy: 12.57%\n",
            "Epoch [9/10], Training Loss: 2.281, Validation Accuracy: 15.49%\n",
            "Epoch [10/10], Training Loss: 2.271, Validation Accuracy: 17.34%\n",
            "Epoch [1/10], Training Loss: 2.260, Validation Accuracy: 18.80%\n",
            "Epoch [2/10], Training Loss: 2.239, Validation Accuracy: 20.08%\n",
            "Epoch [3/10], Training Loss: 2.208, Validation Accuracy: 22.09%\n",
            "Epoch [4/10], Training Loss: 2.166, Validation Accuracy: 22.77%\n",
            "Epoch [5/10], Training Loss: 2.121, Validation Accuracy: 24.34%\n",
            "Epoch [6/10], Training Loss: 2.083, Validation Accuracy: 26.10%\n",
            "Epoch [7/10], Training Loss: 2.051, Validation Accuracy: 27.42%\n",
            "Epoch [8/10], Training Loss: 2.018, Validation Accuracy: 28.60%\n",
            "Epoch [9/10], Training Loss: 1.988, Validation Accuracy: 28.71%\n",
            "Epoch [10/10], Training Loss: 1.960, Validation Accuracy: 30.06%\n",
            "Epoch [1/10], Training Loss: 1.936, Validation Accuracy: 30.73%\n",
            "Epoch [2/10], Training Loss: 1.918, Validation Accuracy: 31.29%\n",
            "Epoch [3/10], Training Loss: 1.900, Validation Accuracy: 32.34%\n",
            "Epoch [4/10], Training Loss: 1.878, Validation Accuracy: 32.18%\n",
            "Epoch [5/10], Training Loss: 1.855, Validation Accuracy: 33.25%\n",
            "Epoch [6/10], Training Loss: 1.831, Validation Accuracy: 32.92%\n",
            "Epoch [7/10], Training Loss: 1.817, Validation Accuracy: 34.60%\n",
            "Epoch [8/10], Training Loss: 1.791, Validation Accuracy: 35.34%\n",
            "Epoch [9/10], Training Loss: 1.772, Validation Accuracy: 35.76%\n",
            "Epoch [10/10], Training Loss: 1.761, Validation Accuracy: 35.98%\n",
            "Epoch [1/10], Training Loss: 1.763, Validation Accuracy: 36.73%\n",
            "Epoch [2/10], Training Loss: 1.745, Validation Accuracy: 37.39%\n",
            "Epoch [3/10], Training Loss: 1.726, Validation Accuracy: 38.06%\n",
            "Epoch [4/10], Training Loss: 1.716, Validation Accuracy: 37.69%\n",
            "Epoch [5/10], Training Loss: 1.694, Validation Accuracy: 38.98%\n",
            "Epoch [6/10], Training Loss: 1.685, Validation Accuracy: 39.43%\n",
            "Epoch [7/10], Training Loss: 1.667, Validation Accuracy: 39.53%\n",
            "Epoch [8/10], Training Loss: 1.659, Validation Accuracy: 39.49%\n",
            "Epoch [9/10], Training Loss: 1.652, Validation Accuracy: 40.19%\n",
            "Epoch [10/10], Training Loss: 1.639, Validation Accuracy: 40.32%\n",
            "Epoch [1/10], Training Loss: 1.620, Validation Accuracy: 40.04%\n",
            "Epoch [2/10], Training Loss: 1.613, Validation Accuracy: 41.08%\n",
            "Epoch [3/10], Training Loss: 1.599, Validation Accuracy: 41.71%\n",
            "Epoch [4/10], Training Loss: 1.592, Validation Accuracy: 41.18%\n",
            "Epoch [5/10], Training Loss: 1.575, Validation Accuracy: 42.60%\n",
            "Epoch [6/10], Training Loss: 1.567, Validation Accuracy: 42.41%\n",
            "Epoch [7/10], Training Loss: 1.556, Validation Accuracy: 42.33%\n",
            "Epoch [8/10], Training Loss: 1.548, Validation Accuracy: 42.57%\n",
            "Epoch [9/10], Training Loss: 1.542, Validation Accuracy: 42.67%\n",
            "Epoch [10/10], Training Loss: 1.535, Validation Accuracy: 43.24%\n",
            "Epoch [1/10], Training Loss: 1.573, Validation Accuracy: 42.75%\n",
            "Epoch [2/10], Training Loss: 1.559, Validation Accuracy: 43.24%\n",
            "Epoch [3/10], Training Loss: 1.547, Validation Accuracy: 44.29%\n",
            "Epoch [4/10], Training Loss: 1.542, Validation Accuracy: 43.42%\n",
            "Epoch [5/10], Training Loss: 1.533, Validation Accuracy: 43.70%\n",
            "Epoch [6/10], Training Loss: 1.522, Validation Accuracy: 44.67%\n",
            "Epoch [7/10], Training Loss: 1.510, Validation Accuracy: 45.21%\n",
            "Epoch [8/10], Training Loss: 1.496, Validation Accuracy: 44.80%\n",
            "Epoch [9/10], Training Loss: 1.488, Validation Accuracy: 45.03%\n",
            "Epoch [10/10], Training Loss: 1.479, Validation Accuracy: 44.99%\n",
            "Epoch [1/10], Training Loss: 1.517, Validation Accuracy: 46.46%\n",
            "Epoch [2/10], Training Loss: 1.501, Validation Accuracy: 46.01%\n",
            "Epoch [3/10], Training Loss: 1.487, Validation Accuracy: 46.51%\n",
            "Epoch [4/10], Training Loss: 1.480, Validation Accuracy: 46.72%\n",
            "Epoch [5/10], Training Loss: 1.476, Validation Accuracy: 46.22%\n",
            "Epoch [6/10], Training Loss: 1.460, Validation Accuracy: 45.14%\n",
            "Epoch [7/10], Training Loss: 1.468, Validation Accuracy: 47.29%\n",
            "Epoch [8/10], Training Loss: 1.443, Validation Accuracy: 47.07%\n",
            "Epoch [9/10], Training Loss: 1.439, Validation Accuracy: 47.83%\n",
            "Epoch [10/10], Training Loss: 1.428, Validation Accuracy: 46.96%\n",
            "Epoch [1/10], Training Loss: 1.481, Validation Accuracy: 47.98%\n",
            "Epoch [2/10], Training Loss: 1.459, Validation Accuracy: 46.79%\n",
            "Epoch [3/10], Training Loss: 1.460, Validation Accuracy: 48.81%\n",
            "Epoch [4/10], Training Loss: 1.443, Validation Accuracy: 48.07%\n",
            "Epoch [5/10], Training Loss: 1.435, Validation Accuracy: 47.72%\n",
            "Epoch [6/10], Training Loss: 1.425, Validation Accuracy: 48.88%\n",
            "Epoch [7/10], Training Loss: 1.417, Validation Accuracy: 48.71%\n",
            "Epoch [8/10], Training Loss: 1.414, Validation Accuracy: 48.24%\n",
            "Epoch [9/10], Training Loss: 1.401, Validation Accuracy: 49.81%\n",
            "Epoch [10/10], Training Loss: 1.403, Validation Accuracy: 48.10%\n",
            "Epoch [1/10], Training Loss: 1.438, Validation Accuracy: 49.41%\n",
            "Epoch [2/10], Training Loss: 1.421, Validation Accuracy: 49.97%\n",
            "Epoch [3/10], Training Loss: 1.410, Validation Accuracy: 49.87%\n",
            "Epoch [4/10], Training Loss: 1.403, Validation Accuracy: 49.74%\n",
            "Epoch [5/10], Training Loss: 1.387, Validation Accuracy: 49.64%\n",
            "Epoch [6/10], Training Loss: 1.387, Validation Accuracy: 50.17%\n",
            "Epoch [7/10], Training Loss: 1.380, Validation Accuracy: 50.31%\n",
            "Epoch [8/10], Training Loss: 1.365, Validation Accuracy: 49.62%\n",
            "Epoch [9/10], Training Loss: 1.357, Validation Accuracy: 50.74%\n",
            "Epoch [10/10], Training Loss: 1.348, Validation Accuracy: 50.79%\n",
            "Epoch [1/10], Training Loss: 1.380, Validation Accuracy: 49.35%\n",
            "Epoch [2/10], Training Loss: 1.366, Validation Accuracy: 50.38%\n",
            "Epoch [3/10], Training Loss: 1.354, Validation Accuracy: 49.99%\n",
            "Epoch [4/10], Training Loss: 1.348, Validation Accuracy: 50.52%\n",
            "Epoch [5/10], Training Loss: 1.334, Validation Accuracy: 50.88%\n",
            "Epoch [6/10], Training Loss: 1.329, Validation Accuracy: 50.53%\n",
            "Epoch [7/10], Training Loss: 1.320, Validation Accuracy: 50.91%\n",
            "Epoch [8/10], Training Loss: 1.318, Validation Accuracy: 51.05%\n",
            "Epoch [9/10], Training Loss: 1.309, Validation Accuracy: 51.53%\n",
            "Epoch [10/10], Training Loss: 1.300, Validation Accuracy: 51.29%\n",
            "Epoch [1/10], Training Loss: 1.354, Validation Accuracy: 51.16%\n",
            "Epoch [2/10], Training Loss: 1.338, Validation Accuracy: 51.79%\n",
            "Epoch [3/10], Training Loss: 1.322, Validation Accuracy: 51.77%\n",
            "Epoch [4/10], Training Loss: 1.330, Validation Accuracy: 52.37%\n",
            "Epoch [5/10], Training Loss: 1.304, Validation Accuracy: 51.86%\n",
            "Epoch [6/10], Training Loss: 1.303, Validation Accuracy: 51.63%\n",
            "Epoch [7/10], Training Loss: 1.287, Validation Accuracy: 52.23%\n",
            "Epoch [8/10], Training Loss: 1.278, Validation Accuracy: 52.12%\n",
            "Epoch [9/10], Training Loss: 1.268, Validation Accuracy: 51.96%\n",
            "Epoch [10/10], Training Loss: 1.259, Validation Accuracy: 51.73%\n",
            "Epoch [1/10], Training Loss: 1.346, Validation Accuracy: 51.91%\n",
            "Epoch [2/10], Training Loss: 1.322, Validation Accuracy: 53.03%\n",
            "Epoch [3/10], Training Loss: 1.313, Validation Accuracy: 53.24%\n",
            "Epoch [4/10], Training Loss: 1.300, Validation Accuracy: 53.53%\n",
            "Epoch [5/10], Training Loss: 1.292, Validation Accuracy: 52.29%\n",
            "Epoch [6/10], Training Loss: 1.283, Validation Accuracy: 53.08%\n",
            "Epoch [7/10], Training Loss: 1.270, Validation Accuracy: 53.09%\n",
            "Epoch [8/10], Training Loss: 1.268, Validation Accuracy: 52.92%\n",
            "Epoch [9/10], Training Loss: 1.264, Validation Accuracy: 53.24%\n",
            "Epoch [10/10], Training Loss: 1.250, Validation Accuracy: 53.04%\n",
            "Epoch [1/10], Training Loss: 1.314, Validation Accuracy: 53.88%\n",
            "Epoch [2/10], Training Loss: 1.291, Validation Accuracy: 54.00%\n",
            "Epoch [3/10], Training Loss: 1.278, Validation Accuracy: 53.63%\n",
            "Epoch [4/10], Training Loss: 1.269, Validation Accuracy: 53.83%\n",
            "Epoch [5/10], Training Loss: 1.248, Validation Accuracy: 53.64%\n",
            "Epoch [6/10], Training Loss: 1.249, Validation Accuracy: 54.36%\n",
            "Epoch [7/10], Training Loss: 1.242, Validation Accuracy: 54.07%\n",
            "Epoch [8/10], Training Loss: 1.245, Validation Accuracy: 53.51%\n",
            "Epoch [9/10], Training Loss: 1.218, Validation Accuracy: 54.03%\n",
            "Epoch [10/10], Training Loss: 1.206, Validation Accuracy: 54.60%\n",
            "Epoch [1/10], Training Loss: 1.290, Validation Accuracy: 53.78%\n",
            "Epoch [2/10], Training Loss: 1.276, Validation Accuracy: 54.20%\n",
            "Epoch [3/10], Training Loss: 1.249, Validation Accuracy: 54.85%\n",
            "Epoch [4/10], Training Loss: 1.232, Validation Accuracy: 54.70%\n",
            "Epoch [5/10], Training Loss: 1.220, Validation Accuracy: 54.84%\n",
            "Epoch [6/10], Training Loss: 1.214, Validation Accuracy: 54.74%\n",
            "Epoch [7/10], Training Loss: 1.205, Validation Accuracy: 55.06%\n",
            "Epoch [8/10], Training Loss: 1.205, Validation Accuracy: 54.71%\n",
            "Epoch [9/10], Training Loss: 1.192, Validation Accuracy: 54.45%\n",
            "Epoch [10/10], Training Loss: 1.185, Validation Accuracy: 55.29%\n",
            "Epoch [1/10], Training Loss: 1.242, Validation Accuracy: 55.03%\n",
            "Epoch [2/10], Training Loss: 1.225, Validation Accuracy: 55.40%\n",
            "Epoch [3/10], Training Loss: 1.204, Validation Accuracy: 55.59%\n",
            "Epoch [4/10], Training Loss: 1.196, Validation Accuracy: 55.60%\n",
            "Epoch [5/10], Training Loss: 1.172, Validation Accuracy: 55.82%\n",
            "Epoch [6/10], Training Loss: 1.168, Validation Accuracy: 55.55%\n",
            "Epoch [7/10], Training Loss: 1.157, Validation Accuracy: 56.25%\n",
            "Epoch [8/10], Training Loss: 1.168, Validation Accuracy: 56.18%\n",
            "Epoch [9/10], Training Loss: 1.143, Validation Accuracy: 56.10%\n",
            "Epoch [10/10], Training Loss: 1.131, Validation Accuracy: 55.38%\n",
            "Epoch [1/10], Training Loss: 1.224, Validation Accuracy: 55.43%\n",
            "Epoch [2/10], Training Loss: 1.188, Validation Accuracy: 55.39%\n",
            "Epoch [3/10], Training Loss: 1.185, Validation Accuracy: 55.23%\n",
            "Epoch [4/10], Training Loss: 1.168, Validation Accuracy: 55.50%\n",
            "Epoch [5/10], Training Loss: 1.152, Validation Accuracy: 56.40%\n",
            "Epoch [6/10], Training Loss: 1.143, Validation Accuracy: 55.77%\n",
            "Epoch [7/10], Training Loss: 1.127, Validation Accuracy: 56.40%\n",
            "Epoch [8/10], Training Loss: 1.126, Validation Accuracy: 56.27%\n",
            "Epoch [9/10], Training Loss: 1.109, Validation Accuracy: 56.13%\n",
            "Epoch [10/10], Training Loss: 1.102, Validation Accuracy: 56.05%\n",
            "Epoch [1/10], Training Loss: 1.224, Validation Accuracy: 56.59%\n",
            "Epoch [2/10], Training Loss: 1.194, Validation Accuracy: 55.82%\n",
            "Epoch [3/10], Training Loss: 1.180, Validation Accuracy: 54.97%\n",
            "Epoch [4/10], Training Loss: 1.170, Validation Accuracy: 55.54%\n",
            "Epoch [5/10], Training Loss: 1.161, Validation Accuracy: 56.87%\n",
            "Epoch [6/10], Training Loss: 1.135, Validation Accuracy: 56.76%\n",
            "Epoch [7/10], Training Loss: 1.133, Validation Accuracy: 56.57%\n",
            "Epoch [8/10], Training Loss: 1.118, Validation Accuracy: 56.73%\n",
            "Epoch [9/10], Training Loss: 1.123, Validation Accuracy: 56.11%\n",
            "Epoch [10/10], Training Loss: 1.103, Validation Accuracy: 56.15%\n",
            "Epoch [1/10], Training Loss: 1.210, Validation Accuracy: 56.89%\n",
            "Epoch [2/10], Training Loss: 1.170, Validation Accuracy: 56.92%\n",
            "Epoch [3/10], Training Loss: 1.154, Validation Accuracy: 57.46%\n",
            "Epoch [4/10], Training Loss: 1.136, Validation Accuracy: 57.03%\n",
            "Epoch [5/10], Training Loss: 1.125, Validation Accuracy: 56.89%\n",
            "Epoch [6/10], Training Loss: 1.106, Validation Accuracy: 56.56%\n",
            "Epoch [7/10], Training Loss: 1.094, Validation Accuracy: 56.77%\n",
            "Epoch [8/10], Training Loss: 1.086, Validation Accuracy: 57.53%\n",
            "Epoch [9/10], Training Loss: 1.073, Validation Accuracy: 56.79%\n",
            "Epoch [10/10], Training Loss: 1.068, Validation Accuracy: 57.58%\n",
            "Epoch [1/10], Training Loss: 1.194, Validation Accuracy: 57.86%\n",
            "Epoch [2/10], Training Loss: 1.155, Validation Accuracy: 57.79%\n",
            "Epoch [3/10], Training Loss: 1.143, Validation Accuracy: 57.76%\n",
            "Epoch [4/10], Training Loss: 1.121, Validation Accuracy: 57.69%\n",
            "Epoch [5/10], Training Loss: 1.108, Validation Accuracy: 57.51%\n",
            "Epoch [6/10], Training Loss: 1.094, Validation Accuracy: 57.60%\n",
            "Epoch [7/10], Training Loss: 1.080, Validation Accuracy: 57.81%\n",
            "Epoch [8/10], Training Loss: 1.075, Validation Accuracy: 57.55%\n",
            "Epoch [9/10], Training Loss: 1.062, Validation Accuracy: 57.80%\n",
            "Epoch [10/10], Training Loss: 1.042, Validation Accuracy: 56.97%\n",
            "Epoch [1/10], Training Loss: 1.148, Validation Accuracy: 58.01%\n",
            "Epoch [2/10], Training Loss: 1.114, Validation Accuracy: 57.90%\n",
            "Epoch [3/10], Training Loss: 1.092, Validation Accuracy: 58.20%\n",
            "Epoch [4/10], Training Loss: 1.081, Validation Accuracy: 58.43%\n",
            "Epoch [5/10], Training Loss: 1.064, Validation Accuracy: 58.64%\n",
            "Epoch [6/10], Training Loss: 1.048, Validation Accuracy: 57.81%\n",
            "Epoch [7/10], Training Loss: 1.032, Validation Accuracy: 58.69%\n",
            "Epoch [8/10], Training Loss: 1.022, Validation Accuracy: 58.64%\n",
            "Epoch [9/10], Training Loss: 1.013, Validation Accuracy: 58.18%\n",
            "Epoch [10/10], Training Loss: 1.004, Validation Accuracy: 58.02%\n",
            "Epoch [1/10], Training Loss: 1.125, Validation Accuracy: 58.42%\n",
            "Epoch [2/10], Training Loss: 1.092, Validation Accuracy: 58.08%\n",
            "Epoch [3/10], Training Loss: 1.064, Validation Accuracy: 58.55%\n",
            "Epoch [4/10], Training Loss: 1.052, Validation Accuracy: 58.12%\n",
            "Epoch [5/10], Training Loss: 1.043, Validation Accuracy: 58.17%\n",
            "Epoch [6/10], Training Loss: 1.022, Validation Accuracy: 57.47%\n",
            "Epoch [7/10], Training Loss: 1.008, Validation Accuracy: 57.95%\n",
            "Epoch [8/10], Training Loss: 1.005, Validation Accuracy: 58.09%\n",
            "Epoch [9/10], Training Loss: 0.988, Validation Accuracy: 57.50%\n",
            "Epoch [10/10], Training Loss: 0.983, Validation Accuracy: 57.92%\n",
            "Epoch [1/10], Training Loss: 1.134, Validation Accuracy: 57.73%\n",
            "Epoch [2/10], Training Loss: 1.110, Validation Accuracy: 57.58%\n",
            "Epoch [3/10], Training Loss: 1.083, Validation Accuracy: 58.42%\n",
            "Epoch [4/10], Training Loss: 1.061, Validation Accuracy: 59.10%\n",
            "Epoch [5/10], Training Loss: 1.051, Validation Accuracy: 58.40%\n",
            "Epoch [6/10], Training Loss: 1.029, Validation Accuracy: 58.07%\n",
            "Epoch [7/10], Training Loss: 1.021, Validation Accuracy: 57.85%\n",
            "Epoch [8/10], Training Loss: 1.001, Validation Accuracy: 58.09%\n",
            "Epoch [9/10], Training Loss: 1.000, Validation Accuracy: 57.08%\n",
            "Epoch [10/10], Training Loss: 0.986, Validation Accuracy: 57.76%\n",
            "Epoch [1/10], Training Loss: 1.121, Validation Accuracy: 58.13%\n",
            "Epoch [2/10], Training Loss: 1.090, Validation Accuracy: 59.39%\n",
            "Epoch [3/10], Training Loss: 1.063, Validation Accuracy: 59.07%\n",
            "Epoch [4/10], Training Loss: 1.037, Validation Accuracy: 58.74%\n",
            "Epoch [5/10], Training Loss: 1.020, Validation Accuracy: 59.23%\n",
            "Epoch [6/10], Training Loss: 1.002, Validation Accuracy: 58.74%\n",
            "Epoch [7/10], Training Loss: 0.986, Validation Accuracy: 58.79%\n",
            "Epoch [8/10], Training Loss: 0.979, Validation Accuracy: 59.31%\n",
            "Epoch [9/10], Training Loss: 0.961, Validation Accuracy: 59.15%\n",
            "Epoch [10/10], Training Loss: 0.951, Validation Accuracy: 59.23%\n",
            "Epoch [1/10], Training Loss: 1.114, Validation Accuracy: 58.55%\n",
            "Epoch [2/10], Training Loss: 1.070, Validation Accuracy: 60.04%\n",
            "Epoch [3/10], Training Loss: 1.040, Validation Accuracy: 59.70%\n",
            "Epoch [4/10], Training Loss: 1.026, Validation Accuracy: 59.35%\n",
            "Epoch [5/10], Training Loss: 1.003, Validation Accuracy: 59.01%\n",
            "Epoch [6/10], Training Loss: 0.989, Validation Accuracy: 59.51%\n",
            "Epoch [7/10], Training Loss: 0.973, Validation Accuracy: 59.61%\n",
            "Epoch [8/10], Training Loss: 0.958, Validation Accuracy: 59.73%\n",
            "Epoch [9/10], Training Loss: 0.946, Validation Accuracy: 59.31%\n",
            "Epoch [10/10], Training Loss: 0.943, Validation Accuracy: 59.50%\n",
            "Epoch [1/10], Training Loss: 1.067, Validation Accuracy: 59.69%\n",
            "Epoch [2/10], Training Loss: 1.030, Validation Accuracy: 59.85%\n",
            "Epoch [3/10], Training Loss: 1.008, Validation Accuracy: 59.63%\n",
            "Epoch [4/10], Training Loss: 0.988, Validation Accuracy: 59.88%\n",
            "Epoch [5/10], Training Loss: 0.966, Validation Accuracy: 58.62%\n",
            "Epoch [6/10], Training Loss: 0.944, Validation Accuracy: 59.62%\n",
            "Epoch [7/10], Training Loss: 0.946, Validation Accuracy: 59.30%\n",
            "Epoch [8/10], Training Loss: 0.922, Validation Accuracy: 59.21%\n",
            "Epoch [9/10], Training Loss: 0.920, Validation Accuracy: 58.03%\n",
            "Epoch [10/10], Training Loss: 0.902, Validation Accuracy: 59.46%\n",
            "Epoch [1/10], Training Loss: 1.052, Validation Accuracy: 59.20%\n",
            "Epoch [2/10], Training Loss: 1.015, Validation Accuracy: 59.26%\n",
            "Epoch [3/10], Training Loss: 0.980, Validation Accuracy: 59.81%\n",
            "Epoch [4/10], Training Loss: 0.958, Validation Accuracy: 60.48%\n",
            "Epoch [5/10], Training Loss: 0.937, Validation Accuracy: 59.16%\n",
            "Epoch [6/10], Training Loss: 0.925, Validation Accuracy: 59.04%\n",
            "Epoch [7/10], Training Loss: 0.905, Validation Accuracy: 59.60%\n",
            "Epoch [8/10], Training Loss: 0.892, Validation Accuracy: 59.54%\n",
            "Epoch [9/10], Training Loss: 0.889, Validation Accuracy: 59.49%\n",
            "Epoch [10/10], Training Loss: 0.865, Validation Accuracy: 59.29%\n",
            "Epoch [1/10], Training Loss: 1.071, Validation Accuracy: 59.56%\n",
            "Epoch [2/10], Training Loss: 1.019, Validation Accuracy: 59.41%\n",
            "Epoch [3/10], Training Loss: 0.987, Validation Accuracy: 60.09%\n",
            "Epoch [4/10], Training Loss: 0.973, Validation Accuracy: 59.87%\n",
            "Epoch [5/10], Training Loss: 0.949, Validation Accuracy: 59.68%\n",
            "Epoch [6/10], Training Loss: 0.931, Validation Accuracy: 59.32%\n",
            "Epoch [7/10], Training Loss: 0.916, Validation Accuracy: 59.72%\n",
            "Epoch [8/10], Training Loss: 0.900, Validation Accuracy: 59.78%\n",
            "Epoch [9/10], Training Loss: 0.895, Validation Accuracy: 59.21%\n",
            "Epoch [10/10], Training Loss: 0.871, Validation Accuracy: 59.93%\n",
            "Epoch [1/10], Training Loss: 1.059, Validation Accuracy: 59.41%\n",
            "Epoch [2/10], Training Loss: 0.999, Validation Accuracy: 60.31%\n",
            "Epoch [3/10], Training Loss: 0.960, Validation Accuracy: 59.90%\n",
            "Epoch [4/10], Training Loss: 0.940, Validation Accuracy: 60.23%\n",
            "Epoch [5/10], Training Loss: 0.935, Validation Accuracy: 59.93%\n",
            "Epoch [6/10], Training Loss: 0.916, Validation Accuracy: 58.96%\n",
            "Epoch [7/10], Training Loss: 0.887, Validation Accuracy: 59.75%\n",
            "Epoch [8/10], Training Loss: 0.873, Validation Accuracy: 59.86%\n",
            "Epoch [9/10], Training Loss: 0.865, Validation Accuracy: 59.23%\n",
            "Epoch [10/10], Training Loss: 0.851, Validation Accuracy: 59.58%\n",
            "Epoch [1/10], Training Loss: 1.054, Validation Accuracy: 60.43%\n",
            "Epoch [2/10], Training Loss: 0.990, Validation Accuracy: 60.03%\n",
            "Epoch [3/10], Training Loss: 0.964, Validation Accuracy: 60.86%\n",
            "Epoch [4/10], Training Loss: 0.936, Validation Accuracy: 60.65%\n",
            "Epoch [5/10], Training Loss: 0.922, Validation Accuracy: 60.98%\n",
            "Epoch [6/10], Training Loss: 0.895, Validation Accuracy: 60.32%\n",
            "Epoch [7/10], Training Loss: 0.882, Validation Accuracy: 60.57%\n",
            "Epoch [8/10], Training Loss: 0.867, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.857, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.838, Validation Accuracy: 59.89%\n",
            "Epoch [1/10], Training Loss: 1.018, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.956, Validation Accuracy: 60.30%\n",
            "Epoch [3/10], Training Loss: 0.934, Validation Accuracy: 60.43%\n",
            "Epoch [4/10], Training Loss: 0.904, Validation Accuracy: 60.67%\n",
            "Epoch [5/10], Training Loss: 0.879, Validation Accuracy: 60.54%\n",
            "Epoch [6/10], Training Loss: 0.871, Validation Accuracy: 60.62%\n",
            "Epoch [7/10], Training Loss: 0.851, Validation Accuracy: 60.61%\n",
            "Epoch [8/10], Training Loss: 0.832, Validation Accuracy: 60.71%\n",
            "Epoch [9/10], Training Loss: 0.815, Validation Accuracy: 60.54%\n",
            "Epoch [10/10], Training Loss: 0.809, Validation Accuracy: 60.29%\n",
            "Epoch [1/10], Training Loss: 0.995, Validation Accuracy: 60.38%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.31%\n",
            "Epoch [3/10], Training Loss: 0.908, Validation Accuracy: 60.30%\n",
            "Epoch [4/10], Training Loss: 0.871, Validation Accuracy: 60.31%\n",
            "Epoch [5/10], Training Loss: 0.859, Validation Accuracy: 60.21%\n",
            "Epoch [6/10], Training Loss: 0.840, Validation Accuracy: 60.12%\n",
            "Epoch [7/10], Training Loss: 0.819, Validation Accuracy: 59.68%\n",
            "Epoch [8/10], Training Loss: 0.811, Validation Accuracy: 59.14%\n",
            "Epoch [9/10], Training Loss: 0.782, Validation Accuracy: 60.05%\n",
            "Epoch [10/10], Training Loss: 0.778, Validation Accuracy: 59.91%\n",
            "Epoch [1/10], Training Loss: 1.014, Validation Accuracy: 60.44%\n",
            "Epoch [2/10], Training Loss: 0.942, Validation Accuracy: 60.07%\n",
            "Epoch [3/10], Training Loss: 0.903, Validation Accuracy: 60.60%\n",
            "Epoch [4/10], Training Loss: 0.883, Validation Accuracy: 60.28%\n",
            "Epoch [5/10], Training Loss: 0.860, Validation Accuracy: 60.03%\n",
            "Epoch [6/10], Training Loss: 0.840, Validation Accuracy: 60.37%\n",
            "Epoch [7/10], Training Loss: 0.818, Validation Accuracy: 60.02%\n",
            "Epoch [8/10], Training Loss: 0.804, Validation Accuracy: 59.95%\n",
            "Epoch [9/10], Training Loss: 0.785, Validation Accuracy: 60.23%\n",
            "Epoch [10/10], Training Loss: 0.778, Validation Accuracy: 59.84%\n",
            "Epoch [1/10], Training Loss: 1.011, Validation Accuracy: 60.24%\n",
            "Epoch [2/10], Training Loss: 0.935, Validation Accuracy: 60.54%\n",
            "Epoch [3/10], Training Loss: 0.891, Validation Accuracy: 61.04%\n",
            "Epoch [4/10], Training Loss: 0.872, Validation Accuracy: 60.20%\n",
            "Epoch [5/10], Training Loss: 0.840, Validation Accuracy: 60.16%\n",
            "Epoch [6/10], Training Loss: 0.822, Validation Accuracy: 60.27%\n",
            "Epoch [7/10], Training Loss: 0.801, Validation Accuracy: 60.83%\n",
            "Epoch [8/10], Training Loss: 0.785, Validation Accuracy: 60.94%\n",
            "Epoch [9/10], Training Loss: 0.764, Validation Accuracy: 60.16%\n",
            "Epoch [10/10], Training Loss: 0.760, Validation Accuracy: 59.79%\n",
            "Epoch [1/10], Training Loss: 0.997, Validation Accuracy: 60.68%\n",
            "Epoch [2/10], Training Loss: 0.937, Validation Accuracy: 60.28%\n",
            "Epoch [3/10], Training Loss: 0.888, Validation Accuracy: 61.30%\n",
            "Epoch [4/10], Training Loss: 0.865, Validation Accuracy: 61.66%\n",
            "Epoch [5/10], Training Loss: 0.828, Validation Accuracy: 60.52%\n",
            "Epoch [6/10], Training Loss: 0.809, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.790, Validation Accuracy: 60.52%\n",
            "Epoch [8/10], Training Loss: 0.779, Validation Accuracy: 61.14%\n",
            "Epoch [9/10], Training Loss: 0.758, Validation Accuracy: 60.26%\n",
            "Epoch [10/10], Training Loss: 0.742, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 0.965, Validation Accuracy: 60.72%\n",
            "Epoch [2/10], Training Loss: 0.895, Validation Accuracy: 60.79%\n",
            "Epoch [3/10], Training Loss: 0.874, Validation Accuracy: 61.17%\n",
            "Epoch [4/10], Training Loss: 0.830, Validation Accuracy: 60.95%\n",
            "Epoch [5/10], Training Loss: 0.810, Validation Accuracy: 61.07%\n",
            "Epoch [6/10], Training Loss: 0.793, Validation Accuracy: 61.24%\n",
            "Epoch [7/10], Training Loss: 0.768, Validation Accuracy: 60.74%\n",
            "Epoch [8/10], Training Loss: 0.752, Validation Accuracy: 61.20%\n",
            "Epoch [9/10], Training Loss: 0.731, Validation Accuracy: 60.69%\n",
            "Epoch [10/10], Training Loss: 0.719, Validation Accuracy: 60.23%\n",
            "Epoch [1/10], Training Loss: 0.944, Validation Accuracy: 60.78%\n",
            "Epoch [2/10], Training Loss: 0.882, Validation Accuracy: 60.43%\n",
            "Epoch [3/10], Training Loss: 0.828, Validation Accuracy: 61.55%\n",
            "Epoch [4/10], Training Loss: 0.799, Validation Accuracy: 61.19%\n",
            "Epoch [5/10], Training Loss: 0.777, Validation Accuracy: 61.10%\n",
            "Epoch [6/10], Training Loss: 0.751, Validation Accuracy: 60.87%\n",
            "Epoch [7/10], Training Loss: 0.742, Validation Accuracy: 60.19%\n",
            "Epoch [8/10], Training Loss: 0.720, Validation Accuracy: 60.82%\n",
            "Epoch [9/10], Training Loss: 0.700, Validation Accuracy: 60.61%\n",
            "Epoch [10/10], Training Loss: 0.687, Validation Accuracy: 60.68%\n",
            "Epoch [1/10], Training Loss: 0.954, Validation Accuracy: 60.14%\n",
            "Epoch [2/10], Training Loss: 0.881, Validation Accuracy: 60.30%\n",
            "Epoch [3/10], Training Loss: 0.839, Validation Accuracy: 60.64%\n",
            "Epoch [4/10], Training Loss: 0.813, Validation Accuracy: 60.02%\n",
            "Epoch [5/10], Training Loss: 0.776, Validation Accuracy: 60.14%\n",
            "Epoch [6/10], Training Loss: 0.764, Validation Accuracy: 60.70%\n",
            "Epoch [7/10], Training Loss: 0.741, Validation Accuracy: 60.34%\n",
            "Epoch [8/10], Training Loss: 0.714, Validation Accuracy: 60.35%\n",
            "Epoch [9/10], Training Loss: 0.696, Validation Accuracy: 59.76%\n",
            "Epoch [10/10], Training Loss: 0.693, Validation Accuracy: 60.50%\n",
            "Epoch [1/10], Training Loss: 0.939, Validation Accuracy: 60.43%\n",
            "Epoch [2/10], Training Loss: 0.878, Validation Accuracy: 60.59%\n",
            "Epoch [3/10], Training Loss: 0.816, Validation Accuracy: 60.20%\n",
            "Epoch [4/10], Training Loss: 0.786, Validation Accuracy: 60.72%\n",
            "Epoch [5/10], Training Loss: 0.766, Validation Accuracy: 60.39%\n",
            "Epoch [6/10], Training Loss: 0.739, Validation Accuracy: 60.43%\n",
            "Epoch [7/10], Training Loss: 0.713, Validation Accuracy: 61.16%\n",
            "Epoch [8/10], Training Loss: 0.692, Validation Accuracy: 61.23%\n",
            "Epoch [9/10], Training Loss: 0.679, Validation Accuracy: 59.28%\n",
            "Epoch [10/10], Training Loss: 0.667, Validation Accuracy: 60.78%\n",
            "Epoch [1/10], Training Loss: 0.951, Validation Accuracy: 60.39%\n",
            "Epoch [2/10], Training Loss: 0.862, Validation Accuracy: 60.70%\n",
            "Epoch [3/10], Training Loss: 0.823, Validation Accuracy: 60.23%\n",
            "Epoch [4/10], Training Loss: 0.783, Validation Accuracy: 61.13%\n",
            "Epoch [5/10], Training Loss: 0.758, Validation Accuracy: 60.77%\n",
            "Epoch [6/10], Training Loss: 0.717, Validation Accuracy: 60.97%\n",
            "Epoch [7/10], Training Loss: 0.709, Validation Accuracy: 60.46%\n",
            "Epoch [8/10], Training Loss: 0.692, Validation Accuracy: 60.69%\n",
            "Epoch [9/10], Training Loss: 0.668, Validation Accuracy: 60.78%\n",
            "Epoch [10/10], Training Loss: 0.646, Validation Accuracy: 60.39%\n",
            "Epoch [1/10], Training Loss: 0.927, Validation Accuracy: 60.80%\n",
            "Epoch [2/10], Training Loss: 0.843, Validation Accuracy: 60.72%\n",
            "Epoch [3/10], Training Loss: 0.808, Validation Accuracy: 61.10%\n",
            "Epoch [4/10], Training Loss: 0.772, Validation Accuracy: 60.64%\n",
            "Epoch [5/10], Training Loss: 0.737, Validation Accuracy: 61.40%\n",
            "Epoch [6/10], Training Loss: 0.710, Validation Accuracy: 60.89%\n",
            "Epoch [7/10], Training Loss: 0.685, Validation Accuracy: 61.20%\n",
            "Epoch [8/10], Training Loss: 0.668, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.657, Validation Accuracy: 60.48%\n",
            "Epoch [10/10], Training Loss: 0.636, Validation Accuracy: 60.77%\n",
            "Epoch [1/10], Training Loss: 0.894, Validation Accuracy: 60.51%\n",
            "Epoch [2/10], Training Loss: 0.806, Validation Accuracy: 61.57%\n",
            "Epoch [3/10], Training Loss: 0.757, Validation Accuracy: 60.34%\n",
            "Epoch [4/10], Training Loss: 0.734, Validation Accuracy: 61.00%\n",
            "Epoch [5/10], Training Loss: 0.702, Validation Accuracy: 61.33%\n",
            "Epoch [6/10], Training Loss: 0.678, Validation Accuracy: 60.51%\n",
            "Epoch [7/10], Training Loss: 0.658, Validation Accuracy: 60.47%\n",
            "Epoch [8/10], Training Loss: 0.634, Validation Accuracy: 61.16%\n",
            "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 60.66%\n",
            "Epoch [10/10], Training Loss: 0.599, Validation Accuracy: 60.93%\n",
            "Epoch [1/10], Training Loss: 0.902, Validation Accuracy: 59.32%\n",
            "Epoch [2/10], Training Loss: 0.814, Validation Accuracy: 59.85%\n",
            "Epoch [3/10], Training Loss: 0.769, Validation Accuracy: 60.51%\n",
            "Epoch [4/10], Training Loss: 0.732, Validation Accuracy: 60.25%\n",
            "Epoch [5/10], Training Loss: 0.703, Validation Accuracy: 59.96%\n",
            "Epoch [6/10], Training Loss: 0.677, Validation Accuracy: 60.69%\n",
            "Epoch [7/10], Training Loss: 0.640, Validation Accuracy: 60.35%\n",
            "Epoch [8/10], Training Loss: 0.624, Validation Accuracy: 60.72%\n",
            "Epoch [9/10], Training Loss: 0.611, Validation Accuracy: 60.21%\n",
            "Epoch [10/10], Training Loss: 0.597, Validation Accuracy: 60.35%\n",
            "Epoch [1/10], Training Loss: 0.910, Validation Accuracy: 60.30%\n",
            "Epoch [2/10], Training Loss: 0.814, Validation Accuracy: 61.13%\n",
            "Epoch [3/10], Training Loss: 0.767, Validation Accuracy: 60.85%\n",
            "Epoch [4/10], Training Loss: 0.713, Validation Accuracy: 60.33%\n",
            "Epoch [5/10], Training Loss: 0.687, Validation Accuracy: 60.88%\n",
            "Epoch [6/10], Training Loss: 0.654, Validation Accuracy: 61.66%\n",
            "Epoch [7/10], Training Loss: 0.636, Validation Accuracy: 60.67%\n",
            "Epoch [8/10], Training Loss: 0.615, Validation Accuracy: 61.08%\n",
            "Epoch [9/10], Training Loss: 0.597, Validation Accuracy: 60.32%\n",
            "Epoch [10/10], Training Loss: 0.587, Validation Accuracy: 60.37%\n",
            "Epoch [1/10], Training Loss: 0.913, Validation Accuracy: 60.73%\n",
            "Epoch [2/10], Training Loss: 0.796, Validation Accuracy: 60.91%\n",
            "Epoch [3/10], Training Loss: 0.748, Validation Accuracy: 61.13%\n",
            "Epoch [4/10], Training Loss: 0.714, Validation Accuracy: 60.75%\n",
            "Epoch [5/10], Training Loss: 0.677, Validation Accuracy: 60.99%\n",
            "Epoch [6/10], Training Loss: 0.648, Validation Accuracy: 60.91%\n",
            "Epoch [7/10], Training Loss: 0.629, Validation Accuracy: 61.09%\n",
            "Epoch [8/10], Training Loss: 0.598, Validation Accuracy: 60.40%\n",
            "Epoch [9/10], Training Loss: 0.574, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.555, Validation Accuracy: 60.67%\n",
            "Epoch [1/10], Training Loss: 0.891, Validation Accuracy: 60.58%\n",
            "Epoch [2/10], Training Loss: 0.792, Validation Accuracy: 60.89%\n",
            "Epoch [3/10], Training Loss: 0.750, Validation Accuracy: 60.62%\n",
            "Epoch [4/10], Training Loss: 0.706, Validation Accuracy: 60.82%\n",
            "Epoch [5/10], Training Loss: 0.674, Validation Accuracy: 60.76%\n",
            "Epoch [6/10], Training Loss: 0.629, Validation Accuracy: 60.65%\n",
            "Epoch [7/10], Training Loss: 0.613, Validation Accuracy: 60.97%\n",
            "Epoch [8/10], Training Loss: 0.590, Validation Accuracy: 61.08%\n",
            "Epoch [9/10], Training Loss: 0.571, Validation Accuracy: 61.13%\n",
            "Epoch [10/10], Training Loss: 0.553, Validation Accuracy: 60.94%\n",
            "Epoch [1/10], Training Loss: 0.860, Validation Accuracy: 60.50%\n",
            "Epoch [2/10], Training Loss: 0.747, Validation Accuracy: 61.58%\n",
            "Epoch [3/10], Training Loss: 0.704, Validation Accuracy: 60.79%\n",
            "Epoch [4/10], Training Loss: 0.661, Validation Accuracy: 61.11%\n",
            "Epoch [5/10], Training Loss: 0.629, Validation Accuracy: 60.52%\n",
            "Epoch [6/10], Training Loss: 0.601, Validation Accuracy: 61.10%\n",
            "Epoch [7/10], Training Loss: 0.573, Validation Accuracy: 60.69%\n",
            "Epoch [8/10], Training Loss: 0.546, Validation Accuracy: 60.52%\n",
            "Epoch [9/10], Training Loss: 0.528, Validation Accuracy: 60.53%\n",
            "Epoch [10/10], Training Loss: 0.506, Validation Accuracy: 60.49%\n",
            "Epoch [1/10], Training Loss: 0.869, Validation Accuracy: 60.20%\n",
            "Epoch [2/10], Training Loss: 0.764, Validation Accuracy: 60.44%\n",
            "Epoch [3/10], Training Loss: 0.689, Validation Accuracy: 60.39%\n",
            "Epoch [4/10], Training Loss: 0.654, Validation Accuracy: 60.46%\n",
            "Epoch [5/10], Training Loss: 0.633, Validation Accuracy: 60.20%\n",
            "Epoch [6/10], Training Loss: 0.593, Validation Accuracy: 59.97%\n",
            "Epoch [7/10], Training Loss: 0.571, Validation Accuracy: 60.05%\n",
            "Epoch [8/10], Training Loss: 0.546, Validation Accuracy: 60.40%\n",
            "Epoch [9/10], Training Loss: 0.527, Validation Accuracy: 60.08%\n",
            "Epoch [10/10], Training Loss: 0.505, Validation Accuracy: 60.39%\n",
            "Epoch [1/10], Training Loss: 0.864, Validation Accuracy: 59.86%\n",
            "Epoch [2/10], Training Loss: 0.743, Validation Accuracy: 60.73%\n",
            "Epoch [3/10], Training Loss: 0.688, Validation Accuracy: 60.59%\n",
            "Epoch [4/10], Training Loss: 0.641, Validation Accuracy: 60.84%\n",
            "Epoch [5/10], Training Loss: 0.606, Validation Accuracy: 60.05%\n",
            "Epoch [6/10], Training Loss: 0.583, Validation Accuracy: 60.82%\n",
            "Epoch [7/10], Training Loss: 0.555, Validation Accuracy: 60.77%\n",
            "Epoch [8/10], Training Loss: 0.525, Validation Accuracy: 60.79%\n",
            "Epoch [9/10], Training Loss: 0.522, Validation Accuracy: 60.70%\n",
            "Epoch [10/10], Training Loss: 0.485, Validation Accuracy: 60.37%\n",
            "Epoch [1/10], Training Loss: 0.870, Validation Accuracy: 60.81%\n",
            "Epoch [2/10], Training Loss: 0.737, Validation Accuracy: 60.95%\n",
            "Epoch [3/10], Training Loss: 0.669, Validation Accuracy: 61.06%\n",
            "Epoch [4/10], Training Loss: 0.639, Validation Accuracy: 60.78%\n",
            "Epoch [5/10], Training Loss: 0.590, Validation Accuracy: 60.66%\n",
            "Epoch [6/10], Training Loss: 0.577, Validation Accuracy: 60.96%\n",
            "Epoch [7/10], Training Loss: 0.543, Validation Accuracy: 60.30%\n",
            "Epoch [8/10], Training Loss: 0.520, Validation Accuracy: 60.44%\n",
            "Epoch [9/10], Training Loss: 0.504, Validation Accuracy: 60.13%\n",
            "Epoch [10/10], Training Loss: 0.475, Validation Accuracy: 60.41%\n",
            "Epoch [1/10], Training Loss: 0.879, Validation Accuracy: 60.65%\n",
            "Epoch [2/10], Training Loss: 0.734, Validation Accuracy: 59.95%\n",
            "Epoch [3/10], Training Loss: 0.676, Validation Accuracy: 60.72%\n",
            "Epoch [4/10], Training Loss: 0.628, Validation Accuracy: 60.43%\n",
            "Epoch [5/10], Training Loss: 0.588, Validation Accuracy: 61.34%\n",
            "Epoch [6/10], Training Loss: 0.563, Validation Accuracy: 60.47%\n",
            "Epoch [7/10], Training Loss: 0.537, Validation Accuracy: 60.78%\n",
            "Epoch [8/10], Training Loss: 0.513, Validation Accuracy: 60.62%\n",
            "Epoch [9/10], Training Loss: 0.489, Validation Accuracy: 60.47%\n",
            "Epoch [10/10], Training Loss: 0.480, Validation Accuracy: 60.79%\n",
            "Confusion Matrix:\n",
            "[[709  35  55  15  23  11   9  16  81  46]\n",
            " [ 36 737  10  13  10   8  13   9  35 129]\n",
            " [ 90  17 522  71  77  75  61  47  23  17]\n",
            " [ 33  25  94 362  70 195  81  78  27  35]\n",
            " [ 49   9 130  41 483  46  73 134  23  12]\n",
            " [ 22   8  88 171  56 492  32 108  12  11]\n",
            " [ 16  18  73  62  53  31 694  29   8  16]\n",
            " [ 33  14  33  40  61  67  11 707   4  30]\n",
            " [ 97  59  17  25   7   7   6  13 720  49]\n",
            " [ 55 147  15  18   5  12   7  33  42 666]]\n",
            "Test Accuracy: 60.92%\n",
            "True Positives (TP): [709 737 522 362 483 492 694 707 720 666]\n",
            "False Positives (FP): [431 332 515 456 362 452 293 467 255 345]\n",
            "True Negatives (TN): [8569 8668 8485 8544 8638 8548 8707 8533 8745 8655]\n",
            "False Negatives (FN): [291 263 478 638 517 508 306 293 280 334]\n",
            ": [10000 10000 10000 10000 10000 10000 10000 10000 10000 10000]\n",
            "Precision: [0.62192982 0.68942937 0.50337512 0.44254279 0.57159763 0.52118644\n",
            " 0.70314083 0.60221465 0.73846154 0.65875371]\n",
            "Recall: [0.709 0.737 0.522 0.362 0.483 0.492 0.694 0.707 0.72  0.666]\n",
            "F1 Score: [0.66261682 0.71242146 0.51251841 0.39823982 0.52357724 0.50617284\n",
            " 0.69854051 0.65041398 0.72911392 0.66235704]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}